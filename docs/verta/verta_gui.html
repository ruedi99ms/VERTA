<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>verta.verta_gui API documentation</title>
<meta name="description" content="VERTA Web GUI
…" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>verta.verta_gui</code></h1>
</header>
<section id="section-intro">
<h1 id="verta-web-gui">VERTA Web GUI</h1>
<p>A Streamlit-based web interface for VERTA (Virtual Environment Route and Trajectory Analyzer).
Provides interactive junction management and real-time analysis.</p>
<h2 id="usage">Usage</h2>
<p>streamlit run verta_gui.py</p>
<p>Or as a Python package:
from verta.verta_gui import launch_gui
launch_gui()</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
VERTA Web GUI
=============

A Streamlit-based web interface for VERTA (Virtual Environment Route and Trajectory Analyzer).
Provides interactive junction management and real-time analysis.

Usage:
    streamlit run verta_gui.py

Or as a Python package:
    from verta.verta_gui import launch_gui
    launch_gui()
&#34;&#34;&#34;

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import json
import os
from typing import List, Dict, Optional, Tuple
import tempfile
import zipfile
from pathlib import Path

import sys
from pathlib import Path

# Package imports - no path manipulation needed

from verta.verta_data_loader import load_folder, load_folder_with_gaze, Trajectory, ColumnMapping
from verta.verta_decisions import discover_decision_chain, discover_branches, assign_branches
from verta.verta_geometry import Circle, entered_junction_idx
from verta.verta_prediction import analyze_junction_choice_patterns, JunctionChoiceAnalyzer
from verta.verta_plotting import plot_flow_graph_map, plot_per_junction_flow_graph, plot_chain_overview
from verta.verta_metrics import _timing_for_traj, time_between_regions, compute_basic_trajectory_metrics, speed_through_junction, junction_transit_speed
from verta.verta_gaze import (
    compute_head_yaw_at_decisions,
    analyze_physiological_at_junctions,
    plot_gaze_directions_at_junctions,
    plot_physiological_by_branch,
    gaze_movement_consistency_report,
    analyze_pupil_dilation_trajectory,
    plot_pupil_trajectory_analysis
)
from verta.verta_intent_recognition import analyze_intent_recognition, IntentRecognitionAnalyzer
from verta.verta_logging import get_logger

# Configure Streamlit page
st.set_page_config(
    page_title=&#34;VERTA&#34;,
    page_icon=&#34;🗺️&#34;,
    layout=&#34;wide&#34;,
    initial_sidebar_state=&#34;expanded&#34;
)

# Custom CSS for better styling
st.markdown(&#34;&#34;&#34;
&lt;style&gt;
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .section-header {
        font-size: 1.5rem;
        font-weight: bold;
        color: #2c3e50;
        margin-top: 2rem;
        margin-bottom: 1rem;
    }
    .junction-list-container {
        max-height: 60vh;
        overflow-y: auto;
        border: 1px solid #e0e0e0;
        border-radius: 5px;
        padding: 10px;
        margin-top: 10px;
    }
    .metric-card {
        background-color: #f8f9fa;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
        margin: 0.5rem 0;
    }
    .success-message {
        background-color: #d4edda;
        color: #155724;
        padding: 1rem;
        border-radius: 0.5rem;
        border: 1px solid #c3e6cb;
    }
    .warning-message {
        background-color: #fff3cd;
        color: #856404;
        padding: 1rem;
        border-radius: 0.5rem;
        border: 1px solid #ffeaa7;
    }
&lt;/style&gt;
&#34;&#34;&#34;, unsafe_allow_html=True)

class RouteAnalyzerGUI:
    &#34;&#34;&#34;Main GUI class for VERTA&#34;&#34;&#34;

    def __init__(self):
        self.initialize_session_state()
        self.logger = get_logger()

    def initialize_session_state(self):
        &#34;&#34;&#34;Initialize Streamlit session state variables&#34;&#34;&#34;
        if &#39;junctions&#39; not in st.session_state:
            st.session_state.junctions = []
        if &#39;junction_r_outer&#39; not in st.session_state:
            st.session_state.junction_r_outer = {}  # Store r_outer for each junction
        if &#39;trajectories&#39; not in st.session_state:
            st.session_state.trajectories = []
        # Unified model: trajectories may include optional gaze/physio fields
        if &#39;gaze_column_mappings&#39; not in st.session_state:
            st.session_state.gaze_column_mappings = {}
        if &#39;analysis_results&#39; not in st.session_state:
            st.session_state.analysis_results = None
        if &#39;current_step&#39; not in st.session_state:
            st.session_state.current_step = &#34;data_upload&#34;
        if &#39;scale_factor&#39; not in st.session_state:
            st.session_state.scale_factor = 0.2  # Default scale factor
        if &#39;data_loaded&#39; not in st.session_state:
            st.session_state.data_loaded = False
        # Flash message shown after reruns (tuple: (level, text))
        if &#39;flash_message&#39; not in st.session_state:
            st.session_state.flash_message = None

        # Track junction state for UI refresh
        if &#39;junction_state_hash&#39; not in st.session_state:
            st.session_state.junction_state_hash = 0

        # Debug: Track session state changes
        if &#39;debug_session_state&#39; not in st.session_state:
            st.session_state.debug_session_state = {
                &#39;trajectories_count&#39;: len(st.session_state.trajectories) if st.session_state.trajectories else 0,
                &#39;gaze_trajectories_count&#39;: 0,
                &#39;last_modified&#39;: &#39;initialize&#39;
            }

    def render_header(self):
        &#34;&#34;&#34;Render the main header&#34;&#34;&#34;
        st.markdown(&#39;&lt;h1 class=&#34;main-header&#34;&gt;🗺️ VERTA&lt;/h1&gt;&#39;, unsafe_allow_html=True)
        st.markdown(&#34;&#34;&#34;
        &lt;div style=&#34;text-align: center; color: #666; margin-bottom: 2rem;&#34;&gt;
            Interactive analysis tool for VR trajectory data and junction-based choice prediction
        &lt;/div&gt;
        &#34;&#34;&#34;, unsafe_allow_html=True)
        # Show any pending flash message at the very top
        self._show_flash()

    def _show_flash(self) -&gt; None:
        &#34;&#34;&#34;Display and clear one-time flash message stored in session state.&#34;&#34;&#34;
        msg = st.session_state.get(&#39;flash_message&#39;)
        if not msg:
            return
        # msg can be a tuple (level, text) or just a string
        if isinstance(msg, tuple) and len(msg) &gt;= 2:
            level, text = msg[0], msg[1]
        else:
            level, text = &#39;success&#39;, str(msg)
        if level == &#39;warning&#39;:
            st.warning(text)
        elif level == &#39;error&#39;:
            st.error(text)
        elif level == &#39;info&#39;:
            st.info(text)
        else:
            st.success(text)
        # Clear after showing once
        st.session_state.flash_message = None

    def render_navigation(self):
        &#34;&#34;&#34;Render the navigation sidebar&#34;&#34;&#34;
        st.sidebar.title(&#34;Navigation&#34;)

        steps = {
            &#34;data_upload&#34;: &#34;📁 Data Upload&#34;,
            &#34;junction_editor&#34;: &#34;🎯 Junction Editor&#34;,
            &#34;analysis&#34;: &#34;📊 Analysis&#34;,
            &#34;visualization&#34;: &#34;📈 Visualization&#34;,
            &#34;export&#34;: &#34;💾 Export Results&#34;
        }

        for step_key, step_name in steps.items():
            if st.sidebar.button(step_name, key=f&#34;nav_{step_key}&#34;):
                st.session_state.current_step = step_key
                st.rerun()

        st.sidebar.markdown(&#34;---&#34;)
        st.sidebar.markdown(&#34;### Current Status&#34;)

        # Status indicators
        data_status = &#34;✅&#34; if (st.session_state.trajectories and getattr(st.session_state, &#39;data_loaded&#39;, False)) else &#34;❌&#34;
        junction_status = &#34;✅&#34; if st.session_state.junctions else &#34;❌&#34;

        st.sidebar.markdown(f&#34;{data_status} Data Loaded&#34;)
        st.sidebar.markdown(f&#34;{junction_status} Junctions Defined&#34;)

        if st.session_state.trajectories and st.session_state.junctions:
            st.sidebar.markdown(&#34;✅ Ready for Analysis&#34;)
        else:
            st.sidebar.markdown(&#34;⚠️ Complete setup steps&#34;)

    def render_data_upload(self):
        &#34;&#34;&#34;Render the data upload interface&#34;&#34;&#34;
        st.markdown(&#39;&lt;h2 class=&#34;section-header&#34;&gt;📁 Data Upload&lt;/h2&gt;&#39;, unsafe_allow_html=True)

        col1, col2 = st.columns([2, 1])

        with col1:
            st.markdown(&#34;### Upload Trajectory Data&#34;)

            # File upload
            uploaded_files = st.file_uploader(
                &#34;Choose CSV files&#34;,
                type=[&#39;csv&#39;],
                accept_multiple_files=True,
                help=&#34;Upload one or more CSV files containing trajectory data&#34;
            )

            # Folder path input
            st.markdown(&#34;### Or specify folder path&#34;)
            folder_path = st.text_input(
                &#34;Folder path:&#34;,
                value=&#34;&#34;,
                help=&#34;Path to folder containing CSV files&#34;
            )

            # Column mapping
            st.markdown(&#34;### Column Mapping&#34;)
            col_x, col_z, col_t = st.columns(3)

            with col_x:
                x_col = st.text_input(&#34;X Column:&#34;, value=&#34;Headset.Head.Position.X&#34;)
            with col_z:
                z_col = st.text_input(&#34;Z Column:&#34;, value=&#34;Headset.Head.Position.Z&#34;)
            with col_t:
                t_col = st.text_input(&#34;Time Column:&#34;, value=&#34;Time&#34;)

            # New: Gaze/Physiology column mapping now lives here
            with st.expander(&#34;🔧 Gaze/Physiology Column Mapping&#34;, expanded=False):
                col_g1, col_g2 = st.columns(2)
                with col_g1:
                    head_forward_x_col = st.text_input(&#34;Head Forward X&#34;, value=st.session_state.gaze_column_mappings.get(&#39;head_forward_x&#39;, &#39;Headset.Head.Forward.X&#39;))
                    head_forward_z_col = st.text_input(&#34;Head Forward Z&#34;, value=st.session_state.gaze_column_mappings.get(&#39;head_forward_z&#39;, &#39;Headset.Head.Forward.Z&#39;))
                    gaze_x_map = st.text_input(&#34;Gaze X&#34;, value=st.session_state.gaze_column_mappings.get(&#39;gaze_x&#39;, &#39;Headset.Gaze.X&#39;))
                    gaze_y_map = st.text_input(&#34;Gaze Y&#34;, value=st.session_state.gaze_column_mappings.get(&#39;gaze_y&#39;, &#39;Headset.Gaze.Y&#39;))
                with col_g2:
                    pupil_l_map = st.text_input(&#34;Pupil Left&#34;, value=st.session_state.gaze_column_mappings.get(&#39;pupil_l&#39;, &#39;Headset.PupilDilation.L&#39;))
                    pupil_r_map = st.text_input(&#34;Pupil Right&#34;, value=st.session_state.gaze_column_mappings.get(&#39;pupil_r&#39;, &#39;Headset.PupilDilation.R&#39;))
                    heart_rate_map = st.text_input(&#34;Heart Rate&#34;, value=st.session_state.gaze_column_mappings.get(&#39;heart_rate&#39;, &#39;Headset.HeartRate&#39;))

                st.session_state.gaze_column_mappings = {
                    &#39;head_forward_x&#39;: head_forward_x_col.strip(),
                    &#39;head_forward_z&#39;: head_forward_z_col.strip(),
                    &#39;gaze_x&#39;: gaze_x_map.strip(),
                    &#39;gaze_y&#39;: gaze_y_map.strip(),
                    &#39;pupil_l&#39;: pupil_l_map.strip(),
                    &#39;pupil_r&#39;: pupil_r_map.strip(),
                    &#39;heart_rate&#39;: heart_rate_map.strip(),
                }

            # Analysis parameters
            st.markdown(&#34;### Analysis Parameters&#34;)
            col_scale, col_threshold = st.columns(2)

            with col_scale:
                scale = st.number_input(&#34;Scale Factor:&#34;, value=st.session_state.get(&#34;scale_factor&#34;, 0.2), min_value=0.01, max_value=1.0, step=0.01)
                st.session_state.scale_factor = scale  # Store scale factor in session state
            with col_threshold:
                motion_threshold = st.number_input(&#34;Motion Threshold:&#34;, value=0.1, min_value=0.01, max_value=1.0, step=0.01)

        with col2:
            st.markdown(&#34;### Quick Actions&#34;)

            if st.button(&#34;🔄 Load Data&#34;, type=&#34;primary&#34;):
                if uploaded_files and folder_path.strip():
                    # Both provided - show warning and ask user to choose
                    st.warning(&#34;⚠️ **Both file uploads and folder path are specified.**&#34;)
                    st.info(&#34;**Current behavior:** File uploads will be processed (folder path will be ignored).&#34;)
                    st.info(&#34;**To use folder path instead:** Clear the file uploads and click &#39;Load Data&#39; again.&#34;)

                    # Process uploaded files (current behavior)
                    self.load_uploaded_files(uploaded_files, x_col, z_col, t_col, scale, motion_threshold)
                elif uploaded_files:
                    # Process uploaded files
                    self.load_uploaded_files(uploaded_files, x_col, z_col, t_col, scale, motion_threshold)
                elif folder_path.strip():
                    # Process folder path
                    self.load_trajectory_data(folder_path, x_col, z_col, t_col, scale, motion_threshold)
                else:
                    st.warning(&#34;⚠️ Please upload files or specify a folder path&#34;)

            if st.button(&#34;📋 Load Sample Data&#34;):
                self.load_sample_data()

            if st.session_state.trajectories:
                st.markdown(&#34;### Data Summary&#34;)
                st.write(f&#34;**Trajectories loaded:** {len(st.session_state.trajectories)}&#34;)

                if len(st.session_state.trajectories) &gt; 0:
                    sample_traj = st.session_state.trajectories[0]
                    st.write(f&#34;**Sample trajectory points:** {len(sample_traj.x)}&#34;)
                    st.write(f&#34;**X range:** {min(sample_traj.x):.1f} to {max(sample_traj.x):.1f}&#34;)
                    st.write(f&#34;**Z range:** {min(sample_traj.z):.1f} to {max(sample_traj.z):.1f}&#34;)

    def load_trajectory_data(self, folder_path: str, x_col: str, z_col: str, t_col: str, scale: float, motion_threshold: float):
        &#34;&#34;&#34;Load trajectory data from folder using unified model&#34;&#34;&#34;
        try:
            # Create progress bar
            progress_bar = st.progress(0)
            status_text = st.empty()

            status_text.text(&#34;🔄 Initializing data loading...&#34;)
            progress_bar.progress(10)

            # Build comprehensive column mapping for VR headset data
            column_mapping = {
                &#39;x&#39;: x_col,
                &#39;z&#39;: z_col,
                &#39;t&#39;: t_col,
                # VR headset gaze/physio columns
                &#39;head_forward_x&#39;: st.session_state.gaze_column_mappings.get(&#39;head_forward_x&#39;, &#39;Headset.Head.Forward.X&#39;),
                &#39;head_forward_y&#39;: st.session_state.gaze_column_mappings.get(&#39;head_forward_y&#39;, &#39;Headset.Head.Forward.Y&#39;),
                &#39;head_forward_z&#39;: st.session_state.gaze_column_mappings.get(&#39;head_forward_z&#39;, &#39;Headset.Head.Forward.Z&#39;),
                &#39;head_up_x&#39;: st.session_state.gaze_column_mappings.get(&#39;head_up_x&#39;, &#39;Headset.Head.Up.X&#39;),
                &#39;head_up_y&#39;: st.session_state.gaze_column_mappings.get(&#39;head_up_y&#39;, &#39;Headset.Head.Up.Y&#39;),
                &#39;head_up_z&#39;: st.session_state.gaze_column_mappings.get(&#39;head_up_z&#39;, &#39;Headset.Head.Up.Z&#39;),
                &#39;gaze_x&#39;: st.session_state.gaze_column_mappings.get(&#39;gaze_x&#39;, &#39;Headset.Gaze.X&#39;),
                &#39;gaze_y&#39;: st.session_state.gaze_column_mappings.get(&#39;gaze_y&#39;, &#39;Headset.Gaze.Y&#39;),
                &#39;pupil_l&#39;: st.session_state.gaze_column_mappings.get(&#39;pupil_l&#39;, &#39;Headset.PupilDilation.L&#39;),
                &#39;pupil_r&#39;: st.session_state.gaze_column_mappings.get(&#39;pupil_r&#39;, &#39;Headset.PupilDilation.R&#39;),
                &#39;heart_rate&#39;: st.session_state.gaze_column_mappings.get(&#39;heart_rate&#39;, &#39;Headset.HeartRate&#39;),
            }

            status_text.text(&#34;🔍 Scanning folder for CSV files...&#34;)
            progress_bar.progress(30)

            # Add a small delay to ensure progress bar is visible
            import time
            time.sleep(0.1)

            # Check what columns are available in the first CSV file and auto-detect gaze columns
            import glob
            import pandas as pd
            csv_files = glob.glob(os.path.join(folder_path, &#34;*.csv&#34;))
            if csv_files:
                sample_df = pd.read_csv(csv_files[0])

                # Auto-detect gaze columns if mappings are empty
                if not st.session_state.gaze_column_mappings:
                    st.info(&#34;🔍 **Auto-detecting gaze columns from CSV file...**&#34;)

                    # Try to find gaze columns by common patterns
                    detected_mappings = {}

                    # Look for pupil dilation columns
                    pupil_cols = [col for col in sample_df.columns if &#39;pupil&#39; in col.lower() and (&#39;l&#39; in col.lower() or &#39;left&#39; in col.lower())]
                    if pupil_cols:
                        detected_mappings[&#39;pupil_l&#39;] = pupil_cols[0]

                    pupil_cols = [col for col in sample_df.columns if &#39;pupil&#39; in col.lower() and (&#39;r&#39; in col.lower() or &#39;right&#39; in col.lower())]
                    if pupil_cols:
                        detected_mappings[&#39;pupil_r&#39;] = pupil_cols[0]

                    # Look for heart rate columns
                    hr_cols = [col for col in sample_df.columns if &#39;heart&#39; in col.lower() or &#39;hr&#39; in col.lower()]
                    if hr_cols:
                        detected_mappings[&#39;heart_rate&#39;] = hr_cols[0]

                    # Look for gaze columns
                    gaze_x_cols = [col for col in sample_df.columns if &#39;gaze&#39; in col.lower() and (&#39;x&#39; in col.lower() or &#39;horizontal&#39; in col.lower())]
                    if gaze_x_cols:
                        detected_mappings[&#39;gaze_x&#39;] = gaze_x_cols[0]

                    gaze_y_cols = [col for col in sample_df.columns if &#39;gaze&#39; in col.lower() and (&#39;y&#39; in col.lower() or &#39;vertical&#39; in col.lower())]
                    if gaze_y_cols:
                        detected_mappings[&#39;gaze_y&#39;] = gaze_y_cols[0]

                    # Look for head forward columns
                    head_fwd_x_cols = [col for col in sample_df.columns if &#39;head&#39; in col.lower() and &#39;forward&#39; in col.lower() and &#39;x&#39; in col.lower()]
                    if head_fwd_x_cols:
                        detected_mappings[&#39;head_forward_x&#39;] = head_fwd_x_cols[0]

                    head_fwd_z_cols = [col for col in sample_df.columns if &#39;head&#39; in col.lower() and &#39;forward&#39; in col.lower() and (&#39;z&#39; in col.lower() or &#39;depth&#39; in col.lower())]
                    if head_fwd_z_cols:
                        detected_mappings[&#39;head_forward_z&#39;] = head_fwd_z_cols[0]

                    # Update session state with detected mappings
                    if detected_mappings:
                        st.session_state.gaze_column_mappings.update(detected_mappings)
                        st.success(f&#34;✅ **Auto-detected gaze columns:** {detected_mappings}&#34;)

                        # Update column mapping with detected values
                        column_mapping.update(detected_mappings)
                    else:
                        st.warning(&#34;⚠️ **No gaze columns auto-detected**&#34;)

                # Check if gaze columns exist (using current mappings)
                gaze_columns = [&#39;Headset.Head.Forward.X&#39;, &#39;Headset.Head.Forward.Z&#39;, &#39;Headset.Gaze.X&#39;,
                              &#39;Headset.Gaze.Y&#39;, &#39;Headset.PupilDilation.L&#39;, &#39;Headset.PupilDilation.R&#39;, &#39;Headset.HeartRate&#39;]

                # Use detected mappings if available
                actual_gaze_columns = []
                for gaze_type in [&#39;head_forward_x&#39;, &#39;head_forward_z&#39;, &#39;gaze_x&#39;, &#39;gaze_y&#39;, &#39;pupil_l&#39;, &#39;pupil_r&#39;, &#39;heart_rate&#39;]:
                    col_name = st.session_state.gaze_column_mappings.get(gaze_type, gaze_columns[[&#39;head_forward_x&#39;, &#39;head_forward_z&#39;, &#39;gaze_x&#39;, &#39;gaze_y&#39;, &#39;pupil_l&#39;, &#39;pupil_r&#39;, &#39;heart_rate&#39;].index(gaze_type)])
                    actual_gaze_columns.append(col_name)

                missing_gaze_cols = [col for col in actual_gaze_columns if col not in sample_df.columns]
                if missing_gaze_cols:
                    st.warning(f&#34;⚠️ **Missing gaze columns:** {missing_gaze_cols}&#34;)
                else:
                    st.success(&#34;✅ **All gaze columns found in CSV!**&#34;)

            status_text.text(&#34;📊 Loading trajectory data...&#34;)
            progress_bar.progress(60)

            # Add a small delay to ensure progress bar is visible
            import time
            time.sleep(0.1)

            # Create progress callback function
            def update_progress(current, total, message):
                progress_percent = int(60 + (current / total) * 30)  # 60-90% range
                progress_bar.progress(progress_percent)
                status_text.text(message)

            # Use unified loader - always returns Trajectory objects with optional gaze fields
            trajectories = load_folder(
                folder=folder_path,
                pattern=&#34;*.csv&#34;,
                columns=column_mapping,
                require_time=False,
                scale=scale,
                motion_threshold=motion_threshold,
                progress_callback=update_progress
            )

            status_text.text(&#34;💾 Storing trajectories in session...&#34;)
            progress_bar.progress(90)

            # Add another small delay to show progress
            time.sleep(0.1)

            # Store unified trajectories
            st.session_state.trajectories = trajectories

            # Update status display
            st.session_state.data_loaded = True

            progress_bar.progress(100)
            status_text.text(&#34;✅ Data loading completed!&#34;)

            # Clear progress elements first
            progress_bar.empty()
            status_text.empty()

            # Queue success flash for top-of-page after rerun
            st.session_state.flash_message = (&#39;success&#39;, f&#34;🎉 Successfully loaded {len(trajectories)} trajectories!&#34;)

            # Force rerun to update status display and show flash at top
            st.rerun()

        except Exception as e:
            st.error(f&#34;❌ Error loading data: {str(e)}&#34;)
            if &#39;progress_bar&#39; in locals():
                progress_bar.empty()
            if &#39;status_text&#39; in locals():
                status_text.empty()

    def load_uploaded_files(self, uploaded_files, x_col: str, z_col: str, t_col: str, scale: float, motion_threshold: float):
        &#34;&#34;&#34;Load trajectory data from uploaded files using unified model&#34;&#34;&#34;
        try:
            with st.spinner(&#34;Loading uploaded files...&#34;):
                import pandas as pd
                import io
                import numpy as np
                import tempfile
                import os
                from verta.verta_data_loader import Trajectory, TrajectoryLoader, ColumnMapping

                # Create progress bar
                progress_bar = st.progress(0)
                status_text = st.empty()

                status_text.text(&#34;🔄 Initializing file processing...&#34;)
                progress_bar.progress(5)

                # Build comprehensive column mapping for VR headset data (same as folder loading)
                column_mapping = {
                    &#39;x&#39;: x_col,
                    &#39;z&#39;: z_col,
                    &#39;t&#39;: t_col,
                    # VR headset gaze/physio columns
                    &#39;head_forward_x&#39;: st.session_state.gaze_column_mappings.get(&#39;head_forward_x&#39;, &#39;Headset.Head.Forward.X&#39;),
                    &#39;head_forward_y&#39;: st.session_state.gaze_column_mappings.get(&#39;head_forward_y&#39;, &#39;Headset.Head.Forward.Y&#39;),
                    &#39;head_forward_z&#39;: st.session_state.gaze_column_mappings.get(&#39;head_forward_z&#39;, &#39;Headset.Head.Forward.Z&#39;),
                    &#39;head_up_x&#39;: st.session_state.gaze_column_mappings.get(&#39;head_up_x&#39;, &#39;Headset.Head.Up.X&#39;),
                    &#39;head_up_y&#39;: st.session_state.gaze_column_mappings.get(&#39;head_up_y&#39;, &#39;Headset.Head.Up.Y&#39;),
                    &#39;head_up_z&#39;: st.session_state.gaze_column_mappings.get(&#39;head_up_z&#39;, &#39;Headset.Head.Up.Z&#39;),
                    &#39;gaze_x&#39;: st.session_state.gaze_column_mappings.get(&#39;gaze_x&#39;, &#39;Headset.Gaze.X&#39;),
                    &#39;gaze_y&#39;: st.session_state.gaze_column_mappings.get(&#39;gaze_y&#39;, &#39;Headset.Gaze.Y&#39;),
                    &#39;pupil_l&#39;: st.session_state.gaze_column_mappings.get(&#39;pupil_l&#39;, &#39;Headset.PupilDilation.L&#39;),
                    &#39;pupil_r&#39;: st.session_state.gaze_column_mappings.get(&#39;pupil_r&#39;, &#39;Headset.PupilDilation.R&#39;),
                    &#39;heart_rate&#39;: st.session_state.gaze_column_mappings.get(&#39;heart_rate&#39;, &#39;Headset.HeartRate&#39;),
                }

                # Auto-detect gaze columns from first uploaded file (same as folder loading)
                if uploaded_files and not st.session_state.gaze_column_mappings:
                    st.info(&#34;🔍 **Auto-detecting gaze columns from uploaded file...**&#34;)

                    # Read first file to detect columns
                    first_file = uploaded_files[0]
                    df_sample = pd.read_csv(io.StringIO(first_file.read().decode(&#39;utf-8&#39;)))
                    first_file.seek(0)  # Reset file pointer

                    # Try to find gaze columns by common patterns (same logic as folder loading)
                    detected_mappings = {}

                    # Look for pupil dilation columns
                    pupil_cols = [col for col in df_sample.columns if &#39;pupil&#39; in col.lower() and (&#39;l&#39; in col.lower() or &#39;left&#39; in col.lower())]
                    if pupil_cols:
                        detected_mappings[&#39;pupil_l&#39;] = pupil_cols[0]

                    pupil_cols = [col for col in df_sample.columns if &#39;pupil&#39; in col.lower() and (&#39;r&#39; in col.lower() or &#39;right&#39; in col.lower())]
                    if pupil_cols:
                        detected_mappings[&#39;pupil_r&#39;] = pupil_cols[0]

                    # Look for heart rate columns
                    hr_cols = [col for col in df_sample.columns if &#39;heart&#39; in col.lower() or &#39;hr&#39; in col.lower()]
                    if hr_cols:
                        detected_mappings[&#39;heart_rate&#39;] = hr_cols[0]

                    # Look for gaze columns
                    gaze_x_cols = [col for col in df_sample.columns if &#39;gaze&#39; in col.lower() and (&#39;x&#39; in col.lower() or &#39;horizontal&#39; in col.lower())]
                    if gaze_x_cols:
                        detected_mappings[&#39;gaze_x&#39;] = gaze_x_cols[0]

                    gaze_y_cols = [col for col in df_sample.columns if &#39;gaze&#39; in col.lower() and (&#39;y&#39; in col.lower() or &#39;vertical&#39; in col.lower())]
                    if gaze_y_cols:
                        detected_mappings[&#39;gaze_y&#39;] = gaze_y_cols[0]

                    # Look for head forward columns
                    head_fwd_x_cols = [col for col in df_sample.columns if &#39;head&#39; in col.lower() and &#39;forward&#39; in col.lower() and &#39;x&#39; in col.lower()]
                    if head_fwd_x_cols:
                        detected_mappings[&#39;head_forward_x&#39;] = head_fwd_x_cols[0]

                    head_fwd_z_cols = [col for col in df_sample.columns if &#39;head&#39; in col.lower() and &#39;forward&#39; in col.lower() and (&#39;z&#39; in col.lower() or &#39;depth&#39; in col.lower())]
                    if head_fwd_z_cols:
                        detected_mappings[&#39;head_forward_z&#39;] = head_fwd_z_cols[0]

                    # Update session state with detected mappings
                    if detected_mappings:
                        st.session_state.gaze_column_mappings.update(detected_mappings)
                        st.success(f&#34;✅ **Auto-detected gaze columns:** {detected_mappings}&#34;)

                        # Update column mapping with detected values
                        column_mapping.update(detected_mappings)
                    else:
                        st.warning(&#34;⚠️ **No gaze columns auto-detected**&#34;)

                # Create temporary directory to store uploaded files
                with tempfile.TemporaryDirectory() as temp_dir:
                    status_text.text(&#34;📁 Saving uploaded files to temporary directory...&#34;)
                    progress_bar.progress(10)

                    # Save uploaded files to temporary directory
                    temp_files = []
                    for i, uploaded_file in enumerate(uploaded_files):
                        temp_file_path = os.path.join(temp_dir, uploaded_file.name)
                        with open(temp_file_path, &#39;wb&#39;) as f:
                            f.write(uploaded_file.getvalue())
                        temp_files.append(temp_file_path)

                    status_text.text(&#34;📊 Loading trajectories using unified loader...&#34;)
                    progress_bar.progress(20)

                    # Use the same unified loader as folder loading
                    loader = TrajectoryLoader(ColumnMapping.from_dict(column_mapping))

                    # Create progress callback function
                    def update_progress(current, total, message):
                        progress_percent = int(20 + (current / total) * 70)  # 20-90% range
                        progress_bar.progress(progress_percent)
                        status_text.text(message)

                    # Load trajectories using unified loader
                    trajectories = loader.load_folder(
                        folder=temp_dir,
                        pattern=&#34;*.csv&#34;,
                        trajectory_class=Trajectory,
                        require_time=False,
                        scale=scale,
                        motion_threshold=motion_threshold,
                        progress_callback=update_progress
                    )

                if trajectories:
                    status_text.text(&#34;💾 Storing trajectories in session...&#34;)
                    progress_bar.progress(90)

                    st.session_state.trajectories = trajectories

                    # Update status display
                    st.session_state.data_loaded = True

                    progress_bar.progress(100)
                    status_text.text(&#34;✅ File processing completed!&#34;)

                    # Clear progress elements first
                    progress_bar.empty()
                    status_text.empty()

                    # Queue success flash for top-of-page after rerun
                    st.session_state.flash_message = (
                        &#39;success&#39;,
                        f&#34;🎉 Successfully loaded {len(trajectories)} trajectories from {len(uploaded_files)} files!&#34;
                    )

                    # Force rerun to update status display and show flash at top
                    st.rerun()
                else:
                    st.error(&#34;❌ No valid trajectories could be loaded from uploaded files&#34;)

        except Exception as e:
            st.error(f&#34;❌ Error loading uploaded files: {str(e)}&#34;)

    def load_sample_data(self):
        &#34;&#34;&#34;Load sample data for demonstration&#34;&#34;&#34;
        try:
            with st.spinner(&#34;Loading sample data...&#34;):
                # Create sample trajectories
                np.random.seed(42)
                trajectories = []

                for i in range(10):
                    # Create a sample trajectory
                    n_points = np.random.randint(1000, 5000)
                    t = np.linspace(0, 100, n_points)
                    x = np.cumsum(np.random.normal(0, 0.5, n_points)) + 500
                    z = np.cumsum(np.random.normal(0, 0.5, n_points)) + 200

                    trajectory = Trajectory(
                        tid=str(i),
                        x=x,
                        z=z,
                        t=t
                    )
                    trajectories.append(trajectory)

                st.session_state.trajectories = trajectories
                st.success(f&#34;✅ Loaded {len(trajectories)} sample trajectories!&#34;)

        except Exception as e:
            st.error(f&#34;❌ Error loading sample data: {str(e)}&#34;)

    def load_assign_trajectories(self, assign_params: dict = None):
        &#34;&#34;&#34;Load trajectories for assign function based on trajectory option&#34;&#34;&#34;
        try:
            # Get scale factor from assign parameters
            assign_scale = assign_params.get(&#34;assign_scale&#34;, 0.2) if assign_params else 0.2
            trajectory_option = assign_params.get(&#34;trajectory_option&#34;, &#34;Upload files&#34;) if assign_params else &#34;Upload files&#34;

            if trajectory_option == &#34;Upload files&#34;:
                trajectory_files = assign_params.get(&#34;trajectory_files&#34;) if assign_params else None
                if not trajectory_files:
                    st.error(&#34;❌ No trajectory files uploaded&#34;)
                    return None

                # Process uploaded files
                trajectories = []
                for uploaded_file in trajectory_files:
                    # Save uploaded file to temporary location
                    with tempfile.NamedTemporaryFile(delete=False, suffix=&#39;.csv&#39;) as tmp_file:
                        tmp_file.write(uploaded_file.getvalue())
                        tmp_path = tmp_file.name

                    # Load trajectory data
                    df = pd.read_csv(tmp_path)

                    # Use default column names if not specified
                    x_col = &#34;Headset.Head.Position.X&#34; if &#34;Headset.Head.Position.X&#34; in df.columns else df.columns[0]
                    z_col = &#34;Headset.Head.Position.Z&#34; if &#34;Headset.Head.Position.Z&#34; in df.columns else df.columns[1]
                    t_col = &#34;Time&#34; if &#34;Time&#34; in df.columns else df.columns[2] if len(df.columns) &gt; 2 else None

                    # Filter out NaN values in coordinate columns
                    coord_mask = df[[x_col, z_col]].notnull().all(axis=1)
                    df_clean = df[coord_mask].copy()

                    if len(df_clean) == 0:
                        st.warning(f&#34;⚠️ Skipping {uploaded_file.name}: All coordinates are NaN&#34;)
                        continue

                    if len(df_clean) &lt; len(df):
                        st.info(f&#34;ℹ️ Cleaned {uploaded_file.name}: Removed {len(df) - len(df_clean)} rows with NaN coordinates&#34;)

                    # Create trajectory with scale factor applied
                    trajectory = Trajectory(
                        tid=uploaded_file.name,
                        x=df_clean[x_col].values * assign_scale,  # Apply scale factor
                        z=df_clean[z_col].values * assign_scale,  # Apply scale factor
                        t=df_clean[t_col].values if t_col else np.arange(len(df_clean))
                    )
                    trajectories.append(trajectory)

                    # Clean up temporary file
                    os.unlink(tmp_path)

                return trajectories

            else:  # Select folder
                trajectory_folder = assign_params.get(&#34;trajectory_folder&#34;) if assign_params else None
                if not trajectory_folder or not trajectory_folder.strip():
                    st.error(&#34;❌ No trajectory folder path specified&#34;)
                    return None

                # Load from folder
                column_mapping = {
                    &#34;x&#34;: &#34;Headset.Head.Position.X&#34;,
                    &#34;z&#34;: &#34;Headset.Head.Position.Z&#34;,
                    &#34;t&#34;: &#34;Time&#34;
                }

                trajectories = load_folder(
                    folder=trajectory_folder,
                    pattern=&#34;*.csv&#34;,
                    columns=column_mapping,
                    scale=assign_scale,  # Use assign-specific scale factor
                    motion_threshold=0.1
                )

                if not trajectories:
                    st.error(f&#34;❌ No trajectories found in folder: {trajectory_folder}&#34;)
                    return None

                # Show data cleaning summary
                total_trajectories = len(trajectories)
                st.info(f&#34;ℹ️ Loaded {total_trajectories} trajectories from folder&#34;)
                st.info(f&#34;ℹ️ NaN filtering applied during loading (built into load_folder function)&#34;)

                return trajectories

        except Exception as e:
            st.error(f&#34;❌ Failed to load assign trajectories: {str(e)}&#34;)
            return None

    def load_assign_centers(self, assign_params: dict = None):
        &#34;&#34;&#34;Load junction centers for assign function based on centers option&#34;&#34;&#34;
        try:
            centers_option = assign_params.get(&#34;centers_option&#34;, &#34;Use session centers&#34;) if assign_params else &#34;Use session centers&#34;

            if centers_option == &#34;Use session centers&#34;:
                # Get centers from previous discover analysis
                if &#34;branches&#34; not in st.session_state.analysis_results:
                    st.error(&#34;❌ No centers found from previous discover analysis&#34;)
                    return None

                centers_dict = {}
                for junction_key, branch_data in st.session_state.analysis_results[&#34;branches&#34;].items():
                    if &#34;centers&#34; in branch_data:
                        centers_dict[junction_key] = branch_data[&#34;centers&#34;]

                return centers_dict

            elif centers_option == &#34;Upload files&#34;:
                centers_files = assign_params.get(&#34;centers_files&#34;) if assign_params else None
                if not centers_files:
                    st.error(&#34;❌ No centers files uploaded&#34;)
                    return None

                centers_dict = {}
                for i, centers_file in enumerate(centers_files):
                    # Save uploaded file to temporary location
                    with tempfile.NamedTemporaryFile(delete=False, suffix=&#39;.npy&#39;) as tmp_file:
                        tmp_file.write(centers_file.getvalue())
                        tmp_path = tmp_file.name

                    # Load centers
                    centers = np.load(tmp_path)
                    centers_dict[f&#34;junction_{i}&#34;] = centers

                    # Clean up temporary file
                    os.unlink(tmp_path)

                return centers_dict

            else:  # Select folder
                centers_folder = assign_params.get(&#34;centers_folder&#34;) if assign_params else None
                if not centers_folder or not centers_folder.strip():
                    st.error(&#34;❌ No centers folder path specified&#34;)
                    return None

                # Load centers from folder (search subfolders)
                centers_dict = {}
                for root, dirs, files in os.walk(centers_folder):
                    for file in files:
                        if file.startswith(&#34;branch_centers_j&#34;) and file.endswith(&#34;.npy&#34;):
                            # Extract junction number from filename
                            junction_num = file.split(&#34;_&#34;)[-1].split(&#34;.&#34;)[0]
                            centers_path = os.path.join(root, file)
                            centers = np.load(centers_path)
                            centers_dict[f&#34;junction_{junction_num}&#34;] = centers

                if not centers_dict:
                    st.error(f&#34;❌ No center files found in folder: {centers_folder}&#34;)
                    st.info(&#34;💡 Looking for files named: branch_centers_j*.npy&#34;)
                    return None

                return centers_dict

        except Exception as e:
            st.error(f&#34;❌ Failed to load assign centers: {str(e)}&#34;)
            return None

    def render_junction_editor(self):
        &#34;&#34;&#34;Render the interactive junction editor&#34;&#34;&#34;
        st.markdown(&#39;&lt;h2 class=&#34;section-header&#34;&gt;🎯 Junction Editor&lt;/h2&gt;&#39;, unsafe_allow_html=True)

        col1, col2 = st.columns([2, 1])

        with col1:
            st.markdown(&#34;### Interactive Junction Management&#34;)

            # Instructions
            st.info(&#34;&#34;&#34;
            **How to use:**
            1. **Add junctions** using the controls on the right
            2. **Edit existing junctions** by changing position, radius, or r_outer values
            3. **Hover over junctions** on the map to see their properties
            4. **Delete junctions** using the 🗑️ button
            &#34;&#34;&#34;)

            # Create interactive plot for junction editing
            if st.session_state.trajectories:
                self.render_junction_plot()
            else:
                st.warning(&#34;⚠️ Please load trajectory data first&#34;)

        with col2:
            st.markdown(&#34;### Junction Controls&#34;)

            # Add new junction section (always visible)
            with st.container():
                st.markdown(&#34;#### Add New Junction&#34;)
                col_x, col_z = st.columns(2)

                with col_x:
                    new_x = st.number_input(&#34;X Position:&#34;, value=500.0, step=10.0)
                with col_z:
                    new_z = st.number_input(&#34;Z Position:&#34;, value=300.0, step=10.0)

                col_radius, col_r_outer = st.columns(2)
                with col_radius:
                    new_radius = st.number_input(&#34;Radius:&#34;, value=30.0, min_value=5.0, max_value=100.0, step=5.0)
                with col_r_outer:
                    new_r_outer = st.number_input(&#34;R Outer:&#34;, value=50.0, min_value=10.0, max_value=200.0, step=5.0)

                if st.button(&#34;➕ Add Junction&#34;):
                    new_junction = Circle(cx=new_x, cz=new_z, r=new_radius)
                    st.session_state.junctions.append(new_junction)
                    st.session_state.junction_r_outer[len(st.session_state.junctions)-1] = new_r_outer

                    # Update junction state hash to force UI refresh
                    st.session_state.junction_state_hash += 1

                    st.success(f&#34;Added junction at ({new_x}, {new_z}) with r_outer={new_r_outer}&#34;)
                    st.rerun()

            st.markdown(&#34;---&#34;)

            # Bulk operations (always visible)
            st.markdown(&#34;#### Bulk Operations&#34;)
            col_clear, col_sample = st.columns(2)

            with col_clear:
                if st.button(&#34;🗑️ Clear All&#34;):
                    st.session_state.junctions = []
                    st.session_state.junction_r_outer = {}

                    # Update junction state hash to force UI refresh
                    st.session_state.junction_state_hash += 1

                    st.rerun()

            with col_sample:
                if st.button(&#34;📋 Load Sample&#34;):
                    self.load_sample_junctions()

            st.markdown(&#34;---&#34;)

            # Scrollable junction list
            st.markdown(&#34;#### Current Junctions&#34;)
            if st.session_state.junctions:
                # Create a scrollable container for the junction list
                st.markdown(f&#34;**Total Junctions: {len(st.session_state.junctions)}**&#34;)
                with st.container():
                    st.markdown(&#39;&lt;div class=&#34;junction-list-container&#34;&gt;&#39;, unsafe_allow_html=True)

                    # Use a scrollable area for the junction list
                    for i, junction in enumerate(st.session_state.junctions):
                        with st.expander(f&#34;Junction {i} - ({junction.cx:.1f}, {junction.cz:.1f})&#34;, expanded=False):
                            # Junction info and delete button
                            col_del, col_info = st.columns([1, 4])

                            with col_del:
                                if st.button(&#34;🗑️&#34;, key=f&#34;del_{i}&#34;, help=&#34;Delete this junction&#34;):
                                    # Store the deleted junction info for debugging
                                    deleted_junction = st.session_state.junctions[i]

                                    # Remove the junction from the list
                                    st.session_state.junctions.pop(i)

                                    # Remove the corresponding r_outer entry
                                    if i in st.session_state.junction_r_outer:
                                        del st.session_state.junction_r_outer[i]

                                    # Reindex remaining junctions and r_outer values
                                    new_r_outer = {}
                                    for j, junction in enumerate(st.session_state.junctions):
                                        old_idx = j + (1 if j &gt;= i else 0)
                                        if old_idx in st.session_state.junction_r_outer:
                                            new_r_outer[j] = st.session_state.junction_r_outer[old_idx]
                                    st.session_state.junction_r_outer = new_r_outer

                                    # Show success message
                                    st.success(f&#34;Deleted Junction {i} at ({deleted_junction.cx:.1f}, {deleted_junction.cz:.1f})&#34;)

                                    # Update junction state hash to force UI refresh
                                    st.session_state.junction_state_hash += 1

                                    # Force a complete rerun to refresh all UI elements
                                    st.rerun()

                            with col_info:
                                st.write(f&#34;Position: ({junction.cx:.1f}, {junction.cz:.1f})&#34;)
                                st.write(f&#34;Radius: {junction.r}&#34;)
                                st.write(f&#34;R_outer: {st.session_state.junction_r_outer.get(i, 50.0)}&#34;)

                            # Position editing
                            st.markdown(&#34;**Edit Position:**&#34;)
                            col_x_edit, col_z_edit = st.columns(2)

                            with col_x_edit:
                                new_x = st.number_input(
                                    f&#34;X:&#34;,
                                    value=float(junction.cx),
                                    step=1.0,
                                    key=f&#34;x_edit_{i}&#34;
                                )

                            with col_z_edit:
                                new_z = st.number_input(
                                    f&#34;Z:&#34;,
                                    value=float(junction.cz),
                                    step=1.0,
                                    key=f&#34;z_edit_{i}&#34;
                                )

                            # Radius editing
                            new_radius = st.number_input(
                                f&#34;Radius:&#34;,
                                value=float(junction.r),
                                min_value=5.0,
                                max_value=100.0,
                                step=1.0,
                                key=f&#34;radius_edit_{i}&#34;
                            )

                            # R_outer control
                            current_r_outer = st.session_state.junction_r_outer.get(i, 50.0)
                            new_r_outer = st.number_input(
                                f&#34;R Outer:&#34;,
                                value=current_r_outer,
                                min_value=10.0,
                                max_value=200.0,
                                step=5.0,
                                key=f&#34;r_outer_{i}&#34;
                            )

                            # Update junction if any values changed
                            if (new_x != junction.cx or new_z != junction.cz or
                                new_radius != junction.r or new_r_outer != current_r_outer):

                                # Update junction
                                st.session_state.junctions[i] = Circle(cx=new_x, cz=new_z, r=new_radius)
                                st.session_state.junction_r_outer[i] = new_r_outer

                                # Update junction state hash to force UI refresh
                                st.session_state.junction_state_hash += 1

                                st.rerun()

                    st.markdown(&#39;&lt;/div&gt;&#39;, unsafe_allow_html=True)
            else:
                st.info(&#34;No junctions defined yet&#34;)

    def render_junction_plot(self):
        &#34;&#34;&#34;Render interactive plot for junction editing&#34;&#34;&#34;
        if not st.session_state.trajectories:
            return

        # Create plot
        fig = go.Figure()

        # Add ALL trajectories (with sampling for performance)
        all_trajectories = st.session_state.trajectories
        for i, traj in enumerate(all_trajectories):
            # Sample every 20th point for performance with many trajectories
            sample_rate = max(1, len(traj.x) // 1000)  # Adaptive sampling
            fig.add_trace(go.Scatter(
                x=traj.x[::sample_rate],
                y=traj.z[::sample_rate],
                mode=&#39;lines&#39;,
                line=dict(color=&#39;lightgray&#39;, width=0.5),
                name=f&#39;Trajectory {i}&#39;,
                showlegend=False,
                opacity=0.6
            ))

        # Add junctions with r_outer circles
        for i, junction in enumerate(st.session_state.junctions):
            # Get r_outer for this junction
            r_outer = st.session_state.junction_r_outer.get(i, 50.0)

            # Junction center with hover info
            fig.add_trace(go.Scatter(
                x=[junction.cx],
                y=[junction.cz],
                mode=&#39;markers&#39;,
                marker=dict(size=20, color=&#39;red&#39;, symbol=&#39;circle&#39;),
                name=f&#39;J{i}&#39;,
                text=f&#39;J{i}&lt;br&gt;Pos: ({junction.cx:.1f}, {junction.cz:.1f})&lt;br&gt;Radius: {junction.r}&lt;br&gt;R_outer: {r_outer}&#39;,
                textposition=&#39;middle center&#39;,
                textfont=dict(color=&#39;white&#39;, size=14, family=&#34;Arial Black&#34;),
                hovertemplate=f&#39;&lt;b&gt;Junction {i}&lt;/b&gt;&lt;br&gt;&#39; +
                             f&#39;Position: ({junction.cx:.1f}, {junction.cz:.1f})&lt;br&gt;&#39; +
                             f&#39;Radius: {junction.r}&lt;br&gt;&#39; +
                             f&#39;R_outer: {r_outer}&lt;br&gt;&#39; +
                             &#39;&lt;extra&gt;&lt;/extra&gt;&#39;
            ))

            # Junction radius circle (decision radius)
            theta = np.linspace(0, 2*np.pi, 100)
            circle_x = junction.cx + junction.r * np.cos(theta)
            circle_z = junction.cz + junction.r * np.sin(theta)

            fig.add_trace(go.Scatter(
                x=circle_x,
                y=circle_z,
                mode=&#39;lines&#39;,
                line=dict(color=&#39;orange&#39;, width=3),
                showlegend=False,
                hovertemplate=f&#39;&lt;b&gt;Junction {i} - Decision Radius&lt;/b&gt;&lt;br&gt;&#39; +
                             f&#39;Position: ({junction.cx:.1f}, {junction.cz:.1f})&lt;br&gt;&#39; +
                             f&#39;Decision Radius: {junction.r}&lt;br&gt;&#39; +
                             f&#39;R_outer: {r_outer}&lt;br&gt;&#39; +
                             &#39;&lt;extra&gt;&lt;/extra&gt;&#39;,
                name=f&#39;J{i} Decision Radius&#39;
            ))

            # R_outer circle (analysis radius)
            r_outer_x = junction.cx + r_outer * np.cos(theta)
            r_outer_z = junction.cz + r_outer * np.sin(theta)

            fig.add_trace(go.Scatter(
                x=r_outer_x,
                y=r_outer_z,
                mode=&#39;lines&#39;,
                line=dict(color=&#39;blue&#39;, width=2, dash=&#39;dash&#39;),
                showlegend=False,
                hovertemplate=f&#39;&lt;b&gt;Junction {i} - Analysis Radius&lt;/b&gt;&lt;br&gt;&#39; +
                             f&#39;Position: ({junction.cx:.1f}, {junction.cz:.1f})&lt;br&gt;&#39; +
                             f&#39;Decision Radius: {junction.r}&lt;br&gt;&#39; +
                             f&#39;R_outer: {r_outer}&lt;br&gt;&#39; +
                             &#39;&lt;extra&gt;&lt;/extra&gt;&#39;,
                name=f&#39;J{i} R_outer&#39;
            ))

            # Add invisible junction area for better hover capture
            # Create a filled circle area that will capture hover events
            theta_dense = np.linspace(0, 2*np.pi, 200)
            area_x = junction.cx + r_outer * np.cos(theta_dense)
            area_z = junction.cz + r_outer * np.sin(theta_dense)

            fig.add_trace(go.Scatter(
                x=area_x,
                y=area_z,
                mode=&#39;lines&#39;,
                fill=&#39;toself&#39;,
                fillcolor=&#39;rgba(255, 0, 0, 0.05)&#39;,  # Very light red fill
                line=dict(width=0),  # No visible line
                showlegend=False,
                hovertemplate=f&#39;&lt;b&gt;Junction {i} Area&lt;/b&gt;&lt;br&gt;&#39; +
                             f&#39;Position: ({junction.cx:.1f}, {junction.cz:.1f})&lt;br&gt;&#39; +
                             f&#39;Decision Radius: {junction.r}&lt;br&gt;&#39; +
                             f&#39;R_outer: {r_outer}&lt;br&gt;&#39; +
                             &#39;&lt;extra&gt;&lt;/extra&gt;&#39;,
                name=f&#39;J{i} Area&#39;,
                hoveron=&#39;fills&#39;  # Only show hover on filled area
            ))

        # Update layout
        fig.update_layout(
            title=&#34;Interactive Junction Editor - All Trajectories&#34;,
            xaxis_title=&#34;X Position&#34;,
            yaxis_title=&#34;Z Position&#34;,
            hovermode=&#39;closest&#39;,
            showlegend=False
        )

        # Set equal aspect ratio for both axes
        fig.update_xaxes(scaleanchor=&#34;y&#34;, scaleratio=1)
        fig.update_yaxes(scaleanchor=&#34;x&#34;, scaleratio=1)

        # Add legend manually
        fig.add_trace(go.Scatter(
            x=[None], y=[None],
            mode=&#39;markers&#39;,
            marker=dict(size=10, color=&#39;red&#39;),
            name=&#39;Junction Center&#39;,
            showlegend=True
        ))
        fig.add_trace(go.Scatter(
            x=[None], y=[None],
            mode=&#39;lines&#39;,
            line=dict(color=&#39;orange&#39;, width=3),
            name=&#39;Decision Radius&#39;,
            showlegend=True
        ))
        fig.add_trace(go.Scatter(
            x=[None], y=[None],
            mode=&#39;lines&#39;,
            line=dict(color=&#39;blue&#39;, width=2, dash=&#39;dash&#39;),
            name=&#39;R_outer (Analysis Radius)&#39;,
            showlegend=True
        ))
        fig.add_trace(go.Scatter(
            x=[None], y=[None],
            mode=&#39;lines&#39;,
            line=dict(color=&#39;lightgray&#39;, width=1),
            name=&#39;Trajectories&#39;,
            showlegend=True
        ))

        st.plotly_chart(fig, config={&#39;displayModeBar&#39;: True}, width=&#39;stretch&#39;)

    def load_sample_junctions(self):
        &#34;&#34;&#34;Load sample junctions&#34;&#34;&#34;
        sample_junctions = [
            Circle(cx=685, cz=170, r=30),
            Circle(cx=550, cz=-90, r=30),
            Circle(cx=730, cz=440, r=20),
            Circle(cx=520, cz=340, r=40),
            Circle(cx=500, cz=515, r=20),
            Circle(cx=575, cz=430, r=15),
            Circle(cx=500, cz=205, r=20)
        ]

        # Sample r_outer values
        sample_r_outer = [100.0, 50.0, 45.0, 45.0, 45.0, 30.0, 45.0]

        st.session_state.junctions = sample_junctions
        st.session_state.junction_r_outer = {i: r_outer for i, r_outer in enumerate(sample_r_outer)}
        st.success(&#34;✅ Loaded sample junctions with r_outer values!&#34;)
        st.rerun()

    def render_analysis(self):
        &#34;&#34;&#34;Render the analysis interface&#34;&#34;&#34;
        st.markdown(&#39;&lt;h2 class=&#34;section-header&#34;&gt;📊 Analysis&lt;/h2&gt;&#39;, unsafe_allow_html=True)

        if not st.session_state.trajectories:
            st.warning(&#34;⚠️ Please load trajectory data first&#34;)
            return

        col1, col2 = st.columns([3, 1])

        with col1:
            st.markdown(&#34;### Analysis Configuration&#34;)

            # Analysis type selection
            analysis_type_options = {
                &#34;discover&#34;: &#34;🔍 Discover Branches - Find decision branches at junctions&#34;,
                &#34;assign&#34;: &#34;📊 Assign Trajectories - Assign trajectories to discovered branches&#34;,
                &#34;metrics&#34;: &#34;📈 Movement Metrics - Calculate trajectory movement patterns and timing&#34;,
                &#34;gaze&#34;: &#34;👁️ Gaze &amp; Physiology - Analyze eye tracking and physiological data&#34;,
                &#34;predict&#34;: &#34;🔮 Predict Choices - Predict junction choice patterns&#34;,
                &#34;intent&#34;: &#34;🧠 Intent Recognition - Predict route choices BEFORE decision points (ML)&#34;,
                &#34;enhanced&#34;: &#34;🚨 Enhanced Analysis - Evacuation planning, risk assessment, and efficiency metrics&#34;
            }

            # Reset analysis type if we&#39;re coming from another tab
            if st.session_state.current_step == &#34;analysis&#34;:
                # Clear any cached analysis type to ensure fresh selection
                if &#34;cached_analysis_type&#34; in st.session_state:
                    del st.session_state.cached_analysis_type

            analysis_type = st.selectbox(
                &#34;Analysis Type:&#34;,
                list(analysis_type_options.keys()),
                format_func=lambda x: analysis_type_options[x],
                help=&#34;Select the type of analysis to perform&#34;,
                key=f&#34;analysis_type_select_{st.session_state.current_step}&#34;
            )

            # Store the selected analysis type
            st.session_state.cached_analysis_type = analysis_type

            # Initialize default values
            decision_mode = &#34;pathlen&#34;
            cluster_method = &#34;dbscan&#34;
            seed = 42

            # Initialize cluster parameters with defaults
            dbscan_eps = 0.5
            dbscan_min_samples = 5
            dbscan_angle_eps = 15.0
            kmeans_k = 3
            kmeans_k_min = 2
            kmeans_k_max = 6
            auto_k_min = 2
            auto_k_max = 6
            auto_min_sep_deg = 12.0
            auto_angle_eps = 15.0

            # Initialize decision mode parameters with defaults
            radial_r_outer = 50.0
            radial_epsilon = 0.05
            pathlen_path_length = 100.0
            pathlen_linger_delta = 0.0
            hybrid_r_outer = 50.0
            hybrid_path_length = 100.0
            hybrid_linger_delta = 0.0

            # Initialize metrics parameters with defaults
            metrics_decision_mode = &#34;pathlen&#34;
            metrics_distance = 100.0
            metrics_r_outer = 50.0
            metrics_trend_window = 5
            metrics_min_outward = 0.0

            # Initialize discover parameters with defaults
            discover_decision_mode = &#34;hybrid&#34;
            discover_r_outer = 50.0
            discover_epsilon = 0.05
            discover_path_length = 100.0
            discover_linger_delta = 0.0
            discover_hybrid_r_outer = 50.0
            discover_hybrid_path_length = 100.0

            # Show parameters based on analysis type
            if analysis_type == &#34;predict&#34;:
                # Predict analysis uses only spatial tracking - no parameters needed
                st.info(&#34;ℹ️ Predict analysis uses spatial tracking only. No additional parameters required.&#34;)

                # Set default values for compatibility (not used in analysis)
                cluster_method = &#34;kmeans&#34;
                seed = 42
                decision_mode = &#34;hybrid&#34;

                # REMOVED: All cluster method parameters - not needed for spatial tracking only

            elif analysis_type == &#34;intent&#34;:
                # Intent Recognition - ML-based early prediction
                st.markdown(&#34;#### 🧠 Intent Recognition Configuration&#34;)
                st.info(&#34;🤖 Machine Learning-based prediction of route choices **before** users reach decision points.&#34;)

                # Check if Discover has been run
                has_discover_results = (st.session_state.analysis_results and
                                       &#39;branches&#39; in st.session_state.analysis_results)

                if has_discover_results:
                    st.success(&#34;✅ Will use branch assignments from your previous &#39;Discover Branches&#39; analysis&#34;)
                else:
                    st.warning(&#34;⚠️ No &#39;Discover Branches&#39; results found. Will use default clustering parameters.&#34;)
                    st.info(&#34;💡 **Recommended:** Run &#39;Discover Branches&#39; analysis first to control clustering settings!&#34;)

                # Prediction distances
                st.markdown(&#34;##### Prediction Distances&#34;)
                dist_col1, dist_col2, dist_col3, dist_col4 = st.columns(4)

                with dist_col1:
                    intent_dist_100 = st.checkbox(&#34;100 units&#34;, value=True, help=&#34;Predict 100 units before junction&#34;)
                with dist_col2:
                    intent_dist_75 = st.checkbox(&#34;75 units&#34;, value=True, help=&#34;Predict 75 units before junction&#34;)
                with dist_col3:
                    intent_dist_50 = st.checkbox(&#34;50 units&#34;, value=True, help=&#34;Predict 50 units before junction&#34;)
                with dist_col4:
                    intent_dist_25 = st.checkbox(&#34;25 units&#34;, value=True, help=&#34;Predict 25 units before junction&#34;)

                # Build prediction distances list
                intent_prediction_distances = []
                if intent_dist_100:
                    intent_prediction_distances.append(100.0)
                if intent_dist_75:
                    intent_prediction_distances.append(75.0)
                if intent_dist_50:
                    intent_prediction_distances.append(50.0)
                if intent_dist_25:
                    intent_prediction_distances.append(25.0)

                if not intent_prediction_distances:
                    st.warning(&#34;⚠️ Select at least one prediction distance!&#34;)
                    intent_prediction_distances = [50.0]  # Default

                # Model configuration
                st.markdown(&#34;##### Model Configuration&#34;)
                model_col1, model_col2 = st.columns(2)

                with model_col1:
                    intent_model_type = st.selectbox(
                        &#34;ML Model:&#34;,
                        [&#34;random_forest&#34;, &#34;gradient_boosting&#34;],
                        index=0,
                        help=&#34;Random Forest: Fast, robust | Gradient Boosting: More accurate but slower&#34;
                    )

                with model_col2:
                    intent_cv_folds = st.number_input(
                        &#34;Cross-validation Folds:&#34;,
                        value=5,
                        min_value=2,
                        max_value=10,
                        help=&#34;Number of folds for cross-validation&#34;
                    )

                # Feature configuration
                with st.expander(&#34;🔧 Advanced: Feature Configuration&#34;):
                    st.markdown(&#34;**Features Used:**&#34;)
                    st.markdown(&#34;&#34;&#34;
                    - ✅ **Spatial**: Distance, approach angle, lateral offset
                    - ✅ **Kinematic**: Speed, acceleration, curvature, sinuosity
                    - ✅ **Gaze** (if available): Gaze angle, alignment, head rotation
                    - ✅ **Physiological** (if available): Heart rate, pupil dilation
                    - ✅ **Contextual**: Previous junction choices
                    &#34;&#34;&#34;)

                    intent_test_split = st.slider(
                        &#34;Test Set Size (%):&#34;,
                        min_value=10,
                        max_value=40,
                        value=20,
                        help=&#34;Percentage of data reserved for testing&#34;
                    )

                # Store intent parameters in session state
                if &#39;intent_params&#39; not in st.session_state:
                    st.session_state.intent_params = {}

                st.session_state.intent_params = {
                    &#39;prediction_distances&#39;: intent_prediction_distances,
                    &#39;model_type&#39;: intent_model_type,
                    &#39;cv_folds&#39;: intent_cv_folds,
                    &#39;test_split&#39;: intent_test_split / 100.0
                }

                # Set default values for compatibility
                cluster_method = &#34;kmeans&#34;
                seed = 42
                decision_mode = &#34;hybrid&#34;

            elif analysis_type == &#34;discover&#34;:
                # Clustering parameters (used by discover analysis)
                st.markdown(&#34;#### Clustering Parameters&#34;)
                col_method, col_seed = st.columns(2)

                with col_method:
                    cluster_method = st.selectbox(
                        &#34;Cluster Method:&#34;,
                        [&#34;dbscan&#34;, &#34;kmeans&#34;, &#34;auto&#34;],
                        index=0,  # Default to DBSCAN
                        help=&#34;Clustering method for discover analysis&#34;
                    )

                with col_seed:
                    seed = st.number_input(
                        &#34;Random Seed:&#34;,
                        value=42,
                        min_value=0,
                        max_value=10000,
                        step=1,
                        help=&#34;Random seed for reproducibility&#34;
                    )

                # Decision mode parameters (needed for discover analysis)
                st.markdown(&#34;#### Decision Mode Parameters&#34;)
                col_decision_mode, col_decision_param = st.columns(2)

                with col_decision_mode:
                    discover_decision_mode = st.selectbox(
                        &#34;Decision Mode:&#34;,
                        [&#34;radial&#34;, &#34;pathlen&#34;, &#34;hybrid&#34;],
                        index=2,  # Default to hybrid
                        help=&#34;Decision mode for discover analysis&#34;
                    )

                with col_decision_param:
                    if discover_decision_mode == &#34;radial&#34;:
                        st.info(&#34;ℹ️ Using junction-specific r_outer values from the Junctions tab&#34;)
                        # r_outer will be overridden by junction-specific values
                        discover_r_outer = 50.0
                        discover_epsilon = st.number_input(
                            &#34;Epsilon:&#34;,
                            value=0.05,
                            min_value=0.01,
                            max_value=1.0,
                            step=0.01,
                            help=&#34;Epsilon parameter&#34;
                        )
                    elif discover_decision_mode == &#34;pathlen&#34;:
                        discover_path_length = st.number_input(
                            &#34;Path Length:&#34;,
                            value=100.0,
                            min_value=10.0,
                            max_value=500.0,
                            step=10.0,
                            help=&#34;Path length for pathlen mode&#34;
                        )
                        discover_linger_delta = st.number_input(
                            &#34;Linger Delta:&#34;,
                            value=0.0,
                            min_value=0.0,
                            max_value=50.0,
                            step=1.0,
                            help=&#34;Linger distance beyond junction&#34;
                        )
                    elif discover_decision_mode == &#34;hybrid&#34;:
                        st.info(&#34;ℹ️ Using junction-specific r_outer values from the Junctions tab&#34;)
                        # r_outer will be overridden by junction-specific values
                        discover_hybrid_r_outer = 50.0
                        discover_hybrid_path_length = st.number_input(
                            &#34;Hybrid Path Length:&#34;,
                            value=100.0,
                            min_value=10.0,
                            max_value=500.0,
                            step=10.0,
                            help=&#34;Path length for hybrid mode&#34;
                        )
                        discover_hybrid_linger_delta = st.number_input(
                            &#34;Hybrid Linger Delta:&#34;,
                            value=0.0,
                            min_value=0.0,
                            max_value=50.0,
                            step=1.0,
                            help=&#34;Linger distance beyond junction for hybrid mode&#34;
                        )

                # Dynamic parameters based on cluster method (for discover analysis)
                st.markdown(&#34;#### Cluster Method Parameters&#34;)
                if cluster_method == &#34;dbscan&#34;:
                    col_eps, col_min_samples = st.columns(2)
                    with col_eps:
                        dbscan_eps = st.number_input(
                            &#34;DBSCAN Epsilon (eps):&#34;,
                            value=0.5,
                            min_value=0.1,
                            max_value=10.0,
                            step=0.1,
                            help=&#34;Maximum distance between samples in the same neighborhood&#34;
                        )
                    with col_min_samples:
                        dbscan_min_samples = st.number_input(
                            &#34;DBSCAN Min Samples:&#34;,
                            value=5,
                            min_value=1,
                            max_value=50,
                            step=1,
                            help=&#34;Minimum number of samples in a neighborhood&#34;
                        )

                    # Add angle_eps parameter for DBSCAN
                    dbscan_angle_eps = st.number_input(
                        &#34;DBSCAN Angle Epsilon (degrees):&#34;,
                        value=11.0,
                        min_value=1.0,
                        max_value=90.0,
                        step=1.0,
                        help=&#34;Angle epsilon for DBSCAN clustering (angular separation between clusters)&#34;
                    )
                elif cluster_method == &#34;kmeans&#34;:
                    col_k, col_k_range = st.columns(2)
                    with col_k:
                        kmeans_k = st.number_input(
                            &#34;K-Means K (number of clusters):&#34;,
                            value=3,
                            min_value=2,
                            max_value=20,
                            step=1,
                            help=&#34;Number of clusters to form&#34;
                        )
                    with col_k_range:
                        kmeans_k_min = st.number_input(
                            &#34;K-Means K Min:&#34;,
                            value=2,
                            min_value=2,
                            max_value=10,
                            step=1,
                            help=&#34;Minimum number of clusters for auto selection&#34;
                        )
                        kmeans_k_max = st.number_input(
                            &#34;K-Means K Max:&#34;,
                            value=6,
                            min_value=3,
                            max_value=20,
                            step=1,
                            help=&#34;Maximum number of clusters for auto selection&#34;
                        )
                elif cluster_method == &#34;auto&#34;:
                    col_k_range, col_separation = st.columns(2)
                    with col_k_range:
                        auto_k_min = st.number_input(
                            &#34;Auto K Min:&#34;,
                            value=2,
                            min_value=2,
                            max_value=10,
                            step=1,
                            help=&#34;Minimum number of clusters for auto selection&#34;
                        )
                        auto_k_max = st.number_input(
                            &#34;Auto K Max:&#34;,
                            value=6,
                            min_value=3,
                            max_value=20,
                            step=1,
                            help=&#34;Maximum number of clusters for auto selection&#34;
                        )
                    with col_separation:
                        auto_min_sep_deg = st.number_input(
                            &#34;Auto Min Separation (degrees):&#34;,
                            value=12.0,
                            min_value=1.0,
                            max_value=90.0,
                            step=1.0,
                            help=&#34;Minimum separation in degrees between clusters&#34;
                        )
                        auto_angle_eps = st.number_input(
                            &#34;Auto Angle Epsilon (degrees):&#34;,
                            value=11.0,
                            min_value=1.0,
                            max_value=90.0,
                            step=1.0,
                            help=&#34;Angle epsilon for auto clustering&#34;
                        )

            elif analysis_type == &#34;enhanced&#34;:
                # Enhanced analysis parameters (same as discover since it uses discover_decision_chain)
                st.markdown(&#34;#### Enhanced Analysis Parameters&#34;)
                st.info(&#34;🚨 Enhanced analysis uses the same clustering and decision parameters as discover analysis, then performs evacuation planning, risk assessment, and efficiency analysis.&#34;)

                # Clustering parameters (same as discover)
                st.markdown(&#34;##### Clustering Parameters&#34;)
                col_method, col_seed = st.columns(2)

                with col_method:
                    cluster_method = st.selectbox(
                        &#34;Cluster Method:&#34;,
                        [&#34;dbscan&#34;, &#34;kmeans&#34;, &#34;auto&#34;],
                        index=0,  # Default to DBSCAN
                        help=&#34;Clustering method for enhanced analysis&#34;
                    )

                with col_seed:
                    seed = st.number_input(
                        &#34;Random Seed:&#34;,
                        value=42,
                        min_value=0,
                        max_value=10000,
                        step=1,
                        help=&#34;Random seed for reproducibility&#34;
                    )

                # Decision mode parameters (same as discover)
                st.markdown(&#34;##### Decision Mode Parameters&#34;)
                col_decision_mode, col_decision_param = st.columns(2)

                with col_decision_mode:
                    discover_decision_mode = st.selectbox(
                        &#34;Decision Mode:&#34;,
                        [&#34;radial&#34;, &#34;pathlen&#34;, &#34;hybrid&#34;],
                        index=2,  # Default to hybrid
                        help=&#34;Decision mode for enhanced analysis&#34;
                    )

                with col_decision_param:
                    if discover_decision_mode == &#34;radial&#34;:
                        st.info(&#34;ℹ️ Using junction-specific r_outer values from the Junctions tab&#34;)
                        # r_outer will be overridden by junction-specific values
                        discover_r_outer = 50.0
                        discover_epsilon = st.number_input(
                            &#34;Epsilon:&#34;,
                            value=0.05,
                            min_value=0.01,
                            max_value=1.0,
                            step=0.01,
                            help=&#34;Epsilon parameter&#34;
                        )
                    elif discover_decision_mode == &#34;pathlen&#34;:
                        discover_path_length = st.number_input(
                            &#34;Path Length:&#34;,
                            value=100.0,
                            min_value=10.0,
                            max_value=500.0,
                            step=10.0,
                            help=&#34;Path length for pathlen mode&#34;
                        )
                        discover_linger_delta = st.number_input(
                            &#34;Linger Delta:&#34;,
                            value=0.0,
                            min_value=0.0,
                            max_value=50.0,
                            step=1.0,
                            help=&#34;Linger distance beyond junction&#34;
                        )
                    elif discover_decision_mode == &#34;hybrid&#34;:
                        st.info(&#34;ℹ️ Using junction-specific r_outer values from the Junctions tab&#34;)
                        # r_outer will be overridden by junction-specific values
                        discover_hybrid_r_outer = 50.0
                        discover_hybrid_path_length = st.number_input(
                            &#34;Hybrid Path Length:&#34;,
                            value=100.0,
                            min_value=10.0,
                            max_value=500.0,
                            step=10.0,
                            help=&#34;Path length for hybrid mode&#34;
                        )
                        discover_hybrid_linger_delta = st.number_input(
                            &#34;Hybrid Linger Delta:&#34;,
                            value=0.0,
                            min_value=0.0,
                            max_value=50.0,
                            step=1.0,
                            help=&#34;Linger distance beyond junction for hybrid mode&#34;
                        )

                # Dynamic parameters based on cluster method (same as discover)
                st.markdown(&#34;##### Cluster Method Parameters&#34;)
                if cluster_method == &#34;dbscan&#34;:
                    col_eps, col_min_samples = st.columns(2)
                    with col_eps:
                        dbscan_eps = st.number_input(
                            &#34;DBSCAN Epsilon (eps):&#34;,
                            value=0.5,
                            min_value=0.1,
                            max_value=10.0,
                            step=0.1,
                            help=&#34;Maximum distance between samples in the same neighborhood&#34;
                        )
                    with col_min_samples:
                        dbscan_min_samples = st.number_input(
                            &#34;DBSCAN Min Samples:&#34;,
                            value=5,
                            min_value=1,
                            max_value=50,
                            step=1,
                            help=&#34;Minimum number of samples in a neighborhood&#34;
                        )

                    # Add angle_eps parameter for DBSCAN
                    dbscan_angle_eps = st.number_input(
                        &#34;DBSCAN Angle Epsilon (degrees):&#34;,
                        value=11.0,
                        min_value=1.0,
                        max_value=90.0,
                        step=1.0,
                        help=&#34;Angle epsilon for DBSCAN clustering (angular separation between clusters)&#34;
                    )
                elif cluster_method == &#34;kmeans&#34;:
                    col_k, col_k_range = st.columns(2)
                    with col_k:
                        kmeans_k = st.number_input(
                            &#34;K-Means K (number of clusters):&#34;,
                            value=3,
                            min_value=2,
                            max_value=20,
                            step=1,
                            help=&#34;Number of clusters to form&#34;
                        )
                    with col_k_range:
                        kmeans_k_min = st.number_input(
                            &#34;K-Means K Min:&#34;,
                            value=2,
                            min_value=2,
                            max_value=10,
                            step=1,
                            help=&#34;Minimum number of clusters for auto selection&#34;
                        )
                        kmeans_k_max = st.number_input(
                            &#34;K-Means K Max:&#34;,
                            value=6,
                            min_value=3,
                            max_value=20,
                            step=1,
                            help=&#34;Maximum number of clusters for auto selection&#34;
                        )
                elif cluster_method == &#34;auto&#34;:
                    col_k_range, col_separation = st.columns(2)
                    with col_k_range:
                        auto_k_min = st.number_input(
                            &#34;Auto K Min:&#34;,
                            value=2,
                            min_value=2,
                            max_value=10,
                            step=1,
                            help=&#34;Minimum number of clusters for auto selection&#34;
                        )
                        auto_k_max = st.number_input(
                            &#34;Auto K Max:&#34;,
                            value=6,
                            min_value=3,
                            max_value=20,
                            step=1,
                            help=&#34;Maximum number of clusters for auto selection&#34;
                        )
                    with col_separation:
                        auto_min_sep_deg = st.number_input(
                            &#34;Auto Min Separation (degrees):&#34;,
                            value=12.0,
                            min_value=1.0,
                            max_value=90.0,
                            step=1.0,
                            help=&#34;Minimum separation in degrees between clusters&#34;
                        )
                        auto_angle_eps = st.number_input(
                            &#34;Auto Angle Epsilon (degrees):&#34;,
                            value=11.0,
                            min_value=1.0,
                            max_value=90.0,
                            step=1.0,
                            help=&#34;Angle epsilon for auto clustering&#34;
                        )

            elif analysis_type == &#34;metrics&#34;:
                # Metrics-specific parameters
                st.markdown(&#34;#### Metrics Parameters&#34;)
                st.info(&#34;Metrics analysis computes timing and distance metrics for trajectories. The decision mode determines how junction timing is calculated - &#39;pathlen&#39; measures time to reach a distance threshold, &#39;radial&#39; measures time to exit a radius, and &#39;hybrid&#39; tries radial first then falls back to pathlen.&#34;)

                col_metrics_mode, col_metrics_distance = st.columns(2)

                with col_metrics_mode:
                    st.session_state.metrics_decision_mode = st.selectbox(
                        &#34;Decision Mode:&#34;,
                        [&#34;pathlen&#34;, &#34;radial&#34;, &#34;hybrid&#34;],
                        index=2,  # Default to hybrid
                        help=&#34;Decision mode for junction timing analysis&#34;
                    )

                with col_metrics_distance:
                    st.session_state.metrics_distance = st.number_input(
                        &#34;Distance Threshold:&#34;,
                        value=100.0,
                        min_value=10.0,
                        max_value=500.0,
                        step=10.0,
                        help=&#34;Path length for decision timing (pathlen mode) or outer radius (radial mode)&#34;
                    )

                # Additional metrics parameters
                col_trend_window, col_min_outward = st.columns(2)

                with col_trend_window:
                    st.session_state.metrics_trend_window = st.number_input(
                        &#34;Metrics Trend Window:&#34;,
                        value=5,
                        min_value=1,
                        max_value=20,
                        step=1,
                        help=&#34;Trend window for radial mode&#34;
                    )

                st.session_state.metrics_min_outward = st.number_input(
                    &#34;Metrics Min Outward:&#34;,
                    value=0.0,
                    min_value=0.0,
                    max_value=10.0,
                    step=0.1,
                    help=&#34;Minimum outward movement for radial mode&#34;
                )

                # Show info about using junction-specific r_outer values
                if st.session_state.metrics_decision_mode in [&#34;radial&#34;, &#34;hybrid&#34;]:
                    st.info(&#34;ℹ️ Using junction-specific r_outer values from the Junctions tab&#34;)

            elif analysis_type == &#34;gaze&#34;:
                # Gaze analysis parameters
                st.markdown(&#34;#### Gaze &amp; Physiological Analysis Parameters&#34;)

                # Prefer gaze trajectories if available
                active_trajs = st.session_state.trajectories

                # Check if we have proper gaze trajectory data
                has_gaze_data = self._check_for_gaze_data(active_trajs)

                # Show simple status message
                if active_trajs:
                    from verta.verta_data_loader import has_vr_headset_data
                    has_vr = any(has_vr_headset_data(traj) for traj in active_trajs)

                    if has_vr:
                        st.success(&#34;✅ This dataset contains VR headset data!&#34;)

                # Show analysis options
                if has_gaze_data:

                    # Check if we have existing branch assignments
                    has_existing_assignments = (st.session_state.analysis_results is not None and
                                             &#34;branches&#34; in st.session_state.analysis_results)

                    if has_existing_assignments:
                        st.success(&#34;✅ **Existing branch assignments found!**&#34;)
                        st.info(&#34;🔍 Gaze analysis will use existing branch assignments from previous discover analysis.&#34;)
                        st.write(&#34;**💡 To create new assignments:** Run &#39;🔍 Discover Branches&#39; analysis first, then return here for gaze analysis.&#34;)

                        # Always use existing assignments
                        st.session_state.use_existing_assignments = True
                        st.session_state.run_custom_discover = False

                    else:
                        st.warning(&#34;⚠️ **No existing branch assignments found!**&#34;)
                        st.error(&#34;**Prerequisite:** You must run &#39;🔍 Discover Branches&#39; analysis first to create branch assignments.&#34;)
                        st.write(&#34;**Steps:**&#34;)
                        st.write(&#34;1. Go to the &#39;🔍 Discover Branches&#39; analysis&#34;)
                        st.write(&#34;2. Run the discover analysis to create branch assignments&#34;)
                        st.write(&#34;3. Return here to run gaze analysis&#34;)

                        # Disable gaze analysis if no assignments
                        st.session_state.use_existing_assignments = False
                        st.session_state.run_custom_discover = False

                        # Show a disabled button
                        st.button(&#34;Run Analysis&#34;, disabled=True, help=&#34;Run &#39;🔍 Discover Branches&#39; analysis first to create branch assignments&#34;)
                        return

                    # Pupil Dilation Heatmap Settings
                    st.markdown(&#34;---&#34;)
                    st.markdown(&#34;#### 🗺️ Pupil Dilation Heatmap Settings&#34;)
                    st.info(&#34;Configure spatial heatmap visualization of pupil dilation patterns&#34;)

                    col_grid, col_norm = st.columns(2)

                    with col_grid:
                        # Initialize session state with default value if not set
                        if &#39;pupil_heatmap_cell_size&#39; not in st.session_state:
                            st.session_state.pupil_heatmap_cell_size = 10.0

                        st.session_state.pupil_heatmap_cell_size = st.slider(
                            &#34;Cell Size (coordinate units):&#34;,
                            min_value=1.0,
                            max_value=200.0,
                            value=float(st.session_state.pupil_heatmap_cell_size),
                            step=1.0,
                            help=&#34;Size of each grid cell in coordinate units (smaller = finer resolution)&#34;
                        )

                    with col_norm:
                        # Initialize session state with default value if not set
                        if &#39;pupil_heatmap_normalization&#39; not in st.session_state:
                            st.session_state.pupil_heatmap_normalization = &#39;relative&#39;

                        st.session_state.pupil_heatmap_normalization = st.selectbox(
                            &#34;Normalization:&#34;,
                            [&#34;relative&#34;, &#34;zscore&#34;],
                            index=0 if st.session_state.pupil_heatmap_normalization == &#39;relative&#39; else 1,
                            help=&#34;Relative: % change from baseline. Z-score: standard deviations from mean&#34;
                        )

                    # Show expected grid dimensions
                    if st.session_state.trajectories:
                        # Calculate approximate grid dimensions
                        all_x = np.concatenate([t.x for t in st.session_state.trajectories[:10] if hasattr(t, &#39;x&#39;)])
                        all_z = np.concatenate([t.z for t in st.session_state.trajectories[:10] if hasattr(t, &#39;z&#39;)])
                        x_range = np.max(all_x) - np.min(all_x)
                        z_range = np.max(all_z) - np.min(all_z)
                        cell_size = st.session_state.pupil_heatmap_cell_size
                        grid_x = int(np.ceil(x_range / cell_size))
                        grid_z = int(np.ceil(z_range / cell_size))

                        st.info(f&#34;📏 **Expected grid:** {grid_x} × {grid_z} cells ({cell_size}×{cell_size} units each)&#34;)

                    # Run Analysis button
                    if st.button(&#34;Run Analysis&#34;, type=&#34;primary&#34;):
                        # Run gaze analysis
                        self.run_analysis(&#34;gaze&#34;, &#34;hybrid&#34;, &#34;dbscan&#34;, 42)

                else:
                    st.info(&#34;ℹ️ Gaze/Physiological analysis requires VR headset data with eye tracking.&#34;)

            elif analysis_type == &#34;assign&#34;:
                st.markdown(&#34;#### Assign Parameters&#34;)

                # Add scaling warning
                st.info(&#34;💡 **Important**: Ensure the scale factor used for trajectories matches the scale factor used during discover analysis. Mismatched scaling will cause assignment failures.&#34;)

                # Scale factor input for assign analysis
                st.markdown(&#34;**📏 Scale Factor for Assignment:**&#34;)
                col_scale_assign, col_scale_info = st.columns([1, 2])

                with col_scale_assign:
                    assign_scale = st.number_input(
                        &#34;Scale Factor:&#34;,
                        value=st.session_state.get(&#34;scale_factor&#34;, 0.2),
                        min_value=0.01,
                        max_value=1.0,
                        step=0.01,
                        key=&#34;assign_scale_factor&#34;,
                        help=&#34;Scale factor to apply to trajectory coordinates&#34;
                    )

                with col_scale_info:
                    # Show scale factor from discover analysis if available
                    analysis_results = st.session_state.get(&#34;analysis_results&#34;)
                    if analysis_results and &#34;branches&#34; in analysis_results:
                        discover_scale = None
                        for junction_key, branch_data in analysis_results[&#34;branches&#34;].items():
                            if &#34;scale&#34; in branch_data:
                                discover_scale = branch_data[&#34;scale&#34;]
                                break

                        if discover_scale is not None:
                            st.info(f&#34;🔍 **Discover used scale**: {discover_scale:.2f}&#34;)
                            if abs(assign_scale - discover_scale) &gt; 0.01:
                                st.warning(f&#34;⚠️ **Scale mismatch detected!** Consider using {discover_scale:.2f} for consistency.&#34;)
                        else:
                            st.info(&#34;🔍 No discover scale factor found&#34;)
                    else:
                        st.info(&#34;🔍 No discover analysis found&#34;)

                # Simplified data input options
                st.markdown(&#34;**📁 Data Input Options:**&#34;)

                # Trajectories input
                st.markdown(&#34;**Trajectories:**&#34;)
                trajectory_option = st.radio(
                    &#34;Trajectory Source:&#34;,
                    [&#34;Upload files&#34;, &#34;Select folder&#34;],
                    key=&#34;assign_trajectory_option&#34;,
                    help=&#34;Upload new trajectories to be assigned to existing branches&#34;
                )

                # Centers input
                st.markdown(&#34;**Junction Centers:**&#34;)
                centers_option = st.radio(
                    &#34;Centers Source:&#34;,
                    [&#34;Use session centers&#34;, &#34;Upload files&#34;, &#34;Select folder&#34;],
                    key=&#34;assign_centers_option&#34;,
                    help=&#34;Choose how to provide junction center data&#34;
                )

                # File upload and folder selection based on options
                if trajectory_option == &#34;Upload files&#34;:
                    st.markdown(&#34;**📤 Upload Trajectory Files:**&#34;)
                    trajectory_files = st.file_uploader(
                        &#34;Choose trajectory CSV files:&#34;,
                        type=[&#39;csv&#39;],
                        accept_multiple_files=True,
                        key=&#34;assign_trajectory_files&#34;,
                        help=&#34;Upload CSV files containing trajectory data to be assigned to existing branches&#34;
                    )
                else:  # Select folder
                    st.markdown(&#34;**📁 Select Trajectory Folder:**&#34;)
                    trajectory_folder = st.text_input(
                        &#34;Trajectory folder path:&#34;,
                        key=&#34;assign_trajectory_folder&#34;,
                        help=&#34;Enter the path to the folder containing trajectory CSV files to be assigned to existing branches&#34;
                    )

                if centers_option == &#34;Upload files&#34;:
                    st.markdown(&#34;**📤 Upload Center Files:**&#34;)
                    centers_files = st.file_uploader(
                        &#34;Choose center files (.npy or .zip):&#34;,
                        type=[&#39;npy&#39;, &#39;zip&#39;],
                        accept_multiple_files=True,
                        key=&#34;assign_centers_files&#34;,
                        help=&#34;Upload .npy files containing junction centers or .zip files with multiple centers&#34;
                    )
                elif centers_option == &#34;Select folder&#34;:
                    st.markdown(&#34;**📁 Select Centers Folder:**&#34;)
                    centers_folder = st.text_input(
                        &#34;Centers folder path:&#34;,
                        key=&#34;assign_centers_folder&#34;,
                        help=&#34;Enter the path to the folder containing junction centers (will search subfolders for branch_centers_j*.npy files)&#34;
                    )

                # Assignment parameters
                st.markdown(&#34;**⚙️ Assignment Parameters:**&#34;)
                # Decision mode selector (mirror discover logic with selectbox)
                # Default from discover if available
                default_mode = &#34;hybrid&#34;
                if centers_option == &#34;Use session centers&#34; and st.session_state.get(&#34;analysis_results&#34;) and &#34;branches&#34; in st.session_state.analysis_results:
                    # Try to fetch from first matching junction block
                    for _jk, _bd in st.session_state.analysis_results[&#34;branches&#34;].items():
                        if isinstance(_bd, dict) and &#34;decision_mode&#34; in _bd:
                            default_mode = _bd.get(&#34;decision_mode&#34;, default_mode)
                            break

                col_decision_mode, col_decision_param = st.columns(2)
                with col_decision_mode:
                    assign_decision_mode = st.selectbox(
                        &#34;Decision Mode:&#34;,
                        [&#34;pathlen&#34;, &#34;radial&#34;, &#34;hybrid&#34;],
                        index=[&#34;pathlen&#34;,&#34;radial&#34;,&#34;hybrid&#34;].index(default_mode) if default_mode in [&#34;pathlen&#34;,&#34;radial&#34;,&#34;hybrid&#34;] else 2,
                        key=&#34;assign_decision_mode&#34;,
                        help=&#34;How to compute initial direction vectors for assignment&#34;
                    )

                # Auto-rediscovery (always available when uploading new trajectories)
                st.markdown(&#34;**🧭 Auto-Discover New Branches (optional):**&#34;)
                auto_col1, auto_col2, auto_col3 = st.columns(3)
                with auto_col1:
                    st.checkbox(
                        &#34;Enable auto-rediscover&#34;,
                        value=False,
                        key=&#34;assign_auto_rediscover&#34;,
                        help=&#34;If outlier assignments form a dense region of size ≥ min samples, rerun discovery for this junction using all trajectories (existing + newly uploaded).&#34;
                    )
                with auto_col2:
                    st.number_input(
                        &#34;Min samples for new branch&#34;,
                        value=5,
                        min_value=2,
                        max_value=100,
                        step=1,
                        key=&#34;assign_auto_min_samples&#34;,
                        help=&#34;Minimum outlier vectors required to trigger rediscovery.&#34;
                    )
                with auto_col3:
                    st.number_input(
                        &#34;Angle eps (deg)&#34;,
                        value=11.0,
                        min_value=1.0,
                        max_value=90.0,
                        step=1.0,
                        key=&#34;assign_auto_angle_eps&#34;,
                        help=&#34;Angular neighborhood size for detecting a dense outlier region.&#34;
                    )

                # Decision mode parameters in second column (mirror discover UI)
                with col_decision_param:
                    # Fetch defaults from discover if using session centers
                    pref_path_length = 100.0
                    pref_linger = 0.0
                    pref_epsilon = 0.05
                    if centers_option == &#34;Use session centers&#34; and st.session_state.get(&#34;analysis_results&#34;) and &#34;branches&#34; in st.session_state.analysis_results:
                        for _jk, _bd in st.session_state.analysis_results[&#34;branches&#34;].items():
                            if isinstance(_bd, dict):
                                if &#34;path_length&#34; in _bd:
                                    pref_path_length = float(_bd.get(&#34;path_length&#34;, pref_path_length))
                                if &#34;linger_delta&#34; in _bd:
                                    pref_linger = float(_bd.get(&#34;linger_delta&#34;, pref_linger))
                                if &#34;epsilon&#34; in _bd:
                                    pref_epsilon = float(_bd.get(&#34;epsilon&#34;, pref_epsilon))
                                break

                    if assign_decision_mode == &#34;radial&#34;:
                        st.info(&#34;ℹ️ Using r_outer from junctions (or stored discover results)&#34;)
                        assign_r_outer = None  # Will be fetched per junction
                        assign_epsilon = st.number_input(
                            &#34;Epsilon:&#34;,
                            value=pref_epsilon,
                            min_value=0.001,
                            max_value=1.0,
                            step=0.001,
                            format=&#34;%.3f&#34;,
                            key=&#34;assign_epsilon&#34;,
                            help=&#34;Minimum movement threshold&#34;
                        )
                        assign_path_length = 100.0
                        assign_linger_delta = 0.0
                    elif assign_decision_mode == &#34;pathlen&#34;:
                        assign_path_length = st.number_input(
                            &#34;Path Length:&#34;,
                            value=pref_path_length,
                            min_value=10.0,
                            max_value=500.0,
                            step=10.0,
                            key=&#34;assign_path_length&#34;,
                            help=&#34;Path length for decision point&#34;
                        )
                        assign_linger_delta = st.number_input(
                            &#34;Linger Delta:&#34;,
                            value=pref_linger,
                            min_value=0.0,
                            max_value=200.0,
                            step=1.0,
                            key=&#34;assign_linger_delta&#34;,
                            help=&#34;Linger distance beyond junction&#34;
                        )
                        assign_epsilon = st.number_input(
                            &#34;Epsilon:&#34;,
                            value=pref_epsilon,
                            min_value=0.001,
                            max_value=1.0,
                            step=0.001,
                            format=&#34;%.3f&#34;,
                            key=&#34;assign_epsilon&#34;,
                            help=&#34;Minimum movement threshold&#34;
                        )
                        assign_r_outer = None
                    elif assign_decision_mode == &#34;hybrid&#34;:
                        st.info(&#34;ℹ️ Using r_outer from junctions (or stored discover results)&#34;)
                        assign_r_outer = None  # Will be fetched per junction
                        assign_path_length = st.number_input(
                            &#34;Hybrid Path Length:&#34;,
                            value=pref_path_length,
                            min_value=10.0,
                            max_value=500.0,
                            step=10.0,
                            key=&#34;assign_path_length&#34;,
                            help=&#34;Path length for hybrid mode&#34;
                        )
                        assign_linger_delta = st.number_input(
                            &#34;Hybrid Linger Delta:&#34;,
                            value=pref_linger,
                            min_value=0.0,
                            max_value=200.0,
                            step=1.0,
                            key=&#34;assign_linger_delta&#34;,
                            help=&#34;Linger distance for hybrid mode&#34;
                        )
                        assign_epsilon = st.number_input(
                            &#34;Epsilon:&#34;,
                            value=pref_epsilon,
                            min_value=0.001,
                            max_value=1.0,
                            step=0.001,
                            format=&#34;%.3f&#34;,
                            key=&#34;assign_epsilon&#34;,
                            help=&#34;Minimum movement threshold&#34;
                        )

                # Junction parameters for external data (only show if not using session centers)
                if centers_option != &#34;Use session centers&#34;:
                    st.markdown(&#34;**🎯 Junction Parameters (for external data):**&#34;)
                    st.info(&#34;If using external trajectory data, you may need to specify junction parameters manually.&#34;)

                    col_junction_cx, col_junction_cz, col_junction_r = st.columns(3)

                    with col_junction_cx:
                        assign_junction_cx = st.number_input(
                            &#34;Junction Center X:&#34;,
                            value=0.0,
                            key=&#34;assign_junction_cx&#34;,
                            help=&#34;X coordinate of junction center&#34;
                        )

                    with col_junction_cz:
                        assign_junction_cz = st.number_input(
                            &#34;Junction Center Z:&#34;,
                            value=0.0,
                            key=&#34;assign_junction_cz&#34;,
                            help=&#34;Z coordinate of junction center&#34;
                        )

                    with col_junction_r:
                        assign_junction_r = st.number_input(
                            &#34;Junction Radius:&#34;,
                            value=50.0,
                            min_value=1.0,
                            max_value=200.0,
                            step=1.0,
                            key=&#34;assign_junction_r&#34;,
                            help=&#34;Radius of the junction area&#34;
                        )

                # Legacy code removed - using simplified interface above

        with col2:
            st.markdown(&#34;### Run Analysis&#34;)

            # Check if junctions are defined for analysis
            has_junctions = bool(st.session_state.junctions)

            if not has_junctions:
                st.warning(&#34;⚠️ **No junctions defined!** Please define junctions in the Junction Editor before running analysis.&#34;)
                st.info(&#34;💡 **Tip:** Go to the Junction Editor tab to define junctions for your analysis.&#34;)

            if st.button(&#34;🚀 Run Analysis&#34;, type=&#34;primary&#34;, disabled=not has_junctions):
                # Collect cluster method parameters
                cluster_params = {}
                if analysis_type == &#34;discover&#34; or analysis_type == &#34;enhanced&#34;:
                    if cluster_method == &#34;dbscan&#34;:
                        cluster_params = {&#34;eps&#34;: dbscan_eps, &#34;min_samples&#34;: dbscan_min_samples, &#34;angle_eps&#34;: dbscan_angle_eps}
                    elif cluster_method == &#34;kmeans&#34;:
                        cluster_params = {&#34;k&#34;: kmeans_k, &#34;k_min&#34;: kmeans_k_min, &#34;k_max&#34;: kmeans_k_max}
                    elif cluster_method == &#34;auto&#34;:
                        cluster_params = {&#34;k_min&#34;: auto_k_min, &#34;k_max&#34;: auto_k_max, &#34;min_sep_deg&#34;: auto_min_sep_deg, &#34;angle_eps&#34;: auto_angle_eps}

                # Collect decision mode parameters
                decision_params = {}
                if analysis_type == &#34;predict&#34;:
                    if decision_mode == &#34;radial&#34;:
                        decision_params = {&#34;r_outer&#34;: radial_r_outer, &#34;epsilon&#34;: radial_epsilon}
                    elif decision_mode == &#34;pathlen&#34;:
                        decision_params = {&#34;path_length&#34;: pathlen_path_length, &#34;linger_delta&#34;: pathlen_linger_delta}
                    elif decision_mode == &#34;hybrid&#34;:
                        decision_params = {&#34;r_outer&#34;: hybrid_r_outer, &#34;path_length&#34;: hybrid_path_length, &#34;linger_delta&#34;: hybrid_linger_delta}
                elif analysis_type == &#34;discover&#34; or analysis_type == &#34;enhanced&#34;:
                    if discover_decision_mode == &#34;radial&#34;:
                        decision_params = {&#34;r_outer&#34;: discover_r_outer, &#34;epsilon&#34;: discover_epsilon}
                    elif discover_decision_mode == &#34;pathlen&#34;:
                        decision_params = {&#34;path_length&#34;: discover_path_length, &#34;linger_delta&#34;: discover_linger_delta}
                    elif discover_decision_mode == &#34;hybrid&#34;:
                        decision_params = {&#34;r_outer&#34;: discover_hybrid_r_outer, &#34;path_length&#34;: discover_hybrid_path_length, &#34;linger_delta&#34;: discover_hybrid_linger_delta}

                # Add metrics-specific parameters if analysis type is metrics
                if analysis_type == &#34;metrics&#34;:
                    decision_params.update({
                        &#34;decision_mode&#34;: st.session_state.get(&#34;metrics_decision_mode&#34;, &#34;pathlen&#34;),
                        &#34;distance&#34;: st.session_state.get(&#34;metrics_distance&#34;, 100.0),
                        &#34;r_outer&#34;: st.session_state.get(&#34;metrics_r_outer&#34;, 50.0),
                        &#34;trend_window&#34;: st.session_state.get(&#34;metrics_trend_window&#34;, 5),
                        &#34;min_outward&#34;: st.session_state.get(&#34;metrics_min_outward&#34;, 0.0)
                    })

                # Collect assign parameters if needed
                assign_params = {}
                if analysis_type == &#34;assign&#34;:
                    # Get assignment parameters
                    assign_path_length = st.session_state.get(&#34;assign_path_length&#34;, 100.0)
                    assign_epsilon = st.session_state.get(&#34;assign_epsilon&#34;, 0.05)
                    # Auto-rediscover controls
                    assign_auto_rediscover = st.session_state.get(&#34;assign_auto_rediscover&#34;, False)
                    assign_auto_min_samples = st.session_state.get(&#34;assign_auto_min_samples&#34;, 5)
                    assign_auto_angle_eps = st.session_state.get(&#34;assign_auto_angle_eps&#34;, 15.0)

                    # Get trajectory and centers options
                    trajectory_option = st.session_state.get(&#34;assign_trajectory_option&#34;, &#34;Upload files&#34;)
                    centers_option = st.session_state.get(&#34;assign_centers_option&#34;, &#34;Use session centers&#34;)

                    # Collect trajectory data
                    trajectory_files = None
                    trajectory_folder = None
                    if trajectory_option == &#34;Upload files&#34;:
                        trajectory_files = st.session_state.get(&#34;assign_trajectory_files&#34;)
                    else:  # Select folder
                        trajectory_folder = st.session_state.get(&#34;assign_trajectory_folder&#34;)

                    # Collect centers data
                    centers_files = None
                    centers_folder = None
                    if centers_option == &#34;Upload files&#34;:
                        centers_files = st.session_state.get(&#34;assign_centers_files&#34;)
                    elif centers_option == &#34;Select folder&#34;:
                        centers_folder = st.session_state.get(&#34;assign_centers_folder&#34;)

                    assign_params = {
                        &#34;path_length&#34;: assign_path_length,
                        &#34;epsilon&#34;: assign_epsilon,
                        &#34;assign_scale&#34;: assign_scale,  # Add assign-specific scale factor
                        &#34;decision_mode&#34;: assign_decision_mode,
                        &#34;r_outer&#34;: assign_r_outer,
                        &#34;linger_delta&#34;: assign_linger_delta,
                        &#34;auto_rediscover&#34;: assign_auto_rediscover,
                        &#34;auto_min_samples&#34;: assign_auto_min_samples,
                        &#34;auto_angle_eps&#34;: assign_auto_angle_eps,
                        &#34;trajectory_option&#34;: trajectory_option,
                        &#34;centers_option&#34;: centers_option,
                        &#34;trajectory_files&#34;: trajectory_files,
                        &#34;trajectory_folder&#34;: trajectory_folder,
                        &#34;centers_files&#34;: centers_files,
                        &#34;centers_folder&#34;: centers_folder,
                        &#34;junction_cx&#34;: st.session_state.get(&#34;assign_junction_cx&#34;, 0.0),
                        &#34;junction_cz&#34;: st.session_state.get(&#34;assign_junction_cz&#34;, 0.0),
                        &#34;junction_r&#34;: st.session_state.get(&#34;assign_junction_r&#34;, 50.0)
                    }

                self.run_analysis(analysis_type, decision_mode, cluster_method, seed, cluster_params, decision_params, assign_params, discover_decision_mode)

            if st.session_state.analysis_results:
                st.markdown(&#34;### Analysis Results&#34;)
                st.success(&#34;✅ Analysis completed successfully!&#34;)


                # Show summary based on analysis type
                if analysis_type == &#34;discover&#34;:
                    if &#34;branches&#34; in st.session_state.analysis_results:
                        st.write(f&#34;**Branches discovered:** {len(st.session_state.analysis_results[&#39;branches&#39;])}&#34;)

                elif analysis_type == &#34;assign&#34;:
                    if &#34;assignments&#34; in st.session_state.analysis_results:
                        st.write(f&#34;**Trajectories assigned:** {len(st.session_state.analysis_results[&#39;assignments&#39;])}&#34;)

                        # Show debug information if available
                        if &#34;assign_debug_info&#34; in st.session_state and st.session_state.assign_debug_info:
                            st.markdown(&#34;### 🔍 Debug Information&#34;)
                            for junction_key, debug_info in st.session_state.assign_debug_info.items():
                                with st.expander(f&#34;Debug Info for {junction_key}&#34;, expanded=False):
                                    st.write(&#34;**Junction Parameters:**&#34;)
                                    st.write(f&#34;- Center: {debug_info[&#39;junction_params&#39;][&#39;center&#39;]}&#34;)
                                    st.write(f&#34;- Radius: {debug_info[&#39;junction_params&#39;][&#39;radius&#39;]}&#34;)
                                    st.write(f&#34;- R_outer: {debug_info[&#39;junction_params&#39;][&#39;r_outer&#39;]}&#34;)

                                    st.write(&#34;**Assignment Parameters:**&#34;)
                                    st.write(f&#34;- Path length: {debug_info[&#39;assignment_params&#39;][&#39;path_length&#39;]}&#34;)
                                    st.write(f&#34;- Epsilon: {debug_info[&#39;assignment_params&#39;][&#39;epsilon&#39;]}&#34;)

                                    st.write(&#34;**Data Info:**&#34;)
                                    st.write(f&#34;- Centers shape: {debug_info[&#39;data_info&#39;][&#39;centers_shape&#39;]}&#34;)
                                    st.write(f&#34;- Trajectories: {debug_info[&#39;data_info&#39;][&#39;trajectories&#39;]}&#34;)

                                    st.write(&#34;**Assignment Distribution:**&#34;)
                                    total_trajectories = sum(debug_info[&#39;assignment_distribution&#39;].values())
                                    for branch, count in debug_info[&#39;assignment_distribution&#39;].items():
                                        percentage = (count / total_trajectories) * 100
                                        st.write(f&#34;- Branch {branch}: {count} trajectories ({percentage:.1f}%)&#34;)

                                    # Add troubleshooting info if most trajectories are -2/-1
                                    neg2_count = debug_info[&#39;assignment_distribution&#39;].get(-2, 0)
                                    neg1_count = debug_info[&#39;assignment_distribution&#39;].get(-1, 0)

                                    if (neg2_count + neg1_count) / total_trajectories &gt; 0.8:  # More than 80% are -2/-1
                                        st.warning(&#34;⚠️ **Troubleshooting:** Most trajectories are getting -2/-1 assignments!&#34;)
                                        st.write(&#34;**Possible solutions:**&#34;)
                                        st.write(&#34;1. **Increase junction radius** - Current radius might be too small&#34;)
                                        st.write(&#34;2. **Adjust junction center** - Center might not match trajectory paths&#34;)
                                        st.write(&#34;3. **Check trajectory data** - Ensure trajectories actually pass through junction area&#34;)
                                        st.write(&#34;4. **Use manual junction parameters** - Try different center coordinates and radius&#34;)

                                    st.write(&#34;**First 10 Assignments:**&#34;)
                                    if debug_info[&#39;assignments_sample&#39;]:
                                        st.dataframe(pd.DataFrame(debug_info[&#39;assignments_sample&#39;]), width=&#39;stretch&#39;)

                elif analysis_type == &#34;metrics&#34;:
                    if &#34;metrics&#34; in st.session_state.analysis_results:
                        st.write(f&#34;**Metrics computed:** {len(st.session_state.analysis_results[&#39;metrics&#39;])}&#34;)

                        # Show debug information for metrics
                        st.markdown(&#34;---&#34;)
                        st.markdown(&#34;### 🔍 Debug Information&#34;)

                        # Get debug information from the first few trajectories
                        if st.session_state.trajectories:
                            st.write(&#34;**Debug Status:**&#34;)
                            st.write(f&#34;- Total trajectories: {len(st.session_state.trajectories)}&#34;)

                            # Sample first 5 trajectories for debug
                            time_data_debug = []
                            for i, traj in enumerate(st.session_state.trajectories[:5]):
                                time_debug = {
                                    &#34;trajectory_id&#34;: i,
                                    &#34;time_data_type&#34;: str(type(traj.t)),
                                    &#34;time_data_shape&#34;: traj.t.shape if traj.t is not None else None,
                                    &#34;time_data_sample&#34;: traj.t[:3].tolist() if traj.t is not None and len(traj.t) &gt; 0 else None,
                                    &#34;time_data_dtype&#34;: str(traj.t.dtype) if traj.t is not None else None,
                                    &#34;time_is_none&#34;: traj.t is None,
                                    &#34;time_length&#34;: len(traj.t) if traj.t is not None else 0,
                                    # Add position data diagnostics
                                    &#34;x_data_type&#34;: str(type(traj.x)),
                                    &#34;x_data_shape&#34;: traj.x.shape if traj.x is not None else None,
                                    &#34;x_data_sample&#34;: traj.x[:3].tolist() if traj.x is not None and len(traj.x) &gt; 0 else None,
                                    &#34;x_data_dtype&#34;: str(traj.x.dtype) if traj.x is not None else None,
                                    &#34;x_is_none&#34;: traj.x is None,
                                    &#34;x_length&#34;: len(traj.x) if traj.x is not None else 0,
                                    &#34;z_data_type&#34;: str(type(traj.z)),
                                    &#34;z_data_shape&#34;: traj.z.shape if traj.z is not None else None,
                                    &#34;z_data_sample&#34;: traj.z[:3].tolist() if traj.z is not None and len(traj.z) &gt; 0 else None,
                                    &#34;z_data_dtype&#34;: str(traj.z.dtype) if traj.z is not None else None,
                                    &#34;z_is_none&#34;: traj.z is None,
                                    &#34;z_length&#34;: len(traj.z) if traj.z is not None else 0
                                }
                                time_data_debug.append(time_debug)

                            st.write(f&#34;- time_data_debug length: {len(time_data_debug)}&#34;)

                            if time_data_debug:
                                with st.expander(&#34;🔍 Trajectory Data Debug Information&#34;, expanded=True):
                                    st.write(&#34;**First 5 trajectories data analysis:**&#34;)
                                    for debug_info in time_data_debug:
                                        st.write(f&#34;**Trajectory {debug_info[&#39;trajectory_id&#39;]}:**&#34;)

                                        # Time data
                                        st.write(&#34;**Time Data:**&#34;)
                                        st.write(f&#34;- Is None: {debug_info[&#39;time_is_none&#39;]}&#34;)
                                        st.write(f&#34;- Length: {debug_info[&#39;time_length&#39;]}&#34;)
                                        st.write(f&#34;- Type: {debug_info[&#39;time_data_type&#39;]}&#34;)
                                        st.write(f&#34;- Shape: {debug_info[&#39;time_data_shape&#39;]}&#34;)
                                        st.write(f&#34;- Dtype: {debug_info[&#39;time_data_dtype&#39;]}&#34;)
                                        st.write(f&#34;- Sample: {debug_info[&#39;time_data_sample&#39;]}&#34;)

                                        # Position data
                                        st.write(&#34;**Position Data (X):**&#34;)
                                        st.write(f&#34;- Is None: {debug_info[&#39;x_is_none&#39;]}&#34;)
                                        st.write(f&#34;- Length: {debug_info[&#39;x_length&#39;]}&#34;)
                                        st.write(f&#34;- Type: {debug_info[&#39;x_data_type&#39;]}&#34;)
                                        st.write(f&#34;- Shape: {debug_info[&#39;x_data_shape&#39;]}&#34;)
                                        st.write(f&#34;- Dtype: {debug_info[&#39;x_data_dtype&#39;]}&#34;)
                                        st.write(f&#34;- Sample: {debug_info[&#39;x_data_sample&#39;]}&#34;)

                                        st.write(&#34;**Position Data (Z):**&#34;)
                                        st.write(f&#34;- Is None: {debug_info[&#39;z_is_none&#39;]}&#34;)
                                        st.write(f&#34;- Length: {debug_info[&#39;z_length&#39;]}&#34;)
                                        st.write(f&#34;- Type: {debug_info[&#39;z_data_type&#39;]}&#34;)
                                        st.write(f&#34;- Shape: {debug_info[&#39;z_data_shape&#39;]}&#34;)
                                        st.write(f&#34;- Dtype: {debug_info[&#39;z_data_dtype&#39;]}&#34;)
                                        st.write(f&#34;- Sample: {debug_info[&#39;z_data_sample&#39;]}&#34;)

                                        st.write(&#34;---&#34;)
                            else:
                                st.info(&#34;No time data debug information available&#34;)

                elif analysis_type == &#34;gaze&#34;:
                    if &#34;gaze_results&#34; in st.session_state.analysis_results:
                        st.write(f&#34;**Gaze analysis completed**&#34;)

                elif analysis_type == &#34;predict&#34;:
                    if &#34;choice_patterns&#34; in st.session_state.analysis_results:
                        st.write(f&#34;**Choice patterns analyzed**&#34;)

    def run_analysis(self, analysis_type: str, decision_mode: str, cluster_method: str, seed: int, cluster_params: dict = None, decision_params: dict = None, assign_params: dict = None, discover_decision_mode: str = &#34;hybrid&#34;):
        &#34;&#34;&#34;Run the selected analysis&#34;&#34;&#34;
        try:
            with st.spinner(f&#34;Running {analysis_type} analysis...&#34;):

                if analysis_type == &#34;discover&#34;:
                    # Unified multi-junction discovery for consistent decisions and assignments
                    import os
                    output_dir = &#34;gui_outputs&#34;
                    os.makedirs(output_dir, exist_ok=True)

                    # Cluster/decision parameters
                    k_value = cluster_params.get(&#34;k&#34;, 3) if cluster_params else 3
                    min_samples = cluster_params.get(&#34;min_samples&#34;, 5) if cluster_params else 5
                    k_min = cluster_params.get(&#34;k_min&#34;, 2) if cluster_params else 2
                    k_max = cluster_params.get(&#34;k_max&#34;, 6) if cluster_params else 6
                    min_sep_deg = cluster_params.get(&#34;min_sep_deg&#34;, 12.0) if cluster_params else 12.0
                    angle_eps = cluster_params.get(&#34;angle_eps&#34;, 15.0) if cluster_params else 15.0

                    path_length = decision_params.get(&#34;path_length&#34;, 100.0) if decision_params else 100.0
                    epsilon = decision_params.get(&#34;epsilon&#34;, 0.05) if decision_params else 0.05
                    linger_delta = decision_params.get(&#34;linger_delta&#34;, 5.0) if decision_params else 5.0
                    r_outer_list = [st.session_state.junction_r_outer.get(i, 50.0) for i in range(len(st.session_state.junctions))]

                    # Run consolidated discovery
                    chain_df, centers_list, decisions_chain_df = discover_decision_chain(
                        trajectories=st.session_state.trajectories,
                        junctions=st.session_state.junctions,
                        path_length=path_length,
                        epsilon=epsilon,
                        seed=seed,
                        decision_mode=discover_decision_mode,
                        r_outer_list=r_outer_list,
                        linger_delta=linger_delta,
                        out_dir=output_dir,
                        cluster_method=cluster_method,
                        k=k_value,
                        k_min=k_min,
                        k_max=k_max,
                        min_sep_deg=min_sep_deg,
                        angle_eps=angle_eps,
                        min_samples=min_samples,
                    )

                    # Build per-junction results view from chain_df/centers_list
                    results = {}
                    for i, junction in enumerate(st.session_state.junctions):
                        junction_key = f&#34;junction_{i}&#34;
                        col = f&#34;branch_j{i}&#34;
                        if col in chain_df.columns:
                            df_i = chain_df[[&#34;trajectory&#34;, col]].copy()
                            df_i = df_i.rename(columns={col: &#34;branch&#34;})
                            # summary counts for main branches (&gt;=0)
                            vc = df_i[df_i[&#34;branch&#34;] &gt;= 0][&#34;branch&#34;].value_counts().sort_index()
                            summary_i = vc.rename_axis(&#34;branch&#34;).to_frame(&#34;count&#34;).reset_index()
                            total_i = int(summary_i[&#34;count&#34;].sum()) if len(summary_i) else 0
                            summary_i[&#34;percent&#34;] = summary_i[&#34;count&#34;] / max(1, total_i) * 100.0
                        else:
                            import pandas as _pd
                            df_i = _pd.DataFrame(columns=[&#34;trajectory&#34;,&#34;branch&#34;])  # empty
                            summary_i = _pd.DataFrame(columns=[&#34;branch&#34;,&#34;count&#34;,&#34;percent&#34;])  # empty

                        results[junction_key] = {
                            &#34;assignments&#34;: df_i,
                            &#34;summary&#34;: summary_i,
                            &#34;centers&#34;: centers_list[i] if i &lt; len(centers_list) else None,
                            &#34;junction&#34;: junction,
                            &#34;r_outer&#34;: r_outer_list[i] if i &lt; len(r_outer_list) else None,
                            &#34;path_length&#34;: path_length,
                            &#34;epsilon&#34;: epsilon,
                            &#34;linger_delta&#34;: linger_delta,  # Store linger_delta for gaze analysis
                            &#34;decision_mode&#34;: discover_decision_mode,
                            &#34;scale&#34;: st.session_state.get(&#34;scale_factor&#34;, 1.0),
                        }

                    # Flow graph generation removed - discover should only do discovery, not flow analysis

                    # Persist results and decisions for gaze reuse
                    if st.session_state.analysis_results is None:
                        st.session_state.analysis_results = {}
                    st.session_state.analysis_results[&#34;branches&#34;] = results

                    # Debug: Check chain_decisions DataFrame
                    st.write(f&#34;🔍 **Chain Decisions Debug:**&#34;)
                    st.write(f&#34;- decisions_chain_df is not None: {decisions_chain_df is not None}&#34;)
                    if decisions_chain_df is not None:
                        st.write(f&#34;- decisions_chain_df length: {len(decisions_chain_df)}&#34;)
                        st.write(f&#34;- decisions_chain_df columns: {list(decisions_chain_df.columns)}&#34;)
                        if not decisions_chain_df.empty:
                            st.write(f&#34;- Junction indices in decisions_chain_df: {sorted(decisions_chain_df[&#39;junction_index&#39;].unique())}&#34;)
                        else:
                            st.write(&#34;- decisions_chain_df is empty!&#34;)
                    else:
                        st.write(&#34;- decisions_chain_df is None!&#34;)

                    # Store branch assignments (chain_df) as chain_decisions for gaze analysis
                    if chain_df is not None and len(chain_df) &gt; 0:
                        st.session_state.analysis_results.setdefault(&#34;branches&#34;, {})
                        st.session_state.analysis_results[&#34;branches&#34;][&#34;chain_decisions&#34;] = chain_df
                        st.write(f&#34;✅ **Stored branch assignments (chain_df) with {len(chain_df)} rows in session state**&#34;)
                        st.write(f&#34;🔍 **Branch assignment columns:** {list(chain_df.columns)}&#34;)

                        # Debug: Check for branch_jX columns specifically
                        branch_cols = [col for col in chain_df.columns if col.startswith(&#39;branch_j&#39;)]
                        st.write(f&#34;🔍 **Branch columns found:** {branch_cols}&#34;)
                        if len(branch_cols) &gt; 0:
                            st.write(f&#34;🔍 **Sample branch data:** {chain_df[branch_cols].head()}&#34;)
                        else:
                            st.error(&#34;❌ **No branch_jX columns found in chain_df!**&#34;)
                    else:
                        st.write(f&#34;❌ **Not storing branch assignments - chain_df is None or empty**&#34;)

                    # Also store decision points separately for reference
                    if decisions_chain_df is not None and len(decisions_chain_df) &gt; 0:
                        st.session_state.analysis_results.setdefault(&#34;branches&#34;, {})
                        st.session_state.analysis_results[&#34;branches&#34;][&#34;decision_points&#34;] = decisions_chain_df
                        st.write(f&#34;✅ **Stored decision points with {len(decisions_chain_df)} rows in session state**&#34;)
                    else:
                        st.write(f&#34;❌ **Not storing decision points - DataFrame is None or empty**&#34;)

                    # Add debugging information for flow analysis
                    try:
                        st.markdown(&#34;#### 🔍 Flow Analysis Debug&#34;)

                        # Count trajectories that visit multiple junctions
                        multi_junction_trajectories = 0
                        junction_visit_counts = {}

                        for i, junction in enumerate(st.session_state.junctions):
                            junction_key = f&#34;junction_{i}&#34;
                            if junction_key in results and &#34;assignments&#34; in results[junction_key]:
                                assignments = results[junction_key][&#34;assignments&#34;]
                                if not assignments.empty:
                                    visited_trajectories = set(assignments[&#34;trajectory&#34;].unique())
                                    junction_visit_counts[i] = visited_trajectories

                        # Find trajectories that visit multiple junctions
                        all_trajectories = set()
                        for trajectories in junction_visit_counts.values():
                            all_trajectories.update(trajectories)

                        for traj_id in all_trajectories:
                            visited_junctions = [i for i, trajs in junction_visit_counts.items() if traj_id in trajs]
                            if len(visited_junctions) &gt; 1:
                                multi_junction_trajectories += 1

                        st.info(f&#34;📊 **Flow Analysis Summary:**&#34;)
                        st.write(f&#34;- Total trajectories: {len(st.session_state.trajectories)}&#34;)
                        st.write(f&#34;- Trajectories visiting multiple junctions: {multi_junction_trajectories}&#34;)
                        st.write(f&#34;- Junction visit counts: {[len(trajs) for trajs in junction_visit_counts.values()]}&#34;)

                        if multi_junction_trajectories == 0:
                            st.warning(&#34;⚠️ **No trajectories visit multiple junctions!** This explains the zero flow matrix.&#34;)
                            st.write(&#34;**Possible causes:**&#34;)
                            st.write(&#34;1. Trajectories are too short to reach multiple junctions&#34;)
                            st.write(&#34;2. Junction r_outer values are too small&#34;)
                            st.write(&#34;3. Junctions are too far apart&#34;)
                            st.write(&#34;4. Trajectory data needs different scaling&#34;)

                    except Exception as e:
                        st.warning(f&#34;Debug analysis failed: {str(e)}&#34;)

                    #st.success(f&#34;✅ Discover analysis completed successfully for all {len(st.session_state.junctions)} junctions!&#34;)
                    self.generate_cli_command(&#34;discover&#34;, results, cluster_method, cluster_params, decision_mode, decision_params)

                elif analysis_type == &#34;assign&#34;:
                    # Assign trajectories to branches using simplified interface
                    import numpy as np

                    # Load trajectories based on trajectory option
                    trajectories = self.load_assign_trajectories(assign_params)
                    if trajectories is None:
                        return

                    # Load centers based on centers option
                    centers_dict = self.load_assign_centers(assign_params)
                    if centers_dict is None:
                        return

                    # Validate that we have compatible data
                    if not trajectories:
                        st.error(&#34;❌ No trajectories loaded for assignment&#34;)
                        return

                    if not centers_dict:
                        st.error(&#34;❌ No junction centers loaded for assignment&#34;)
                        return

                    # Get assignment parameters
                    path_length = assign_params.get(&#34;path_length&#34;, 100.0) if assign_params else 100.0
                    epsilon = assign_params.get(&#34;epsilon&#34;, 0.05) if assign_params else 0.05

                    results = {}
                    successful_assignments = 0

                    # Process each junction
                    for junction_key, centers in centers_dict.items():
                        try:
                            # Extract junction number from key (e.g., &#34;junction_0&#34; -&gt; 0)
                            junction_num = int(junction_key.split(&#39;_&#39;)[1])

                            # Get assignment parameters - use stored values from discover analysis if available
                            centers_option = assign_params.get(&#34;centers_option&#34;, &#34;Use session centers&#34;) if assign_params else &#34;Use session centers&#34;
                            if centers_option == &#34;Use session centers&#34;:
                                if junction_key in st.session_state.analysis_results[&#34;branches&#34;]:
                                    branch_data = st.session_state.analysis_results[&#34;branches&#34;][junction_key]
                                    # Use stored parameters from discover analysis
                                    stored_path_length = branch_data.get(&#34;path_length&#34;, path_length)
                                    stored_epsilon = branch_data.get(&#34;epsilon&#34;, epsilon)
                                    stored_scale = branch_data.get(&#34;scale&#34;, 1.0)
                                    st.info(f&#34;📊 Using assignment parameters from discover analysis: path_length={stored_path_length:.1f}, epsilon={stored_epsilon:.3f}, scale={stored_scale:.1f}&#34;)
                                else:
                                    stored_path_length = path_length
                                    stored_epsilon = epsilon
                                    stored_scale = 1.0
                            else:
                                stored_path_length = path_length
                                stored_epsilon = epsilon
                                stored_scale = 1.0

                            # Get junction and r_outer - prioritize stored parameters from discover analysis
                            if centers_option == &#34;Use session centers&#34;:
                                # Try to get junction parameters from discover analysis first
                                if junction_key in st.session_state.analysis_results[&#34;branches&#34;]:
                                    branch_data = st.session_state.analysis_results[&#34;branches&#34;][junction_key]
                                    if &#34;junction&#34; in branch_data and &#34;r_outer&#34; in branch_data:
                                        junction = branch_data[&#34;junction&#34;]
                                        r_outer = branch_data[&#34;r_outer&#34;]
                                        stored_scale = branch_data.get(&#34;scale&#34;, 1.0)
                                        st.info(f&#34;📊 Using junction parameters from discover analysis: center=({junction.cx:.1f}, {junction.cz:.1f}), radius={junction.r:.1f}, r_outer={r_outer:.1f}&#34;)
                                        st.info(f&#34;📊 Scale factor from discover analysis: {stored_scale:.1f}&#34;)

                                        # Check if current trajectories use different scale factor
                                        if hasattr(st.session_state, &#39;trajectories&#39;) and st.session_state.trajectories:
                                            # Estimate scale factor from trajectory coordinates
                                            sample_traj = st.session_state.trajectories[0]
                                            if hasattr(sample_traj, &#39;x&#39;) and len(sample_traj.x) &gt; 0:
                                                # Simple heuristic: if coordinates are much larger than expected, scale might be different
                                                max_coord = max(abs(sample_traj.x.max()), abs(sample_traj.z.max()))
                                                if max_coord &gt; 1000 and stored_scale &lt; 0.5:
                                                    st.warning(f&#34;⚠️ Scale factor mismatch detected! Discover used {stored_scale:.1f}, but current trajectories appear to use a different scale.&#34;)
                                                    st.warning(f&#34;⚠️ This may cause assignment failures. Consider using the same scale factor as discover analysis.&#34;)
                                    else:
                                        # Fallback to session state junctions
                                        if junction_num &lt; len(st.session_state.junctions):
                                            junction = st.session_state.junctions[junction_num]
                                            r_outer = st.session_state.junction_r_outer.get(junction_num, 50.0)
                                        else:
                                            st.error(f&#34;❌ No junction parameters found for {junction_key}&#34;)
                                            continue
                                else:
                                    st.error(f&#34;❌ No discover analysis data found for {junction_key}&#34;)
                                    continue
                            elif junction_num &lt; len(st.session_state.junctions):
                                junction = st.session_state.junctions[junction_num]
                                r_outer = st.session_state.junction_r_outer.get(junction_num, 50.0)
                            else:
                                # If using external data, use manual parameters or estimate from trajectory data
                                manual_cx = assign_params.get(&#34;junction_cx&#34;, 0.0)
                                manual_cz = assign_params.get(&#34;junction_cz&#34;, 0.0)
                                manual_r = assign_params.get(&#34;junction_r&#34;, 50.0)

                                if manual_cx != 0.0 or manual_cz != 0.0 or manual_r != 50.0:
                                    # Use manual parameters
                                    junction = Circle(cx=manual_cx, cz=manual_cz, r=manual_r)
                                    r_outer = manual_r * 2.0
                                    st.info(f&#34;📊 Using manual junction: center=({manual_cx:.1f}, {manual_cz:.1f}), radius={manual_r:.1f}&#34;)
                                else:
                                    # Estimate junction from trajectory data
                                    st.warning(f&#34;⚠️ No junction defined for {junction_key}. Attempting to estimate from trajectory data...&#34;)

                                    # Estimate junction center from trajectory data
                                    all_x = np.concatenate([tr.x for tr in trajectories])
                                    all_z = np.concatenate([tr.z for tr in trajectories])

                                    # Show trajectory data range for debugging
                                    st.info(f&#34;📊 Trajectory data range:&#34;)
                                    st.write(f&#34;- X range: {np.min(all_x):.1f} to {np.max(all_x):.1f}&#34;)
                                    st.write(f&#34;- Z range: {np.min(all_z):.1f} to {np.max(all_z):.1f}&#34;)

                                    # Use median as center (more robust than mean)
                                    estimated_cx = float(np.median(all_x))
                                    estimated_cz = float(np.median(all_z))

                                    # Estimate radius based on data spread - use a more conservative approach
                                    distances = np.sqrt((all_x - estimated_cx)**2 + (all_z - estimated_cz)**2)
                                    estimated_r = float(np.percentile(distances, 75))  # Use 75th percentile for radius

                                    junction = Circle(cx=estimated_cx, cz=estimated_cz, r=max(estimated_r, 20.0))
                                    r_outer = estimated_r * 3.0  # Make r_outer much larger than junction radius

                                    st.info(f&#34;📊 Estimated junction: center=({estimated_cx:.1f}, {estimated_cz:.1f}), radius={estimated_r:.1f}&#34;)
                                    st.warning(f&#34;⚠️ Using estimated junction for {junction_key}. Consider defining junctions manually for better results.&#34;)
                                    st.info(f&#34;💡 Tip: If trajectories still get -2/-1, try increasing the junction radius or adjusting the center coordinates.&#34;)

                            # Create output directory for this junction
                            import os
                            out_dir = os.path.join(&#34;gui_outputs&#34;, f&#34;junction_{junction_num}&#34;)
                            os.makedirs(out_dir, exist_ok=True)

                            # Run assignment
                            # Determine decision parameters to use
                            dm = assign_params.get(&#34;decision_mode&#34;, &#34;pathlen&#34;)
                            ld = assign_params.get(&#34;linger_delta&#34;, 0.0)
                            dm_r_outer = assign_params.get(&#34;r_outer&#34;, r_outer)

                            assignments = assign_branches(
                                trajectories=trajectories,
                                centers=centers,
                                junction=junction,
                                path_length=stored_path_length,
                                epsilon=stored_epsilon,
                                decision_mode=dm,
                                r_outer=dm_r_outer,
                                linger_delta=ld,
                                out_dir=out_dir
                            )

                            # Optional: auto-rediscover if outlier cluster among new assignments is large enough
                            try:
                                auto_flag = st.session_state.get(&#34;assign_auto_rediscover&#34;, False)
                                min_samples_new = int(st.session_state.get(&#34;assign_auto_min_samples&#34;, 5))
                                angle_eps_new = float(st.session_state.get(&#34;assign_auto_angle_eps&#34;, 15.0))
                                # Auto-rediscover: detect dense outlier regions among newly uploaded trajectories
                                if auto_flag and len(assignments) &gt; 0:
                                    # Identify outliers (-1) that entered junction and have usable vectors
                                    from verta.verta_decisions import compute_assignment_vectors
                                    from verta.verta_clustering import cluster_angles_dbscan
                                    # Compute vectors for these trajectories with same decision params
                                    vec_df = compute_assignment_vectors(
                                        trajectories=trajectories,
                                        junction=junction,
                                        path_length=stored_path_length,
                                        decision_mode=dm,
                                        r_outer=dm_r_outer,
                                        epsilon=stored_epsilon,
                                    )
                                    # Merge to filter to current outliers only
                                    outlier_ids = set(assignments[assignments[&#34;branch&#34;] == -1][&#34;trajectory&#34;].tolist())
                                    if outlier_ids:
                                        use = vec_df[(vec_df[&#34;trajectory&#34;].isin(outlier_ids)) &amp; (vec_df[&#34;entered&#34;]) &amp; (vec_df[&#34;usable&#34;])].copy()
                                        if len(use) &gt;= min_samples_new:
                                            V = np.vstack([use[[&#34;vx&#34;,&#34;vz&#34;]].to_numpy()]) if len(use) else np.zeros((0,2))
                                            if V.size:
                                                labels_o, centers_o = cluster_angles_dbscan(V, eps_deg=angle_eps_new, min_samples=min_samples_new)
                                                # If any valid cluster exists (label &gt;=0), trigger rediscovery using all available trajectories
                                                if (labels_o &gt;= 0).any():
                                                    st.info(f&#34;🔄 Auto-rediscover triggered for {junction_key}: detected dense outlier region (min_samples={min_samples_new}, angle_eps={angle_eps_new}°)&#34;)
                                                    # Build combined trajectory set: existing session trajectories + newly provided
                                                    all_trajs = []
                                                    if hasattr(st.session_state, &#39;trajectories&#39;) and st.session_state.trajectories:
                                                        all_trajs.extend(st.session_state.trajectories)
                                                    all_trajs.extend([t for t in trajectories if t not in all_trajs])
                                                    # Rerun discovery for this junction
                                                    from verta.verta_decisions import discover_branches
                                                    new_assign, _sum, new_centers = discover_branches(
                                                        trajectories=all_trajs,
                                                        junction=junction,
                                                        k=centers.shape[0] if centers is not None and centers.size else 3,
                                                        path_length=stored_path_length,
                                                        epsilon=stored_epsilon,
                                                        seed=seed,
                                                        decision_mode=dm,
                                                        r_outer=dm_r_outer,
                                                        out_dir=out_dir,
                                                        cluster_method=cluster_method,
                                                        k_min=st.session_state.get(&#34;discover_k_min&#34;, 2),
                                                        k_max=st.session_state.get(&#34;discover_k_max&#34;, 6),
                                                        min_sep_deg=st.session_state.get(&#34;discover_min_sep_deg&#34;, 12.0),
                                                        angle_eps=st.session_state.get(&#34;discover_angle_eps&#34;, 15.0),
                                                        min_samples=min_samples_new,
                                                        junction_number=junction_num,
                                                        all_junctions=[junction]
                                                    )
                                                    # Update centers to the rediscovered ones and reassign current trajectories for display
                                                    centers = new_centers
                                                    assignments = assign_branches(
                                                        trajectories=trajectories,
                                                        centers=centers,
                                                        junction=junction,
                                                        path_length=stored_path_length,
                                                        epsilon=stored_epsilon,
                                                        decision_mode=dm,
                                                        r_outer=dm_r_outer,
                                                        linger_delta=ld,
                                                        out_dir=out_dir
                                                    )
                                                    st.warning(&#34;⚠️ Branch IDs may have been renumbered due to rediscovery.&#34;)
                            except Exception as _e:
                                # Keep assignment results even if auto-rediscover path fails
                                pass

                            # Enhanced debugging for assignment issues
                            if assignments is not None and len(assignments) &gt; 0:
                                # Check if assignments is a string (error message) or pandas DataFrame
                                if isinstance(assignments, str):
                                    st.error(f&#34;🚨 **Assignment Error:** {assignments}&#34;)
                                    st.error(&#34;This indicates the assign_branches function returned an error message instead of assignment results.&#34;)
                                    st.error(&#34;Check the assign_branches function implementation or input parameters.&#34;)
                                elif hasattr(assignments, &#39;iterrows&#39;):  # pandas DataFrame
                                    # Count assignment types from DataFrame
                                    assignment_counts = assignments[&#39;branch&#39;].value_counts().to_dict()

                                    total_trajectories = len(assignments)
                                    neg2_count = assignment_counts.get(-2, 0)
                                    neg1_count = assignment_counts.get(-1, 0)

                                    # If most trajectories are -2/-1, provide enhanced debugging
                                    if (neg2_count + neg1_count) / total_trajectories &gt; 0.8:
                                        st.error(f&#34;🚨 **Assignment Issue Detected for {junction_key}:**&#34;)
                                        st.error(f&#34;   -2 (never entered): {neg2_count} trajectories ({neg2_count/total_trajectories*100:.1f}%)&#34;)
                                        st.error(f&#34;   -1 (no usable vector): {neg1_count} trajectories ({neg1_count/total_trajectories*100:.1f}%)&#34;)

                                        # Enhanced debugging analysis
                                        st.info(f&#34;🔍 **Enhanced Debug Analysis:**&#34;)

                                        # Show trajectory data ranges
                                        all_x = []
                                        all_z = []
                                        for traj in trajectories:
                                            all_x.extend(traj.x)
                                            all_z.extend(traj.z)

                                        if all_x and all_z:
                                            st.info(f&#34;📊 **Trajectory Data Ranges:**&#34;)
                                            st.info(f&#34;   X: {min(all_x):.1f} to {max(all_x):.1f} (range: {max(all_x)-min(all_x):.1f})&#34;)
                                            st.info(f&#34;   Z: {min(all_z):.1f} to {max(all_z):.1f} (range: {max(all_z)-min(all_z):.1f})&#34;)
                                            st.info(f&#34;   Total points: {len(all_x)}&#34;)

                                            # Show how many trajectories actually pass through the junction area
                                            trajectories_in_junction = 0
                                            trajectories_with_usable_vectors = 0

                                            for traj in trajectories:
                                                # Check if trajectory passes through junction area
                                                distances = np.sqrt((traj.x - junction.cx)**2 + (traj.z - junction.cz)**2)
                                                if np.any(distances &lt;= junction.r):
                                                    trajectories_in_junction += 1

                                                    # Check if trajectory has usable vectors (length &gt; epsilon)
                                                    if len(traj.x) &gt; 1:
                                                        dx = np.diff(traj.x)
                                                        dz = np.diff(traj.z)
                                                        movement = np.sqrt(dx**2 + dz**2)
                                                        if np.any(movement &gt; stored_epsilon):
                                                            trajectories_with_usable_vectors += 1

                                            st.info(f&#34;📊 **Junction Analysis:**&#34;)
                                            st.info(f&#34;   Trajectories passing through junction: {trajectories_in_junction}/{len(trajectories)}&#34;)
                                            st.info(f&#34;   Trajectories with usable vectors: {trajectories_with_usable_vectors}/{len(trajectories)}&#34;)

                                            # Analyze movement patterns for -1 assignments
                                            if neg1_count &gt; neg2_count:  # More -1 than -2 assignments
                                                st.error(f&#34;🚨 **Critical Issue: Most trajectories are -1 (entered junction but no usable vectors)!**&#34;)

                                                # Analyze movement patterns
                                                st.info(f&#34;🔍 **Movement Analysis:**&#34;)
                                                all_movements = []
                                                nan_trajectories = 0
                                                for traj in trajectories:
                                                    if len(traj.x) &gt; 1:
                                                        # Check for NaN values
                                                        if np.any(np.isnan(traj.x)) or np.any(np.isnan(traj.z)):
                                                            nan_trajectories += 1
                                                            continue

                                                        dx = np.diff(traj.x)
                                                        dz = np.diff(traj.z)
                                                        movement = np.sqrt(dx**2 + dz**2)
                                                        # Filter out NaN movements
                                                        valid_movements = movement[~np.isnan(movement)]
                                                        all_movements.extend(valid_movements)

                                                if nan_trajectories &gt; 0:
                                                    st.error(f&#34;🚨 **CRITICAL: {nan_trajectories} trajectories contain NaN coordinates!**&#34;)
                                                    st.error(&#34;This will cause assignment failures. Check your trajectory data for missing/invalid coordinates.&#34;)

                                                if all_movements:
                                                    percentile_5 = np.percentile(all_movements, 5)
                                                    percentile_10 = np.percentile(all_movements, 10)
                                                    percentile_25 = np.percentile(all_movements, 25)
                                                    mean_movement = np.mean(all_movements)

                                                    st.info(f&#34;📊 **Movement Statistics:**&#34;)
                                                    st.info(f&#34;   Mean movement: {mean_movement:.4f}&#34;)
                                                    st.info(f&#34;   5th percentile: {percentile_5:.4f}&#34;)
                                                    st.info(f&#34;   10th percentile: {percentile_10:.4f}&#34;)
                                                    st.info(f&#34;   25th percentile: {percentile_25:.4f}&#34;)
                                                    st.info(f&#34;   Current epsilon: {stored_epsilon:.3f}&#34;)

                                                    # Suggest epsilon adjustment
                                                    if stored_epsilon &gt; percentile_25:
                                                        suggested_epsilon = percentile_10
                                                        st.warning(f&#34;⚠️ **Epsilon too high!** Try: {suggested_epsilon:.4f} (current: {stored_epsilon:.3f})&#34;)
                                                    elif stored_epsilon &lt; percentile_5:
                                                        suggested_epsilon = percentile_10
                                                        st.warning(f&#34;⚠️ **Epsilon too low!** Try: {suggested_epsilon:.4f} (current: {stored_epsilon:.3f})&#34;)
                                                    else:
                                                        suggested_epsilon = percentile_5
                                                        st.warning(f&#34;⚠️ **Try lower epsilon:** {suggested_epsilon:.4f} (current: {stored_epsilon:.3f})&#34;)
                                                else:
                                                    st.error(f&#34;🚨 **NO VALID MOVEMENTS FOUND!**&#34;)
                                                    st.error(&#34;All trajectories have NaN coordinates or invalid movement data.&#34;)
                                                    st.error(&#34;This explains why all trajectories get -1 assignments.&#34;)
                                                    st.error(&#34;**SOLUTION**: Check your trajectory data for missing/invalid coordinates.&#34;)

                                                    # Show sample trajectory analysis
                                                    st.info(f&#34;🔍 **Sample Trajectory Analysis (first 3):**&#34;)
                                                    for i, traj in enumerate(trajectories[:3]):
                                                        if len(traj.x) &gt; 1:
                                                            # Check for NaN values
                                                            has_nan = np.any(np.isnan(traj.x)) or np.any(np.isnan(traj.z))
                                                            if has_nan:
                                                                st.error(f&#34;   Trajectory {i}: ⚠️ CONTAINS NaN COORDINATES!&#34;)
                                                                continue

                                                            dx = np.diff(traj.x)
                                                            dz = np.diff(traj.z)
                                                            movement = np.sqrt(dx**2 + dz**2)
                                                            max_movement = np.max(movement) if len(movement) &gt; 0 else 0
                                                            mean_movement = np.mean(movement) if len(movement) &gt; 0 else 0

                                                            # Check if trajectory passes through junction
                                                            distances = np.sqrt((traj.x - junction.cx)**2 + (traj.z - junction.cz)**2)
                                                            in_junction = np.any(distances &lt;= junction.r)
                                                            min_distance = np.min(distances)

                                                            st.info(f&#34;   Trajectory {i}: max_movement={max_movement:.3f}, mean_movement={mean_movement:.3f}, in_junction={in_junction}, min_distance={min_distance:.1f}&#34;)

                                            # Suggest junction radius adjustment
                                            if trajectories_in_junction &lt; len(trajectories) * 0.5:
                                                st.warning(f&#34;⚠️ **Low junction coverage!** Only {trajectories_in_junction}/{len(trajectories)} trajectories pass through the junction area.&#34;)

                                                # Calculate suggested radius to cover more trajectories
                                                all_distances = []
                                                for traj in trajectories:
                                                    distances = np.sqrt((traj.x - junction.cx)**2 + (traj.z - junction.cz)**2)
                                                    all_distances.extend(distances)

                                                suggested_radius = np.percentile(all_distances, 80)  # Cover 80% of trajectory points
                                                st.warning(f&#34;⚠️ **Suggested radius:** {suggested_radius:.1f} (current: {junction.r:.1f})&#34;)
                                                st.warning(f&#34;⚠️ **Suggested center:** ({junction.cx:.1f}, {junction.cz:.1f}) - verify this matches your junction location&#34;)
                                else:
                                    st.warning(f&#34;⚠️ Unexpected assignment format: {type(assignments)} - {assignments}&#34;)
                                    st.warning(&#34;Expected pandas DataFrame or string, but got something else.&#34;)

                            results[junction_key] = {
                                &#34;assignments&#34;: assignments,
                                &#34;centers&#34;: centers,
                                &#34;junction&#34;: junction,
                                &#34;path_length&#34;: path_length,
                                &#34;epsilon&#34;: epsilon
                            }

                            successful_assignments += 1
                            st.success(f&#34;✅ Completed assignment for {junction_key} ({len(assignments)} trajectories)&#34;)

                            # Store debug information in session state
                            branch_counts = assignments[&#39;branch&#39;].value_counts().sort_index()
                            debug_info = {
                                &#34;junction_params&#34;: {
                                    &#34;center&#34;: f&#34;({junction.cx:.1f}, {junction.cz:.1f})&#34;,
                                    &#34;radius&#34;: f&#34;{junction.r:.1f}&#34;,
                                    &#34;r_outer&#34;: f&#34;{r_outer:.1f}&#34;
                                },
                                &#34;assignment_params&#34;: {
                                    &#34;path_length&#34;: f&#34;{path_length:.1f}&#34;,
                                    &#34;epsilon&#34;: f&#34;{epsilon:.3f}&#34;
                                },
                                &#34;data_info&#34;: {
                                    &#34;centers_shape&#34;: str(centers.shape),
                                    &#34;trajectories&#34;: len(trajectories)
                                },
                                &#34;assignment_distribution&#34;: dict(branch_counts),
                                &#34;assignments_sample&#34;: assignments.head(10).to_dict(&#39;records&#39;)
                            }

                            # Store in session state
                            if &#34;assign_debug_info&#34; not in st.session_state:
                                st.session_state.assign_debug_info = {}
                            st.session_state.assign_debug_info[junction_key] = debug_info

                        except Exception as e:
                            st.error(f&#34;❌ Assignment failed for {junction_key}: {str(e)}&#34;)
                            continue

                    # Store results - preserve existing analysis results
                    if st.session_state.analysis_results is None:
                        st.session_state.analysis_results = {}
                    st.session_state.analysis_results[&#34;assignments&#34;] = results

                    # Show summary
                    total_junctions = len(centers_dict)
                    if successful_assignments == total_junctions:
                        st.success(f&#34;✅ Assign analysis completed successfully for all {total_junctions} junctions!&#34;)
                    else:
                        st.warning(f&#34;⚠️ Assign analysis completed for {successful_assignments}/{total_junctions} junctions&#34;)

                    # Show assignment statistics
                    if results:
                        st.markdown(&#34;### Assignment Summary&#34;)
                        total_trajectories = len(trajectories)
                        st.write(f&#34;**Total trajectories processed:** {total_trajectories}&#34;)
                        st.write(f&#34;**Junctions processed:** {successful_assignments}&#34;)

                        # Show branch distribution for first junction as example
                        first_junction = list(results.keys())[0]
                        assignments_df = results[first_junction][&#34;assignments&#34;]
                        branch_counts = assignments_df[&#34;branch&#34;].value_counts().sort_index()

                        st.markdown(f&#34;**Branch distribution for {first_junction}:**&#34;)
                        for branch, count in branch_counts.items():
                            percentage = (count / len(assignments_df)) * 100
                            st.write(f&#34;- Branch {branch}: {count} trajectories ({percentage:.1f}%)&#34;)

                    # Generate CLI command for easy copying
                    self.generate_cli_command(&#34;assign&#34;, results, cluster_method, cluster_params, decision_mode, decision_params)

                elif analysis_type == &#34;metrics&#34;:
                    # Compute timing metrics
                    metrics = []
                    trajectories_with_time_data = 0
                    trajectories_without_time_data = 0
                    time_data_debug = []

                    # Get junctions from session state or estimate from data (ONCE, before the loop)
                    junctions_to_use = st.session_state.junctions if st.session_state.junctions else []

                    # If no junctions defined, estimate from trajectory data
                    if not junctions_to_use:
                        st.info(&#34;📊 No junctions defined. Estimating junctions from trajectory data...&#34;)

                        # Estimate junction from trajectory data (similar to assign analysis)
                        all_x = np.concatenate([tr.x for tr in st.session_state.trajectories])
                        all_z = np.concatenate([tr.z for tr in st.session_state.trajectories])

                        # Use median as center (more robust than mean)
                        estimated_cx = float(np.median(all_x))
                        estimated_cz = float(np.median(all_z))

                        # Estimate radius based on data spread
                        distances = np.sqrt((all_x - estimated_cx)**2 + (all_z - estimated_cz)**2)
                        estimated_r = float(np.percentile(distances, 75))  # Use 75th percentile for radius

                        # Create estimated junction
                        estimated_junction = Circle(cx=estimated_cx, cz=estimated_cz, r=max(estimated_r, 20.0))
                        junctions_to_use = [estimated_junction]

                        st.info(f&#34;📊 Estimated junction: center=({estimated_cx:.1f}, {estimated_cz:.1f}), radius={estimated_r:.1f}&#34;)

                        # Debug: Show trajectory data range
                        st.info(f&#34;📊 Trajectory data range:&#34;)
                        st.write(f&#34;- X range: {np.min(all_x):.1f} to {np.max(all_x):.1f}&#34;)
                        st.write(f&#34;- Z range: {np.min(all_z):.1f} to {np.max(all_z):.1f}&#34;)
                        st.write(f&#34;- Total trajectories: {len(st.session_state.trajectories)}&#34;)
                        st.write(f&#34;- Total coordinate points: {len(all_x)}&#34;)

                    # Debug: Count how many trajectories pass through the estimated junction
                    if junctions_to_use:
                        junction = junctions_to_use[0]
                        trajectories_through_junction = 0
                        for traj in st.session_state.trajectories:
                            dist = np.hypot(traj.x - junction.cx, traj.z - junction.cz)
                            if np.any(dist &lt;= junction.r):
                                trajectories_through_junction += 1

                        st.info(f&#34;📊 Junction Analysis:&#34;)
                        st.write(f&#34;- Trajectories passing through estimated junction: {trajectories_through_junction}/{len(st.session_state.trajectories)} ({trajectories_through_junction/len(st.session_state.trajectories)*100:.1f}%)&#34;)

                    for i, traj in enumerate(st.session_state.trajectories):
                        try:
                            # Debug time data for first few trajectories
                            if i &lt; 5:  # Debug first 5 trajectories
                                time_debug = {
                            &#34;trajectory_id&#34;: i,
                                    &#34;time_data_type&#34;: str(type(traj.t)),
                                    &#34;time_data_shape&#34;: traj.t.shape if traj.t is not None else None,
                                    &#34;time_data_sample&#34;: traj.t[:3].tolist() if traj.t is not None and len(traj.t) &gt; 0 else None,
                                    &#34;time_data_dtype&#34;: str(traj.t.dtype) if traj.t is not None else None
                                }
                                time_data_debug.append(time_debug)

                            # Compute basic trajectory metrics
                            basic_metrics = compute_basic_trajectory_metrics(traj)

                            # Track time data availability
                            if basic_metrics[&#34;total_time&#34;] &gt; 0:
                                trajectories_with_time_data += 1
                            else:
                                trajectories_without_time_data += 1

                            # Compute junction-specific timing metrics
                            junction_metrics = {}

                            # Compute timing for each junction
                            for j, junction in enumerate(junctions_to_use):
                                try:
                                    # First check if trajectory actually passes through this junction
                                    entered, _ = entered_junction_idx(traj.x, traj.z, junction)

                                    if not entered:
                                        # Trajectory doesn&#39;t pass through this junction, set NaN
                                        junction_metrics[f&#34;junction_{j}_time&#34;] = float(&#39;nan&#39;)
                                        junction_metrics[f&#34;junction_{j}_mode&#34;] = &#34;no_entry&#34;
                                        # Set speed metrics to NaN as well
                                        junction_metrics[f&#34;junction_{j}_speed&#34;] = float(&#39;nan&#39;)
                                        junction_metrics[f&#34;junction_{j}_speed_mode&#34;] = &#34;no_entry&#34;
                                        junction_metrics[f&#34;junction_{j}_entry_speed&#34;] = float(&#39;nan&#39;)
                                        junction_metrics[f&#34;junction_{j}_exit_speed&#34;] = float(&#39;nan&#39;)
                                        junction_metrics[f&#34;junction_{j}_avg_transit_speed&#34;] = float(&#39;nan&#39;)
                                        continue

                                    # Use the selected decision mode from GUI (with defaults)
                                    decision_mode = getattr(st.session_state, &#39;metrics_decision_mode&#39;, &#39;pathlen&#39;)
                                    distance = getattr(st.session_state, &#39;metrics_distance&#39;, 100.0)
                                    # Use r_outer from junction definitions like other functions do
                                    r_outer_list = [st.session_state.junction_r_outer.get(i, 50.0) for i in range(len(st.session_state.junctions))]
                                    r_outer = r_outer_list[j] if j &lt; len(r_outer_list) else 50.0
                                    trend_window = getattr(st.session_state, &#39;metrics_trend_window&#39;, 5)
                                    min_outward = getattr(st.session_state, &#39;metrics_min_outward&#39;, 0.0)

                                    # Dynamic path length adjustment for all modes
                                    if len(traj.x) &gt; 1:
                                        dx = np.diff(traj.x)
                                        dz = np.diff(traj.z)
                                        segments = np.hypot(dx, dz)
                                        total_distance = float(np.sum(segments))

                                        # Apply dynamic adjustment based on mode
                                        if decision_mode == &#34;pathlen&#34;:
                                            # Use 5% of total distance, but at least 0.1 and at most 10.0
                                            dynamic_distance = max(0.1, min(10.0, total_distance * 0.05))
                                            distance = dynamic_distance
                                        elif decision_mode == &#34;radial&#34;:
                                            # For radial mode, use a smaller r_outer based on trajectory data
                                            dynamic_r_outer = max(5.0, min(50.0, total_distance * 0.1))
                                            r_outer = dynamic_r_outer

                                        # Debug: Show dynamic parameters for first few trajectories
                                        if i &lt; 5:
                                            if decision_mode == &#34;pathlen&#34;:
                                                st.write(f&#34;🔍 Trajectory {i}: total_distance={total_distance:.2f}, dynamic_distance={dynamic_distance:.2f}&#34;)
                                            elif decision_mode == &#34;radial&#34;:
                                                st.write(f&#34;🔍 Trajectory {i}: total_distance={total_distance:.2f}, dynamic_r_outer={dynamic_r_outer:.2f}&#34;)

                                    # Try the timing calculation with the dynamic parameters
                                    t_val, mode_used = _timing_for_traj(
                                        tr=traj,
                                        junction=junction,
                                        decision_mode=decision_mode,
                                        distance=distance,
                                        r_outer=r_outer,
                                        trend_window=trend_window,
                                        min_outward=min_outward,
                                    )

                                    # Compute speed analysis for this junction
                                    speed_val, speed_mode = speed_through_junction(
                                        tr=traj,
                                        junction=junction,
                                        decision_mode=decision_mode,
                                        path_length=distance,
                                        r_outer=r_outer,
                                        window=trend_window,
                                        min_outward=min_outward,
                                    )

                                    # Compute junction transit speeds
                                    entry_speed, exit_speed, avg_transit_speed = junction_transit_speed(traj, junction)

                                    # If still NaN and we have some movement, try with even smaller parameters
                                    if np.isnan(t_val) and len(traj.x) &gt; 1:
                                        if decision_mode == &#34;pathlen&#34;:
                                            # Try with just 1% of total distance, minimum 0.01
                                            fallback_distance = max(0.01, total_distance * 0.01)
                                            t_val, mode_used = _timing_for_traj(
                                                tr=traj,
                                                junction=junction,
                                                decision_mode=decision_mode,
                                                distance=fallback_distance,
                                                r_outer=r_outer,
                                                trend_window=trend_window,
                                                min_outward=min_outward,
                                            )
                                        elif decision_mode == &#34;radial&#34;:
                                            # Try with even smaller r_outer
                                            fallback_r_outer = max(2.0, total_distance * 0.05)
                                            t_val, mode_used = _timing_for_traj(
                                                tr=traj,
                                                junction=junction,
                                                decision_mode=decision_mode,
                                                distance=distance,
                                                r_outer=fallback_r_outer,
                                                trend_window=trend_window,
                                                min_outward=min_outward,
                                            )

                                        if i &lt; 5 and not np.isnan(t_val):
                                            if decision_mode == &#34;pathlen&#34;:
                                                st.write(f&#34;🔍 Trajectory {i}: Fallback worked! fallback_distance={fallback_distance:.3f}&#34;)
                                            elif decision_mode == &#34;radial&#34;:
                                                st.write(f&#34;🔍 Trajectory {i}: Fallback worked! fallback_r_outer={fallback_r_outer:.3f}&#34;)

                                    junction_metrics[f&#34;junction_{j}_time&#34;] = t_val
                                    junction_metrics[f&#34;junction_{j}_mode&#34;] = mode_used
                                    # Add speed analysis metrics
                                    junction_metrics[f&#34;junction_{j}_speed&#34;] = speed_val
                                    junction_metrics[f&#34;junction_{j}_speed_mode&#34;] = speed_mode
                                    junction_metrics[f&#34;junction_{j}_entry_speed&#34;] = entry_speed
                                    junction_metrics[f&#34;junction_{j}_exit_speed&#34;] = exit_speed
                                    junction_metrics[f&#34;junction_{j}_avg_transit_speed&#34;] = avg_transit_speed
                                except Exception as e:
                                    st.warning(f&#34;⚠️ Junction {j} timing failed for trajectory {i}: {e}&#34;)
                                    junction_metrics[f&#34;junction_{j}_time&#34;] = float(&#39;nan&#39;)
                                    junction_metrics[f&#34;junction_{j}_mode&#34;] = &#34;error&#34;

                            # Combine basic and junction metrics
                            combined_metrics = {
                                &#34;trajectory_id&#34;: i,
                                &#34;trajectory_tid&#34;: traj.tid,
                                **basic_metrics,
                                **junction_metrics
                            }
                            metrics.append(combined_metrics)

                        except Exception as e:
                            st.error(f&#34;❌ Failed to compute metrics for trajectory {i} ({traj.tid}): {e}&#34;)
                            # Add error entry to maintain consistency
                            error_metrics = {
                                &#34;trajectory_id&#34;: i,
                                &#34;trajectory_tid&#34;: traj.tid,
                                &#34;total_time&#34;: 0.0,
                                &#34;total_distance&#34;: 0.0,
                                &#34;average_speed&#34;: 0.0,
                                &#34;error&#34;: str(e)
                            }
                            metrics.append(error_metrics)

                    # Show time data debug information (always show for metrics analysis)
                    st.markdown(&#34;---&#34;)
                    st.markdown(&#34;### 🔍 Debug Information&#34;)

                    # Debug: Show what we have
                    st.write(f&#34;**Debug Status:**&#34;)
                    st.write(f&#34;- time_data_debug length: {len(time_data_debug)}&#34;)
                    st.write(f&#34;- trajectories_without_time_data: {trajectories_without_time_data}&#34;)
                    st.write(f&#34;- trajectories_with_time_data: {trajectories_with_time_data}&#34;)

                    if time_data_debug:
                        with st.expander(&#34;🔍 Time Data Debug Information&#34;, expanded=True):
                            st.write(&#34;**First 5 trajectories time data analysis:**&#34;)
                            for debug_info in time_data_debug:
                                st.write(f&#34;**Trajectory {debug_info[&#39;trajectory_id&#39;]}:**&#34;)
                                st.write(f&#34;- Type: {debug_info[&#39;time_data_type&#39;]}&#34;)
                                st.write(f&#34;- Shape: {debug_info[&#39;time_data_shape&#39;]}&#34;)
                                st.write(f&#34;- Dtype: {debug_info[&#39;time_data_dtype&#39;]}&#34;)
                                st.write(f&#34;- Sample: {debug_info[&#39;time_data_sample&#39;]}&#34;)
                                st.write(&#34;---&#34;)
                    else:
                        st.info(&#34;No time data debug information available&#34;)

                    # Show detailed analysis of failing trajectories
                    if trajectories_without_time_data &gt; 0:
                        with st.expander(&#34;🔍 Detailed Time Data Analysis&#34;, expanded=True):
                            st.write(f&#34;**Analysis of {trajectories_without_time_data} trajectories with invalid time data:**&#34;)

                            # Sample a few failing trajectories for detailed analysis
                            failing_samples = []
                            for i, traj in enumerate(st.session_state.trajectories):
                                if i &gt;= 10:  # Limit to first 10 for performance
                                    break
                                try:
                                    basic_metrics = compute_basic_trajectory_metrics(traj)
                                    if basic_metrics[&#34;total_time&#34;] == 0:
                                        failing_samples.append({
                                            &#34;id&#34;: i,
                                            &#34;tid&#34;: traj.tid,
                                            &#34;time_data&#34;: traj.t[:5].tolist() if traj.t is not None and len(traj.t) &gt; 0 else None,
                                            &#34;time_dtype&#34;: str(traj.t.dtype) if traj.t is not None else None,
                                            &#34;time_shape&#34;: traj.t.shape if traj.t is not None else None,
                                            &#34;time_is_none&#34;: traj.t is None,
                                            &#34;time_length&#34;: len(traj.t) if traj.t is not None else 0
                                        })
                                except Exception as e:
                                    failing_samples.append({
                                        &#34;id&#34;: i,
                                        &#34;tid&#34;: traj.tid,
                                        &#34;error&#34;: str(e)
                                    })

                            if failing_samples:
                                st.write(&#34;**Sample failing trajectories:**&#34;)
                                for sample in failing_samples:
                                    st.write(f&#34;**Trajectory {sample[&#39;id&#39;]} ({sample[&#39;tid&#39;]}):**&#34;)
                                    if &#39;error&#39; in sample:
                                        st.write(f&#34;- Error: {sample[&#39;error&#39;]}&#34;)
                                    else:
                                        st.write(f&#34;- Time data is None: {sample[&#39;time_is_none&#39;]}&#34;)
                                        st.write(f&#34;- Time data length: {sample[&#39;time_length&#39;]}&#34;)
                                        st.write(f&#34;- Time data sample: {sample[&#39;time_data&#39;]}&#34;)
                                        st.write(f&#34;- Time dtype: {sample[&#39;time_dtype&#39;]}&#34;)
                                        st.write(f&#34;- Time shape: {sample[&#39;time_shape&#39;]}&#34;)
                                    st.write(&#34;---&#34;)

                            # Check if time data is completely missing
                            none_count = sum(1 for traj in st.session_state.trajectories[:10] if traj.t is None)
                            if none_count &gt; 0:
                                st.warning(f&#34;⚠️ {none_count} out of 10 sample trajectories have NO time data (t=None)&#34;)
                                st.write(&#34;**This suggests:**&#34;)
                                st.write(&#34;- Time column mapping is incorrect&#34;)
                                st.write(&#34;- Time column doesn&#39;t exist in CSV files&#34;)
                                st.write(&#34;- Time column is completely empty&#34;)

                                # Try to diagnose column mapping issue
                                st.write(&#34;**Column Mapping Diagnosis:**&#34;)
                                st.write(&#34;The GUI is looking for a time column, but it might not exist or be named differently.&#34;)
                                st.write(&#34;**Common time column names:**&#34;)
                                st.write(&#34;- &#39;Time&#39; (most common)&#34;)
                                st.write(&#34;- &#39;time&#39;&#34;)
                                st.write(&#34;- &#39;t&#39;&#34;)
                                st.write(&#34;- &#39;timestamp&#39;&#34;)
                                st.write(&#34;- &#39;Timestamp&#39;&#34;)
                                st.write(&#34;- &#39;TIME&#39;&#34;)
                                st.write(&#34;&#34;)
                                st.write(&#34;**To fix this:**&#34;)
                                st.write(&#34;1. Check the Data Upload tab&#34;)
                                st.write(&#34;2. Look at the &#39;Time Column&#39; field&#34;)
                                st.write(&#34;3. Make sure it matches a column name in your CSV files&#34;)
                                st.write(&#34;4. If unsure, try common names like &#39;Time&#39;, &#39;time&#39;, or &#39;t&#39;&#34;)

                            st.write(&#34;**Common time data issues:**&#34;)
                            st.write(&#34;- Empty strings or null values&#34;)
                            st.write(&#34;- Non-numeric text (e.g., &#39;Time: 1.23&#39;, &#39;1.23s&#39;)&#34;)
                            st.write(&#34;- Mixed data types in the same column&#34;)
                            st.write(&#34;- Missing or corrupted time data&#34;)
                            st.write(&#34;- Incorrect column mapping&#34;)

                    # Store results - preserve existing analysis results
                    if st.session_state.analysis_results is None:
                        st.session_state.analysis_results = {}
                    st.session_state.analysis_results[&#34;metrics&#34;] = metrics

                    # Save metrics to CSV file
                    try:
                        import os
                        import pandas as pd
                        os.makedirs(&#34;gui_outputs&#34;, exist_ok=True)
                        df = pd.DataFrame(metrics)
                        csv_path = os.path.join(&#34;gui_outputs&#34;, &#34;metrics_results.csv&#34;)
                        df.to_csv(csv_path, index=False)
                        st.info(f&#34;📁 Metrics saved to: {csv_path}&#34;)
                    except Exception as e:
                        st.warning(f&#34;⚠️ Could not save metrics to file: {e}&#34;)

                    # Generate and save metrics plots for export and visualization reuse
                    try:
                        import os
                        import glob
                        import pandas as pd
                        import matplotlib.pyplot as plt
                        import math

                        metrics_dir = os.path.join(&#34;gui_outputs&#34;, &#34;metrics&#34;)
                        os.makedirs(metrics_dir, exist_ok=True)

                        metrics_df = pd.DataFrame(metrics)

                        # Helper to safely save and close figures
                        def _save_fig(path):
                            plt.tight_layout()
                            plt.savefig(path, dpi=150)
                            plt.close()

                        # ---- Utilities for KDE and distribution fitting ----
                        def _kde_curve(values):
                            arr = np.asarray(values, dtype=float)
                            arr = arr[np.isfinite(arr)]
                            if len(arr) &lt; 2:
                                return None
                            std = np.std(arr)
                            if std == 0:
                                return None
                            n = len(arr)
                            # Silverman&#39;s rule of thumb
                            h = 1.06 * std * (n ** (-1.0 / 5.0))
                            xs = np.linspace(np.percentile(arr, 1), np.percentile(arr, 99), 200)
                            diffs = (xs[:, None] - arr[None, :]) / h
                            kernel = np.exp(-0.5 * diffs * diffs) / (math.sqrt(2.0 * math.pi))
                            density = np.sum(kernel, axis=1) / (n * h)
                            return xs, density

                        def _loglik_normal(arr, mu, sigma):
                            if sigma &lt;= 0:
                                return -np.inf
                            return np.sum(-0.5 * np.log(2 * np.pi) - np.log(sigma) - 0.5 * ((arr - mu) / sigma) ** 2)

                        def _loglik_lognormal(arr, mu_log, sigma_log):
                            if sigma_log &lt;= 0:
                                return -np.inf
                            if np.any(arr &lt;= 0):
                                return -np.inf
                            z = (np.log(arr) - mu_log) / sigma_log
                            return np.sum(-0.5 * np.log(2 * np.pi) - np.log(sigma_log) - np.log(arr) - 0.5 * z * z)

                        def _loglik_gamma(arr, k, theta):
                            if k &lt;= 0 or theta &lt;= 0:
                                return -np.inf
                            if np.any(arr &lt;= 0):
                                return -np.inf
                            # log pdf = (k-1)ln x - x/theta - k ln theta - lgamma(k)
                            return np.sum((k - 1) * np.log(arr) - arr / theta - k * np.log(theta) - math.lgamma(k))

                        def _fit_and_plot_overlays(ax, arr, xlabel, base_color, alt_color):
                            # KDE overlay
                            kde = _kde_curve(arr)
                            if kde is not None:
                                xs_kde, dens_kde = kde
                                ax.plot(xs_kde, dens_kde, color=alt_color, linewidth=2, alpha=0.9, label=&#34;KDE&#34;)

                            # Candidate distributions and AIC
                            candidates = []
                            n = len(arr)
                            if n &gt;= 2:
                                # Normal
                                mu = float(np.mean(arr))
                                sigma = float(np.std(arr))
                                ll_n = _loglik_normal(arr, mu, sigma) if sigma &gt; 0 else -np.inf
                                aic_n = 2 * 2 - 2 * ll_n  # 2 params
                                candidates.append({
                                    &#34;name&#34;: &#34;Normal&#34;,
                                    &#34;params&#34;: (mu, sigma),
                                    &#34;aic&#34;: aic_n,
                                    &#34;pdf&#34;: lambda x: (1.0 / (sigma * math.sqrt(2 * math.pi))) * np.exp(-0.5 * ((x - mu) / sigma) ** 2),
                                    &#34;label&#34;: f&#34;Normal (μ={mu:.2f}, σ={sigma:.2f})&#34;
                                })

                                # Log-normal (positive values)
                                pos = arr[arr &gt; 0]
                                if len(pos) &gt;= 2 and np.std(np.log(pos)) &gt; 0:
                                    mu_l = float(np.mean(np.log(pos)))
                                    sigma_l = float(np.std(np.log(pos)))
                                    ll_l = _loglik_lognormal(pos, mu_l, sigma_l)
                                    aic_l = 2 * 2 - 2 * ll_l  # 2 params
                                    candidates.append({
                                        &#34;name&#34;: &#34;LogNormal&#34;,
                                        &#34;params&#34;: (mu_l, sigma_l),
                                        &#34;aic&#34;: aic_l,
                                        &#34;pdf&#34;: lambda x: np.where(x &gt; 0, (1.0 / (x * sigma_l * math.sqrt(2 * math.pi))) * np.exp(-0.5 * ((np.log(x) - mu_l) / sigma_l) ** 2), 0.0),
                                        &#34;label&#34;: f&#34;LogNormal (μlog={mu_l:.2f}, σlog={sigma_l:.2f})&#34;
                                    })

                                # Gamma (method of moments)
                                if np.all(arr &gt; 0) and np.var(arr) &gt; 0:
                                    mean = float(np.mean(arr))
                                    var = float(np.var(arr))
                                    k = mean * mean / var
                                    theta = var / mean
                                    ll_g = _loglik_gamma(arr, k, theta)
                                    aic_g = 2 * 2 - 2 * ll_g  # 2 params (k, theta)
                                    candidates.append({
                                        &#34;name&#34;: &#34;Gamma&#34;,
                                        &#34;params&#34;: (k, theta),
                                        &#34;aic&#34;: aic_g,
                                        &#34;pdf&#34;: lambda x: np.where(x &gt; 0, (x ** (k - 1)) * np.exp(-x / theta) / (math.gamma(k) * (theta ** k)), 0.0),
                                        &#34;label&#34;: f&#34;Gamma (k={k:.2f}, θ={theta:.2f})&#34;
                                    })

                            if candidates:
                                best = min(candidates, key=lambda c: c[&#34;aic&#34;])
                                xs = np.linspace(np.percentile(arr, 1), np.percentile(arr, 99), 200)
                                ax.plot(xs, best[&#34;pdf&#34;](xs), color=base_color, linewidth=2.5, label=f&#34;Best: {best[&#39;label&#39;]} (AIC {best[&#39;aic&#39;]:.1f})&#34;)
                                ax.legend()

                        # 1) Total Time Distribution (+ KDE and best-fit distribution)
                        if &#34;total_time&#34; in metrics_df.columns and metrics_df[&#34;total_time&#34;].notna().any():
                            plt.figure(figsize=(8, 4))
                            vals = metrics_df[&#34;total_time&#34;].dropna().to_numpy()
                            ax = plt.gca()
                            ax.hist(vals, bins=30, color=&#34;#4C78A8&#34;, alpha=0.55, density=True, edgecolor=&#34;none&#34;)
                            _fit_and_plot_overlays(ax, vals, xlabel=&#34;Seconds&#34;, base_color=&#34;#1F77B4&#34;, alt_color=&#34;#4C78A8&#34;)
                            ax.set_title(&#34;Total Time Distribution (s)&#34;)
                            ax.set_xlabel(&#34;Seconds&#34;)
                            ax.set_ylabel(&#34;Density&#34;)
                            _save_fig(os.path.join(metrics_dir, &#34;total_time_distribution.png&#34;))

                        # 2) Average Speed Distribution (+ KDE and best-fit distribution)
                        if &#34;average_speed&#34; in metrics_df.columns and metrics_df[&#34;average_speed&#34;].notna().any():
                            plt.figure(figsize=(8, 4))
                            vals = metrics_df[&#34;average_speed&#34;].dropna().to_numpy()
                            ax = plt.gca()
                            ax.hist(vals, bins=30, color=&#34;#F58518&#34;, alpha=0.55, density=True, edgecolor=&#34;none&#34;)
                            _fit_and_plot_overlays(ax, vals, xlabel=&#34;Speed&#34;, base_color=&#34;#DD8452&#34;, alt_color=&#34;#F58518&#34;)
                            ax.set_title(&#34;Average Speed Distribution&#34;)
                            ax.set_xlabel(&#34;Speed&#34;)
                            ax.set_ylabel(&#34;Density&#34;)
                            _save_fig(os.path.join(metrics_dir, &#34;average_speed_distribution.png&#34;))

                        # 3) Total Distance Distribution (+ KDE and best-fit distribution)
                        if &#34;total_distance&#34; in metrics_df.columns and metrics_df[&#34;total_distance&#34;].notna().any():
                            plt.figure(figsize=(8, 4))
                            vals = metrics_df[&#34;total_distance&#34;].dropna().to_numpy()
                            ax = plt.gca()
                            ax.hist(vals, bins=30, color=&#34;#54A24B&#34;, alpha=0.55, density=True, edgecolor=&#34;none&#34;)
                            _fit_and_plot_overlays(ax, vals, xlabel=&#34;Distance&#34;, base_color=&#34;#2CA02C&#34;, alt_color=&#34;#54A24B&#34;)
                            ax.set_title(&#34;Total Distance Distribution&#34;)
                            ax.set_xlabel(&#34;Distance&#34;)
                            ax.set_ylabel(&#34;Density&#34;)
                            _save_fig(os.path.join(metrics_dir, &#34;total_distance_distribution.png&#34;))

                        # Discover junction-related columns
                        junction_time_cols = [c for c in metrics_df.columns if c.startswith(&#34;junction_&#34;) and c.endswith(&#34;_time&#34;)]
                        speed_cols = [c for c in metrics_df.columns if c.startswith(&#34;junction_&#34;) and c.endswith(&#34;_speed&#34;)]

                        # 4) Speed vs Time Correlation (means per junction for selected speed metrics)
                        # Use average transit speed if available; else fall back to generic _speed
                        suffix_candidates = [&#34;_avg_transit_speed&#34;, &#34;_speed&#34;]
                        chosen_speed_cols = []
                        for sfx in suffix_candidates:
                            candidate = [c for c in speed_cols if c.endswith(sfx)]
                            if candidate:
                                chosen_speed_cols = candidate
                                break
                        if chosen_speed_cols and junction_time_cols:
                            means_speed = []
                            means_time = []
                            labels = []
                            for sc in chosen_speed_cols:
                                jn = sc.split(&#34;_&#34;)[1]
                                tc = f&#34;junction_{jn}_time&#34;
                                if tc in metrics_df.columns:
                                    df_pair = metrics_df[[sc, tc]].dropna()
                                    if len(df_pair) &gt; 0:
                                        means_speed.append(df_pair[sc].mean())
                                        means_time.append(df_pair[tc].mean())
                                        labels.append(f&#34;J{jn}&#34;)
                            if means_speed and means_time:
                                plt.figure(figsize=(6, 6))
                                plt.scatter(means_time, means_speed, c=&#34;#4C78A8&#34;)
                                for x, y, lab in zip(means_time, means_speed, labels):
                                    plt.annotate(lab, (x, y), xytext=(5, 5), textcoords=&#39;offset points&#39;, fontsize=8)
                                plt.title(&#34;Speed vs Time (means per junction)&#34;)
                                plt.xlabel(&#34;Time (s)&#34;)
                                plt.ylabel(&#34;Speed&#34;)
                                _save_fig(os.path.join(metrics_dir, &#34;speed_vs_time_correlation.png&#34;))

                        # 5) Entry vs Exit Speed by Junction (grouped bars)
                        entry_cols = [c for c in metrics_df.columns if c.endswith(&#34;_entry_speed&#34;)]
                        exit_cols = [c for c in metrics_df.columns if c.endswith(&#34;_exit_speed&#34;)]
                        if entry_cols and exit_cols:
                            data = []
                            labels_j = []
                            for ec in entry_cols:
                                jn = ec.split(&#34;_&#34;)[1]
                                xc = f&#34;junction_{jn}_exit_speed&#34;
                                if xc in metrics_df.columns:
                                    e_vals = metrics_df[ec].dropna()
                                    x_vals = metrics_df[xc].dropna()
                                    if len(e_vals) &gt; 0 and len(x_vals) &gt; 0:
                                        data.append((e_vals.mean(), x_vals.mean()))
                                        labels_j.append(f&#34;J{jn}&#34;)
                            if data:
                                entry_means = [d[0] for d in data]
                                exit_means = [d[1] for d in data]
                                x = np.arange(len(labels_j))
                                width = 0.35
                                plt.figure(figsize=(max(6, len(labels_j) * 0.6), 4))
                                plt.bar(x - width/2, entry_means, width, label=&#39;Entry&#39;, color=&#39;#72B7B2&#39;)
                                plt.bar(x + width/2, exit_means, width, label=&#39;Exit&#39;, color=&#39;#E45756&#39;)
                                plt.xticks(x, labels_j)
                                plt.title(&#34;Entry vs Exit Speed by Junction&#34;)
                                plt.xlabel(&#34;Junction&#34;)
                                plt.ylabel(&#34;Speed&#34;)
                                plt.legend()
                                _save_fig(os.path.join(metrics_dir, &#34;entry_exit_speed_by_junction.png&#34;))

                        # 6) Junction Timing Comparison (average times)
                        if junction_time_cols:
                            jt_labels = []
                            jt_means = []
                            for tc in sorted(junction_time_cols, key=lambda c: int(c.split(&#39;_&#39;)[1])):
                                jn = tc.split(&#34;_&#34;)[1]
                                vals = metrics_df[tc].dropna()
                                if len(vals) &gt; 0:
                                    jt_labels.append(f&#34;J{jn}&#34;)
                                    jt_means.append(vals.mean())
                            if jt_labels:
                                x = np.arange(len(jt_labels))
                                plt.figure(figsize=(max(6, len(jt_labels) * 0.6), 4))
                                plt.bar(x, jt_means, color=&#34;#4C78A8&#34;)
                                plt.xticks(x, jt_labels)
                                plt.title(&#34;Average Junction Timing&#34;)
                                plt.xlabel(&#34;Junction&#34;)
                                plt.ylabel(&#34;Time (s)&#34;)
                                _save_fig(os.path.join(metrics_dir, &#34;junction_timing_comparison.png&#34;))

                        # 7) Individual Junction Timing Distributions (per junction) (+ KDE and best-fit distribution)
                        if junction_time_cols:
                            for tc in sorted(junction_time_cols, key=lambda c: int(c.split(&#39;_&#39;)[1])):
                                jn = tc.split(&#34;_&#34;)[1]
                                vals = metrics_df[tc].dropna().to_numpy()
                                if len(vals) &gt; 0:
                                    plt.figure(figsize=(8, 4))
                                    ax = plt.gca()
                                    ax.hist(vals, bins=30, color=&#34;#B279A2&#34;, alpha=0.55, density=True, edgecolor=&#34;none&#34;)
                                    _fit_and_plot_overlays(ax, vals, xlabel=&#34;Seconds&#34;, base_color=&#34;#A05FA3&#34;, alt_color=&#34;#B279A2&#34;)
                                    ax.set_title(f&#34;Junction {jn} Timing Distribution (s)&#34;)
                                    ax.set_xlabel(&#34;Seconds&#34;)
                                    ax.set_ylabel(&#34;Density&#34;)
                                    _save_fig(os.path.join(metrics_dir, f&#34;timing_distribution_J{jn}.png&#34;))

                        # Store paths in session for downstream tabs
                        try:
                            images = {}
                            for p in glob.glob(os.path.join(metrics_dir, &#34;*.png&#34;)):
                                images[os.path.basename(p)] = p
                            st.session_state.analysis_results.setdefault(&#34;metrics_images&#34;, images)
                        except Exception:
                            pass

                    except Exception as e:
                        st.warning(f&#34;⚠️ Could not generate metrics plots: {e}&#34;)

                    # Provide detailed success message
                    success_msg = f&#34;✅ Computed metrics for {len(metrics)} trajectories&#34;
                    if trajectories_with_time_data &gt; 0:
                        success_msg += f&#34; ({trajectories_with_time_data} with time data)&#34;
                    if trajectories_without_time_data &gt; 0:
                        success_msg += f&#34; ({trajectories_without_time_data} without valid time data)&#34;

                    st.success(success_msg)

                    # Show warning if many trajectories lack time data
                    if trajectories_without_time_data &gt; len(metrics) * 0.5:
                        st.warning(f&#34;⚠️ {trajectories_without_time_data} trajectories lack valid time data. This may indicate time data format issues. Check the debug information above.&#34;)

                        # Provide suggestions for fixing time data issues
                        with st.expander(&#34;💡 Suggestions for Fixing Time Data Issues&#34;, expanded=False):
                            st.write(&#34;**To fix time data issues, try these solutions:**&#34;)
                            st.write(&#34;&#34;)
                            st.write(&#34;**1. Check Column Mapping:**&#34;)
                            st.write(&#34;- Verify the time column is correctly mapped&#34;)
                            st.write(&#34;- Look for columns like &#39;Time&#39;, &#39;timestamp&#39;, &#39;t&#39;, etc.&#34;)
                            st.write(&#34;&#34;)
                            st.write(&#34;**2. Check Data Format:**&#34;)
                            st.write(&#34;- Time data should be numeric (e.g., 0, 1.5, 2.3)&#34;)
                            st.write(&#34;- Avoid text formats like &#39;Time: 1.23&#39; or &#39;1.23s&#39;&#34;)
                            st.write(&#34;- Remove quotes around time values&#34;)
                            st.write(&#34;&#34;)
                            st.write(&#34;**3. Data Cleaning:**&#34;)
                            st.write(&#34;- Remove empty rows or null values&#34;)
                            st.write(&#34;- Ensure consistent data types in time column&#34;)
                            st.write(&#34;- Check for mixed formats in the same column&#34;)
                            st.write(&#34;&#34;)
                            st.write(&#34;**4. File Format:**&#34;)
                            st.write(&#34;- Ensure CSV files are properly formatted&#34;)
                            st.write(&#34;- Check for encoding issues (UTF-8 recommended)&#34;)
                            st.write(&#34;- Verify file is not corrupted&#34;)
                            st.write(&#34;&#34;)
                            st.write(&#34;**5. Manual Inspection:**&#34;)
                            st.write(&#34;- Open the CSV file in a text editor&#34;)
                            st.write(&#34;- Look at the first few rows of the time column&#34;)
                            st.write(&#34;- Check for patterns in the data format&#34;)

                    # Generate CLI command for easy copying
                    # Build results dict for CLI command generation
                    metrics_results = {}
                    for i, junction in enumerate(junctions_to_use):
                        junction_key = f&#34;junction_{i}&#34;
                        metrics_results[junction_key] = {
                            &#34;junction&#34;: junction,
                            &#34;r_outer&#34;: st.session_state.junction_r_outer.get(i, 50.0) if i &lt; len(st.session_state.junctions) else 50.0,
                            &#34;decision_mode&#34;: getattr(st.session_state, &#39;metrics_decision_mode&#39;, &#39;pathlen&#39;),
                            &#34;distance&#34;: getattr(st.session_state, &#39;metrics_distance&#39;, 100.0),
                            &#34;r_outer_value&#34;: st.session_state.junction_r_outer.get(i, 50.0) if i &lt; len(st.session_state.junctions) else 50.0,
                            &#34;trend_window&#34;: getattr(st.session_state, &#39;metrics_trend_window&#39;, 5),
                            &#34;min_outward&#34;: getattr(st.session_state, &#39;metrics_min_outward&#39;, 0.0),
                        }
                    self.generate_cli_command(&#34;metrics&#34;, metrics_results, cluster_method, cluster_params, decision_mode, decision_params)

                elif analysis_type == &#34;gaze&#34;:
                    # Analyze gaze and physiological data
                    gaze_results = {}

                    # Create dedicated debug container for gaze analysis
                    gaze_debug_container = st.empty()

                    def update_gaze_debug_display():
                        &#34;&#34;&#34;Update the persistent gaze debug display&#34;&#34;&#34;

                    # Initialize gaze debug info
                    st.session_state[&#39;gaze_debug_info&#39;] = {}

                    # Check if we have proper gaze trajectory data (prefer trajectories that actually carry data)
                    active_trajs = st.session_state.trajectories
                    # Build a filtered list that actually has gaze OR physio data
                    try:
                        from verta.verta_data_loader import has_gaze_data as _has_gaze, has_physio_data as _has_physio
                        trajs_with_signals = [t for t in active_trajs if (_has_gaze(t) or _has_physio(t))]

                        # Debug: Show filtering results
                        st.info(f&#34;🔍 **Trajectory Filtering Debug:**&#34;)
                        st.write(f&#34;- Total trajectories: {len(active_trajs)}&#34;)
                        st.write(f&#34;- Trajectories with gaze/physio data: {len(trajs_with_signals)}&#34;)

                        if len(trajs_with_signals) &lt; len(active_trajs):
                            skipped_count = len(active_trajs) - len(trajs_with_signals)
                            st.info(f&#34;ℹ️ Skipped {skipped_count} trajectories without gaze/physiological data&#34;)

                    except Exception as e:
                        st.warning(f&#34;⚠️ Error filtering trajectories: {e}&#34;)
                        trajs_with_signals = active_trajs

                    # Use the filtered list if it has any valid trajectories
                    if trajs_with_signals and len(trajs_with_signals) &gt; 0:
                        active_trajs = trajs_with_signals
                    else:
                        st.warning(&#34;⚠️ No trajectories with gaze/physiological data found&#34;)
                        active_trajs = []
                    has_gaze_data = self._check_for_gaze_data(active_trajs)
                    # If global check still fails but we have some physio data, allow comprehensive path to proceed
                    try:
                        from verta.verta_data_loader import has_physio_data as _has_physio
                        has_any_physio = any(_has_physio(t) for t in active_trajs)
                    except Exception:
                        has_any_physio = False

                    # Get column mappings from session state (if they were specified)
                    column_mappings = self._get_gaze_column_mappings()

                    if not has_gaze_data and not has_any_physio and not column_mappings:
                        st.warning(&#34;⚠️ **No Gaze Data Available**&#34;)
                        st.info(&#34;&#34;&#34;
                        **Gaze analysis requires trajectories with:**
                        - Head tracking data (`head_forward_x`, `head_forward_z`)
                        - Eye tracking data (`gaze_x`, `gaze_y`)
                        - Physiological data (`pupil_l`, `pupil_r`, `heart_rate`)

                        **Current trajectories only have position data (x, z, t).**

                        Proceeding with movement-only fallback so visualizations still render.
                        &#34;&#34;&#34;)

                        # Compute movement-only fallback per junction
                        for i, junction in enumerate(st.session_state.junctions):
                            junction_key = f&#34;junction_{i}&#34;
                            r_outer = st.session_state.junction_r_outer.get(i, 50.0)
                            try:
                                movement_df = self._analyze_movement_patterns_optimized(
                                    active_trajs, junction, r_outer, decision_mode, path_length=100.0, epsilon=0.05
                                )
                                # Tag as movement analysis for downstream UI
                                if hasattr(movement_df, &#39;assign&#39;):
                                    movement_df = movement_df.assign(analysis_type=&#39;movement&#39;)
                                gaze_results[junction_key] = movement_df
                            except Exception as e:
                                st.warning(f&#34;⚠️ Movement fallback failed for {junction_key}: {e}&#34;)
                                gaze_results[junction_key] = None
                    else:
                        # Perform gaze analysis (use the active trajectories we already determined)

                        # Debug: Track session state before analysis
                        st.session_state.debug_session_state = {
                            &#39;trajectories_count&#39;: len(st.session_state.trajectories) if st.session_state.trajectories else 0,
                            &#39;gaze_trajectories_count&#39;: 0,
                            &#39;last_modified&#39;: &#39;before_gaze_analysis&#39;
                        }

                        # Create global heatmap ONCE (outside junction loop)
                        global_heatmap_data = None
                        cell_size = st.session_state.get(&#39;pupil_heatmap_cell_size&#39;, 50.0)
                        normalization = st.session_state.get(&#39;pupil_heatmap_normalization&#39;, &#39;relative&#39;)

                        # Define output directory for global plots
                        import os
                        global_out_dir = os.path.join(&#34;gui_outputs&#34;, &#34;gaze_plots&#34;)
                        os.makedirs(global_out_dir, exist_ok=True)

                        st.info(&#34;🗺️ Creating global pupil dilation heatmap...&#34;)
                        try:
                            global_heatmap_data = create_pupil_dilation_heatmap(
                                trajectories=active_trajs,
                                junctions=st.session_state.junctions,
                                cell_size=cell_size,
                                normalization=normalization
                            )
                            st.success(&#34;✅ Global heatmap created&#34;)

                            # Store global heatmap data for consistent scaling calculation
                            st.session_state[&#39;global_heatmap_data&#39;] = global_heatmap_data

                            # Generate global heatmap plot
                            try:
                                from verta.verta_gaze import plot_pupil_dilation_heatmap
                                import matplotlib.pyplot as plt
                                import os

                                # Create global heatmap plot
                                global_plot_path = os.path.join(global_out_dir, &#34;global_pupil_heatmap.png&#34;)

                                fig = plot_pupil_dilation_heatmap(
                                    heatmap_data=global_heatmap_data,
                                    junctions=st.session_state.junctions,
                                    trajectories=active_trajs,
                                    all_trajectories=active_trajs,
                                    title=&#34;Global Pupil Dilation Heatmap&#34;,
                                    show_sample_counts=False,
                                    show_minimap=False,
                                    vmin=None,  # Let the function determine scaling
                                    vmax=None
                                )

                                # Save the plot
                                fig.savefig(global_plot_path, dpi=150, bbox_inches=&#34;tight&#34;)
                                plt.close(fig)

                                st.info(f&#34;📁 Global heatmap plot saved to: {global_plot_path}&#34;)
                                st.write(f&#34;🔍 **Debug:** Global plot saved to: `{global_plot_path}`&#34;)
                                st.write(f&#34;🔍 **Debug:** File exists after save: {os.path.exists(global_plot_path)}&#34;)

                            except Exception as plot_e:
                                st.warning(f&#34;⚠️ Could not generate global heatmap plot: {plot_e}&#34;)

                        except Exception as e:
                            st.warning(f&#34;⚠️ Could not create global heatmap: {e}&#34;)

                        # Debug: Check junctions
                        st.info(f&#34;🔍 **Junction Debug:**&#34;)
                        st.write(f&#34;- Number of junctions: {len(st.session_state.junctions)}&#34;)
                        if st.session_state.junctions:
                            for i, junction in enumerate(st.session_state.junctions):
                                r_outer = st.session_state.junction_r_outer.get(i, 50.0)
                                st.write(f&#34;- Junction {i}: Circle(cx={junction.cx}, cz={junction.cz}, r={junction.r}), r_outer={r_outer}&#34;)
                        else:
                            st.error(&#34;❌ **No junctions defined!** Gaze analysis requires junctions to be defined.&#34;)
                            st.write(&#34;**Solution:** Go to the Junction Editor tab and define at least one junction.&#34;)
                            return

                        # CRITICAL FIX: Perform comprehensive gaze analysis for ALL junctions at once
                        # This ensures all junctions have access to the complete assignments DataFrame
                        st.info(&#34;🔍 **Performing comprehensive gaze analysis for all junctions...**&#34;)

                        # Create output directory for all junctions
                        import os
                        import pandas as pd
                        out_dir = os.path.join(&#34;gui_outputs&#34;, &#34;gaze_analysis&#34;)
                        os.makedirs(out_dir, exist_ok=True)

                        # Get r_outer values for all junctions
                        r_outer_list = [st.session_state.junction_r_outer.get(i, 50.0) for i in range(len(st.session_state.junctions))]

                        # Perform comprehensive gaze analysis for ALL junctions at once
                        try:
                            gaze_data_all = self._perform_comprehensive_gaze_analysis_all_junctions(
                                trajectories=active_trajs,
                                junctions=st.session_state.junctions,
                                r_outer_list=r_outer_list,
                                decision_mode=decision_mode,
                                path_length=100.0,
                                epsilon=0.05,
                                linger_delta=0.0,
                                out_dir=out_dir
                            )
                        except Exception as e:
                            st.error(f&#34;❌ **Comprehensive gaze analysis failed:** {e}&#34;)
                            st.write(f&#34;**Error type:** {type(e).__name__}&#34;)
                            st.write(f&#34;**Error message:** {str(e)}&#34;)

                            # Fall back to individual junction analysis
                            st.info(&#34;🔄 **Falling back to individual junction analysis...**&#34;)
                            for i, junction in enumerate(st.session_state.junctions):
                                junction_key = f&#34;junction_{i}&#34;
                                r_outer = r_outer_list[i]

                                try:
                                    gaze_data = self._perform_gaze_analysis_with_mappings(
                                        trajectories=active_trajs,
                                        junction=junction,
                                        r_outer=r_outer,
                                        decision_mode=decision_mode,
                                        path_length=100.0,
                                        epsilon=0.05,
                                        linger_delta=0.0,
                                        out_dir=out_dir,
                                        column_mappings=column_mappings,
                                        scale_factor=1.0
                                    )

                                    # Normalize columns so downstream plots find expected names
                                    if isinstance(gaze_data, dict):
                                        gaze_data = self._normalize_gaze_result_frames(gaze_data)

                                    # Store the comprehensive gaze analysis results
                                    gaze_results[junction_key] = gaze_data

                                    st.success(f&#34;✅ Completed fallback gaze analysis for {junction_key}&#34;)

                                    # Generate gaze plots immediately after analysis
                                    try:
                                        st.info(f&#34;📊 Generating gaze plots for {junction_key}...&#34;)
                                        self._generate_gaze_plots_during_analysis(gaze_data, junction_key, out_dir)
                                        st.success(f&#34;✅ Generated plots for {junction_key}&#34;)
                                    except Exception as e:
                                        st.warning(f&#34;⚠️ Plot generation failed for {junction_key}: {e}&#34;)

                                    # Update debug display after each junction
                                    update_gaze_debug_display()

                                except Exception as e:
                                    st.warning(f&#34;⚠️ Fallback gaze analysis failed for {junction_key}: {e}&#34;)

                                    # Store error information
                                    gaze_results[junction_key] = {
                                        &#39;error&#39;: str(e),
                                        &#39;error_type&#39;: type(e).__name__,
                                        &#39;junction&#39;: junction,
                                        &#39;r_outer&#39;: r_outer,
                                        &#39;physiological&#39;: None,
                                        &#39;pupil_dilation&#39;: None,
                                        &#39;head_yaw&#39;: None
                                    }

                                    # Update debug display even on error
                                    update_gaze_debug_display()
                                    continue

                            # Skip the rest of the multi-junction processing
                            gaze_data_all = None

                        # The comprehensive analysis returns data for all junctions
                        # We need to split it by junction for storage
                        if gaze_data_all is not None and isinstance(gaze_data_all, dict) and &#39;head_yaw&#39; in gaze_data_all:
                                # Split the results by junction
                                head_yaw_df = gaze_data_all[&#39;head_yaw&#39;]
                                physio_df = gaze_data_all.get(&#39;physiological&#39;)
                                pupil_df = gaze_data_all.get(&#39;pupil_dilation&#39;)
                                all_heatmaps = gaze_data_all.get(&#39;pupil_heatmap_junction&#39;, {})

                                # Process each junction&#39;s data
                                for i, junction in enumerate(st.session_state.junctions):
                                    junction_key = f&#34;junction_{i}&#34;

                                    # Filter data for this junction
                                    junction_head_yaw = head_yaw_df[head_yaw_df[&#39;junction&#39;] == i] if not head_yaw_df.empty else pd.DataFrame()
                                    junction_physio = physio_df[physio_df[&#39;junction&#39;] == i] if physio_df is not None and not physio_df.empty else None
                                    junction_pupil = pupil_df[pupil_df[&#39;junction&#39;] == i] if pupil_df is not None and not pupil_df.empty else None

                                    # Get heatmap data for this junction
                                    junction_heatmap = all_heatmaps.get(i) if all_heatmaps else None

                                    # Create junction-specific gaze data
                                    gaze_data = {
                                        &#39;head_yaw&#39;: junction_head_yaw,
                                        &#39;physiological&#39;: junction_physio,
                                        &#39;pupil_dilation&#39;: junction_pupil,
                                        &#39;pupil_heatmap_junction&#39;: {i: junction_heatmap} if junction_heatmap else {},
                                        &#39;junction&#39;: junction,
                                        &#39;r_outer&#39;: r_outer_list[i]
                                    }

                                    st.info(f&#34;🔍 **Processing junction {i}: {junction_key}**&#34;)
                                    st.write(f&#34;- Junction: Circle(cx={junction.cx}, cz={junction.cz}, r={junction.r})&#34;)
                                    st.write(f&#34;- R_outer: {r_outer_list[i]}&#34;)
                                    st.write(f&#34;- Head yaw records: {len(junction_head_yaw)}&#34;)
                                    st.write(f&#34;- Physiological records: {len(junction_physio) if junction_physio is not None else 0}&#34;)
                                    st.write(f&#34;- Pupil records: {len(junction_pupil) if junction_pupil is not None else 0}&#34;)

                                    # Normalize columns so downstream plots find expected names
                                    if isinstance(gaze_data, dict):
                                        gaze_data = self._normalize_gaze_result_frames(gaze_data)

                                    # Store the comprehensive gaze analysis results
                                    gaze_results[junction_key] = gaze_data

                                    # Debug: Verify data was saved correctly
                                    st.info(f&#34;🔍 **Data Storage Verification for {junction_key}:**&#34;)
                                    if isinstance(gaze_data, dict):
                                        for data_type, data in gaze_data.items():
                                            if data is not None:
                                                if hasattr(data, &#39;shape&#39;):
                                                    st.write(f&#34;- {data_type}: {data.shape} DataFrame&#34;)
                                                elif isinstance(data, list):
                                                    st.write(f&#34;- {data_type}: {len(data)} records&#34;)
                                                else:
                                                    st.write(f&#34;- {data_type}: {type(data).__name__}&#34;)
                                            else:
                                                st.write(f&#34;- {data_type}: None&#34;)
                                    else:
                                        st.write(f&#34;- Raw data type: {type(gaze_data).__name__}&#34;)

                                    st.success(f&#34;✅ Completed gaze analysis for {junction_key}&#34;)

                                    # Generate gaze plots immediately after analysis
                                    try:
                                        st.info(f&#34;📊 Generating gaze plots for {junction_key}...&#34;)
                                        self._generate_gaze_plots_during_analysis(gaze_data, junction_key, out_dir)
                                        st.success(f&#34;✅ Generated plots for {junction_key}&#34;)
                                    except Exception as e:
                                        st.warning(f&#34;⚠️ Plot generation failed for {junction_key}: {e}&#34;)

                                    # Update debug display after each junction
                                    update_gaze_debug_display()
                        else:
                            st.error(&#34;❌ **Comprehensive gaze analysis failed to return expected data structure**&#34;)
                            st.write(f&#34;Returned data type: {type(gaze_data_all)}&#34;)
                            if isinstance(gaze_data_all, dict):
                                st.write(f&#34;Keys: {list(gaze_data_all.keys())}&#34;)

                            # Fall back to individual junction analysis
                            st.info(&#34;🔄 **Falling back to individual junction analysis...**&#34;)
                            for i, junction in enumerate(st.session_state.junctions):
                                junction_key = f&#34;junction_{i}&#34;
                                r_outer = r_outer_list[i]

                                try:
                                    gaze_data = self._perform_gaze_analysis_with_mappings(
                                        trajectories=active_trajs,
                                        junction=junction,
                                        r_outer=r_outer,
                                        decision_mode=decision_mode,
                                        path_length=100.0,
                                        epsilon=0.05,
                                        linger_delta=0.0,
                                        out_dir=out_dir,
                                        column_mappings=column_mappings,
                                        scale_factor=1.0
                                    )

                                    # Normalize columns so downstream plots find expected names
                                    if isinstance(gaze_data, dict):
                                        gaze_data = self._normalize_gaze_result_frames(gaze_data)

                                    # Store the comprehensive gaze analysis results
                                    gaze_results[junction_key] = gaze_data

                                    st.success(f&#34;✅ Completed fallback gaze analysis for {junction_key}&#34;)

                                    # Generate gaze plots immediately after analysis
                                    try:
                                        st.info(f&#34;📊 Generating gaze plots for {junction_key}...&#34;)
                                        self._generate_gaze_plots_during_analysis(gaze_data, junction_key, out_dir)
                                        st.success(f&#34;✅ Generated plots for {junction_key}&#34;)
                                    except Exception as e:
                                        st.warning(f&#34;⚠️ Plot generation failed for {junction_key}: {e}&#34;)

                                    # Update debug display after each junction
                                    update_gaze_debug_display()

                                except Exception as e:
                                    st.warning(f&#34;⚠️ Fallback gaze analysis failed for {junction_key}: {e}&#34;)

                                    # Store error information
                                    gaze_results[junction_key] = {
                                        &#39;error&#39;: str(e),
                                        &#39;error_type&#39;: type(e).__name__,
                                        &#39;junction&#39;: junction,
                                        &#39;r_outer&#39;: r_outer,
                                        &#39;physiological&#39;: None,
                                        &#39;pupil_dilation&#39;: None,
                                        &#39;head_yaw&#39;: None
                                    }

                                    # Update debug display even on error
                                    update_gaze_debug_display()
                                    continue

                    # Store results - preserve existing analysis results
                    if st.session_state.analysis_results is None:
                        st.session_state.analysis_results = {}
                    st.session_state.analysis_results[&#34;gaze_results&#34;] = gaze_results

                    # Store global heatmap separately (only once)
                    if global_heatmap_data is not None:
                        st.session_state.analysis_results[&#34;pupil_heatmap_global&#34;] = global_heatmap_data

                    # Show summary
                    successful_junctions = len([k for k, v in gaze_results.items() if v is not None])
                    total_junctions = len(st.session_state.junctions)
                    if successful_junctions == total_junctions:
                        st.success(f&#34;✅ Gaze analysis completed successfully for all {total_junctions} junctions!&#34;)
                    else:
                        st.warning(f&#34;⚠️ Gaze analysis completed for {successful_junctions}/{total_junctions} junctions&#34;)

                    # Generate CLI command for easy copying
                    # Build results dict for CLI command generation
                    gaze_results_dict = {}
                    for i, junction in enumerate(st.session_state.junctions):
                        junction_key = f&#34;junction_{i}&#34;
                        gaze_results_dict[junction_key] = {
                            &#34;junction&#34;: junction,
                            &#34;r_outer&#34;: st.session_state.junction_r_outer.get(i, 50.0),
                            &#34;decision_mode&#34;: decision_mode,
                            &#34;path_length&#34;: decision_params.get(&#34;path_length&#34;, 100.0) if decision_params else 100.0,
                            &#34;epsilon&#34;: decision_params.get(&#34;epsilon&#34;, 0.05) if decision_params else 0.05,
                            &#34;linger_delta&#34;: decision_params.get(&#34;linger_delta&#34;, 5.0) if decision_params else 5.0,
                        }
                    self.generate_cli_command(&#34;gaze&#34;, gaze_results_dict, cluster_method, cluster_params, decision_mode, decision_params)

                elif analysis_type == &#34;predict&#34;:
                    # Run prediction analysis using spatial tracking only
                    # Create output directory for prediction results
                    import os
                    output_dir = &#34;gui_outputs&#34;
                    os.makedirs(output_dir, exist_ok=True)

                    # Skip discover_decision_chain - use spatial tracking only
                    # Create empty chain_df for compatibility
                    import pandas as pd
                    chain_df = pd.DataFrame(columns=[&#39;trajectory&#39;])
                    chain_df[&#39;trajectory&#39;] = [i for i in range(len(st.session_state.trajectories))]

                    print(f&#34;🔍 DEBUG: Using spatial tracking only - skipping discover_decision_chain&#34;)
                    print(f&#34;🔍 DEBUG: Created empty chain_df with {len(chain_df)} trajectories&#34;)

                    # Define r_outer_list for predict analysis
                    r_outer_list = [st.session_state.junction_r_outer.get(i, 50.0) for i in range(len(st.session_state.junctions))]

                    print(f&#34;🔍 DEBUG: Starting analyze_junction_choice_patterns with spatial tracking only...&#34;)

                    # Debug: Check what trajectories visit multiple junctions
                    print(f&#34;\n🔍 DEBUG: Analyzing trajectory junction visits...&#34;)
                    multi_junction_trajectories = 0
                    consecutive_junction_trajectories = 0

                    # COMMENTED OUT: This code referenced norm_df which we removed
                    # for idx, row in norm_df.iterrows():
                    #     traj_id = row[&#39;trajectory&#39;]
                    #     visited_junctions = []
                    #     for i in range(7):  # 7 junctions
                    #         col = f&#39;branch_j{i}&#39;
                    #         if pd.notna(row[col]) and row[col] &gt;= 0:  # Valid branch assignment
                    #             visited_junctions.append(i)
                    #
                    #     if len(visited_junctions) &gt; 1:
                    #         multi_junction_trajectories += 1
                    #         # Check if junctions are consecutive (for flow analysis)
                    #         if len(visited_junctions) &gt;= 2:
                    #             consecutive_junction_trajectories += 1
                    #             if consecutive_junction_trajectories &lt;= 5:  # Show first 5 examples
                    #                 print(f&#34;🔍 DEBUG: Trajectory {traj_id} visits junctions: {visited_junctions}&#34;)

                    print(f&#34;🔍 DEBUG: Trajectories visiting multiple junctions: {multi_junction_trajectories}&#34;)
                    print(f&#34;🔍 DEBUG: Trajectories with consecutive visits: {consecutive_junction_trajectories}&#34;)

                    # Debug: Check r_outer_list values
                    print(f&#34;\n🔍 DEBUG: r_outer_list values: {r_outer_list}&#34;)
                    print(f&#34;🔍 DEBUG: r_outer_list length: {len(r_outer_list)}&#34;)
                    print(f&#34;🔍 DEBUG: r_outer_list types: {[type(r) for r in r_outer_list]}&#34;)

                    # Debug: Check junction radii
                    print(f&#34;\n🔍 DEBUG: Junction radii:&#34;)
                    for i, junction in enumerate(st.session_state.junctions):
                        print(f&#34;  Junction {i}: radius={junction.r}, r_outer={r_outer_list[i] if i &lt; len(r_outer_list) else &#39;N/A&#39;}&#34;)

                    # Debug: Test trajectory sequence tracking with multiple trajectories
                    print(f&#34;\n🔍 DEBUG: Testing trajectory sequence tracking...&#34;)
                    from verta.verta_plotting import _track_trajectory_junction_sequence

                    # Test trajectories 1-3 instead of trajectory 0 (which has limited range)
                    test_trajectories = st.session_state.trajectories[1:4]  # Trajectories 1-3

                    for test_idx, test_traj in enumerate(test_trajectories):
                        traj_id = getattr(test_traj, &#39;tid&#39;, test_idx + 1)
                        print(f&#34;\n🔍 DEBUG: === Testing Trajectory {traj_id} ===&#34;)

                        # Debug: Check trajectory data
                        print(f&#34;🔍 DEBUG: Trajectory {traj_id} data:&#34;)
                        print(f&#34;  - Length: {len(test_traj.x)} points&#34;)
                        print(f&#34;  - X range: {min(test_traj.x):.2f} to {max(test_traj.x):.2f}&#34;)
                        print(f&#34;  - Z range: {min(test_traj.z):.2f} to {max(test_traj.z):.2f}&#34;)

                        # Debug: Check junction data
                        print(f&#34;🔍 DEBUG: Junction data:&#34;)
                        for i, junction in enumerate(st.session_state.junctions):
                            print(f&#34;  Junction {i}: center=({junction.cx:.2f}, {junction.cz:.2f}), radius={junction.r}, r_outer={r_outer_list[i]}&#34;)

                        # Test spatial tracking
                        test_sequence = _track_trajectory_junction_sequence(test_traj, st.session_state.junctions, r_outer_list)
                        print(f&#34;🔍 DEBUG: Trajectory {traj_id} sequence: {test_sequence}&#34;)

                        if len(test_sequence) &gt; 0:
                            print(f&#34;🔍 DEBUG: ✅ Trajectory {traj_id} has valid sequence!&#34;)
                            print(f&#34;🔍 DEBUG: Stopping debug testing - spatial tracking is working!&#34;)
                            break
                        else:
                            print(f&#34;🔍 DEBUG: ❌ Trajectory {traj_id} has no sequence&#34;)

                    # If no trajectories worked, test trajectory 0 as fallback
                    if all(len(_track_trajectory_junction_sequence(traj, st.session_state.junctions, r_outer_list)) == 0 for traj in test_trajectories):
                        print(f&#34;\n🔍 DEBUG: === Testing Trajectory 0 as fallback ===&#34;)
                        test_traj = st.session_state.trajectories[0]
                        test_sequence = _track_trajectory_junction_sequence(test_traj, st.session_state.junctions, r_outer_list)
                        print(f&#34;🔍 DEBUG: Trajectory 0 sequence: {test_sequence}&#34;)

                    # Debug: Show sample of norm_df data
                    # print(f&#34;\n🔍 DEBUG: Sample norm_df data (first 10 rows):&#34;)
                    # print(norm_df.head(10))

                    # Then analyze junction choice patterns
                    print(&#34;\n&#34; + &#34;=&#34;*60)
                    print(&#34;🔍 PREDICT ANALYSIS - FLOW GRAPH DEBUG OUTPUT&#34;)
                    print(&#34;=&#34;*60)

                    # Run prediction analysis using spatial tracking only
                    results = analyze_junction_choice_patterns(
                        trajectories=st.session_state.trajectories,
                        chain_df=chain_df,  # Empty chain_df for compatibility
                        junctions=st.session_state.junctions,
                        output_dir=output_dir,
                        r_outer_list=r_outer_list,
                        gui_mode=False  # Enable console debug output
                    )

                    print(&#34;=&#34;*60)
                    print(&#34;✅ Predict analysis completed&#34;)
                    print(&#34;=&#34;*60 + &#34;\n&#34;)

                    # Store results - preserve existing analysis results
                    if st.session_state.analysis_results is None:
                        st.session_state.analysis_results = {}
                    st.session_state.analysis_results[&#34;predictions&#34;] = results

                    # Generate CLI command for easy copying
                    self.generate_cli_command(&#34;predict&#34;, results, cluster_method, cluster_params, decision_mode, decision_params)

                elif analysis_type == &#34;intent&#34;:
                    # Run intent recognition analysis
                    import pandas as pd
                    import numpy as np
                    import os

                    st.info(&#34;🧠 Running Intent Recognition Analysis...&#34;)

                    # Get parameters
                    intent_params = st.session_state.get(&#39;intent_params&#39;, {
                        &#39;prediction_distances&#39;: [100.0, 75.0, 50.0, 25.0],
                        &#39;model_type&#39;: &#39;random_forest&#39;,
                        &#39;cv_folds&#39;: 5,
                        &#39;test_split&#39;: 0.2
                    })

                    # Check if we have time data
                    has_time = all(tr.t is not None and len(tr.t) &gt; 0 for tr in st.session_state.trajectories[:5])
                    if not has_time:
                        st.warning(&#34;⚠️ Time data not detected. Intent recognition requires temporal information for velocity/acceleration features.&#34;)
                        st.info(&#34;💡 Tip: Ensure your CSV files have a time column specified in column mapping.&#34;)

                    # Check for sklearn
                    try:
                        import sklearn
                    except ImportError:
                        st.error(&#34;❌ scikit-learn not installed!&#34;)
                        st.markdown(&#34;&#34;&#34;
                        Intent recognition requires scikit-learn. Install with:
                        ```bash
                        pip install scikit-learn
                        ```
                        &#34;&#34;&#34;)
                        return

                    # Create output directory
                    output_dir = &#34;gui_outputs/intent_recognition&#34;
                    os.makedirs(output_dir, exist_ok=True)

                    # For each junction, run intent recognition
                    intent_results = {}

                    progress_bar = st.progress(0)
                    status_text = st.empty()

                    for junction_idx, junction in enumerate(st.session_state.junctions):
                        status_text.text(f&#34;Analyzing junction {junction_idx + 1}/{len(st.session_state.junctions)}...&#34;)

                        try:
                            # Create junction output directory
                            junction_output = os.path.join(output_dir, f&#34;junction_{junction_idx}&#34;)
                            os.makedirs(junction_output, exist_ok=True)

                            # Try to load existing branch assignments from previous Discover analysis
                            assignments_df = None
                            centers = None

                            # Check if we have results from a previous analysis
                            if st.session_state.analysis_results and &#39;branches&#39; in st.session_state.analysis_results:
                                branch_results = st.session_state.analysis_results[&#39;branches&#39;]
                                junction_key = f&#34;junction_{junction_idx}&#34;

                                if junction_key in branch_results:
                                    assignments_df = branch_results[junction_key].get(&#39;assignments&#39;)
                                    centers = branch_results[junction_key].get(&#39;centers&#39;)

                                    if assignments_df is not None:
                                        st.info(f&#34;📋 Using existing branch assignments from previous Discover analysis for Junction {junction_idx}&#34;)

                            # If no existing assignments, run discovery
                            if assignments_df is None:
                                st.warning(f&#34;⚠️ No existing branch assignments found for Junction {junction_idx}&#34;)
                                st.info(f&#34;🔍 Running branch discovery with default parameters...&#34;)
                                st.info(&#34;💡 **Tip:** Run &#39;Discover Branches&#39; analysis first to control clustering parameters!&#34;)

                                r_outer = st.session_state.junction_r_outer.get(junction_idx, 50.0)

                                # Run branch discovery with default parameters
                                assignments_df, summary_df, centers = discover_branches(
                                    trajectories=st.session_state.trajectories,
                                    junction=junction,
                                    k=3,
                                    decision_mode=&#34;hybrid&#34;,
                                    r_outer=r_outer,
                                    path_length=100.0,
                                    epsilon=0.05,
                                    cluster_method=&#34;auto&#34;,
                                    out_dir=junction_output
                                )

                            # Filter valid branches (&gt;= 0)
                            valid_assignments = assignments_df[assignments_df[&#39;branch&#39;] &gt;= 0]

                            if len(valid_assignments) &lt; 10:
                                st.warning(f&#34;⚠️ Junction {junction_idx}: Insufficient valid trajectories ({len(valid_assignments)}). Skipping intent analysis.&#34;)
                                intent_results[f&#34;junction_{junction_idx}&#34;] = {
                                    &#39;error&#39;: &#39;insufficient_data&#39;,
                                    &#39;n_valid_trajectories&#39;: len(valid_assignments)
                                }
                                continue

                            # Count unique branches
                            n_branches = len(assignments_df[assignments_df[&#39;branch&#39;] &gt;= 0][&#39;branch&#39;].unique())
                            st.success(f&#34;✅ Using {n_branches} branches with {len(valid_assignments)} valid trajectories&#34;)

                            # Run intent recognition
                            st.info(f&#34;🤖 Training intent recognition models...&#34;)

                            results = analyze_intent_recognition(
                                trajectories=st.session_state.trajectories,
                                junction=junction,
                                actual_branches=assignments_df,
                                output_dir=junction_output,
                                prediction_distances=intent_params[&#39;prediction_distances&#39;],
                                previous_choices=None  # TODO: Could add multi-junction support
                            )

                            if &#39;error&#39; in results:
                                st.error(f&#34;❌ Junction {junction_idx}: {results[&#39;error&#39;]}&#34;)
                                intent_results[f&#34;junction_{junction_idx}&#34;] = results
                            else:
                                st.success(f&#34;✅ Junction {junction_idx}: Intent recognition complete!&#34;)

                                # Display quick summary
                                models_trained = results[&#39;training_results&#39;].get(&#39;models_trained&#39;, {})
                                if models_trained:
                                    avg_acc = np.mean([m[&#39;cv_mean_accuracy&#39;] for m in models_trained.values()])
                                    st.metric(f&#34;Junction {junction_idx} Avg Accuracy&#34;, f&#34;{avg_acc:.1%}&#34;)

                                intent_results[f&#34;junction_{junction_idx}&#34;] = results

                        except Exception as e:
                            st.error(f&#34;❌ Junction {junction_idx} failed: {str(e)}&#34;)
                            intent_results[f&#34;junction_{junction_idx}&#34;] = {
                                &#39;error&#39;: str(e),
                                &#39;error_type&#39;: type(e).__name__
                            }

                        progress_bar.progress((junction_idx + 1) / len(st.session_state.junctions))

                    status_text.text(&#34;✅ Intent recognition analysis complete!&#34;)
                    progress_bar.empty()

                    # Store results
                    if st.session_state.analysis_results is None:
                        st.session_state.analysis_results = {}
                    st.session_state.analysis_results[&#34;intent_recognition&#34;] = intent_results

                    # Display summary
                    st.markdown(&#34;### 📊 Intent Recognition Summary&#34;)

                    successful_junctions = [k for k, v in intent_results.items()
                                          if &#39;error&#39; not in v]

                    if successful_junctions:
                        st.success(f&#34;✅ Successfully analyzed {len(successful_junctions)}/{len(st.session_state.junctions)} junctions&#34;)

                        # Create summary table
                        summary_data = []
                        for junction_key in successful_junctions:
                            results = intent_results[junction_key]
                            models = results[&#39;training_results&#39;].get(&#39;models_trained&#39;, {})

                            for dist, model_info in models.items():
                                summary_data.append({
                                    &#39;Junction&#39;: junction_key.replace(&#39;junction_&#39;, &#39;J&#39;),
                                    &#39;Distance (units)&#39;: dist,
                                    &#39;Accuracy&#39;: f&#34;{model_info[&#39;cv_mean_accuracy&#39;]:.1%}&#34;,
                                    &#39;Std Dev&#39;: f&#34;±{model_info[&#39;cv_std_accuracy&#39;]:.1%}&#34;,
                                    &#39;Samples&#39;: model_info[&#39;n_samples&#39;]
                                })

                        if summary_data:
                            summary_df = pd.DataFrame(summary_data)
                            st.dataframe(summary_df, width=&#39;stretch&#39;)

                        # Show interpretation
                        st.markdown(&#34;#### 💡 Interpretation&#34;)
                        avg_accuracy = np.mean([float(d[&#39;Accuracy&#39;].strip(&#39;%&#39;)) / 100 for d in summary_data])

                        if avg_accuracy &gt; 0.85:
                            st.success(&#34;🟢 **Excellent Predictability**: User intent is highly predictable. Early intervention systems will be very effective!&#34;)
                        elif avg_accuracy &gt; 0.70:
                            st.info(&#34;🟡 **Good Predictability**: Clear patterns exist. Adaptive systems can benefit users.&#34;)
                        else:
                            st.warning(&#34;🔴 **Moderate Predictability**: Behavior is variable. Consider per-user models or additional features.&#34;)
                    else:
                        st.error(&#34;❌ Intent recognition failed for all junctions&#34;)

                    st.info(f&#34;📁 Detailed results saved to: {output_dir}&#34;)

                    # Generate CLI command for easy copying
                    # Build results dict for CLI command generation
                    intent_results_dict = {}
                    for i, junction in enumerate(st.session_state.junctions):
                        junction_key = f&#34;junction_{i}&#34;
                        intent_results_dict[junction_key] = {
                            &#34;junction&#34;: junction,
                            &#34;r_outer&#34;: st.session_state.junction_r_outer.get(i, 50.0),
                            &#34;decision_mode&#34;: decision_mode,
                            &#34;path_length&#34;: decision_params.get(&#34;path_length&#34;, 100.0) if decision_params else 100.0,
                            &#34;epsilon&#34;: decision_params.get(&#34;epsilon&#34;, 0.05) if decision_params else 0.05,
                            &#34;linger_delta&#34;: decision_params.get(&#34;linger_delta&#34;, 5.0) if decision_params else 5.0,
                            &#34;prediction_distances&#34;: intent_params.get(&#39;prediction_distances&#39;, [100.0, 75.0, 50.0, 25.0]),
                            &#34;model_type&#34;: intent_params.get(&#39;model_type&#39;, &#39;random_forest&#39;),
                            &#34;cv_folds&#34;: intent_params.get(&#39;cv_folds&#39;, 5),
                            &#34;test_split&#34;: intent_params.get(&#39;test_split&#39;, 0.2),
                        }
                    self.generate_cli_command(&#34;intent&#34;, intent_results_dict, cluster_method, cluster_params, decision_mode, decision_params)

                elif analysis_type == &#34;enhanced&#34;:
                    # Run enhanced analysis for evacuation planning and risk assessment
                    import os
                    output_dir = &#34;gui_outputs&#34;
                    os.makedirs(output_dir, exist_ok=True)

                    # First run discovery to get chain data
                    k_value = cluster_params.get(&#34;k&#34;, 3) if cluster_params else 3
                    min_samples = cluster_params.get(&#34;min_samples&#34;, 5) if cluster_params else 5
                    k_min = cluster_params.get(&#34;k_min&#34;, 2) if cluster_params else 2
                    k_max = cluster_params.get(&#34;k_max&#34;, 6) if cluster_params else 6
                    min_sep_deg = cluster_params.get(&#34;min_sep_deg&#34;, 12.0) if cluster_params else 12.0
                    angle_eps = cluster_params.get(&#34;angle_eps&#34;, 15.0) if cluster_params else 15.0

                    path_length = decision_params.get(&#34;path_length&#34;, 100.0) if decision_params else 100.0
                    epsilon = decision_params.get(&#34;epsilon&#34;, 0.05) if decision_params else 0.05
                    linger_delta = decision_params.get(&#34;linger_delta&#34;, 5.0) if decision_params else 5.0
                    r_outer_list = [st.session_state.junction_r_outer.get(i, 50.0) for i in range(len(st.session_state.junctions))]

                    # Run discovery first
                    chain_df, centers_list, decisions_chain_df = discover_decision_chain(
                        trajectories=st.session_state.trajectories,
                        junctions=st.session_state.junctions,
                        path_length=path_length,
                        epsilon=epsilon,
                        seed=seed,
                        decision_mode=discover_decision_mode,
                        r_outer_list=r_outer_list,
                        linger_delta=linger_delta,
                        out_dir=output_dir,
                        cluster_method=cluster_method,
                        k=k_value,
                        k_min=k_min,
                        k_max=k_max,
                        min_sep_deg=min_sep_deg,
                        angle_eps=angle_eps,
                        min_samples=min_samples,
                    )

                    # Run enhanced analysis
                    enhanced_results = self._run_enhanced_analysis(
                        trajectories=st.session_state.trajectories,
                        chain_df=chain_df,
                        junctions=st.session_state.junctions,
                        r_outer_list=r_outer_list,
                        centers_list=centers_list,
                        decisions_df=decisions_chain_df
                    )

                    # Store results
                    if st.session_state.analysis_results is None:
                        st.session_state.analysis_results = {}
                    st.session_state.analysis_results[&#34;enhanced&#34;] = enhanced_results

                #st.success(f&#34;✅ {analysis_type.capitalize()} analysis completed!&#34;)
                #st.rerun()

        except Exception as e:
            st.error(f&#34;❌ Analysis failed: {str(e)}&#34;)
            st.exception(e)

    def _run_enhanced_analysis(self, trajectories, chain_df, junctions, r_outer_list, centers_list, decisions_df):
        &#34;&#34;&#34;Run enhanced analysis for evacuation planning and risk assessment&#34;&#34;&#34;
        import numpy as np
        import pandas as pd
        import matplotlib.pyplot as plt
        import os

        results = {
            &#34;evacuation_analysis&#34;: {},
            &#34;recommendations&#34;: [],
            &#34;risk_assessment&#34;: {},
            &#34;efficiency_metrics&#34;: {}
        }

        # 1. Evacuation Analysis - Identify bottlenecks and optimal routes
        st.info(&#34;🚨 Running evacuation analysis...&#34;)
        evacuation_results = self._analyze_evacuation_patterns(
            trajectories, chain_df, junctions, r_outer_list, centers_list
        )
        results[&#34;evacuation_analysis&#34;] = evacuation_results

        # 2. Generate Recommendations
        st.info(&#34;💡 Generating recommendations...&#34;)
        recommendations = self._generate_recommendations(evacuation_results, chain_df, junctions)
        results[&#34;recommendations&#34;] = recommendations

        # 3. Risk Assessment
        st.info(&#34;⚠️ Assessing risks...&#34;)
        risk_results = self._assess_risks(trajectories, chain_df, junctions, r_outer_list)
        results[&#34;risk_assessment&#34;] = risk_results

        # 4. Efficiency Metrics
        st.info(&#34;📊 Computing efficiency metrics...&#34;)
        efficiency_results = self._compute_efficiency_metrics(trajectories, chain_df, junctions, r_outer_list)
        results[&#34;efficiency_metrics&#34;] = efficiency_results

        # Save enhanced analysis results to CSV files
        try:
            enhanced_data_dir = os.path.join(&#34;gui_outputs&#34;, &#34;enhanced_analysis&#34;)
            os.makedirs(enhanced_data_dir, exist_ok=True)

            # Save evacuation analysis results
            if evacuation_results[&#34;bottlenecks&#34;]:
                bottlenecks_df = pd.DataFrame(evacuation_results[&#34;bottlenecks&#34;])
                bottlenecks_file = os.path.join(enhanced_data_dir, &#34;evacuation_bottlenecks.csv&#34;)
                bottlenecks_df.to_csv(bottlenecks_file, index=False)
                st.info(f&#34;📁 Evacuation bottlenecks saved to: {bottlenecks_file}&#34;)

            if evacuation_results[&#34;optimal_routes&#34;]:
                optimal_routes_df = pd.DataFrame(evacuation_results[&#34;optimal_routes&#34;])
                optimal_routes_file = os.path.join(enhanced_data_dir, &#34;optimal_routes.csv&#34;)
                optimal_routes_df.to_csv(optimal_routes_file, index=False)
                st.info(f&#34;📁 Optimal routes saved to: {optimal_routes_file}&#34;)

            # Save flow analysis results
            if evacuation_results[&#34;flow_analysis&#34;]:
                flow_data = []
                for junction_key, flow_info in evacuation_results[&#34;flow_analysis&#34;].items():
                    flow_data.append({
                        &#34;junction&#34;: junction_key,
                        &#34;total_trajectories&#34;: flow_info[&#34;total_trajectories&#34;],
                        &#34;entropy&#34;: flow_info[&#34;entropy&#34;],
                        &#34;branch_distribution&#34;: str(flow_info[&#34;branch_distribution&#34;])
                    })
                flow_df = pd.DataFrame(flow_data)
                flow_file = os.path.join(enhanced_data_dir, &#34;flow_analysis.csv&#34;)
                flow_df.to_csv(flow_file, index=False)
                st.info(f&#34;📁 Flow analysis saved to: {flow_file}&#34;)

            # Save recommendations
            if recommendations:
                recommendations_df = pd.DataFrame(recommendations)
                recommendations_file = os.path.join(enhanced_data_dir, &#34;recommendations.csv&#34;)
                recommendations_df.to_csv(recommendations_file, index=False)
                st.info(f&#34;📁 Recommendations saved to: {recommendations_file}&#34;)

            # Save risk assessment results
            if risk_results[&#34;high_risk_junctions&#34;]:
                risk_df = pd.DataFrame(risk_results[&#34;high_risk_junctions&#34;])
                risk_file = os.path.join(enhanced_data_dir, &#34;risk_assessment.csv&#34;)
                risk_df.to_csv(risk_file, index=False)
                st.info(f&#34;📁 Risk assessment saved to: {risk_file}&#34;)

            # Save efficiency metrics
            if efficiency_results:
                efficiency_data = []
                for metric_name, metric_value in efficiency_results.items():
                    if isinstance(metric_value, dict):
                        for key, value in metric_value.items():
                            efficiency_data.append({
                                &#34;metric_category&#34;: metric_name,
                                &#34;metric_name&#34;: key,
                                &#34;value&#34;: value
                            })
                    else:
                        efficiency_data.append({
                            &#34;metric_category&#34;: &#34;overall&#34;,
                            &#34;metric_name&#34;: metric_name,
                            &#34;value&#34;: metric_value
                        })

                if efficiency_data:
                    efficiency_df = pd.DataFrame(efficiency_data)
                    efficiency_file = os.path.join(enhanced_data_dir, &#34;efficiency_metrics.csv&#34;)
                    efficiency_df.to_csv(efficiency_file, index=False)
                    st.info(f&#34;📁 Efficiency metrics saved to: {efficiency_file}&#34;)

            # Save overall enhanced analysis summary
            summary_data = {
                &#34;overall_risk_score&#34;: risk_results.get(&#34;overall_risk_score&#34;, 0),
                &#34;total_bottlenecks&#34;: len(evacuation_results[&#34;bottlenecks&#34;]),
                &#34;total_optimal_routes&#34;: len(evacuation_results[&#34;optimal_routes&#34;]),
                &#34;total_recommendations&#34;: len(recommendations),
                &#34;high_risk_junctions_count&#34;: len(risk_results.get(&#34;high_risk_junctions&#34;, [])),
                &#34;analysis_timestamp&#34;: pd.Timestamp.now().isoformat()
            }

            summary_df = pd.DataFrame([summary_data])
            summary_file = os.path.join(enhanced_data_dir, &#34;enhanced_analysis_summary.csv&#34;)
            summary_df.to_csv(summary_file, index=False)
            st.info(f&#34;📁 Enhanced analysis summary saved to: {summary_file}&#34;)

        except Exception as e:
            st.warning(f&#34;⚠️ Could not save enhanced analysis files: {e}&#34;)

        return results

    def _analyze_evacuation_patterns(self, trajectories, chain_df, junctions, r_outer_list, centers_list):
        &#34;&#34;&#34;Analyze evacuation patterns and identify bottlenecks&#34;&#34;&#34;
        import numpy as np
        import pandas as pd

        evacuation_results = {
            &#34;bottlenecks&#34;: [],
            &#34;optimal_routes&#34;: [],
            &#34;flow_analysis&#34;: {},
            &#34;capacity_analysis&#34;: {}
        }

        # Analyze flow patterns for each junction
        for i, junction in enumerate(junctions):
            branch_col = f&#34;branch_j{i}&#34;
            if branch_col in chain_df.columns:
                junction_assignments = chain_df[[&#39;trajectory&#39;, branch_col]].copy()
                junction_assignments = junction_assignments.rename(columns={branch_col: &#39;branch&#39;})
                junction_assignments = junction_assignments[junction_assignments[&#39;branch&#39;] &gt;= 0]

                # Calculate branch flow rates
                branch_counts = junction_assignments[&#39;branch&#39;].value_counts()
                total_trajectories = len(junction_assignments)

                # Identify bottlenecks (branches with high concentration)
                for branch, count in branch_counts.items():
                    concentration = count / total_trajectories
                    if concentration &gt; 0.6:  # More than 60% use same route
                        evacuation_results[&#34;bottlenecks&#34;].append({
                            &#34;junction&#34;: i,
                            &#34;branch&#34;: branch,
                            &#34;concentration&#34;: concentration,
                            &#34;trajectory_count&#34;: count,
                            &#34;risk_level&#34;: &#34;HIGH&#34; if concentration &gt; 0.8 else &#34;MEDIUM&#34;
                        })

                # Identify optimal routes (balanced distribution)
                if len(branch_counts) &gt; 1:
                    entropy = -sum((count/total_trajectories) * np.log2(count/total_trajectories)
                                for count in branch_counts.values)
                    max_entropy = np.log2(len(branch_counts))
                    balance_ratio = entropy / max_entropy

                    if balance_ratio &gt; 0.7:  # Well-balanced distribution
                        evacuation_results[&#34;optimal_routes&#34;].append({
                            &#34;junction&#34;: i,
                            &#34;balance_ratio&#34;: balance_ratio,
                            &#34;entropy&#34;: entropy,
                            &#34;branch_count&#34;: len(branch_counts)
                        })

                evacuation_results[&#34;flow_analysis&#34;][f&#34;junction_{i}&#34;] = {
                    &#34;total_trajectories&#34;: total_trajectories,
                    &#34;branch_distribution&#34;: branch_counts.to_dict(),
                    &#34;entropy&#34;: entropy if len(branch_counts) &gt; 1 else 0
                }

        return evacuation_results

    def _generate_recommendations(self, evacuation_results, chain_df, junctions):
        &#34;&#34;&#34;Generate actionable recommendations based on analysis&#34;&#34;&#34;
        recommendations = []

        # Recommendations based on bottlenecks
        for bottleneck in evacuation_results[&#34;bottlenecks&#34;]:
            if bottleneck[&#34;risk_level&#34;] == &#34;HIGH&#34;:
                recommendations.append({
                    &#34;type&#34;: &#34;Signage&#34;,
                    &#34;priority&#34;: &#34;HIGH&#34;,
                    &#34;junction&#34;: bottleneck[&#34;junction&#34;],
                    &#34;message&#34;: f&#34;Add directional signage at Junction {bottleneck[&#39;junction&#39;]} to distribute traffic away from Branch {int(bottleneck[&#39;branch&#39;])} (currently {bottleneck[&#39;concentration&#39;]:.1%} of traffic)&#34;
                })
                recommendations.append({
                    &#34;type&#34;: &#34;Route Modification&#34;,
                    &#34;priority&#34;: &#34;HIGH&#34;,
                    &#34;junction&#34;: bottleneck[&#34;junction&#34;],
                    &#34;message&#34;: f&#34;Consider widening or adding alternative routes at Junction {bottleneck[&#39;junction&#39;]} to reduce bottleneck risk&#34;
                })

        # Recommendations based on optimal routes
        for route in evacuation_results[&#34;optimal_routes&#34;]:
            recommendations.append({
                &#34;type&#34;: &#34;Maintenance&#34;,
                &#34;priority&#34;: &#34;LOW&#34;,
                &#34;junction&#34;: route[&#34;junction&#34;],
                &#34;message&#34;: f&#34;Junction {route[&#39;junction&#39;]} shows good traffic distribution (balance ratio: {route[&#39;balance_ratio&#39;]:.2f}) - maintain current design&#34;
            })

        # General recommendations
        if len(evacuation_results[&#34;bottlenecks&#34;]) &gt; len(junctions) * 0.5:
            recommendations.append({
                &#34;type&#34;: &#34;System-wide&#34;,
                &#34;priority&#34;: &#34;MEDIUM&#34;,
                &#34;junction&#34;: &#34;ALL&#34;,
                &#34;message&#34;: &#34;High number of bottlenecks detected - consider system-wide evacuation route optimization&#34;
            })

        return recommendations

    def _assess_risks(self, trajectories, chain_df, junctions, r_outer_list):
        &#34;&#34;&#34;Assess potential safety risks in flow patterns&#34;&#34;&#34;
        risk_results = {
            &#34;high_risk_junctions&#34;: [],
            &#34;overall_risk_score&#34;: 0
        }

        total_risk_score = 0

        for i, junction in enumerate(junctions):
            branch_col = f&#34;branch_j{i}&#34;
            if branch_col in chain_df.columns:
                junction_assignments = chain_df[[&#39;trajectory&#39;, branch_col]].copy()
                junction_assignments = junction_assignments.rename(columns={branch_col: &#39;branch&#39;})
                junction_assignments = junction_assignments[junction_assignments[&#39;branch&#39;] &gt;= 0]

                branch_counts = junction_assignments[&#39;branch&#39;].value_counts()
                total_trajectories = len(junction_assignments)

                # Calculate unified risk factors
                risk_factors = []
                risk_score = 0.0

                # 1. Concentration Risk (0.0-1.0)
                if len(branch_counts) &gt; 0:
                    max_concentration = branch_counts.max() / total_trajectories
                    if max_concentration &gt; 0.7:
                        concentration_risk = (max_concentration - 0.7) / 0.3  # Scale 0.7-1.0 to 0.0-1.0
                        risk_factors.append((&#34;high_concentration&#34;, concentration_risk))
                        risk_score += concentration_risk

                # 2. Diversity Risk (0.0-1.0)
                if len(branch_counts) &lt; 2:
                    diversity_risk = 1.0
                    risk_factors.append((&#34;low_diversity&#34;, diversity_risk))
                    risk_score += diversity_risk
                elif len(branch_counts) == 2:
                    diversity_risk = 0.3  # Moderate risk for only 2 routes
                    risk_factors.append((&#34;limited_diversity&#34;, diversity_risk))
                    risk_score += diversity_risk

                # 3. Crowding Risk (0.0-1.0)
                if total_trajectories &gt; 50:
                    if total_trajectories &gt; 100:
                        crowding_risk = 1.0  # High crowding
                        risk_factors.append((&#34;high_crowding&#34;, crowding_risk))
                    else:
                        crowding_risk = (total_trajectories - 50) / 50  # Scale 50-100 to 0.0-1.0
                        risk_factors.append((&#34;moderate_crowding&#34;, crowding_risk))
                    risk_score += crowding_risk

                # Normalize total risk score to 0-1 scale
                # Max possible: 1.0 (concentration) + 1.0 (diversity) + 1.0 (crowding) = 3.0
                risk_score = min(risk_score / 3.0, 1.0)
                total_risk_score += risk_score

                # Classify risk level
                if risk_score &gt;= 0.7:
                    risk_level = &#34;HIGH&#34;
                elif risk_score &gt;= 0.4:
                    risk_level = &#34;MEDIUM&#34;
                else:
                    risk_level = &#34;LOW&#34;

                # Include all junctions with risk score &gt;= 0.4 for comprehensive assessment
                if risk_score &gt;= 0.4:
                    risk_results[&#34;high_risk_junctions&#34;].append({
                        &#34;junction&#34;: i,
                        &#34;risk_score&#34;: risk_score,
                        &#34;risk_level&#34;: risk_level,
                        &#34;risk_factors&#34;: risk_factors,
                        &#34;trajectory_count&#34;: total_trajectories,
                        &#34;concentration&#34;: max_concentration if len(branch_counts) &gt; 0 else 0,
                        &#34;route_count&#34;: len(branch_counts)
                    })

        # Calculate overall risk score
        risk_results[&#34;overall_risk_score&#34;] = total_risk_score / len(junctions) if junctions else 0

        return risk_results

    def _compute_efficiency_metrics(self, trajectories, chain_df, junctions, r_outer_list):
        &#34;&#34;&#34;Compute efficiency metrics for navigation&#34;&#34;&#34;
        efficiency_results = {
            &#34;average_travel_times&#34;: {},
            &#34;route_efficiency&#34;: {},
            &#34;capacity_utilization&#34;: {},
            &#34;overall_efficiency&#34;: 0
        }

        total_efficiency = 0

        for i, junction in enumerate(junctions):
            branch_col = f&#34;branch_j{i}&#34;
            if branch_col in chain_df.columns:
                junction_assignments = chain_df[[&#39;trajectory&#39;, branch_col]].copy()
                junction_assignments = junction_assignments.rename(columns={branch_col: &#39;branch&#39;})
                junction_assignments = junction_assignments[junction_assignments[&#39;branch&#39;] &gt;= 0]

                # Calculate efficiency metrics for this junction
                branch_counts = junction_assignments[&#39;branch&#39;].value_counts()
                total_trajectories = len(junction_assignments)

                # Route efficiency (entropy-based)
                if len(branch_counts) &gt; 1:
                    entropy = -sum((count/total_trajectories) * np.log2(count/total_trajectories)
                                for count in branch_counts.values)
                    max_entropy = np.log2(len(branch_counts))
                    route_efficiency = entropy / max_entropy
                else:
                    route_efficiency = 0

                # Capacity utilization
                capacity_utilization = total_trajectories / 100.0  # Assuming capacity of 100
                capacity_utilization = min(capacity_utilization, 1.0)  # Cap at 100%

                efficiency_results[&#34;route_efficiency&#34;][f&#34;junction_{i}&#34;] = route_efficiency
                efficiency_results[&#34;capacity_utilization&#34;][f&#34;junction_{i}&#34;] = capacity_utilization

                total_efficiency += route_efficiency

        # Calculate overall efficiency
        efficiency_results[&#34;overall_efficiency&#34;] = total_efficiency / len(junctions) if junctions else 0

        return efficiency_results

    def render_visualization(self):
        &#34;&#34;&#34;Render the visualization interface&#34;&#34;&#34;
        st.markdown(&#39;&lt;h2 class=&#34;section-header&#34;&gt;📈 Visualization&lt;/h2&gt;&#39;, unsafe_allow_html=True)

        if not st.session_state.analysis_results:
            st.warning(&#34;⚠️ Please run an analysis first&#34;)
            return

        # Debug: Show what analysis results are available
        with st.expander(&#34;🔍 Debug: Available Analysis Results&#34;, expanded=False):
            if st.session_state.analysis_results is not None:
                st.write(&#34;Analysis results keys:&#34;, list(st.session_state.analysis_results.keys()))
            else:
                st.write(&#34;No analysis results available&#34;)

        # Show different visualizations based on analysis type
        # If multiple analysis types are available, let user choose
        # Prioritize &#34;branches&#34; as the default selection
        available_analyses = []

        # Add &#34;branches&#34; first if available (for default selection)
        if &#34;branches&#34; in st.session_state.analysis_results:
            available_analyses.append(&#34;branches&#34;)

        # Add other analysis types
        if st.session_state.analysis_results is not None:
            if &#34;metrics&#34; in st.session_state.analysis_results:
                available_analyses.append(&#34;metrics&#34;)
            if &#34;assignments&#34; in st.session_state.analysis_results:
                available_analyses.append(&#34;assignments&#34;)
        if &#34;predictions&#34; in st.session_state.analysis_results:
            available_analyses.append(&#34;predictions&#34;)
        if &#34;choice_patterns&#34; in st.session_state.analysis_results:
            available_analyses.append(&#34;choice_patterns&#34;)
        if &#34;intent_recognition&#34; in st.session_state.analysis_results:
            available_analyses.append(&#34;intent_recognition&#34;)
        if &#34;enhanced&#34; in st.session_state.analysis_results:
            available_analyses.append(&#34;enhanced&#34;)
        if &#34;gaze_results&#34; in st.session_state.analysis_results:
            available_analyses.append(&#34;gaze_results&#34;)

        if len(available_analyses) &gt; 1:
            # Multiple analysis types available - let user choose
            st.markdown(&#34;### Multiple Analysis Results Available&#34;)
            selected_analysis = st.selectbox(
                &#34;Choose analysis to visualize:&#34;,
                available_analyses,
                help=&#34;Select which analysis results to display&#34;
            )

            if selected_analysis == &#34;metrics&#34;:
                self.render_metrics_visualizations()
            elif selected_analysis == &#34;assignments&#34;:
                self.render_assign_visualizations()
            elif selected_analysis == &#34;branches&#34;:
                self.render_discover_visualizations()
            elif selected_analysis == &#34;predictions&#34;:
                self.render_predict_visualizations()
            elif selected_analysis == &#34;choice_patterns&#34;:
                self.render_flow_graphs()
                self.render_conditional_probabilities()
                self.render_pattern_analysis()
            elif selected_analysis == &#34;intent_recognition&#34;:
                self.render_intent_visualizations()
            elif selected_analysis == &#34;enhanced&#34;:
                self.render_enhanced_visualizations()
            elif selected_analysis == &#34;gaze_results&#34;:
                self.render_gaze_visualizations()
        else:
            # Single analysis type - show automatically
            if st.session_state.analysis_results is not None:
                if &#34;metrics&#34; in st.session_state.analysis_results:
                    self.render_metrics_visualizations()
                elif &#34;assignments&#34; in st.session_state.analysis_results:
                    self.render_assign_visualizations()
                elif &#34;branches&#34; in st.session_state.analysis_results:
                    self.render_discover_visualizations()
                    # Also show flow graphs if available
                    if &#34;flow_graph_map&#34; in st.session_state.analysis_results:
                        self.render_flow_graphs()
                elif &#34;predictions&#34; in st.session_state.analysis_results:
                    self.render_predict_visualizations()
                elif &#34;intent_recognition&#34; in st.session_state.analysis_results:
                    self.render_intent_visualizations()
                elif &#34;enhanced&#34; in st.session_state.analysis_results:
                    self.render_enhanced_visualizations()
                elif &#34;gaze_results&#34; in st.session_state.analysis_results:
                    self.render_gaze_visualizations()
                else:
                    st.info(&#34;No visualizations available for this analysis type&#34;)
                    st.write(&#34;Available analysis results:&#34;, list(st.session_state.analysis_results.keys()))
            else:
                st.info(&#34;No analysis results available. Please run an analysis first.&#34;)

    def render_predict_visualizations(self):
        &#34;&#34;&#34;Render predict analysis visualizations&#34;&#34;&#34;
        st.markdown(&#34;### Predict Analysis Results&#34;)

        # Check if predict analysis results exist
        if (st.session_state.analysis_results is None or
            &#34;predictions&#34; not in st.session_state.analysis_results):
            st.info(&#34;No predict analysis results available. Run predict analysis first.&#34;)
            return

        predictions_data = st.session_state.analysis_results[&#34;predictions&#34;]

        # Display flow graphs
        st.markdown(&#34;#### Flow Graphs&#34;)
        col1, col2 = st.columns(2)

        with col1:
            st.markdown(&#34;##### Overall Flow Graph&#34;)
            flow_map_path = os.path.join(&#34;gui_outputs&#34;, &#34;Flow_Graph_Map.png&#34;)
            if os.path.exists(flow_map_path):
                st.image(flow_map_path, width=&#39;stretch&#39;)
            else:
                st.info(&#34;Flow graph map not available&#34;)

        with col2:
            st.markdown(&#34;##### Per-Junction Flow Graph&#34;)
            per_junction_path = os.path.join(&#34;gui_outputs&#34;, &#34;Per_Junction_Flow_Graph.png&#34;)
            if os.path.exists(per_junction_path):
                st.image(per_junction_path, width=&#39;stretch&#39;)
            else:
                st.info(&#34;Per-junction flow graph not available&#34;)

        # Display conditional probability heatmap
        st.markdown(&#34;#### Conditional Probability Analysis&#34;)
        heatmap_path = os.path.join(&#34;gui_outputs&#34;, &#34;conditional_probability_heatmap.png&#34;)
        if os.path.exists(heatmap_path):
            st.image(heatmap_path, width=&#39;stretch&#39;)
        else:
            st.info(&#34;Conditional probability heatmap not available&#34;)

        # Display behavioral pattern analysis
        st.markdown(&#34;#### Behavioral Pattern Distribution&#34;)
        pattern_path = os.path.join(&#34;gui_outputs&#34;, &#34;behavioral_patterns.png&#34;)
        if os.path.exists(pattern_path):
            st.image(pattern_path, width=&#39;stretch&#39;)
        else:
            st.info(&#34;Behavioral pattern analysis not available&#34;)

        # Display summary statistics
        if &#34;summary&#34; in predictions_data:
            st.markdown(&#34;#### Analysis Summary&#34;)
            summary = predictions_data[&#34;summary&#34;]

            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric(&#34;Total Trajectories&#34;, summary.get(&#34;total_trajectories&#34;, 0))
            with col2:
                st.metric(&#34;Total Junctions&#34;, summary.get(&#34;total_junctions&#34;, 0))
            with col3:
                st.metric(&#34;Total Transitions&#34;, summary.get(&#34;total_transitions&#34;, 0))
            with col4:
                st.metric(&#34;Unique Patterns&#34;, summary.get(&#34;unique_patterns&#34;, 0))

        # Interactive Junction Prediction Tool
        st.markdown(&#34;#### Interactive Junction Prediction&#34;)
        self._render_interactive_prediction_tool(predictions_data)

    def _render_interactive_prediction_tool(self, predictions_data):
        &#34;&#34;&#34;Render interactive junction prediction tool&#34;&#34;&#34;
        if &#34;conditional_probabilities&#34; not in predictions_data:
            st.info(&#34;No conditional probability data available for predictions.&#34;)
            return

        conditional_probs = predictions_data[&#34;conditional_probabilities&#34;]

        # Get available junctions from the conditional probabilities
        available_junctions = []
        for origin_key in conditional_probs.keys():
            junction_num = int(origin_key.split(&#39;_&#39;)[1][1:])  # Extract from &#34;from_J0&#34;
            available_junctions.append(junction_num)

        available_junctions = sorted(set(available_junctions))

        if not available_junctions:
            st.info(&#34;No junction data available for predictions.&#34;)
            return

        st.markdown(&#34;Select a decision junction and analyze probabilities for connected junctions:&#34;)

        col1, col2 = st.columns(2)

        with col1:
            # Decision junction selection
            decision_junction = st.selectbox(
                &#34;Decision Junction&#34;,
                options=available_junctions,
                format_func=lambda x: f&#34;J{x}&#34;,
                key=&#34;prediction_decision_junction&#34;
            )

        with col2:
            # Direction selection
            direction = st.selectbox(
                &#34;Analysis Direction&#34;,
                options=[&#34;Predecessor Analysis&#34;, &#34;Successor Analysis&#34;],
                key=&#34;prediction_direction&#34;
            )

        # Get connected junctions based on direction
        if direction == &#34;Predecessor Analysis&#34;:
            # Find junctions that lead TO the decision junction
            connected_junctions = []
            for origin_key, destinations in conditional_probs.items():
                origin_num = int(origin_key.split(&#39;_&#39;)[1][1:])
                if f&#34;J{decision_junction}&#34; in destinations:
                    connected_junctions.append(origin_num)

            if not connected_junctions:
                st.info(f&#34;No predecessors found for J{decision_junction}&#34;)
                return

            # Predecessor selection
            predecessor = st.selectbox(
                &#34;Select Predecessor Junction&#34;,
                options=sorted(connected_junctions),
                format_func=lambda x: f&#34;J{x}&#34;,
                key=&#34;prediction_predecessor&#34;
            )

            # Calculate probabilities
            self._calculate_predecessor_probabilities(conditional_probs, decision_junction, predecessor)

        else:  # Successor Analysis
            # Find junctions that the decision junction leads TO
            origin_key = f&#34;from_J{decision_junction}&#34;
            if origin_key not in conditional_probs:
                st.info(f&#34;No successors found for J{decision_junction}&#34;)
                return

            destinations = conditional_probs[origin_key]
            connected_junctions = [int(dest[1:]) for dest in destinations.keys()]

            if not connected_junctions:
                st.info(f&#34;No successors found for J{decision_junction}&#34;)
                return

            # Successor selection
            successor = st.selectbox(
                &#34;Select Successor Junction&#34;,
                options=sorted(connected_junctions),
                format_func=lambda x: f&#34;J{x}&#34;,
                key=&#34;prediction_successor&#34;
            )

            # Calculate probabilities
            self._calculate_successor_probabilities(conditional_probs, decision_junction, successor)

    def _calculate_predecessor_probabilities(self, conditional_probs, decision_junction, predecessor):
        &#34;&#34;&#34;Calculate probabilities for predecessor analysis - what happens AFTER decision junction when coming FROM predecessor&#34;&#34;&#34;
        st.markdown(f&#34;### Analysis: J{predecessor} → J{decision_junction} → ?&#34;)
        st.markdown(f&#34;**Question**: What junctions do trajectories visit AFTER J{decision_junction} when they came FROM J{predecessor}? (excluding self-loops)**&#34;)

        # Get cached sequences from analysis results
        if &#34;cached_sequences&#34; not in st.session_state.analysis_results.get(&#34;predictions&#34;, {}):
            st.error(&#34;No trajectory sequence data available. Please rerun the predict analysis.&#34;)
            return

        cached_sequences = st.session_state.analysis_results[&#34;predictions&#34;][&#34;cached_sequences&#34;]

        # Find trajectories that follow the J{predecessor} → J{decision_junction} sequence
        relevant_trajectories = []
        successor_counts = {}

        for traj_idx, sequence in cached_sequences.items():
            # Check if this trajectory follows the predecessor → decision sequence
            for i in range(len(sequence) - 1):
                if sequence[i] == predecessor and sequence[i + 1] == decision_junction:
                    relevant_trajectories.append(traj_idx)

                    # Find what happens after the decision junction (only count different junctions)
                    if i + 2 &lt; len(sequence):  # There&#39;s a junction after the decision junction
                        successor = sequence[i + 2]
                        # Only count as successor if it&#39;s a different junction (no self-loops)
                        if successor != decision_junction:
                            successor_counts[successor] = successor_counts.get(successor, 0) + 1
                    break

        if not relevant_trajectories:
            st.info(f&#34;No trajectories found that follow the J{predecessor} → J{decision_junction} sequence&#34;)
            return

        total_trajectories = len(relevant_trajectories)
        st.markdown(f&#34;**Found {total_trajectories} trajectories that follow J{predecessor} → J{decision_junction}**&#34;)

        if not successor_counts:
            st.info(f&#34;None of these trajectories continue to another junction after J{decision_junction}&#34;)
            return

        # Calculate probabilities
        successor_probs = {}
        for successor, count in successor_counts.items():
            prob = (count / total_trajectories) * 100
            successor_probs[f&#34;J{successor}&#34;] = prob

        # Create visualization
        import matplotlib.pyplot as plt
        import pandas as pd

        successor_names = list(successor_probs.keys())
        probabilities = list(successor_probs.values())

        fig, ax = plt.subplots(figsize=(10, 6))
        bars = ax.bar(successor_names, probabilities, color=&#39;skyblue&#39;, alpha=0.7)

        ax.set_xlabel(&#39;Successor Junction&#39;)
        ax.set_ylabel(&#39;Probability (%)&#39;)
        ax.set_title(f&#39;Direct Successor Probabilities: J{predecessor} → J{decision_junction} → ?&#39;)
        ax.set_ylim(0, max(probabilities) * 1.1)

        # Add value labels on bars
        for bar, prob in zip(bars, probabilities):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                   f&#39;{prob:.1f}%&#39;, ha=&#39;center&#39;, va=&#39;bottom&#39;)

        plt.xticks(rotation=45)
        plt.tight_layout()
        st.pyplot(fig)

        # Show detailed table
        st.markdown(&#34;#### Detailed Analysis:&#34;)
        prob_data = []
        for successor, prob in successor_probs.items():
            count = successor_counts[int(successor[1:])]
            prob_data.append({
                &#39;Successor Junction&#39;: successor,
                &#39;Trajectory Count&#39;: count,
                &#39;Probability (%)&#39;: f&#34;{prob:.1f}&#34;,
                &#39;Sequence&#39;: f&#34;J{predecessor} → J{decision_junction} → {successor}&#34;
            })

        df = pd.DataFrame(prob_data)
        st.dataframe(df, width=&#39;stretch&#39;)

        # Show trajectory examples
        st.markdown(&#34;#### Example Trajectory Sequences:&#34;)
        example_count = 0
        for traj_idx in relevant_trajectories[:5]:  # Show first 5 examples
            sequence = cached_sequences[traj_idx]
            seq_str = &#34; → &#34;.join([f&#34;J{j}&#34; for j in sequence])
            st.write(f&#34;**Trajectory {traj_idx}**: {seq_str}&#34;)
            example_count += 1

        if len(relevant_trajectories) &gt; 5:
            st.write(f&#34;... and {len(relevant_trajectories) - 5} more trajectories&#34;)

    def _calculate_successor_probabilities(self, conditional_probs, decision_junction, successor):
        &#34;&#34;&#34;Calculate probabilities for successor analysis - what happened BEFORE decision junction when going TO successor&#34;&#34;&#34;
        st.markdown(f&#34;### Analysis: ? → J{decision_junction} → J{successor}&#34;)
        st.markdown(f&#34;**Question**: What junctions did trajectories visit BEFORE J{decision_junction} when they went TO J{successor}? (excluding self-loops)**&#34;)

        # Get cached sequences from analysis results
        if &#34;cached_sequences&#34; not in st.session_state.analysis_results.get(&#34;predictions&#34;, {}):
            st.error(&#34;No trajectory sequence data available. Please rerun the predict analysis.&#34;)
            return

        cached_sequences = st.session_state.analysis_results[&#34;predictions&#34;][&#34;cached_sequences&#34;]

        # Find trajectories that follow the J{decision_junction} → J{successor} sequence
        relevant_trajectories = []
        predecessor_counts = {}

        for traj_idx, sequence in cached_sequences.items():
            # Check if this trajectory follows the decision → successor sequence
            for i in range(len(sequence) - 1):
                if sequence[i] == decision_junction and sequence[i + 1] == successor:
                    relevant_trajectories.append(traj_idx)

                    # Find what happened before the decision junction (only count different junctions)
                    if i &gt; 0:  # There&#39;s a junction before the decision junction
                        predecessor = sequence[i - 1]
                        # Only count as predecessor if it&#39;s a different junction (no self-loops)
                        if predecessor != decision_junction:
                            predecessor_counts[predecessor] = predecessor_counts.get(predecessor, 0) + 1
                        else:
                            # Skip self-loops and look further back
                            j = i - 1
                            while j &gt;= 0 and sequence[j] == decision_junction:
                                j -= 1
                            if j &gt;= 0:  # Found a different junction
                                predecessor = sequence[j]
                                predecessor_counts[predecessor] = predecessor_counts.get(predecessor, 0) + 1
                    break

        if not relevant_trajectories:
            st.info(f&#34;No trajectories found that follow the J{decision_junction} → J{successor} sequence&#34;)
            return

        total_trajectories = len(relevant_trajectories)
        st.markdown(f&#34;**Found {total_trajectories} trajectories that follow J{decision_junction} → J{successor}**&#34;)

        if not predecessor_counts:
            st.info(f&#34;None of these trajectories came from another junction before J{decision_junction}&#34;)
            return

        # Calculate probabilities
        predecessor_probs = {}
        for predecessor, count in predecessor_counts.items():
            prob = (count / total_trajectories) * 100
            predecessor_probs[f&#34;J{predecessor}&#34;] = prob

        # Create visualization
        import matplotlib.pyplot as plt
        import pandas as pd

        predecessor_names = list(predecessor_probs.keys())
        probabilities = list(predecessor_probs.values())

        fig, ax = plt.subplots(figsize=(10, 6))
        bars = ax.bar(predecessor_names, probabilities, color=&#39;lightgreen&#39;, alpha=0.7)

        ax.set_xlabel(&#39;Predecessor Junction&#39;)
        ax.set_ylabel(&#39;Probability (%)&#39;)
        ax.set_title(f&#39;Direct Predecessor Probabilities: ? → J{decision_junction} → J{successor}&#39;)
        ax.set_ylim(0, max(probabilities) * 1.1)

        # Add value labels on bars
        for bar, prob in zip(bars, probabilities):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                   f&#39;{prob:.1f}%&#39;, ha=&#39;center&#39;, va=&#39;bottom&#39;)

        plt.xticks(rotation=45)
        plt.tight_layout()
        st.pyplot(fig)

        # Show detailed table
        st.markdown(&#34;#### Detailed Analysis:&#34;)
        prob_data = []
        for predecessor, prob in predecessor_probs.items():
            count = predecessor_counts[int(predecessor[1:])]
            prob_data.append({
                &#39;Predecessor Junction&#39;: predecessor,
                &#39;Trajectory Count&#39;: count,
                &#39;Probability (%)&#39;: f&#34;{prob:.1f}&#34;,
                &#39;Sequence&#39;: f&#34;{predecessor} → J{decision_junction} → J{successor}&#34;
            })

        df = pd.DataFrame(prob_data)
        st.dataframe(df, width=&#39;stretch&#39;)

        # Show trajectory examples
        st.markdown(&#34;#### Example Trajectory Sequences:&#34;)
        example_count = 0
        for traj_idx in relevant_trajectories[:5]:  # Show first 5 examples
            sequence = cached_sequences[traj_idx]
            seq_str = &#34; → &#34;.join([f&#34;J{j}&#34; for j in sequence])
            st.write(f&#34;**Trajectory {traj_idx}**: {seq_str}&#34;)
            example_count += 1

        if len(relevant_trajectories) &gt; 5:
            st.write(f&#34;... and {len(relevant_trajectories) - 5} more trajectories&#34;)

    def render_flow_graphs(self):
        &#34;&#34;&#34;Render flow graph visualizations&#34;&#34;&#34;
        st.markdown(&#34;### Flow Graphs&#34;)

        col1, col2 = st.columns(2)

        with col1:
            st.markdown(&#34;#### Overall Flow Graph&#34;)
            if &#34;flow_graph_map&#34; in st.session_state.analysis_results:
                st.image(st.session_state.analysis_results[&#34;flow_graph_map&#34;], width=&#39;stretch&#39;)

        with col2:
            st.markdown(&#34;#### Per-Junction Flow Graph&#34;)
            if &#34;per_junction_flow_graph&#34; in st.session_state.analysis_results:
                st.image(st.session_state.analysis_results[&#34;per_junction_flow_graph&#34;], width=&#39;stretch&#39;)

    def render_discover_visualizations(self):
        &#34;&#34;&#34;Render discover analysis visualizations&#34;&#34;&#34;
        st.markdown(&#34;### Discover Analysis Results&#34;)

        # Display decision intercepts for each junction
        for junction_key, branches_data in st.session_state.analysis_results[&#34;branches&#34;].items():
            if junction_key == &#34;chain_decisions&#34;:  # Skip the chain decisions data
                continue

            st.markdown(f&#34;#### {junction_key.replace(&#39;_&#39;, &#39; &#39;).title()}&#34;)

            # Show decision intercepts plot
            junction_num = junction_key.split(&#39;_&#39;)[1]
            junction_dir = os.path.join(&#34;gui_outputs&#34;, f&#34;junction_{junction_num}&#34;)

            # Display available plots
            intercepts_path = os.path.join(junction_dir, &#34;Decision_Intercepts.png&#34;)
            if os.path.exists(intercepts_path):
                st.image(intercepts_path, caption=f&#34;Decision Intercepts - {junction_key}&#34;, width=&#39;stretch&#39;)
            else:
                st.warning(f&#34;Decision intercepts plot not found for {junction_key}&#34;)

            # Check for other available plots that might be generated
            other_plots = [
                (&#34;Decision_Map.png&#34;, &#34;Decision Map&#34;),
                (&#34;Branch_Counts.png&#34;, &#34;Branch Counts&#34;),
                (&#34;Branch_Directions.png&#34;, &#34;Branch Directions&#34;)
            ]

            for plot_file, plot_name in other_plots:
                plot_path = os.path.join(junction_dir, plot_file)
                if os.path.exists(plot_path):
                    st.image(plot_path, caption=f&#34;{plot_name} - {junction_key}&#34;, width=&#39;stretch&#39;)

            # Show branch summary
            if &#34;summary&#34; in branches_data and branches_data[&#34;summary&#34;] is not None:
                st.markdown(&#34;**Branch Summary:**&#34;)
                st.dataframe(branches_data[&#34;summary&#34;], width=&#39;stretch&#39;)

            # Show assignments preview
            if &#34;assignments&#34; in branches_data and branches_data[&#34;assignments&#34;] is not None:
                st.markdown(&#34;**Branch Assignments (first 20):**&#34;)
                st.dataframe(branches_data[&#34;assignments&#34;].head(20), width=&#39;stretch&#39;)

    def render_assign_visualizations(self):
        &#34;&#34;&#34;Render assign analysis visualizations&#34;&#34;&#34;
        st.markdown(&#34;### Assign Analysis Results&#34;)

        # Display assignment results for each junction
        for junction_key, assignments_data in st.session_state.analysis_results[&#34;assignments&#34;].items():
            st.markdown(f&#34;#### {junction_key.replace(&#39;_&#39;, &#39; &#39;).title()}&#34;)

            # Extract the actual assignments DataFrame from the nested structure
            if isinstance(assignments_data, dict) and &#34;assignments&#34; in assignments_data:
                assignments_df = assignments_data[&#34;assignments&#34;]
            else:
                assignments_df = assignments_data

            # Show assignment data
            if assignments_df is not None and hasattr(assignments_df, &#39;head&#39;):
                st.markdown(&#34;**Branch Assignments:**&#34;)
                st.dataframe(assignments_df.head(20), width=&#39;stretch&#39;)

                if len(assignments_df) &gt; 20:
                    st.info(f&#34;Showing first 20 of {len(assignments_df)} assignments&#34;)

                # Show assignment statistics
                if &#39;branch&#39; in assignments_df.columns:
                    branch_counts = assignments_df[&#39;branch&#39;].value_counts()
                    st.markdown(&#34;**Branch Distribution:**&#34;)
                    st.bar_chart(branch_counts)

                    # Show detailed statistics
                    st.markdown(&#34;**Assignment Statistics:**&#34;)
                    total_trajectories = len(assignments_df)
                    for branch, count in branch_counts.items():
                        percentage = (count / total_trajectories) * 100
                        st.write(f&#34;- Branch {branch}: {count} trajectories ({percentage:.1f}%)&#34;)
            else:
                st.info(f&#34;No assignment data available for {junction_key}&#34;)

    def render_metrics_visualizations(self):
        &#34;&#34;&#34;Render metrics analysis visualizations&#34;&#34;&#34;
        st.markdown(&#34;### Metrics Analysis Results&#34;)

        metrics_data = st.session_state.analysis_results[&#34;metrics&#34;]
        metrics_images = st.session_state.analysis_results.get(&#34;metrics_images&#34;, {})

        if metrics_data:
            # Convert to DataFrame for better display
            import pandas as pd
            df = pd.DataFrame(metrics_data)

            # Display metrics table
            st.markdown(&#34;**Trajectory Metrics:**&#34;)
            st.dataframe(df, width=&#39;stretch&#39;)

            # Create distribution visualizations (prefer pre-generated images)
            col1, col2 = st.columns(2)

            with col1:
                st.markdown(&#34;**Total Time Distribution**&#34;)
                img = metrics_images.get(&#34;total_time_distribution.png&#34;)
                if img and os.path.exists(img):
                    st.image(img, width=&#39;stretch&#39;)
                else:
                    if &#39;total_time&#39; in df.columns:
                        valid_times = df[&#39;total_time&#39;].dropna()
                        if len(valid_times) &gt; 0:
                            sorted_times = valid_times.sort_values().reset_index(drop=True)
                            st.bar_chart(sorted_times)
                            st.caption(f&#34;Range: {sorted_times.min():.1f}s - {sorted_times.max():.1f}s&#34;)
                            st.caption(f&#34;Mean: {sorted_times.mean():.1f}s, Median: {sorted_times.median():.1f}s&#34;)
                        else:
                            st.info(&#34;No valid time data available&#34;)

            with col2:
                st.markdown(&#34;**Average Speed Distribution**&#34;)
                img = metrics_images.get(&#34;average_speed_distribution.png&#34;)
                if img and os.path.exists(img):
                    st.image(img, width=&#39;stretch&#39;)
                else:
                    if &#39;average_speed&#39; in df.columns:
                        valid_speeds = df[&#39;average_speed&#39;].dropna()
                        if len(valid_speeds) &gt; 0:
                            sorted_speeds = valid_speeds.sort_values().reset_index(drop=True)
                            st.bar_chart(sorted_speeds)
                            st.caption(f&#34;Range: {sorted_speeds.min():.2f} - {sorted_speeds.max():.2f}&#34;)
                            st.caption(f&#34;Mean: {sorted_speeds.mean():.2f}, Median: {sorted_speeds.median():.2f}&#34;)
                        else:
                            st.info(&#34;No valid speed data available&#34;)

            # Add distance visualization
            col3, col4 = st.columns(2)

            with col3:
                st.markdown(&#34;**Total Distance Distribution**&#34;)
                img = metrics_images.get(&#34;total_distance_distribution.png&#34;)
                if img and os.path.exists(img):
                    st.image(img, width=&#39;stretch&#39;)
                else:
                    if &#39;total_distance&#39; in df.columns:
                        valid_distances = df[&#39;total_distance&#39;].dropna()
                        if len(valid_distances) &gt; 0:
                            sorted_distances = valid_distances.sort_values().reset_index(drop=True)
                            st.bar_chart(sorted_distances)
                            st.caption(f&#34;Range: {sorted_distances.min():.1f} - {sorted_distances.max():.1f}&#34;)
                            st.caption(f&#34;Mean: {sorted_distances.mean():.1f}, Median: {sorted_distances.median():.1f}&#34;)
                        else:
                            st.info(&#34;No valid distance data available&#34;)

            with col4:
                st.markdown(&#34;**Summary Statistics**&#34;)
                if len(df) &gt; 0:
                    summary_stats = {
                        &#34;Total Trajectories&#34;: len(df),
                        &#34;Avg Total Time&#34;: f&#34;{df[&#39;total_time&#39;].mean():.2f}s&#34; if &#39;total_time&#39; in df.columns else &#34;N/A&#34;,
                        &#34;Avg Total Distance&#34;: f&#34;{df[&#39;total_distance&#39;].mean():.2f}&#34; if &#39;total_distance&#39; in df.columns else &#34;N/A&#34;,
                        &#34;Avg Speed&#34;: f&#34;{df[&#39;average_speed&#39;].mean():.2f}&#34; if &#39;average_speed&#39; in df.columns else &#34;N/A&#34;
                    }
                    for key, value in summary_stats.items():
                        st.metric(key, value)

            # Define junction columns early for use in speed analysis
            junction_cols = [col for col in df.columns if col.startswith(&#39;junction_&#39;) and col.endswith(&#39;_time&#39;)]

            # Speed analysis visualizations
            speed_cols = [col for col in df.columns if col.startswith(&#39;junction_&#39;) and col.endswith(&#39;_speed&#39;)]
            if speed_cols:
                st.markdown(&#34;### Junction Speed Analysis&#34;)

                # Create speed analysis summary
                speed_summary = []
                for col in speed_cols:
                    junction_num = col.split(&#39;_&#39;)[1]
                    speed_mode_col = f&#34;junction_{junction_num}_speed_mode&#34;
                    entry_speed_col = f&#34;junction_{junction_num}_entry_speed&#34;
                    exit_speed_col = f&#34;junction_{junction_num}_exit_speed&#34;
                    avg_transit_col = f&#34;junction_{junction_num}_avg_transit_speed&#34;

                    valid_speeds = df[col].dropna()
                    total_trajectories = len(df)
                    valid_count = len(valid_speeds)

                    if valid_count &gt; 0:
                        speed_summary.append({
                            &#34;Junction&#34;: f&#34;Junction {junction_num}&#34;,
                            &#34;Avg Speed Through&#34;: f&#34;{valid_speeds.mean():.2f}&#34;,
                            &#34;Std Speed Through&#34;: f&#34;{valid_speeds.std():.2f}&#34;,
                            &#34;Valid Count&#34;: valid_count,
                            &#34;NaN Count&#34;: total_trajectories - valid_count
                        })

                if speed_summary:
                    speed_df = pd.DataFrame(speed_summary)
                    st.markdown(&#34;**Junction Speed Statistics:**&#34;)
                    st.dataframe(speed_df, width=&#39;stretch&#39;)

                    # One concise explanation above both diagrams
                    st.markdown(&#34;### Speed Analysis&#34;)
                    st.info(&#34;&#34;&#34;
                    **Available Speed Metrics:** Entry (2–5 s before), Exit (2–5 s after), and Average Transit (inside junction).
                    Use the selector in the correlation plot to switch the speed metric.
                    &#34;&#34;&#34;)

                    # Show correlation (left) and entry/exit bars (right) side-by-side
                    col_speed1, col_speed2 = st.columns(2)

                    with col_speed1:
                        st.markdown(&#34;**Speed vs Time Correlation**&#34;)
                        img = metrics_images.get(&#34;speed_vs_time_correlation.png&#34;)
                        if img and os.path.exists(img):
                            st.image(img, width=&#39;stretch&#39;)
                        else:
                            st.info(&#34;Correlation plot not available yet. Re-run metrics analysis to generate.&#34;)

                    with col_speed2:
                        st.markdown(&#34;**Entry vs Exit Speed Analysis**&#34;)
                        st.caption(&#34;**Entry Speed**: Average speed in 2-5 second window before entering junction&#34;)
                        st.caption(&#34;**Exit Speed**: Average speed in 2-5 second window after leaving junction&#34;)
                        img = metrics_images.get(&#34;entry_exit_speed_by_junction.png&#34;)
                        if img and os.path.exists(img):
                            st.image(img, width=&#39;stretch&#39;)
                        else:
                            st.info(&#34;Entry/Exit bar chart not available yet. Re-run metrics analysis to generate.&#34;)

                # Detailed speed metrics table
                st.markdown(&#34;### Detailed Speed Metrics&#34;)
                speed_detail_cols = [col for col in df.columns if &#39;speed&#39; in col.lower()]
                if speed_detail_cols:
                    speed_detail_df = df[speed_detail_cols + [&#39;trajectory_id&#39;, &#39;trajectory_tid&#39;]]
                    st.dataframe(speed_detail_df, width=&#39;stretch&#39;)

            # Junction-specific metrics if available
            if junction_cols:
                st.markdown(&#34;### Junction Timing Analysis&#34;)

                # Check for NaN values and provide explanation
                total_junction_measurements = len(df) * len(junction_cols)
                valid_junction_measurements = sum(len(df[col].dropna()) for col in junction_cols)
                nan_count = total_junction_measurements - valid_junction_measurements

                if nan_count &gt; 0:
                    st.info(f&#34;ℹ️ **Note**: {nan_count} out of {total_junction_measurements} junction timing measurements returned NaN. This typically means trajectories didn&#39;t pass through those junctions or timing couldn&#39;t be computed.&#34;)

                # Create junction timing summary
                junction_summary = []
                for col in junction_cols:
                    junction_num = col.split(&#39;_&#39;)[1]
                    mode_col = f&#34;junction_{junction_num}_mode&#34;
                    if mode_col in df.columns:
                        valid_times = df[col].dropna()
                        total_trajectories = len(df)
                        valid_count = len(valid_times)
                        nan_count_junction = total_trajectories - valid_count

                        if valid_count &gt; 0:
                            junction_summary.append({
                                &#34;Junction&#34;: f&#34;Junction {junction_num}&#34;,
                                &#34;Avg Time&#34;: f&#34;{valid_times.mean():.2f}s&#34;,
                                &#34;Std Time&#34;: f&#34;{valid_times.std():.2f}s&#34;,
                                &#34;Valid Count&#34;: valid_count,
                                &#34;NaN Count&#34;: nan_count_junction
                            })
                        else:
                            junction_summary.append({
                                &#34;Junction&#34;: f&#34;Junction {junction_num}&#34;,
                                &#34;Avg Time&#34;: &#34;N/A&#34;,
                                &#34;Std Time&#34;: &#34;N/A&#34;,
                                &#34;Valid Count&#34;: 0,
                                &#34;NaN Count&#34;: total_trajectories
                            })

                if junction_summary:
                    junction_df = pd.DataFrame(junction_summary)
                    st.markdown(&#34;**Junction Statistics (Only trajectories that actually pass through each junction):**&#34;)
                    st.dataframe(junction_df, width=&#39;stretch&#39;)

                    # Junction timing visualization
                    st.markdown(&#34;**Junction Timing Comparison**&#34;)
                    img = metrics_images.get(&#34;junction_timing_comparison.png&#34;)
                    if img and os.path.exists(img):
                        st.image(img, width=&#39;stretch&#39;)
                    else:
                        st.info(&#34;Timing comparison chart not available yet. Re-run metrics analysis to generate.&#34;)

                    # Show individual junction timing distributions
                    st.markdown(&#34;**Individual Junction Timing Distributions**&#34;)
                    if metrics_images:
                        # display per-junction histograms if present
                        for name, path in sorted(metrics_images.items()):
                            if name.startswith(&#34;timing_distribution_J&#34;) and os.path.exists(path):
                                jlabel = name.replace(&#34;timing_distribution_&#34;, &#34;&#34;).replace(&#34;.png&#34;, &#34;&#34;)
                                st.markdown(f&#34;**{jlabel.replace(&#39;_&#39;, &#39; &#39;)}**&#34;)
                                st.image(path, width=&#39;stretch&#39;)
                    else:
                        for col in junction_cols:
                            junction_num = col.split(&#39;_&#39;)[1]
                            valid_times = df[col].dropna()
                            if len(valid_times) &gt; 0:
                                st.markdown(f&#34;**Junction {junction_num} Timing Distribution**&#34;)
                                sorted_times = valid_times.sort_values().reset_index(drop=True)
                                st.bar_chart(sorted_times)
                                st.caption(f&#34;Range: {sorted_times.min():.2f}s - {sorted_times.max():.2f}s, Mean: {sorted_times.mean():.2f}s&#34;)


    def _analyze_movement_patterns_at_junction(self, trajectories, junction, r_outer, decision_mode, path_length, epsilon):
        &#34;&#34;&#34;Analyze movement patterns at a junction for regular trajectories (without head tracking data).&#34;&#34;&#34;
        import numpy as np
        import pandas as pd

        results = []

        for traj_idx, trajectory in enumerate(trajectories):
            # Find decision point using the same logic as the gaze analysis
            if decision_mode == &#34;radial&#34; or (decision_mode == &#34;hybrid&#34; and r_outer &gt; junction.r):
                # Use radial decision point
                decision_idx = self._find_radial_decision_point(trajectory, junction, r_outer)
            else:
                # Use path length decision point
                decision_idx = self._find_path_length_decision_point(trajectory, junction, path_length, epsilon)

            if decision_idx is None:
                # Fallback to nearest point to junction center
                decision_idx = self._find_nearest_to_center(trajectory, junction)

            if decision_idx is not None and decision_idx &lt; len(trajectory.x):
                # Calculate movement direction at decision point with better edge case handling
                movement_yaw = np.nan
                if decision_idx &gt; 0 and decision_idx &lt; len(trajectory.x) - 1:
                    dx = trajectory.x[decision_idx + 1] - trajectory.x[decision_idx - 1]
                    dz = trajectory.z[decision_idx + 1] - trajectory.z[decision_idx - 1]
                    movement_magnitude = np.hypot(dx, dz)
                    if movement_magnitude &gt; 1e-3:  # Increased threshold for numerical stability
                        movement_yaw = np.degrees(np.arctan2(dx, dz))

                # Calculate approach direction (direction from previous point to decision point)
                approach_yaw = np.nan
                if decision_idx &gt; 0:
                    dx_approach = trajectory.x[decision_idx] - trajectory.x[decision_idx - 1]
                    dz_approach = trajectory.z[decision_idx] - trajectory.z[decision_idx - 1]
                    approach_magnitude = np.hypot(dx_approach, dz_approach)
                    if approach_magnitude &gt; 1e-3:
                        approach_yaw = np.degrees(np.arctan2(dx_approach, dz_approach))

                # Calculate exit direction (direction from decision point to next point)
                exit_yaw = np.nan
                if decision_idx &lt; len(trajectory.x) - 1:
                    dx_exit = trajectory.x[decision_idx + 1] - trajectory.x[decision_idx]
                    dz_exit = trajectory.z[decision_idx + 1] - trajectory.z[decision_idx]
                    exit_magnitude = np.hypot(dx_exit, dz_exit)
                    if exit_magnitude &gt; 1e-3:
                        exit_yaw = np.degrees(np.arctan2(dx_exit, dz_exit))

                # Calculate distance from junction center
                distance_from_center = np.sqrt(
                    (trajectory.x[decision_idx] - junction.cx)**2 +
                    (trajectory.z[decision_idx] - junction.cz)**2
                )

                # Calculate trajectory length for context
                trajectory_length = len(trajectory.x)

                results.append({
                    &#34;trajectory&#34;: traj_idx,
                    &#34;junction&#34;: 0,  # Single junction analysis
                    &#34;decision_idx&#34;: decision_idx,
                    &#34;trajectory_length&#34;: trajectory_length,
                    &#34;decision_ratio&#34;: decision_idx / trajectory_length if trajectory_length &gt; 0 else 0,
                    &#34;movement_yaw&#34;: movement_yaw,
                    &#34;approach_yaw&#34;: approach_yaw,
                    &#34;exit_yaw&#34;: exit_yaw,
                    &#34;distance_from_center&#34;: distance_from_center,
                    &#34;decision_x&#34;: trajectory.x[decision_idx],
                    &#34;decision_z&#34;: trajectory.z[decision_idx],
                    &#34;time_at_decision&#34;: self._safe_get_time_value(trajectory, decision_idx)
                })

        return pd.DataFrame(results)

    def _analyze_movement_patterns_optimized(self, trajectories, junction, r_outer, decision_mode, path_length, epsilon):
        &#34;&#34;&#34;Optimized movement pattern analysis for a single junction.&#34;&#34;&#34;
        import numpy as np
        import pandas as pd

        results = []

        # Pre-calculate junction center for efficiency
        jx, jz = junction.cx, junction.cz

        for traj_idx, trajectory in enumerate(trajectories):
            # Fast decision point detection
            decision_idx = self._find_decision_point_fast(trajectory, jx, jz, r_outer, decision_mode, path_length, epsilon)

            if decision_idx is not None and decision_idx &lt; len(trajectory.x):
                # Calculate movement direction with optimized approach
                movement_yaw = np.nan
                if decision_idx &gt; 0 and decision_idx &lt; len(trajectory.x) - 1:
                    dx = trajectory.x[decision_idx + 1] - trajectory.x[decision_idx - 1]
                    dz = trajectory.z[decision_idx + 1] - trajectory.z[decision_idx - 1]
                    movement_magnitude = np.hypot(dx, dz)
                    if movement_magnitude &gt; 1e-3:
                        movement_yaw = np.degrees(np.arctan2(dx, dz))

                # Calculate approach and exit directions
                approach_yaw = np.nan
                if decision_idx &gt; 0:
                    dx_approach = trajectory.x[decision_idx] - trajectory.x[decision_idx - 1]
                    dz_approach = trajectory.z[decision_idx] - trajectory.z[decision_idx - 1]
                    approach_magnitude = np.hypot(dx_approach, dz_approach)
                    if approach_magnitude &gt; 1e-3:
                        approach_yaw = np.degrees(np.arctan2(dx_approach, dz_approach))

                exit_yaw = np.nan
                if decision_idx &lt; len(trajectory.x) - 1:
                    dx_exit = trajectory.x[decision_idx + 1] - trajectory.x[decision_idx]
                    dz_exit = trajectory.z[decision_idx + 1] - trajectory.z[decision_idx]
                    exit_magnitude = np.hypot(dx_exit, dz_exit)
                    if exit_magnitude &gt; 1e-3:
                        exit_yaw = np.degrees(np.arctan2(dx_exit, dz_exit))

                # Calculate distance from junction center
                distance_from_center = np.sqrt(
                    (trajectory.x[decision_idx] - jx)**2 +
                    (trajectory.z[decision_idx] - jz)**2
                )

                # Calculate trajectory position metrics
                trajectory_length = len(trajectory.x)
                decision_ratio = decision_idx / trajectory_length if trajectory_length &gt; 0 else 0

                results.append({
                    &#34;trajectory&#34;: traj_idx,
                    &#34;junction&#34;: 0,  # Single junction analysis
                    &#34;decision_idx&#34;: decision_idx,
                    &#34;trajectory_length&#34;: trajectory_length,
                    &#34;decision_ratio&#34;: decision_ratio,
                    &#34;movement_yaw&#34;: movement_yaw,
                    &#34;approach_yaw&#34;: approach_yaw,
                    &#34;exit_yaw&#34;: exit_yaw,
                    &#34;distance_from_center&#34;: distance_from_center,
                    &#34;decision_x&#34;: trajectory.x[decision_idx],
                    &#34;decision_z&#34;: trajectory.z[decision_idx],
                    &#34;time_at_decision&#34;: self._safe_get_time_value(trajectory, decision_idx)
                })

        return pd.DataFrame(results)

    def _find_decision_point_fast(self, trajectory, jx, jz, r_outer, decision_mode, path_length, epsilon):
        &#34;&#34;&#34;Fast decision point detection optimized for performance.&#34;&#34;&#34;
        import numpy as np

        # Vectorized distance calculation
        distances = np.sqrt((trajectory.x - jx)**2 + (trajectory.z - jz)**2)

        if decision_mode == &#34;radial&#34; or (decision_mode == &#34;hybrid&#34; and r_outer &gt; 50.0):
            # Find first point within junction
            within_junction = distances &lt;= r_outer
            if np.any(within_junction):
                return np.argmax(within_junction)  # First True value
        else:
            # Find closest point and search around it
            closest_idx = np.argmin(distances)
            search_window = min(int(path_length), len(trajectory.x) // 10, 50)
            start_idx = max(0, closest_idx - search_window)
            end_idx = min(len(trajectory.x), closest_idx + search_window)

            # Look for point within junction radius
            for i in range(start_idx, end_idx):
                if distances[i] &lt;= 50.0 + epsilon:  # Use junction radius + epsilon
                    return i
            return closest_idx

        return None

    def _check_for_gaze_data(self, trajectories):
        &#34;&#34;&#34;Check if trajectories have gaze/physiological data using unified model.&#34;&#34;&#34;
        if not trajectories:
            return False

        # Use capability helpers from unified model
        from verta.verta_data_loader import has_gaze_data, has_physio_data, has_vr_headset_data

        # Check if ANY trajectory has gaze/physio capabilities
        has_gaze = any(has_gaze_data(traj) for traj in trajectories)
        has_physio = any(has_physio_data(traj) for traj in trajectories)
        has_vr = any(has_vr_headset_data(traj) for traj in trajectories)

        # Also check the first trajectory for debugging
        sample_traj = trajectories[0]
        sample_has_gaze = has_gaze_data(sample_traj)
        sample_has_physio = has_physio_data(sample_traj)
        sample_has_vr = has_vr_headset_data(sample_traj)

        st.write(f&#34;- Debug _check_for_gaze_data: has_gaze={has_gaze}, has_physio={has_physio}, has_vr_headset={has_vr}&#34;)
        st.write(f&#34;- Sample trajectory (first): has_gaze={sample_has_gaze}, has_physio={sample_has_physio}, has_vr_headset={sample_has_vr}&#34;)

        return has_gaze or has_physio or has_vr



    def _get_gaze_column_mappings(self):
        &#34;&#34;&#34;Get gaze column mappings from session state.&#34;&#34;&#34;
        return getattr(st.session_state, &#39;gaze_column_mappings&#39;, {})

    def _normalize_gaze_result_frames(self, results: dict) -&gt; dict:
        &#34;&#34;&#34;Normalize result column names so GUI plots find expected columns.&#34;&#34;&#34;
        # Physiological: ensure heart_rate_change, pupil_change
        if &#39;physiological&#39; in results and results[&#39;physiological&#39;] is not None:
            phys = results[&#39;physiological&#39;]
            # Handle both DataFrames and lists (converted from DataFrames)
            if hasattr(phys, &#39;rename&#39;):  # DataFrame
                phys = phys.rename(columns={
                    &#39;hr_change&#39;: &#39;heart_rate_change&#39;,
                    &#39;hr_delta&#39;: &#39;heart_rate_change&#39;,
                    &#39;pupil_delta&#39;: &#39;pupil_change&#39;,
                    &#39;pupil_dilation_change&#39;: &#39;pupil_change&#39;
                })
                results[&#39;physiological&#39;] = phys
            elif isinstance(phys, list):  # List of dictionaries (converted DataFrame)
                # Convert back to DataFrame, rename, then convert back to list
                import pandas as pd
                phys_df = pd.DataFrame(phys)
                phys_df = phys_df.rename(columns={
                    &#39;hr_change&#39;: &#39;heart_rate_change&#39;,
                    &#39;hr_delta&#39;: &#39;heart_rate_change&#39;,
                    &#39;pupil_delta&#39;: &#39;pupil_change&#39;,
                    &#39;pupil_dilation_change&#39;: &#39;pupil_change&#39;
                })
                results[&#39;physiological&#39;] = phys_df.to_dict(&#39;records&#39;)

        # Pupil dilation: ensure pupil_change
        if &#39;pupil_dilation&#39; in results and results[&#39;pupil_dilation&#39;] is not None:
            pup = results[&#39;pupil_dilation&#39;]
            if hasattr(pup, &#39;rename&#39;):  # DataFrame
                pup = pup.rename(columns={
                    &#39;pupil_delta&#39;: &#39;pupil_change&#39;,
                    &#39;pupil_dilation_change&#39;: &#39;pupil_change&#39;
                })
                results[&#39;pupil_dilation&#39;] = pup
            elif isinstance(pup, list):  # List of dictionaries (converted DataFrame)
                import pandas as pd
                pup_df = pd.DataFrame(pup)
                pup_df = pup_df.rename(columns={
                    &#39;pupil_delta&#39;: &#39;pupil_change&#39;,
                    &#39;pupil_dilation_change&#39;: &#39;pupil_change&#39;
                })
                results[&#39;pupil_dilation&#39;] = pup_df.to_dict(&#39;records&#39;)

        # Head yaw: ensure head_yaw, yaw_difference, intercept_x, intercept_z
        if &#39;head_yaw&#39; in results and results[&#39;head_yaw&#39;] is not None:
            yaw = results[&#39;head_yaw&#39;]
            if hasattr(yaw, &#39;rename&#39;):  # DataFrame
                yaw = yaw.rename(columns={
                    &#39;yaw&#39;: &#39;head_yaw&#39;,
                    &#39;delta_yaw&#39;: &#39;yaw_difference&#39;,
                    &#39;gaze_movement_diff&#39;: &#39;yaw_difference&#39;,
                    &#39;decision_x&#39;: &#39;intercept_x&#39;,
                    &#39;decision_z&#39;: &#39;intercept_z&#39;
                })
                results[&#39;head_yaw&#39;] = yaw
            elif isinstance(yaw, list):  # List of dictionaries (converted DataFrame)
                import pandas as pd
                yaw_df = pd.DataFrame(yaw)
                yaw_df = yaw_df.rename(columns={
                    &#39;yaw&#39;: &#39;head_yaw&#39;,
                    &#39;delta_yaw&#39;: &#39;yaw_difference&#39;,
                    &#39;gaze_movement_diff&#39;: &#39;yaw_difference&#39;,
                    &#39;decision_x&#39;: &#39;intercept_x&#39;,
                    &#39;decision_z&#39;: &#39;intercept_z&#39;
                })
                results[&#39;head_yaw&#39;] = yaw_df.to_dict(&#39;records&#39;)

        return results

    # (Removed) _plot_head_yaw_arrows_at_intercepts helper per request

    def _perform_gaze_analysis_with_mappings(self, trajectories, junction, r_outer, decision_mode, path_length, epsilon, linger_delta, out_dir, column_mappings, scale_factor=1.0):
        &#34;&#34;&#34;Perform gaze analysis using column mappings with scaling support.&#34;&#34;&#34;
        import pandas as pd
        import numpy as np
        from verta.verta_geometry import Circle

        # Filter trajectories to only include those with gaze or physiological data
        try:
            from verta.verta_data_loader import has_gaze_data as _has_gaze, has_physio_data as _has_physio
            filtered_trajectories = [t for t in trajectories if (_has_gaze(t) or _has_physio(t))]

            st.info(f&#34;🔍 **Trajectory Filtering in Analysis:**&#34;)
            st.write(f&#34;- Total trajectories: {len(trajectories)}&#34;)
            st.write(f&#34;- Trajectories with gaze/physio data: {len(filtered_trajectories)}&#34;)

            if len(filtered_trajectories) &lt; len(trajectories):
                skipped_count = len(trajectories) - len(filtered_trajectories)
                st.info(f&#34;ℹ️ Skipped {skipped_count} trajectories without gaze/physiological data&#34;)

            if not filtered_trajectories:
                st.warning(&#34;⚠️ No trajectories with gaze/physiological data found&#34;)
                return None

            trajectories = filtered_trajectories

        except Exception as e:
            st.warning(f&#34;⚠️ Error filtering trajectories: {e}&#34;)
            # Continue with original trajectories if filtering fails

        # Check if we have standard gaze data first
        has_gaze_data = self._check_for_gaze_data(trajectories)

        # Debug: Show which code path is being used
        st.info(f&#34;🔍 **Gaze Analysis Code Path Debug:**&#34;)
        st.write(f&#34;- Has gaze data: {has_gaze_data}&#34;)
        st.write(f&#34;- Column mappings provided: {bool(column_mappings)}&#34;)
        if column_mappings:
            st.write(f&#34;- Column mappings: {list(column_mappings.keys())}&#34;)

        # Debug: Check what data is actually available in trajectories
        if trajectories:
            sample_traj = trajectories[0]
            st.write(f&#34;**Sample trajectory data availability:**&#34;)
            st.write(f&#34;- head_forward_x: {hasattr(sample_traj, &#39;head_forward_x&#39;) and sample_traj.head_forward_x is not None}&#34;)
            st.write(f&#34;- head_forward_z: {hasattr(sample_traj, &#39;head_forward_z&#39;) and sample_traj.head_forward_z is not None}&#34;)
            st.write(f&#34;- gaze_x: {hasattr(sample_traj, &#39;gaze_x&#39;) and sample_traj.gaze_x is not None}&#34;)
            st.write(f&#34;- gaze_y: {hasattr(sample_traj, &#39;gaze_y&#39;) and sample_traj.gaze_y is not None}&#34;)
            st.write(f&#34;- pupil_l: {hasattr(sample_traj, &#39;pupil_l&#39;) and sample_traj.pupil_l is not None}&#34;)
            st.write(f&#34;- pupil_r: {hasattr(sample_traj, &#39;pupil_r&#39;) and sample_traj.pupil_r is not None}&#34;)
            st.write(f&#34;- heart_rate: {hasattr(sample_traj, &#39;heart_rate&#39;) and sample_traj.heart_rate is not None}&#34;)

            if hasattr(sample_traj, &#39;pupil_l&#39;) and sample_traj.pupil_l is not None:
                st.write(f&#34;- pupil_l length: {len(sample_traj.pupil_l)}&#34;)
                st.write(f&#34;- pupil_l sample: {sample_traj.pupil_l[:5] if len(sample_traj.pupil_l) &gt; 5 else sample_traj.pupil_l}&#34;)
            if hasattr(sample_traj, &#39;heart_rate&#39;) and sample_traj.heart_rate is not None:
                st.write(f&#34;- heart_rate length: {len(sample_traj.heart_rate)}&#34;)
                st.write(f&#34;- heart_rate sample: {sample_traj.heart_rate[:5] if len(sample_traj.heart_rate) &gt; 5 else sample_traj.heart_rate}&#34;)

        if has_gaze_data:
            st.write(&#34;**→ Using comprehensive gaze analysis (with enhanced debugging)**&#34;)
            # Apply scaling to trajectories AND junction if needed
            if scale_factor != 1.0:
                st.info(f&#34;🔧 Applying scale factor {scale_factor} to trajectory coordinates and junction...&#34;)
                scaled_trajectories = []
                for traj in trajectories:
                    # Create a copy with scaled coordinates
                    scaled_traj = Trajectory(
                        tid=traj.tid,
                        x=traj.x * scale_factor,
                        z=traj.z * scale_factor,
                        t=traj.t,
                        head_forward_x=traj.head_forward_x,
                        head_forward_z=traj.head_forward_z,
                        gaze_x=traj.gaze_x,
                        gaze_y=traj.gaze_y,
                        pupil_l=traj.pupil_l,
                        pupil_r=traj.pupil_r,
                        heart_rate=traj.heart_rate
                    )
                    scaled_trajectories.append(scaled_traj)
                trajectories = scaled_trajectories

                # Scale the junction coordinates to match scaled trajectories
                scaled_junction = Circle(
                    cx=junction.cx * scale_factor,
                    cz=junction.cz * scale_factor,
                    r=junction.r * scale_factor
                )
                junction = scaled_junction
            else:
                # Junction coordinates are correct and should NOT be scaled
                # The issue is that trajectories might be getting double-scaled somewhere
                st.info(f&#34;🔧 Using original junction coordinates (no scaling needed)&#34;)
                st.write(f&#34;- Junction coordinates: ({junction.cx}, {junction.cz}), r={junction.r}&#34;)

            # Call the comprehensive gaze analysis function
            return self._perform_comprehensive_gaze_analysis(
                trajectories=trajectories,
                junction=junction,
                r_outer=r_outer,
                decision_mode=decision_mode,
                path_length=path_length,
                epsilon=epsilon,
                linger_delta=linger_delta,
                out_dir=out_dir,
                run_custom_discover=st.session_state.get(&#39;run_custom_discover&#39;, False)
            )

        elif column_mappings:
            st.write(&#34;**→ Using custom gaze analysis (with column mappings)**&#34;)
            # Use custom gaze analysis with column mappings
            gaze_data = self._perform_custom_gaze_analysis(
                trajectories, junction, r_outer, decision_mode, path_length, epsilon, out_dir, column_mappings, scale_factor=1.0
            )
        else:
            # Check if we have any physiological data (even if incomplete)
            has_any_physio = any(
                (hasattr(traj, &#39;pupil_l&#39;) and traj.pupil_l is not None) or
                (hasattr(traj, &#39;pupil_r&#39;) and traj.pupil_r is not None) or
                (hasattr(traj, &#39;heart_rate&#39;) and traj.heart_rate is not None)
                for traj in trajectories
            )

            if has_any_physio:
                st.write(&#34;**→ Using physiological-only analysis (incomplete gaze data)**&#34;)
                # Try to perform analysis with whatever physiological data we have
                return self._perform_comprehensive_gaze_analysis(
                    trajectories=trajectories,
                    junction=junction,
                    r_outer=r_outer,
                    decision_mode=decision_mode,
                    path_length=path_length,
                    epsilon=epsilon,
                    linger_delta=linger_delta,
                    out_dir=out_dir,
                    run_custom_discover=st.session_state.get(&#39;run_custom_discover&#39;, False)
                )
            else:
                st.error(&#34;❌ No gaze data, physiological data, or column mappings available&#34;)
            return None

        # Debug: Check trajectory types
        st.info(f&#34;🔍 **Trajectory Type Debug:**&#34;)
        if trajectories:
            sample_traj = trajectories[0]
            st.write(f&#34;- Sample trajectory type: {type(sample_traj).__name__}&#34;)
            st.write(f&#34;- Sample trajectory ID: {sample_traj.tid}&#34;)
            st.write(f&#34;- Has gaze_x: {hasattr(sample_traj, &#39;gaze_x&#39;) and sample_traj.gaze_x is not None}&#34;)
            st.write(f&#34;- Has heart_rate: {hasattr(sample_traj, &#39;heart_rate&#39;) and sample_traj.heart_rate is not None}&#34;)
            st.write(f&#34;- Has pupil_l: {hasattr(sample_traj, &#39;pupil_l&#39;) and sample_traj.pupil_l is not None}&#34;)

        # CRITICAL FIX: Use the same trajectory objects that were used for discover analysis
        # to ensure trajectory IDs match between assignments and gaze analysis
        if &#34;branches&#34; in st.session_state.analysis_results and st.session_state.analysis_results[&#34;branches&#34;]:
            st.info(&#34;🔧 **Using trajectory objects from discover analysis to ensure ID consistency**&#34;)
            # Use the original trajectories that were used for discover analysis
            discover_trajectories = st.session_state.trajectories
            st.write(f&#34;- Discover trajectories: {len(discover_trajectories)}&#34;)
            st.write(f&#34;- Gaze trajectories: {len(trajectories)}&#34;)

            # DEBUG: Check coordinate ranges
            if discover_trajectories and trajectories:
                sample_discover = discover_trajectories[0]
                sample_gaze = trajectories[0]
                st.error(&#34;🔍 **COORDINATE SYSTEM DEBUG:**&#34;)
                st.write(f&#34;- Discover trajectory {sample_discover.tid}: X range {np.min(sample_discover.x):.1f} to {np.max(sample_discover.x):.1f}&#34;)

                # Check if gaze trajectory has valid coordinates
                if np.all(np.isnan(sample_gaze.x)):
                    st.error(f&#34;❌ **CRITICAL ISSUE:** Gaze trajectory {sample_gaze.tid} has ALL NaN coordinates!&#34;)
                    st.write(&#34;**This explains why arrows are not visible - trajectory coordinates are invalid!**&#34;)
                    st.write(&#34;**Root cause:** Gaze trajectories were loaded BEFORE NaN handling was added&#34;)

                    # Show sample data for debugging
                    st.write(f&#34;**Sample gaze trajectory data:**&#34;)
                    st.write(f&#34;- X values: {sample_gaze.x[:5]} (first 5)&#34;)
                    st.write(f&#34;- Z values: {sample_gaze.z[:5]} (first 5)&#34;)
                    st.write(f&#34;- X type: {type(sample_gaze.x)}&#34;)
                    st.write(f&#34;- Z type: {type(sample_gaze.z)}&#34;)

                    # Provide solution
                    st.error(&#34;🔧 **SOLUTION:** Reload gaze data to apply NaN handling&#34;)
                    st.write(&#34;**Steps to fix:**&#34;)
                    st.write(&#34;1. Go to &#39;Data Loader&#39; tab&#34;)
                    st.write(&#34;2. Re-upload or reload your gaze trajectory files&#34;)
                    st.write(&#34;3. The new NaN handling will clean the coordinate data&#34;)
                    st.write(&#34;4. Run gaze analysis again&#34;)
                else:
                    st.write(f&#34;- Gaze trajectory {sample_gaze.tid}: X range {np.min(sample_gaze.x):.1f} to {np.max(sample_gaze.x):.1f}&#34;)

                    # Check if coordinates are in the same scale
                    discover_scale = np.max(sample_discover.x) / np.max(sample_gaze.x) if np.max(sample_gaze.x) &gt; 0 else 1
                    st.write(f&#34;- Coordinate scale ratio (discover/gaze): {discover_scale:.2f}&#34;)
                    if abs(discover_scale - 1.0) &gt; 0.1:
                        st.error(f&#34;❌ **COORDINATE MISMATCH DETECTED!** Scale ratio: {discover_scale:.2f}&#34;)
                        st.write(&#34;**This explains why arrows are not visible - they&#39;re in different coordinate systems!**&#34;)

                st.write(f&#34;- Junction coordinates: ({junction.cx}, {junction.cz})&#34;)

            # Check if we can convert discover trajectories to Trajectory objects
            if True:
                st.write(&#34;🔄 Using unified Trajectory objects (IDs already consistent)...&#34;)
                # Create a mapping from trajectory ID to trajectories currently loaded
                gaze_traj_map = {gt.tid: gt for gt in trajectories}
                # Create a mapping for discover trajectories to allow coordinate repair when needed
                discover_traj_map = {dt.tid: dt for dt in discover_trajectories}

                # Convert discover trajectories to Trajectory objects where possible
                converted_trajectories = []
                for dt in discover_trajectories:
                    if dt.tid in gaze_traj_map:
                        gt = gaze_traj_map[dt.tid]
                        # Repair NaN coordinate issue by borrowing x/z/t from discover trajectory when needed
                        try:
                            needs_repair = (
                                gt.x is None or gt.z is None or
                                (hasattr(gt.x, &#39;shape&#39;) and hasattr(gt.z, &#39;shape&#39;) and
                                 (np.all(np.isnan(gt.x)) or np.all(np.isnan(gt.z))))
                            )
                        except Exception:
                            needs_repair = True
                        if needs_repair and dt.tid in discover_traj_map:
                            repaired = Trajectory(
                                tid=gt.tid,
                                x=np.asarray(discover_traj_map[dt.tid].x),
                                z=np.asarray(discover_traj_map[dt.tid].z),
                                t=getattr(discover_traj_map[dt.tid], &#39;t&#39;, None),
                                head_forward_x=getattr(gt, &#39;head_forward_x&#39;, None),
                                head_forward_z=getattr(gt, &#39;head_forward_z&#39;, None),
                                gaze_x=getattr(gt, &#39;gaze_x&#39;, None),
                                gaze_y=getattr(gt, &#39;gaze_y&#39;, None),
                                pupil_l=getattr(gt, &#39;pupil_l&#39;, None),
                                pupil_r=getattr(gt, &#39;pupil_r&#39;, None),
                                heart_rate=getattr(gt, &#39;heart_rate&#39;, None),
                            )
                            converted_trajectories.append(repaired)
                        else:
                            converted_trajectories.append(gt)
                    else:
                        st.warning(f&#34;⚠️ No Trajectory found for discover trajectory ID: {dt.tid}&#34;)

                st.write(f&#34;- Converted trajectories: {len(converted_trajectories)}&#34;)
                trajectories = converted_trajectories
            else:
                st.warning(&#34;⚠️ No gaze trajectories available - using discover trajectories (may not have gaze data)&#34;)
                trajectories = discover_trajectories

        # Return the movement data as fallback (this should not be reached if has_gaze_data is True)
        return self._analyze_movement_patterns_optimized(trajectories, junction, r_outer, decision_mode, path_length, epsilon)


    def _perform_custom_gaze_analysis(self, trajectories, junction, r_outer, decision_mode, path_length, epsilon, column_mappings, scale_factor=1.0):
        &#34;&#34;&#34;Perform gaze analysis using custom column mappings with scaling support.&#34;&#34;&#34;
        import pandas as pd
        import numpy as np
        from verta.verta_geometry import Circle

        results = []

        # Apply scaling to junction coordinates if needed
        if scale_factor != 1.0:
            jx = junction.cx * scale_factor
            jz = junction.cz * scale_factor
            junction_radius = junction.r * scale_factor
        else:
            # Junction coordinates are correct and should NOT be scaled
            jx = junction.cx
            jz = junction.cz
            junction_radius = junction.r

        for traj_idx, trajectory in enumerate(trajectories):
            # Apply scaling to coordinates if needed
            if scale_factor != 1.0:
                scaled_x = trajectory.x * scale_factor
                scaled_z = trajectory.z * scale_factor
            else:
                scaled_x = trajectory.x
                scaled_z = trajectory.z

            # Find decision point using scaled coordinates
            decision_idx = self._find_decision_point_fast(trajectory, jx, jz, r_outer, decision_mode, path_length, epsilon)

            if decision_idx is not None and decision_idx &lt; len(scaled_x):
                # Extract gaze data using column mappings
                gaze_data = self._extract_gaze_data_from_trajectory(trajectory, decision_idx, column_mappings)

                # Calculate trajectory position metrics
                trajectory_length = len(scaled_x)
                decision_ratio = decision_idx / trajectory_length if trajectory_length &gt; 0 else 0

                # Calculate distance from junction center using scaled coordinates
                distance_from_center = np.sqrt(
                    (scaled_x[decision_idx] - jx)**2 +
                    (scaled_z[decision_idx] - jz)**2
                )

                results.append({
                    &#34;trajectory&#34;: traj_idx,
                    &#34;junction&#34;: 0,  # Single junction analysis
                    &#34;decision_idx&#34;: decision_idx,
                    &#34;trajectory_length&#34;: trajectory_length,
                    &#34;decision_ratio&#34;: decision_ratio,
                    &#34;distance_from_center&#34;: distance_from_center,
                    &#34;decision_x&#34;: trajectory.x[decision_idx],
                    &#34;decision_z&#34;: trajectory.z[decision_idx],
                    &#34;time_at_decision&#34;: self._safe_get_time_value(trajectory, decision_idx),
                    # Gaze-specific data
                    &#34;head_yaw&#34;: gaze_data.get(&#39;head_yaw&#39;, np.nan),
                    &#34;gaze_x&#34;: gaze_data.get(&#39;gaze_x&#39;, np.nan),
                    &#34;gaze_y&#34;: gaze_data.get(&#39;gaze_y&#39;, np.nan),
                    &#34;pupil_l&#34;: gaze_data.get(&#39;pupil_l&#39;, np.nan),
                    &#34;pupil_r&#34;: gaze_data.get(&#39;pupil_r&#39;, np.nan),
                    &#34;heart_rate&#34;: gaze_data.get(&#39;heart_rate&#39;, np.nan),
                    &#34;analysis_type&#34;: &#34;gaze&#34;
                })

        return pd.DataFrame(results)

    def _extract_gaze_data_from_trajectory(self, trajectory, decision_idx, column_mappings):
        &#34;&#34;&#34;Extract gaze data from trajectory using column mappings.&#34;&#34;&#34;
        import numpy as np

        gaze_data = {}

        # Map the standard gaze fields to the actual column names
        field_mappings = {
            &#39;head_yaw&#39;: [&#39;head_forward_x&#39;, &#39;head_forward_z&#39;],
            &#39;gaze_x&#39;: [&#39;gaze_x&#39;],
            &#39;gaze_y&#39;: [&#39;gaze_y&#39;],
            &#39;pupil_l&#39;: [&#39;pupil_l&#39;],
            &#39;pupil_r&#39;: [&#39;pupil_r&#39;],
            &#39;heart_rate&#39;: [&#39;heart_rate&#39;]
        }

        for field, required_columns in field_mappings.items():
            try:
                if field == &#39;head_yaw&#39;:
                    # Calculate head yaw from forward direction
                    head_x_col = column_mappings.get(&#39;head_forward_x&#39;, &#39;&#39;)
                    head_z_col = column_mappings.get(&#39;head_forward_z&#39;, &#39;&#39;)

                    if head_x_col and head_z_col and hasattr(trajectory, head_x_col) and hasattr(trajectory, head_z_col):
                        head_x = getattr(trajectory, head_x_col)[decision_idx] if decision_idx &lt; len(getattr(trajectory, head_x_col)) else np.nan
                        head_z = getattr(trajectory, head_z_col)[decision_idx] if decision_idx &lt; len(getattr(trajectory, head_z_col)) else np.nan

                        if not (np.isnan(head_x) or np.isnan(head_z)):
                            gaze_data[field] = np.degrees(np.arctan2(head_x, head_z))
                        else:
                            gaze_data[field] = np.nan
                    else:
                        gaze_data[field] = np.nan

                else:
                    # For other fields, directly map the column
                    col_name = column_mappings.get(required_columns[0], &#39;&#39;)
                    if col_name and hasattr(trajectory, col_name):
                        data_array = getattr(trajectory, col_name)
                        if decision_idx &lt; len(data_array):
                            gaze_data[field] = data_array[decision_idx]
                        else:
                            gaze_data[field] = np.nan
                    else:
                        gaze_data[field] = np.nan

            except (IndexError, AttributeError, KeyError):
                gaze_data[field] = np.nan

        return gaze_data

    def _perform_comprehensive_gaze_analysis_all_junctions(self, trajectories, junctions, r_outer_list, decision_mode, path_length, epsilon, linger_delta, out_dir):
        &#34;&#34;&#34;Perform comprehensive gaze analysis for all junctions at once.&#34;&#34;&#34;
        import pandas as pd
        import numpy as np
        from verta.verta_gaze import compute_head_yaw_at_decisions, analyze_physiological_at_junctions, analyze_pupil_dilation_trajectory

        st.info(&#34;🔍 **Performing comprehensive gaze analysis for all junctions...**&#34;)

        # First check if we have existing branch assignments from previous discover analysis
        existing_assignments = None
        use_existing_assignments = False

        if &#34;branches&#34; in st.session_state.analysis_results:
            st.info(&#34;🔍 Found existing branch assignments from previous discover analysis!&#34;)

            # Try to get chain_decisions (contains all junctions&#39; assignments)
            if &#34;chain_decisions&#34; in st.session_state.analysis_results[&#34;branches&#34;]:
                existing_assignments = st.session_state.analysis_results[&#34;branches&#34;][&#34;chain_decisions&#34;]
                st.write(f&#34;🔍 **Chain decisions shape:** {existing_assignments.shape}&#34;)
                st.write(f&#34;🔍 **Chain decisions columns:** {list(existing_assignments.columns)}&#34;)

                # Debug: Check what type of data we&#39;re getting
                if &#39;junction_index&#39; in existing_assignments.columns:
                    st.error(&#34;❌ **ERROR: Found junction_index column in chain_decisions - this is decision points data, not branch assignments!**&#34;)
                    st.write(&#34;🔍 **This means the wrong data was stored as chain_decisions**&#34;)
                else:
                    st.success(&#34;✅ **No junction_index column found - this looks like proper branch assignments**&#34;)

                # Check if we have assignments for all junctions
                branch_cols = [col for col in existing_assignments.columns if col.startswith(&#39;branch_j&#39;)]
                st.write(f&#34;🔍 **Found branch columns:** {branch_cols}&#34;)

                # If no branch_j columns found, check if we have junction_index column
                if len(branch_cols) == 0 and &#39;junction_index&#39; in existing_assignments.columns:
                    st.info(&#34;🔍 **Found junction_index column - this appears to be decision points data, not branch assignments**&#34;)
                    st.write(&#34;**Solution:** Run &#39;🔍 Discover Branches&#39; analysis first to create proper branch assignments&#34;)
                    existing_assignments = None
                elif len(branch_cols) &gt;= len(junctions):
                    use_existing_assignments = True
                    st.success(f&#34;✅ **Found assignments for all {len(junctions)} junctions!**&#34;)
                else:
                    st.warning(f&#34;⚠️ **Only found assignments for {len(branch_cols)} junctions, but have {len(junctions)} junctions**&#34;)
                    existing_assignments = None

        if existing_assignments is not None and use_existing_assignments:
            chain_df = existing_assignments

            # For multi-junction analysis, we&#39;ll pass decision points separately to each gaze function
            # This avoids the complex merging issues with junction-specific data
            decisions_chain_df = st.session_state.analysis_results.get(&#34;branches&#34;, {}).get(&#34;decision_points&#34;)
            if decisions_chain_df is not None and len(decisions_chain_df) &gt; 0:
                st.info(&#34;🔗 **Found decision points - will use precomputed intercept coordinates**&#34;)
                st.write(f&#34;🔍 **Decision points available:** {len(decisions_chain_df)} records&#34;)
            else:
                st.warning(&#34;⚠️ **No decision points found in session state - will calculate from scratch**&#34;)

            st.success(f&#34;✅ Using existing assignments - found {len(chain_df)} assignments&#34;)
            st.info(f&#34;🔍 **GAZE ANALYSIS PATH:** Using existing assignments from previous discover analysis&#34;)
        else:
            st.error(&#34;❌ **No existing assignments found!**&#34;)
            st.write(&#34;**Solution:** Run &#39;🔍 Discover Branches&#39; analysis first to create proper assignments&#34;)
            return {
                &#39;head_yaw&#39;: pd.DataFrame(),
                &#39;physiological&#39;: pd.DataFrame(),
                &#39;pupil_dilation&#39;: pd.DataFrame(),
                &#39;error&#39;: &#39;No assignments found&#39;
            }

        # Preprocess trajectories to convert time values to numeric format
        processed_trajectories = []
        for traj in trajectories:
            if hasattr(traj, &#39;t&#39;) and traj.t is not None:
                # Convert time values to numeric if they&#39;re strings
                if isinstance(traj.t[0], str):
                    try:
                        import pandas as pd
                        # Convert string time format to numeric seconds
                        numeric_times = []
                        for t_val in traj.t:
                            if isinstance(t_val, str):
                                # Parse time string like &#34;00:00:17.425&#34;
                                time_parts = t_val.split(&#39;:&#39;)
                                if len(time_parts) == 3:
                                    hours = float(time_parts[0])
                                    minutes = float(time_parts[1])
                                    seconds = float(time_parts[2])
                                    total_seconds = hours * 3600 + minutes * 60 + seconds
                                    numeric_times.append(total_seconds)
                                else:
                                    numeric_times.append(float(t_val))
                            else:
                                numeric_times.append(float(t_val))
                        traj.t = np.array(numeric_times)
                    except Exception as e:
                        st.warning(f&#34;⚠️ Could not convert time values for trajectory {traj.tid}: {e}&#34;)
                        # Keep original time values
                processed_trajectories.append(traj)
            else:
                processed_trajectories.append(traj)

        # Merge decision points with assignments for each junction to avoid multi-junction merging issues
        chain_df_with_decisions = chain_df.copy()
        if decisions_chain_df is not None and len(decisions_chain_df) &gt; 0:
            st.info(&#34;🔗 **Merging decision points with assignments per junction...**&#34;)
            try:
                from verta.verta_consistency import normalize_assignments

                # For each junction, merge its decision points
                # Ensure junction_index is numeric for comparison (do this once outside the loop)
                decisions_df_numeric = decisions_chain_df.copy()
                st.write(f&#34;🔍 **Debug: Original junction_index values:** {decisions_df_numeric[&#39;junction_index&#39;].unique()[:10]}&#34;)
                st.write(f&#34;🔍 **Debug: Original junction_index types:** {[type(x) for x in decisions_df_numeric[&#39;junction_index&#39;].unique()[:5]]}&#34;)

                decisions_df_numeric[&#34;junction_index&#34;] = pd.to_numeric(decisions_df_numeric[&#34;junction_index&#34;], errors=&#39;coerce&#39;)
                st.write(f&#34;🔍 **Debug: After conversion junction_index values:** {decisions_df_numeric[&#39;junction_index&#39;].unique()[:10]}&#34;)
                st.write(f&#34;🔍 **Debug: NaN count after conversion:** {decisions_df_numeric[&#39;junction_index&#39;].isna().sum()}&#34;)

                for junction_idx in range(len(junctions)):
                    # Filter decision points for this junction
                    junction_decisions = decisions_df_numeric[decisions_df_numeric[&#34;junction_index&#34;] == junction_idx].copy()

                    if len(junction_decisions) &gt; 0:
                        # Remove junction_index column to avoid filtering conflicts in normalize_assignments
                        junction_decisions_clean = junction_decisions.drop(columns=[&#39;junction_index&#39;]).copy()

                        # Debug: Check trajectory ID types
                        st.write(f&#34;🔍 **Debug Junction {junction_idx}:**&#34;)
                        st.write(f&#34;- Chain_df trajectory types: {[type(x) for x in chain_df[&#39;trajectory&#39;].unique()[:5]]}&#34;)
                        st.write(f&#34;- Junction_decisions trajectory types: {[type(x) for x in junction_decisions_clean[&#39;trajectory&#39;].unique()[:5]]}&#34;)
                        st.write(f&#34;- Chain_df trajectory values: {chain_df[&#39;trajectory&#39;].unique()[:5]}&#34;)
                        st.write(f&#34;- Junction_decisions trajectory values: {junction_decisions_clean[&#39;trajectory&#39;].unique()[:5]}&#34;)

                        # Normalize assignments for this junction
                        chain_df_normalized, norm_report = normalize_assignments(
                            assignments_df=chain_df,
                            trajectories=processed_trajectories,
                            junctions=[junctions[junction_idx]],  # Single junction
                            current_junction_idx=None,  # Don&#39;t filter by junction since we already did
                            decisions_df=junction_decisions_clean,
                            prefer_decisions=True,
                            include_outliers=False,
                            strict=False,
                        )

                        # Merge the decision columns from this junction&#39;s normalized data
                        decision_cols = [&#39;decision_idx&#39;, &#39;intercept_x&#39;, &#39;intercept_z&#39;]
                        for col in decision_cols:
                            if col in chain_df_normalized.columns:
                                # Create junction-specific column names
                                junction_col = f&#34;{col}_j{junction_idx}&#34;
                                chain_df_with_decisions[junction_col] = chain_df_normalized[col]

                # Check if any decision points were merged
                decision_cols_merged = [col for col in chain_df_with_decisions.columns if col.startswith(&#39;decision_idx_&#39;)]
                if decision_cols_merged:
                    st.success(f&#34;✅ **Successfully merged decision points for {len(decision_cols_merged)} junctions**&#34;)

                    # Debug: Show coverage of decision points
                    for col in decision_cols_merged:
                        junction_num = col.split(&#39;_&#39;)[-1]
                        non_null_count = chain_df_with_decisions[col].notna().sum()
                        total_count = len(chain_df_with_decisions)
                        st.write(f&#34;🔍 **Junction {junction_num}:** {non_null_count}/{total_count} trajectories have precomputed decision points&#34;)

                    chain_df = chain_df_with_decisions
                else:
                    st.warning(&#34;⚠️ **No decision points merged - will calculate from scratch**&#34;)

            except Exception as e:
                st.warning(f&#34;⚠️ **Could not merge decision points:** {e}&#34;)
                st.write(&#34;**Will calculate decision points from scratch**&#34;)

        # Use the actual gaze analysis functions with proper assignments
        # Get decision mode and parameters from discover analysis if available
        discover_decision_mode = decision_mode  # Default fallback
        discover_path_length = path_length
        discover_epsilon = epsilon
        discover_linger_delta = linger_delta
        discover_r_outer = r_outer_list[0] if r_outer_list else None  # Default fallback

        if &#34;branches&#34; in st.session_state.analysis_results:
            # Try to get parameters from the first junction&#39;s stored data
            for junction_key, branch_data in st.session_state.analysis_results[&#34;branches&#34;].items():
                if isinstance(branch_data, dict) and &#34;decision_mode&#34; in branch_data:
                    discover_decision_mode = branch_data[&#34;decision_mode&#34;]
                    discover_path_length = branch_data.get(&#34;path_length&#34;, path_length)
                    discover_epsilon = branch_data.get(&#34;epsilon&#34;, epsilon)
                    discover_linger_delta = branch_data.get(&#34;linger_delta&#34;, linger_delta)
                    discover_r_outer = branch_data.get(&#34;r_outer&#34;, discover_r_outer)
                    st.info(f&#34;🔧 **Using discover analysis parameters:** decision_mode={discover_decision_mode}, path_length={discover_path_length}, epsilon={discover_epsilon}, linger_delta={discover_linger_delta}, r_outer={discover_r_outer}&#34;)
                    break

        try:
            st.info(&#34;🔬 Analyzing head yaw data for all junctions...&#34;)
            with st.spinner(&#34;Processing head yaw data...&#34;):
                head_yaw_df = compute_head_yaw_at_decisions(
                    trajectories=processed_trajectories,
                    junctions=junctions,
                    assignments_df=chain_df,
                    decision_mode=discover_decision_mode,  # Use discover decision mode
                    r_outer_list=r_outer_list,
                    path_length=discover_path_length,  # Use discover path length
                    epsilon=discover_epsilon,  # Use discover epsilon
                    linger_delta=discover_linger_delta,  # Use discover linger delta
                    base_index=0  # Start from 0 for all junctions
                )

            st.info(&#34;🔬 Analyzing physiological data for all junctions...&#34;)
            with st.spinner(&#34;Processing physiological data...&#34;):
                physio_df = analyze_physiological_at_junctions(
                    trajectories=processed_trajectories,
                    junctions=junctions,
                    assignments_df=chain_df,
                    decision_mode=discover_decision_mode,  # Use discover decision mode
                    r_outer_list=r_outer_list,
                    path_length=discover_path_length,  # Use discover path length
                    epsilon=discover_epsilon,  # Use discover epsilon
                    linger_delta=discover_linger_delta,  # Use discover linger delta
                    physio_window=3.0,
                    base_index=0,
                )

            st.info(&#34;🔬 Analyzing pupil dilation trajectories for all junctions...&#34;)
            with st.spinner(&#34;Processing pupil dilation data...&#34;):
                pupil_df = analyze_pupil_dilation_trajectory(
                    trajectories=processed_trajectories,
                    junctions=junctions,
                    assignments_df=chain_df,
                    decision_mode=discover_decision_mode,  # Use discover decision mode
                    r_outer_list=r_outer_list,
                    path_length=discover_path_length,  # Use discover path length
                    epsilon=discover_epsilon,  # Use discover epsilon
                    linger_delta=discover_linger_delta,  # Use discover linger delta
                    physio_window=3.0,
                    base_index=0,
                )

            # Generate pupil dilation heatmaps for all junctions
            st.info(&#34;🗺️ Generating pupil dilation heatmaps for all junctions...&#34;)
            with st.spinner(&#34;Creating spatial heatmaps...&#34;):
                from verta.verta_gaze import create_per_junction_pupil_heatmap

                # Get heatmap parameters from session state
                cell_size = st.session_state.get(&#39;pupil_heatmap_cell_size&#39;, 3.0)
                normalization = st.session_state.get(&#39;pupil_heatmap_normalization&#39;, &#39;relative&#39;)

                # Generate heatmaps for all junctions
                all_heatmaps = create_per_junction_pupil_heatmap(
                    trajectories=processed_trajectories,
                    junctions=junctions,
                    r_outer_list=r_outer_list,
                    cell_size=cell_size,
                    normalization=normalization,
                    base_index=0  # Start from 0 for all junctions
                )

                st.write(f&#34;🔍 **Generated heatmaps for {len(all_heatmaps)} junctions**&#34;)

            # Debug: Show results summary
            st.info(f&#34;🔍 **Gaze Analysis Results Summary:**&#34;)
            st.write(f&#34;- Head yaw records: {len(head_yaw_df)}&#34;)
            st.write(f&#34;- Physiological records: {len(physio_df)}&#34;)
            st.write(f&#34;- Pupil dilation records: {len(pupil_df)}&#34;)
            st.write(f&#34;- Heatmaps generated: {len(all_heatmaps)}&#34;)

            if len(head_yaw_df) &gt; 0:
                st.write(f&#34;- Junctions with head yaw data: {sorted(head_yaw_df[&#39;junction&#39;].unique())}&#34;)
            if len(physio_df) &gt; 0:
                st.write(f&#34;- Junctions with physiological data: {sorted(physio_df[&#39;junction&#39;].unique())}&#34;)
            if len(pupil_df) &gt; 0:
                st.write(f&#34;- Junctions with pupil data: {sorted(pupil_df[&#39;junction&#39;].unique())}&#34;)

            return {
                &#39;head_yaw&#39;: head_yaw_df,
                &#39;physiological&#39;: physio_df,
                &#39;pupil_dilation&#39;: pupil_df,
                &#39;pupil_heatmap_junction&#39;: all_heatmaps,  # Add heatmaps to results
                &#39;junction&#39;: junctions[0],  # Reference junction
                &#39;r_outer&#39;: r_outer_list[0]  # Reference r_outer
            }

        except Exception as e:
            st.error(f&#34;❌ **Gaze analysis failed:** {e}&#34;)
            st.write(f&#34;**Error type:** {type(e).__name__}&#34;)
            st.write(f&#34;**Error message:** {str(e)}&#34;)

            # Show suggestions based on error type
            if &#34;No assignments found&#34; in str(e):
                st.info(&#34;💡 **Suggestion:** Run &#39;🔍 Discover Branches&#39; analysis first to create proper assignments&#34;)
            elif &#34;trajectory&#34; in str(e).lower():
                st.info(&#34;💡 **Suggestion:** Check if trajectories actually pass through the junctions&#34;)
            elif &#34;column&#34; in str(e).lower():
                st.info(&#34;💡 **Suggestion:** Check your gaze column mappings in the Data tab&#34;)

            return {
                &#39;head_yaw&#39;: pd.DataFrame(),
                &#39;physiological&#39;: pd.DataFrame(),
                &#39;pupil_dilation&#39;: pd.DataFrame(),
                &#39;error&#39;: str(e),
                &#39;error_type&#39;: type(e).__name__
            }

    def _perform_comprehensive_gaze_analysis(self, trajectories, junction, r_outer, decision_mode, path_length, epsilon, linger_delta, out_dir, run_custom_discover=False):
        &#34;&#34;&#34;Perform comprehensive gaze analysis using the actual gaze functions.&#34;&#34;&#34;
        import pandas as pd
        import numpy as np
        from verta.verta_decisions import discover_decision_chain

        # Debug: Check r_outer parameter at function start
        st.write(f&#34;🔍 **DEBUG: r_outer parameter received:** {r_outer} (type: {type(r_outer)})&#34;)

        # Get decision mode and parameters from discover analysis if available
        discover_decision_mode = decision_mode  # Default fallback
        discover_path_length = path_length
        discover_epsilon = epsilon
        discover_linger_delta = linger_delta

        if &#34;branches&#34; in st.session_state.analysis_results:
            # Try to get parameters from the first junction&#39;s stored data
            for junction_key, branch_data in st.session_state.analysis_results[&#34;branches&#34;].items():
                if isinstance(branch_data, dict) and &#34;decision_mode&#34; in branch_data:
                    discover_decision_mode = branch_data[&#34;decision_mode&#34;]
                    discover_path_length = branch_data.get(&#34;path_length&#34;, path_length)
                    discover_epsilon = branch_data.get(&#34;epsilon&#34;, epsilon)
                    discover_linger_delta = branch_data.get(&#34;linger_delta&#34;, linger_delta)
                    st.info(f&#34;🔧 **Using discover analysis parameters:** decision_mode={discover_decision_mode}, path_length={discover_path_length}, epsilon={discover_epsilon}&#34;)
                    break

        # Always define r_outer_list upfront to avoid unbound local errors later
        r_outer_list = [r_outer] if r_outer is not None else [None]

        # First check if we have existing branch assignments from previous discover analysis
        existing_assignments = None
        use_existing_assignments = False

        if &#34;branches&#34; in st.session_state.analysis_results:
            st.info(&#34;🔍 Found existing branch assignments from previous discover analysis!&#34;)

            # Debug: Show available keys
            available_keys = list(st.session_state.analysis_results[&#34;branches&#34;].keys())
            st.write(f&#34;🔍 **Available branch keys:** {available_keys}&#34;)
            st.write(f&#34;🔍 **Looking for junction:** ({junction.cx}, {junction.cz}, r={junction.r})&#34;)

            # First try to find junction-specific assignments (from recent discover analysis)
            junction_found = False
            for junction_key, junction_data in st.session_state.analysis_results[&#34;branches&#34;].items():
                if junction_key.startswith(&#34;junction_&#34;) and isinstance(junction_data, dict):
                    junction_obj = junction_data.get(&#34;junction&#34;)
                    if junction_obj and junction_obj.cx == junction.cx and junction_obj.cz == junction.cz and junction_obj.r == junction.r:
                        st.success(f&#34;✅ **Found junction-specific assignments for {junction_key}!**&#34;)
                        assignments_df = junction_data.get(&#34;assignments&#34;)
                        if assignments_df is not None and not assignments_df.empty:
                            st.write(f&#34;🔍 **Assignments shape:** {assignments_df.shape}&#34;)
                            st.write(f&#34;🔍 **Assignments columns:** {list(assignments_df.columns)}&#34;)
                            st.write(f&#34;🔍 **Sample assignments:**&#34;)
                            st.write(assignments_df.head())

                            existing_assignments = assignments_df
                            use_existing_assignments = True
                            junction_found = True
                            break
                        else:
                            st.warning(f&#34;⚠️ **No assignments found in {junction_key}**&#34;)

            # If no junction-specific assignments found, try chain_decisions as fallback
            if not junction_found and &#34;chain_decisions&#34; in st.session_state.analysis_results[&#34;branches&#34;]:
                st.info(&#34;🔍 **No junction-specific assignments found, trying chain_decisions as fallback**&#34;)
                existing_assignments = st.session_state.analysis_results[&#34;branches&#34;][&#34;chain_decisions&#34;]
                st.write(f&#34;🔍 **Chain decisions shape:** {existing_assignments.shape}&#34;)
                st.write(f&#34;🔍 **Chain decisions columns:** {list(existing_assignments.columns)}&#34;)

                # Check if this junction has assignments in the chain decisions
                try:
                    junction_index = next(i for i, j in enumerate(st.session_state.junctions)
                                          if j.cx == junction.cx and j.cz == junction.cz and j.r == junction.r)

                    # Look for junction-specific branch column
                    branch_col = f&#34;branch_j{junction_index}&#34;
                    if branch_col in existing_assignments.columns:
                        st.success(f&#34;✅ **Found assignments for Junction {junction_index} in chain_decisions!**&#34;)
                        st.write(f&#34;🔍 **Branch column:** {branch_col}&#34;)

                        # Filter to only trajectories with assignments for this junction
                        assigned_mask = existing_assignments[branch_col].notna() &amp; (existing_assignments[branch_col] &gt;= 0)
                        junction_assignments = existing_assignments[assigned_mask].copy()

                        st.write(f&#34;🔍 **Junction {junction_index} assignments:** {len(junction_assignments)} trajectories&#34;)
                        if len(junction_assignments) &gt; 0:
                            st.write(f&#34;🔍 **Sample assignments:**&#34;)
                            st.write(junction_assignments[[&#39;trajectory&#39;, branch_col]].head())

                            # Set the found assignments
                            existing_assignments = junction_assignments
                            use_existing_assignments = True
                        else:
                            st.warning(f&#34;⚠️ **No assignments found for Junction {junction_index} in chain_decisions**&#34;)
                            existing_assignments = None
                    else:
                        st.warning(f&#34;⚠️ **Branch column {branch_col} not found in chain_decisions**&#34;)
                        st.write(f&#34;Available branch columns: {[col for col in existing_assignments.columns if col.startswith(&#39;branch_j&#39;)]}&#34;)
                        existing_assignments = None
                        found_key = None

                except StopIteration:
                    st.warning(&#34;⚠️ **Could not find junction index in session state junctions**&#34;)
                    existing_assignments = None

            # If we found assignments, show summary
            if existing_assignments is not None and use_existing_assignments:
                try:
                    num_rows = len(existing_assignments)
                    st.success(f&#34;✅ **Using existing assignments: {num_rows} trajectories**&#34;)
                except Exception:
                    st.success(&#34;✅ **Using existing assignments**&#34;)

                # Debug: Show sample of assignments data
                st.write(f&#34;🔍 **Sample assignments data:**&#34;)
                if hasattr(existing_assignments, &#39;head&#39;):
                    st.write(existing_assignments.head())
                st.write(f&#34;🔍 **Assignments columns:** {list(existing_assignments.columns)}&#34;)

                # Check if assignments have the expected structure
                if &#39;branch&#39; in existing_assignments.columns:
                    assigned_count = existing_assignments[&#39;branch&#39;].notna().sum()
                    st.write(f&#34;🔍 **Total assigned trajectories:** {assigned_count}&#34;)
                elif any(col.startswith(&#39;branch_j&#39;) for col in existing_assignments.columns):
                    branch_cols = [col for col in existing_assignments.columns if col.startswith(&#39;branch_j&#39;)]
                    st.write(f&#34;🔍 **Found branch columns:** {branch_cols}&#34;)
                else:
                    st.warning(&#34;⚠️ **Unexpected assignments structure**&#34;)

            # If no existing assignments found, we&#39;ll need to run discover analysis
            if existing_assignments is None:
                st.info(&#34;🔍 **No existing assignments found - will run discover analysis**&#34;)
                use_existing_assignments = False
        else:
            st.info(&#34;🔍 **No existing branch assignments found - will run discover analysis**&#34;)
            use_existing_assignments = False

        # Use existing assignments if available and selected, otherwise run discover analysis
        chain_df = None

        # Use the current checkbox selections (fall back to session_state if not set)
        # CRITICAL: Respect user&#39;s choice first, then check for existing assignments
        user_wants_existing = st.session_state.get(&#39;use_existing_assignments&#39;, False)
        user_wants_custom = st.session_state.get(&#39;run_custom_discover&#39;, False)

        # If user explicitly chose custom parameters, ignore existing assignments
        if user_wants_custom:
            use_existing_assignments = False
            run_custom_discover = True
        else:
            use_existing_assignments = use_existing_assignments and user_wants_existing
            run_custom_discover = user_wants_custom

        # Debug: Show what parameters are being used
        st.info(f&#34;🔍 **Parameter Source Debug:**&#34;)
        st.write(f&#34;- Using existing assignments: {use_existing_assignments}&#34;)
        st.write(f&#34;- Running custom discover: {run_custom_discover}&#34;)
        if run_custom_discover and &#39;custom_discover_params&#39; in st.session_state:
            st.write(f&#34;- Custom parameters available: {list(st.session_state.custom_discover_params.keys())}&#34;)
            st.write(f&#34;- Custom decision mode: {st.session_state.custom_discover_params.get(&#39;decision_mode&#39;, &#39;not set&#39;)}&#34;)

            # Show actual parameter values being used
            st.write(&#34;🔧 **Custom Parameters Being Used:**&#34;)
            custom_params = st.session_state.custom_discover_params
            for param_name, param_value in custom_params.items():
                st.write(f&#34;- {param_name}: {param_value}&#34;)

            # Check if parameters might be too restrictive
            st.write(&#34;🔍 **Parameter Restrictiveness Check:**&#34;)
            if custom_params.get(&#39;eps&#39;, 0.5) &lt; 1.0:
                st.warning(f&#34;⚠️ DBSCAN eps={custom_params.get(&#39;eps&#39;, 0.5)} might be too restrictive (try 1.0-2.0)&#34;)
            if custom_params.get(&#39;min_samples&#39;, 5) &gt; 3:
                st.warning(f&#34;⚠️ DBSCAN min_samples={custom_params.get(&#39;min_samples&#39;, 5)} might be too high (try 2-3)&#34;)
            if custom_params.get(&#39;path_length&#39;, 100.0) &gt; 50.0:
                st.warning(f&#34;⚠️ Path length={custom_params.get(&#39;path_length&#39;, 100.0)} might be too high (try 20-50)&#34;)
        else:
            st.write(&#34;- Using default parameters&#34;)

        if existing_assignments is not None and use_existing_assignments:
            # CRITICAL FIX: Ensure we&#39;re using the correct junction-specific assignments
            # If we&#39;re using chain_decisions, we need to extract the specific junction&#39;s assignments
            if &#34;chain_decisions&#34; in str(type(existing_assignments)) or &#34;branch_j&#34; in existing_assignments.columns:
                # This is the chain_decisions DataFrame - extract junction-specific assignments
                junction_index = next(i for i, j in enumerate(st.session_state.junctions)
                                    if j.cx == junction.cx and j.cz == junction.cz and j.r == junction.r)

                st.info(f&#34;🔧 **Extracting Junction {junction_index} assignments from chain_decisions...**&#34;)

                # Create a junction-specific assignments DataFrame
                junction_assignments = existing_assignments[[&#34;trajectory&#34;]].copy()

                # Add the junction-specific branch column
                branch_col = f&#34;branch_j{junction_index}&#34;
                if branch_col in existing_assignments.columns:
                    junction_assignments[&#34;branch&#34;] = existing_assignments[branch_col]

                    # Add decision point columns if available
                    if &#34;decision_idx&#34; in existing_assignments.columns:
                        junction_assignments[&#34;decision_idx&#34;] = existing_assignments[&#34;decision_idx&#34;]
                    if &#34;intercept_x&#34; in existing_assignments.columns:
                        junction_assignments[&#34;intercept_x&#34;] = existing_assignments[&#34;intercept_x&#34;]
                    if &#34;intercept_z&#34; in existing_assignments.columns:
                        junction_assignments[&#34;intercept_z&#34;] = existing_assignments[&#34;intercept_z&#34;]

                    # Filter to only assigned trajectories
                    assigned_mask = junction_assignments[&#34;branch&#34;].notna() &amp; (junction_assignments[&#34;branch&#34;] &gt;= 0)
                    junction_assignments = junction_assignments[assigned_mask]

                    st.success(f&#34;✅ **Extracted Junction {junction_index} assignments:** {len(junction_assignments)} trajectories&#34;)
                    st.write(f&#34;🔍 **Branch column used:** {branch_col}&#34;)
                    st.write(f&#34;🔍 **Sample assignments:**&#34;)
                    st.write(junction_assignments.head())

                    chain_df = junction_assignments
                else:
                    st.error(f&#34;❌ **Branch column {branch_col} not found in chain_decisions!**&#34;)
                    st.write(f&#34;Available branch columns: {[col for col in existing_assignments.columns if col.startswith(&#39;branch_j&#39;)]}&#34;)
                    chain_df = pd.DataFrame()  # Empty DataFrame
            else:
                # This is already a junction-specific assignments DataFrame
                chain_df = existing_assignments
                st.success(f&#34;✅ Using existing assignments - found {len(chain_df)} assignments&#34;)

            st.info(f&#34;🔍 **GAZE ANALYSIS PATH:** Using existing assignments from previous discover analysis&#34;)
        else:
            # Show discover analysis parameters
            st.info(&#34;🔍 Running discover analysis to get branch assignments...&#34;)
            st.info(f&#34;🔍 **GAZE ANALYSIS PATH:** Running discover analysis (not using existing assignments)&#34;)

            # Add parameter adjustment suggestions
            st.warning(&#34;⚠️ **Low assignment rate detected!** Try these adjustments:&#34;)
            st.write(&#34;**Suggested parameter changes:**&#34;)
            st.write(&#34;- **Path length**: Try reducing from 100.0 to 20.0-50.0&#34;)
            st.write(&#34;- **Decision mode**: Try &#39;hybrid&#39; instead of &#39;pathlen&#39;&#34;)
            st.write(&#34;- **Junction position/radius**: Verify they match your trajectory data&#34;)
            st.write(&#34;&#34;)
            st.write(&#34;**Or run &#39;🔍 Discover Branches&#39; analysis first to create proper assignments!**&#34;)

            try:
                # Use custom parameters if provided
                if run_custom_discover and &#39;custom_discover_params&#39; in st.session_state:
                    custom_params = st.session_state.custom_discover_params

                    # Update parameters with custom values
                    cluster_method = custom_params.get(&#39;cluster_method&#39;, &#39;kmeans&#39;)
                    seed = custom_params.get(&#39;seed&#39;, 0)
                    decision_mode = custom_params.get(&#39;decision_mode&#39;, &#39;hybrid&#39;)

                    # Decision mode specific parameters
                    if decision_mode == &#34;radial&#34;:
                        # Don&#39;t override r_outer - use the value passed to the function
                        # r_outer = custom_params.get(&#39;r_outer&#39;, 50.0)  # REMOVED: This was overriding the correct value
                        epsilon = custom_params.get(&#39;epsilon&#39;, 0.05)
                        path_length = None
                        linger_delta = None
                    elif decision_mode == &#34;pathlen&#34;:
                        path_length = custom_params.get(&#39;path_length&#39;, 100.0)
                        linger_delta = custom_params.get(&#39;linger_delta&#39;, 0.0)
                        # Don&#39;t override r_outer - use the value passed to the function
                        # r_outer = None  # REMOVED: This was causing the TypeError
                        epsilon = custom_params.get(&#39;epsilon&#39;, 0.05)  # Provide default epsilon for pathlen mode
                    elif decision_mode == &#34;hybrid&#34;:
                        # Don&#39;t override r_outer - use the value passed to the function
                        # r_outer = custom_params.get(&#39;r_outer&#39;, 50.0)  # REMOVED: This was overriding the correct value
                        path_length = custom_params.get(&#39;path_length&#39;, 100.0)
                        epsilon = custom_params.get(&#39;epsilon&#39;, 0.05)  # Hybrid mode needs epsilon for DBSCAN
                        linger_delta = custom_params.get(&#39;linger_delta&#39;, 0.0)

                    # Cluster method specific parameters
                    if cluster_method == &#34;dbscan&#34;:
                        # Use epsilon parameter for DBSCAN (not eps)
                        epsilon = custom_params.get(&#39;eps&#39;, 0.5)  # Map eps to epsilon
                        min_samples = custom_params.get(&#39;min_samples&#39;, 5)
                        angle_eps = custom_params.get(&#39;angle_eps&#39;, 15.0)
                        # DBSCAN doesn&#39;t use k parameters, but discover_decision_chain expects them
                        k = 3  # Default value for discover_decision_chain
                        k_min = 2  # Default value for discover_decision_chain
                        k_max = 6  # Default value for discover_decision_chain
                        min_sep_deg = 12.0  # Default value for discover_decision_chain

                        # Ensure no None values are passed to discover_decision_chain
                        if epsilon is None:
                            epsilon = 0.5
                        if min_samples is None:
                            min_samples = 5
                        if angle_eps is None:
                            angle_eps = 15.0

                        # Debug: Show DBSCAN parameters
                        st.write(f&#34;**DBSCAN Parameters:** epsilon={epsilon}, min_samples={min_samples}, angle_eps={angle_eps}&#34;)
                    elif cluster_method == &#34;kmeans&#34;:
                        k = custom_params.get(&#39;k&#39;, 3)
                        k_min = custom_params.get(&#39;k_min&#39;, 2)
                        k_max = custom_params.get(&#39;k_max&#39;, 6)
                        eps = None
                        min_samples = None
                        angle_eps = None
                        min_sep_deg = None

                        # Ensure no None values are passed to discover_decision_chain
                        if k is None:
                            k = 3
                        if k_min is None:
                            k_min = 2
                        if k_max is None:
                            k_max = 6
                    elif cluster_method == &#34;auto&#34;:
                        k_min = custom_params.get(&#39;k_min&#39;, 2)
                        k_max = custom_params.get(&#39;k_max&#39;, 6)
                        min_sep_deg = custom_params.get(&#39;min_sep_deg&#39;, 12.0)
                        angle_eps = custom_params.get(&#39;angle_eps&#39;, 15.0)
                        eps = None
                        min_samples = None
                        k = None

                        # Ensure no None values are passed to discover_decision_chain
                        if k_min is None:
                            k_min = 2
                        if k_max is None:
                            k_max = 6
                        if min_sep_deg is None:
                            min_sep_deg = 12.0
                        if angle_eps is None:
                            angle_eps = 15.0

                    st.info(f&#34;🔧 Using custom parameters: cluster_method={cluster_method}, decision_mode={decision_mode}, seed={seed}&#34;)
                else:
                    # Use default parameters
                    cluster_method = &#34;kmeans&#34;
                    seed = 0
                    decision_mode = &#34;hybrid&#34;
                    # Use the r_outer value defined in the junctions tab
                    # Don&#39;t override it with hardcoded logic
                    path_length = 100.0
                    epsilon = 0.05  # Default epsilon for hybrid mode
                    linger_delta = 0.0
                    # Even if not used by the selected cluster method, avoid passing None
                    eps = 0.5
                    min_samples = 5
                    angle_eps = 15.0
                    k = 3
                    k_min = 2
                    k_max = 6
                    min_sep_deg = 12.0  # Default value for discover_decision_chain

                # r_outer_list already initialized above; update if user changed r_outer
                r_outer_list = [r_outer] if r_outer is not None else [None]

                # Debug: Show parameters being used
                st.info(f&#34;🔍 **Discover Analysis Parameters:**&#34;)
                st.write(f&#34;- Junction: Circle(cx={junction.cx}, cz={junction.cz}, r={junction.r})&#34;)
                st.write(f&#34;- Decision mode: {decision_mode}&#34;)
                st.write(f&#34;🔍 **DEBUG: r_outer before debug message:** {r_outer} (type: {type(r_outer)})&#34;)
                st.write(f&#34;- R outer: {r_outer} (from junctions tab, r_outer_list: {r_outer_list})&#34;)
                st.write(f&#34;- Path length: {path_length}&#34;)
                st.write(f&#34;- Cluster method: {cluster_method}, k: {k}&#34;)
                st.write(f&#34;- Trajectories: {len(trajectories)}&#34;)

                # Additional debugging for radial mode issues
                if decision_mode == &#34;radial&#34;:
                    st.error(f&#34;🚨 **RADIAL MODE DETECTED - LIKELY CAUSE OF NO ASSIGNMENTS!**&#34;)
                    st.write(f&#34;- Junction radius: {junction.r}&#34;)
                    st.write(f&#34;- R outer: {r_outer}&#34;)
                    st.write(f&#34;- Ratio (r_outer/junction.r): {r_outer/junction.r:.2f}&#34;)
                    if r_outer &lt;= junction.r:
                        st.error(f&#34;❌ **CRITICAL:** r_outer ({r_outer}) &lt;= junction radius ({junction.r})!&#34;)
                        st.write(&#34;In radial mode, r_outer must be significantly larger than junction radius.&#34;)
                    elif r_outer/junction.r &lt; 2.0:
                        st.warning(f&#34;⚠️ **WARNING:** r_outer/junction.r ratio ({r_outer/junction.r:.2f}) is too low!&#34;)
                        st.write(&#34;For radial mode, r_outer should be at least 2x the junction radius for reliable detection.&#34;)

                    st.error(f&#34;🔧 **RECOMMENDED FIXES:**&#34;)
                    st.write(&#34;1. **Change decision mode to &#39;hybrid&#39;** in the gaze analysis custom parameters&#34;)
                    st.write(&#34;2. **Or increase r_outer values** in the Junctions tab to at least 2x the junction radius&#34;)
                    st.write(&#34;3. **Or use default parameters** instead of custom parameters&#34;)

                    # Show current parameter status
                    st.write(f&#34;**Current decision mode:** {decision_mode}&#34;)
                    st.write(&#34;**To fix:** Change the decision mode to &#39;hybrid&#39; in the analysis parameters&#34;)

                # Debug: Check if trajectories pass through junction
                trajectories_through_junction = 0
                for traj in trajectories[:5]:  # Check first 5 trajectories
                    # Handle NaN values in trajectory coordinates
                    valid_mask = ~(np.isnan(traj.x) | np.isnan(traj.z))
                    if np.any(valid_mask):
                        valid_x = traj.x[valid_mask]
                        valid_z = traj.z[valid_mask]
                        distances = np.sqrt((valid_x - junction.cx)**2 + (valid_z - junction.cz)**2)
                        min_distance = np.min(distances)
                        if min_distance &lt;= junction.r:
                            trajectories_through_junction += 1

                st.write(f&#34;- Sample trajectories through junction: {trajectories_through_junction}/5&#34;)
                if trajectories_through_junction == 0:
                    st.warning(&#34;⚠️ **Warning: No sample trajectories pass through the junction!**&#34;)
                    st.write(&#34;This might explain why no assignments are found.&#34;)
                    st.write(&#34;Check if junction coordinates match your trajectory data.&#34;)

                    # Additional debugging: Show coordinate ranges
                    st.error(&#34;🔍 **Coordinate Range Analysis:**&#34;)
                    all_x = np.concatenate([traj.x for traj in trajectories[:10]])  # Check first 10 trajectories
                    all_z = np.concatenate([traj.z for traj in trajectories[:10]])

                    # Handle NaN values in trajectory coordinates
                    valid_x = all_x[~np.isnan(all_x)]
                    valid_z = all_z[~np.isnan(all_z)]

                    if len(valid_x) &gt; 0 and len(valid_z) &gt; 0:
                        st.write(f&#34;- Trajectory X range: {np.min(valid_x):.1f} to {np.max(valid_x):.1f}&#34;)
                        st.write(f&#34;- Trajectory Z range: {np.min(valid_z):.1f} to {np.max(valid_z):.1f}&#34;)
                        st.write(f&#34;- Junction position: ({junction.cx}, {junction.cz})&#34;)
                        st.write(f&#34;- Junction radius: {junction.r}&#34;)

                        # Check if junction is within trajectory bounds
                        x_in_bounds = np.min(valid_x) &lt;= junction.cx &lt;= np.max(valid_x)
                        z_in_bounds = np.min(valid_z) &lt;= junction.cz &lt;= np.max(valid_z)
                        st.write(f&#34;- Junction X in trajectory bounds: {x_in_bounds}&#34;)
                        st.write(f&#34;- Junction Z in trajectory bounds: {z_in_bounds}&#34;)

                        if not (x_in_bounds and z_in_bounds):
                            st.error(&#34;❌ **CRITICAL:** Junction is outside trajectory coordinate bounds!&#34;)
                            st.write(&#34;**Solution:** Adjust junction coordinates in the Junctions tab to match your trajectory data.&#34;)
                    else:
                        st.error(&#34;❌ **CRITICAL:** All trajectory coordinates are NaN!&#34;)
                        st.write(&#34;**This indicates a data loading or scaling issue.**&#34;)
                        st.write(&#34;**Solutions:**&#34;)
                        st.write(&#34;1. Check if trajectory data was loaded correctly&#34;)
                        st.write(&#34;2. Check if scaling factor is appropriate&#34;)
                        st.write(&#34;3. Check if coordinate columns are mapped correctly&#34;)
                        st.write(&#34;4. Try reloading the data with different parameters&#34;)

                        # Show sample trajectory data for debugging
                        if len(trajectories) &gt; 0:
                            sample_traj = trajectories[0]
                            st.write(f&#34;**Sample trajectory data:**&#34;)
                            st.write(f&#34;- X values: {sample_traj.x[:5]} (first 5)&#34;)
                            st.write(f&#34;- Z values: {sample_traj.z[:5]} (first 5)&#34;)
                            st.write(f&#34;- X type: {type(sample_traj.x)}&#34;)
                            st.write(f&#34;- Z type: {type(sample_traj.z)}&#34;)

                # Debug: Show all parameters being passed to discover_decision_chain
                st.write(f&#34;🔍 **DEBUG: Parameters being passed to discover_decision_chain:**&#34;)
                st.write(f&#34;- path_length: {path_length} (type: {type(path_length)})&#34;)
                st.write(f&#34;- epsilon: {epsilon} (type: {type(epsilon)})&#34;)
                st.write(f&#34;- seed: {seed} (type: {type(seed)})&#34;)
                st.write(f&#34;- decision_mode: {decision_mode} (type: {type(decision_mode)})&#34;)
                st.write(f&#34;- r_outer_list: {r_outer_list} (type: {type(r_outer_list)})&#34;)
                st.write(f&#34;- linger_delta: {linger_delta} (type: {type(linger_delta)})&#34;)
                st.write(f&#34;- cluster_method: {cluster_method} (type: {type(cluster_method)})&#34;)
                st.write(f&#34;- k: {k} (type: {type(k)})&#34;)
                st.write(f&#34;- k_min: {k_min} (type: {type(k_min)})&#34;)
                st.write(f&#34;- k_max: {k_max} (type: {type(k_max)})&#34;)
                st.write(f&#34;- min_sep_deg: {min_sep_deg} (type: {type(min_sep_deg)})&#34;)
                st.write(f&#34;- angle_eps: {angle_eps} (type: {type(angle_eps)})&#34;)
                st.write(f&#34;- min_samples: {min_samples} (type: {type(min_samples)})&#34;)

                try:
                    chain_df, centers_list, decisions_chain_df = discover_decision_chain(
                        trajectories=trajectories,
                        junctions=[junction],
                        path_length=path_length,
                        epsilon=epsilon,
                        seed=seed,
                        decision_mode=decision_mode,
                        r_outer_list=r_outer_list,
                        linger_delta=linger_delta if linger_delta is not None else 0.0,
                        out_dir=out_dir,
                        cluster_method=cluster_method,
                        k=k,
                        k_min=k_min,
                        k_max=k_max,
                        min_sep_deg=min_sep_deg,
                        angle_eps=angle_eps,
                        min_samples=min_samples,
                    )
                    # Save decisions into session state for reuse by gaze
                    try:
                        if decisions_chain_df is not None and len(decisions_chain_df) &gt; 0:
                            st.session_state.analysis_results.setdefault(&#34;branches&#34;, {})
                            st.session_state.analysis_results[&#34;branches&#34;][&#34;chain_decisions&#34;] = decisions_chain_df
                    except Exception:
                        pass
                except Exception as e:
                    st.error(f&#34;❌ **Discover analysis failed - this will prevent gaze analysis!**&#34;)
                    st.write(f&#34;**Error type:** {type(e).__name__}&#34;)
                    st.write(f&#34;**Error message:** {str(e)}&#34;)

                    # Show the most common issues and solutions
                    st.error(&#34;🔧 **Common Solutions:**&#34;)
                    if &#34;int()&#34; in str(e) and &#34;NoneType&#34; in str(e):
                        st.write(&#34;**Issue:** NoneType to int() conversion error&#34;)
                        st.write(&#34;**Solution:** Use &#39;Use existing branch assignments&#39; or run &#39;🔍 Discover Branches&#39; first&#34;)
                    elif &#34;DBSCAN&#34; in str(e) or &#34;eps&#34; in str(e):
                        st.write(&#34;**Issue:** DBSCAN clustering failed&#34;)
                        st.write(&#34;**Solution:** Try &#39;Use existing branch assignments&#39; or adjust DBSCAN parameters&#34;)
                    elif &#34;trajectory&#34; in str(e).lower():
                        st.write(&#34;**Issue:** Trajectory data problem&#34;)
                        st.write(&#34;**Solution:** Check if trajectories pass through the junction&#34;)
                    else:
                        st.write(&#34;**Solution:** Use &#39;Use existing branch assignments&#39; or run &#39;🔍 Discover Branches&#39; first&#34;)

                    st.write(&#34;&#34;)
                    st.write(&#34;**💡 Recommended approach:**&#34;)
                    st.write(&#34;1. **Use existing assignments** (if available)&#34;)
                    st.write(&#34;2. **Or run &#39;🔍 Discover Branches&#39; analysis first**&#34;)
                    st.write(&#34;3. **Then return here for gaze analysis**&#34;)

                    # Fall back to empty assignments
                    import pandas as pd
                    chain_df = pd.DataFrame()
                    centers_list = []
                st.success(f&#34;✅ Discover analysis completed - found {len(chain_df)} assignments&#34;)
                st.write(f&#34;**Assignment Summary:**&#34;)
                st.write(f&#34;- Total trajectories processed: {len(trajectories)}&#34;)
                st.write(f&#34;- Trajectories with assignments: {len(chain_df)}&#34;)
                st.write(f&#34;- Assignment rate: {len(chain_df)/len(trajectories)*100:.1f}%&#34;)

                # Show sample of assignments
                if len(chain_df) &gt; 0:
                    st.write(f&#34;**Sample Assignments:**&#34;)
                    st.dataframe(chain_df.head(10), width=&#39;stretch&#39;)
                else:
                    st.warning(&#34;⚠️ No trajectories were assigned to this junction!&#34;)
                    st.write(&#34;**Possible causes:**&#34;)
                    st.write(&#34;- Junction position/radius may not match trajectory data&#34;)
                    st.write(&#34;- Decision parameters may be too restrictive&#34;)
                    st.write(&#34;- Trajectories may not actually pass through this junction&#34;)

                    # Add specific parameter suggestions
                    st.info(&#34;🔧 **Parameter Adjustment Suggestions:**&#34;)
                    st.write(&#34;**Current parameters:**&#34;)
                    st.write(f&#34;- Path length: {path_length} (try reducing to 20-50)&#34;)
                    st.write(f&#34;- Decision mode: {decision_mode}&#34;)
                    st.write(f&#34;- DBSCAN eps: {epsilon} (try increasing to 1.0-2.0)&#34;)
                    st.write(f&#34;- DBSCAN min_samples: {min_samples} (try reducing to 2-3)&#34;)
                    st.write(f&#34;- R_outer: {r_outer} (try increasing)&#34;)

                    st.write(&#34;**Suggested changes:**&#34;)
                    st.write(&#34;1. **Reduce path_length** from 100.0 to 20.0-50.0&#34;)
                    st.write(&#34;2. **Increase DBSCAN eps** from 0.5 to 1.0-2.0&#34;)
                    st.write(&#34;3. **Reduce min_samples** from 5 to 2-3&#34;)
                    st.write(&#34;4. **Check junction position** - ensure it matches your trajectory data&#34;)

                    # Debug: Show trajectory data around junction
                    st.write(&#34;🔍 **Trajectory Data Around Junction:**&#34;)
                    junction_x, junction_z = junction.cx, junction.cz
                    junction_r = junction.r

                    # Find trajectories that pass near the junction
                    nearby_trajectories = []
                    for i, traj in enumerate(trajectories[:10]):  # Check first 10 trajectories
                        distances = np.sqrt((traj.x - junction_x)**2 + (traj.z - junction_z)**2)
                        min_distance = np.min(distances)
                        if min_distance &lt; junction_r * 3:  # Within 3x junction radius
                            nearby_trajectories.append((i, min_distance))

                    if nearby_trajectories:
                        st.write(f&#34;- Found {len(nearby_trajectories)} trajectories within 3x junction radius&#34;)
                        for traj_idx, min_dist in nearby_trajectories[:5]:
                            st.write(f&#34;  - Trajectory {traj_idx}: min distance = {min_dist:.1f}&#34;)
                    else:
                        st.write(&#34;- No trajectories found within 3x junction radius&#34;)
                        st.write(&#34;- This suggests the junction position may not match your data&#34;)
            except Exception as e:
                st.warning(f&#34;⚠️ Discover analysis failed: {e}&#34;)
                st.info(&#34;🔄 Falling back to empty assignments...&#34;)
                chain_df = pd.DataFrame()

        # Preprocess trajectories to convert time values to numeric format
        processed_trajectories = []
        for traj in trajectories:
            if hasattr(traj, &#39;t&#39;) and traj.t is not None:
                # Convert time values to numeric if they&#39;re strings
                if isinstance(traj.t[0], str):
                    try:
                        import pandas as pd
                        # Convert string time format to numeric seconds
                        numeric_times = []
                        for t_val in traj.t:
                            if isinstance(t_val, str):
                                # Parse time string like &#34;00:00:17.425&#34;
                                time_parts = t_val.split(&#39;:&#39;)
                                if len(time_parts) == 3:
                                    hours = float(time_parts[0])
                                    minutes = float(time_parts[1])
                                    seconds = float(time_parts[2])
                                    total_seconds = hours * 3600 + minutes * 60 + seconds
                                    numeric_times.append(total_seconds)
                                else:
                                    numeric_times.append(float(t_val))
                            else:
                                numeric_times.append(float(t_val))
                        traj.t = np.array(numeric_times)
                    except Exception as e:
                        st.warning(f&#34;⚠️ Could not convert time values for trajectory {traj.tid}: {e}&#34;)
                        # Keep original time values
                processed_trajectories.append(traj)
            else:
                processed_trajectories.append(traj)

        # Store debug information in centralized gaze debug info
        junction_key = f&#34;junction_{junction.cx}_{junction.cz}_{junction.r}&#34;
        if &#39;gaze_debug_info&#39; not in st.session_state:
            st.session_state[&#39;gaze_debug_info&#39;] = {}

        st.session_state[&#39;gaze_debug_info&#39;][junction_key] = {
            &#39;trajectories_count&#39;: len(processed_trajectories),
            &#39;assignments_count&#39;: len(chain_df),
            &#39;junction&#39;: f&#34;Circle(cx={junction.cx}, cz={junction.cz}, r={junction.r})&#34;,
            &#39;decision_mode&#39;: decision_mode,
            &#39;r_outer&#39;: r_outer,
            &#39;status&#39;: &#39;processing&#39;
        }

        # Check if we have any assignments
        if len(chain_df) == 0:
            st.session_state[&#39;gaze_debug_info&#39;][junction_key][&#39;status&#39;] = &#39;no_assignments&#39;

            st.warning(&#34;⚠️ No trajectory assignments found - cannot perform gaze analysis&#34;)
            st.info(&#34;💡 **Suggestions:**&#34;)
            st.write(&#34;1. **Adjust parameters**: Try different decision mode or parameters&#34;)
            st.write(&#34;2. **Check junction position**: Verify junction coordinates match your data&#34;)
            st.write(&#34;3. **Run discover analysis first**: Use &#39;🔍 Discover Branches&#39; to create assignments&#34;)
            st.write(&#34;4. **Check trajectory data**: Ensure trajectories actually pass through junctions&#34;)

            # Return empty results
            return {
                &#39;physiological&#39;: None,
                &#39;pupil_dilation&#39;: None,
                &#39;head_yaw&#39;: None,
                &#39;junction&#39;: junction,
                &#39;r_outer&#39;: r_outer,
                &#39;error&#39;: &#39;No assignments found&#39;
            }

        # Use the actual gaze analysis functions with proper assignments
        try:
            # Update debug status
            st.session_state[&#39;gaze_debug_info&#39;][junction_key][&#39;status&#39;] = &#39;analyzing&#39;

            # Debug: Check what trajectories are being passed to gaze analysis
            st.info(f&#34;🔍 **Gaze Analysis Input Debug:**&#34;)
            st.write(f&#34;- Trajectories passed: {len(processed_trajectories)}&#34;)
            if processed_trajectories:
                sample_traj = processed_trajectories[0]
                st.write(f&#34;- Sample trajectory type: {type(sample_traj).__name__}&#34;)
                st.write(f&#34;- Sample trajectory ID: {sample_traj.tid}&#34;)
                st.write(f&#34;- Has gaze_x: {hasattr(sample_traj, &#39;gaze_x&#39;) and sample_traj.gaze_x is not None}&#34;)
                st.write(f&#34;- Has heart_rate: {hasattr(sample_traj, &#39;heart_rate&#39;) and sample_traj.heart_rate is not None}&#34;)
                st.write(f&#34;- Has pupil_l: {hasattr(sample_traj, &#39;pupil_l&#39;) and sample_traj.pupil_l is not None}&#34;)

            # Debug: Check assignments DataFrame structure
            st.info(f&#34;🔍 **Assignments DataFrame Debug:**&#34;)
            st.write(f&#34;- Assignments shape: {chain_df.shape}&#34;)
            st.write(f&#34;- Assignments columns: {list(chain_df.columns)}&#34;)
            if len(chain_df) &gt; 0:
                st.write(f&#34;- Sample assignment row:&#34;)
                st.write(chain_df.head(1))
                # Safe sorting of trajectory IDs (handle mixed types)
                try:
                    unique_ids = chain_df[&#39;trajectory&#39;].unique()
                    # Convert to strings for safe sorting
                    unique_ids_str = [str(id) for id in unique_ids]
                    sorted_ids = sorted(unique_ids_str)[:10]
                    st.write(f&#34;- Unique trajectory IDs in assignments: {sorted_ids}...&#34;)
                except Exception as e:
                    st.write(f&#34;- Unique trajectory IDs in assignments: {list(unique_ids)[:10]}... (sorting failed: {e})&#34;)
                st.write(f&#34;- Sample trajectory IDs from Trajectory objects: {[tr.tid for tr in processed_trajectories[:5]]}&#34;)

                # Check for ID mismatch (handle mixed types)
                assignment_ids = set(str(id) for id in chain_df[&#39;trajectory&#39;].unique())
                trajectory_ids = set(str(tr.tid) for tr in processed_trajectories)
                common_ids = assignment_ids.intersection(trajectory_ids)
                st.write(f&#34;- Common IDs between assignments and trajectories: {len(common_ids)}&#34;)
                if len(common_ids) &lt; 10:
                    st.write(f&#34;- Common IDs: {sorted(common_ids)}&#34;)
                else:
                    st.write(f&#34;- Common IDs (first 10): {sorted(list(common_ids))[:10]}&#34;)

            # Debug: Check session state at the beginning of gaze analysis
            st.write(f&#34;🔍 **Gaze Analysis Session State Check:**&#34;)
            st.write(f&#34;- analysis_results exists: {st.session_state.analysis_results is not None}&#34;)
            if st.session_state.analysis_results is not None:
                st.write(f&#34;- branches exists: {&#39;branches&#39; in st.session_state.analysis_results}&#34;)
                if &#39;branches&#39; in st.session_state.analysis_results:
                    branches = st.session_state.analysis_results[&#39;branches&#39;]
                    st.write(f&#34;- chain_decisions exists: {&#39;chain_decisions&#39; in branches}&#34;)
                    if &#39;chain_decisions&#39; in branches:
                        decisions_chain_df = branches[&#39;chain_decisions&#39;]
                        st.write(f&#34;- chain_decisions length: {len(decisions_chain_df)}&#34;)
                        st.write(f&#34;- Junction indices in chain_decisions: {sorted(decisions_chain_df[&#39;junction_index&#39;].unique())}&#34;)
                    else:
                        st.write(&#34;- chain_decisions not found in branches!&#34;)
                else:
                    st.write(&#34;- branches not found in analysis_results!&#34;)
            else:
                st.write(&#34;- analysis_results is None!&#34;)

            # Analyze physiological data
            st.info(&#34;🔬 Analyzing physiological data...&#34;)
            with st.spinner(&#34;Processing physiological data...&#34;):
                # Debug: Test trajectory matching before calling the function
                st.write(&#34;🔍 **Pre-analysis Debug:**&#34;)
                test_traj = processed_trajectories[0]
                test_assignments = chain_df[chain_df[&#34;trajectory&#34;] == test_traj.tid]
                st.write(f&#34;- Test trajectory ID: {test_traj.tid}&#34;)
                st.write(f&#34;- Test trajectory assignments: {len(test_assignments)} rows&#34;)
                if len(test_assignments) &gt; 0:
                    st.write(f&#34;- Test assignment: {test_assignments.iloc[0].to_dict()}&#34;)
                else:
                    st.write(&#34;- ❌ No assignments found for test trajectory!&#34;)

                # Fix: Convert single &#39;branch&#39; column to junction-specific format expected by gaze functions
                current_junction_idx = next(i for i, j in enumerate(st.session_state.junctions)
                                           if j.cx == junction.cx and j.cz == junction.cz and j.r == junction.r)
                current_branch_col = f&#39;branch_j{current_junction_idx}&#39;

                if &#39;branch&#39; in chain_df.columns:
                    if current_branch_col not in chain_df.columns:
                        st.info(f&#34;🔧 Converting single &#39;branch&#39; column to &#39;{current_branch_col}&#39; format for gaze analysis...&#34;)
                        chain_df_fixed = chain_df.copy()
                        chain_df_fixed[current_branch_col] = chain_df_fixed[&#39;branch&#39;]
                        st.write(f&#34;- Converted {len(chain_df_fixed)} assignments to gaze analysis format for Junction {current_junction_idx}&#34;)
                    else:
                        chain_df_fixed = chain_df
                else:
                    chain_df_fixed = chain_df

                # CRITICAL FIX: Filter out unassigned trajectories (branch = -1 or NaN)
                original_count = len(chain_df_fixed)
                if current_branch_col in chain_df_fixed.columns:
                    # Filter out unassigned trajectories
                    assigned_mask = (chain_df_fixed[current_branch_col] != -1) &amp; (chain_df_fixed[current_branch_col].notna())
                    chain_df_fixed = chain_df_fixed[assigned_mask]
                    filtered_count = len(chain_df_fixed)
                    unassigned_count = original_count - filtered_count

                    st.write(f&#34;🔍 **Assignment Filtering:**&#34;)
                    st.write(f&#34;- Original assignments: {original_count}&#34;)
                    st.write(f&#34;- Unassigned trajectories (branch=-1 or NaN): {unassigned_count}&#34;)
                    st.write(f&#34;- Assigned trajectories: {filtered_count}&#34;)

                    if unassigned_count &gt; 0:
                        st.info(f&#34;✅ Filtered out {unassigned_count} unassigned trajectories - only processing {filtered_count} assigned trajectories&#34;)
                    else:
                        st.info(f&#34;✅ All {original_count} trajectories are assigned to branches&#34;)

                    # Debug: Show sample of filtered assignments
                    if filtered_count &gt; 0:
                        st.write(f&#34;🔍 **Sample filtered assignments for Junction {current_junction_idx}:**&#34;)
                        sample_assignments = chain_df_fixed[[&#39;trajectory&#39;, current_branch_col]].head()
                        st.write(sample_assignments)

                        # Show branch distribution
                        branch_counts = chain_df_fixed[current_branch_col].value_counts().sort_index()
                        st.write(f&#34;🔍 **Branch distribution:**&#34;)
                        for branch, count in branch_counts.items():
                            st.write(f&#34;  - Branch {branch}: {count} trajectories&#34;)

                        # Debug: Show trajectory ID matching
                        st.write(f&#34;🔍 **Trajectory ID Matching Debug:**&#34;)
                        st.write(f&#34;- Assignments trajectory IDs (first 10): {list(chain_df_fixed[&#39;trajectory&#39;].astype(str))[:10]}&#34;)
                        st.write(f&#34;- Processed trajectories IDs (first 10): {[str(getattr(t, &#39;tid&#39;, getattr(t, &#39;id&#39;, &#39;NA&#39;))) for t in processed_trajectories[:10]]}&#34;)

                        # Check if trajectory ID 0 exists in assignments
                        traj_0_in_assignments = &#39;0&#39; in chain_df_fixed[&#39;trajectory&#39;].astype(str).values
                        st.write(f&#34;- Trajectory ID 0 in assignments: {traj_0_in_assignments}&#34;)

                        # ALWAYS apply trajectory ID mapping to ensure sequential IDs for gaze analysis
                        st.info(&#34;🔧 **APPLYING TRAJECTORY ID MAPPING**: Ensuring sequential trajectory IDs for gaze analysis...&#34;)

                        if True:  # Always apply mapping
                            st.write(f&#34;🔍 **Trajectory ID Analysis for Junction {current_junction_idx}:**&#34;)
                            st.write(&#34;The gaze analysis functions expect sequential trajectory IDs [0, 1, 2, ...] for proper decision point matching.&#34;)

                            # Show the actual range of trajectory IDs in assignments
                            traj_ids = chain_df_fixed[&#39;trajectory&#39;].astype(str).values
                            st.write(f&#34;- Assignment trajectory IDs (first 5): {traj_ids[:5]}&#34;)
                            st.write(f&#34;- Processed trajectory IDs (first 5): {[str(getattr(t, &#39;tid&#39;, getattr(t, &#39;id&#39;, i))) for i, t in enumerate(processed_trajectories[:5])]}&#34;)

                            # Check if trajectory IDs match between assignments and processed trajectories
                            assignment_ids = set(traj_ids)
                            processed_ids = set(str(getattr(t, &#39;tid&#39;, getattr(t, &#39;id&#39;, i))) for i, t in enumerate(processed_trajectories))
                            common_ids = assignment_ids.intersection(processed_ids)
                            st.write(f&#34;- Common trajectory IDs: {len(common_ids)} out of {len(assignment_ids)} assignments&#34;)

                            st.error(&#34;❌ **SOLUTION NEEDED**: The trajectory ID mapping between discover and gaze analysis is broken!&#34;)
                            st.write(&#34;The discover function assigns trajectories with string IDs (filenames) but gaze analysis expects sequential integer IDs [0, 1, 2, ...]&#34;)

                            # IMPLEMENT FIX: Create trajectory ID mapping
                            st.info(&#34;🔧 **IMPLEMENTING FIX**: Creating trajectory ID mapping...&#34;)

                            # Create a mapping from assignment trajectory IDs to processed trajectory indices
                            # Map assignment IDs to sequential indices based on the order they appear in processed_trajectories
                            assignment_ids = chain_df_fixed[&#39;trajectory&#39;].unique()
                            traj_id_to_index = {tid: i for i, tid in enumerate(sorted(assignment_ids))}

                            st.write(f&#34;🔧 **Trajectory ID Mapping Debug:**&#34;)
                            st.write(f&#34;- Assignment trajectory IDs (first 5): {list(chain_df_fixed[&#39;trajectory&#39;].unique()[:5])}&#34;)
                            st.write(f&#34;- Processed trajectory IDs (first 5): {list(traj_id_to_index.keys())[:5]}&#34;)
                            st.write(f&#34;- Mapping dictionary (first 5): {dict(list(traj_id_to_index.items())[:5])}&#34;)

                            # Map assignment trajectory IDs to processed trajectory indices
                            chain_df_fixed[&#39;trajectory_index&#39;] = chain_df_fixed[&#39;trajectory&#39;].map(traj_id_to_index)

                            # Remove rows where trajectory ID couldn&#39;t be mapped
                            original_mapped_count = len(chain_df_fixed)
                            chain_df_fixed = chain_df_fixed.dropna(subset=[&#39;trajectory_index&#39;])
                            mapped_count = len(chain_df_fixed)
                            unmapped_count = original_mapped_count - mapped_count

                            st.write(f&#34;🔧 **Trajectory ID Mapping Results:**&#34;)
                            st.write(f&#34;- Original assignments: {original_mapped_count}&#34;)
                            st.write(f&#34;- Successfully mapped: {mapped_count}&#34;)
                            st.write(f&#34;- Unmapped (removed): {unmapped_count}&#34;)

                            if mapped_count &gt; 0:
                                st.success(f&#34;✅ **FIX APPLIED**: Mapped {mapped_count} trajectory assignments to processed trajectory indices!&#34;)
                                st.write(f&#34;- Sample mapping: {chain_df_fixed[[&#39;trajectory&#39;, &#39;trajectory_index&#39;, current_branch_col]].head()}&#34;)

                                # Update trajectory IDs to be sequential starting from 0
                                chain_df_fixed[&#39;trajectory&#39;] = chain_df_fixed[&#39;trajectory_index&#39;].astype(int)
                                chain_df_fixed = chain_df_fixed.drop(&#39;trajectory_index&#39;, axis=1)

                                st.write(f&#34;🔧 **Updated assignments with sequential IDs:**&#34;)
                                st.write(f&#34;- Sample: {chain_df_fixed[[&#39;trajectory&#39;, current_branch_col]].head()}&#34;)
                            else:
                                st.error(&#34;❌ **FIX FAILED**: No trajectory assignments could be mapped!&#34;)
                                st.write(&#34;This suggests a fundamental mismatch between discover and gaze analysis trajectory handling.&#34;)
                else:
                    st.error(f&#34;❌ Expected column &#39;{current_branch_col}&#39; not found in assignments!&#34;)
                    st.write(f&#34;Available columns: {list(chain_df_fixed.columns)}&#34;)

                # Ensure expected branch column exists for physiological analysis
                chain_df_call = chain_df_fixed.copy()

                # Ensure the junction-specific branch column exists
                if current_branch_col not in chain_df_call.columns:
                    st.error(f&#34;❌ **Missing branch column**: {current_branch_col} not found for Junction {current_junction_idx}&#34;)
                else:
                    st.write(f&#34;🔧 **Using junction-specific column**: {current_branch_col} for Junction {current_junction_idx}&#34;)

                    # CRITICAL FIX: Use verta_consistency.normalize_assignments for proper trajectory ID mapping
                    # This will automatically handle branch column naming and trajectory ID mapping
                    st.info(f&#34;🔧 **Using verta_consistency.normalize_assignments for proper trajectory ID mapping...**&#34;)

                    try:
                        from verta.verta_consistency import normalize_assignments

                        # Use the proper normalization function with the fixed assignments
                        # It will automatically create branch_j{i} for single junction analysis
                        normalized_df, report = normalize_assignments(
                            assignments_df=chain_df_fixed,  # Use the fixed assignments with proper trajectory IDs
                            trajectories=processed_trajectories,
                            junctions=[junction],  # Single junction for this analysis
                            current_junction_idx=current_junction_idx,
                            prefer_decisions=False,  # We already have the assignments
                            include_outliers=False,  # Filter out negative branches
                            strict=False  # Don&#39;t fail on low coverage
                        )

                        st.write(f&#34;🔧 **Normalization report:**&#34;)
                        st.write(f&#34;- Input rows: {report[&#39;input_rows&#39;]}&#34;)
                        st.write(f&#34;- Kept after ID mapping: {report[&#39;kept_after_tid_map&#39;]}&#34;)
                        st.write(f&#34;- Dropped unmapped IDs: {report[&#39;dropped_unmapped_ids&#39;]}&#34;)
                        st.write(f&#34;- Has decisions: {report[&#39;has_decisions&#39;]}&#34;)

                        if report[&#39;kept_after_tid_map&#39;] &gt; 0:
                            st.success(f&#34;✅ **Assignment normalization successful!** {report[&#39;kept_after_tid_map&#39;]} assignments ready for gaze analysis&#34;)

                            # Show available branch columns after normalization
                            branch_cols = [col for col in normalized_df.columns if col.startswith(&#39;branch&#39;)]
                            st.write(f&#34;🔧 **Available branch columns after normalization:** {branch_cols}&#34;)

                            # Show sample data with available branch columns
                            display_cols = [&#39;trajectory&#39;] + branch_cols
                            st.write(f&#34;🔧 **Sample normalized assignments:**&#34;)
                            st.write(normalized_df[display_cols].head())

                            # Use the normalized DataFrame for gaze analysis
                            chain_df_call = normalized_df
                        else:
                            st.error(f&#34;❌ **Assignment normalization failed!** No assignments could be mapped&#34;)
                            st.write(&#34;This suggests a fundamental mismatch between trajectory IDs and assignment IDs&#34;)

                    except Exception as e:
                        st.error(f&#34;❌ **Error during assignment normalization:** {e}&#34;)
                        st.write(&#34;Falling back to manual trajectory ID mapping...&#34;)

                        # Fallback to manual mapping if normalization fails
                        traj_id_mapping = {}
                        for i, traj in enumerate(processed_trajectories):
                            original_id = getattr(traj, &#39;_original_tid&#39;, traj.tid)
                            traj_id_mapping[original_id] = i

                        if &#39;trajectory&#39; in chain_df_call.columns:
                            chain_df_call[&#39;trajectory&#39;] = chain_df_call[&#39;trajectory&#39;].map(traj_id_mapping)
                            chain_df_call = chain_df_call.dropna(subset=[&#39;trajectory&#39;])

                # CRITICAL FIX: Ensure we&#39;re using the correct junction-specific assignments
                # If we have a single-junction assignment (single &#39;branch&#39; column),
                # we need to make sure the decision points are calculated for THIS junction,
                # not some other junction&#39;s decision points.
                st.write(f&#34;🔍 **Junction-Specific Assignment Debug:**&#34;)
                st.write(f&#34;- Current junction index: {current_junction_idx}&#34;)
                st.write(f&#34;- Current branch column: {current_branch_col}&#34;)
                st.write(f&#34;- Available columns: {list(chain_df_call.columns)}&#34;)

                # Show branch column values dynamically
                branch_cols = [col for col in chain_df_call.columns if col.startswith(&#39;branch&#39;)]
                for branch_col in branch_cols:
                    st.write(f&#34;- {branch_col} values: {chain_df_call[branch_col].value_counts().to_dict()}&#34;)

                if not branch_cols:
                    st.write(&#34;- ❌ No branch columns found!&#34;)

                # Create copies of trajectories and remap their IDs to match the assignments DataFrame
                # This ensures the analysis functions can find the correct assignments
                import copy as _copy
                trajectories_for_analysis = [_copy.copy(tr) for tr in processed_trajectories]

                # CRITICAL FIX: Update trajectory IDs to match the assignments DataFrame
                # The assignments DataFrame has original trajectory IDs, so we need to match them
                assignment_ids = chain_df_fixed[&#39;trajectory&#39;].unique()

                # Update trajectory IDs to match assignments DataFrame IDs
                for i, _tr in enumerate(trajectories_for_analysis):
                    if i &lt; len(assignment_ids):
                        _tr.tid = assignment_ids[i]
                    else:
                        _tr.tid = i

                # Normalize assignments using shared consistency layer (replaces ad-hoc fixes)
                try:
                    from .verta_consistency import normalize_assignments
                except Exception:
                    from verta.verta_consistency import normalize_assignments

                decisions_chain_df = st.session_state.analysis_results.get(&#34;branches&#34;, {}).get(&#34;decision_points&#34;)
                if decisions_chain_df is None:
                    # Fallback: try to load from default GUI outputs dir
                    try:
                        import os as _os
                        import pandas as _pd
                        _p = _os.path.join(&#34;gui_outputs&#34;, &#34;branch_decisions_chain.csv&#34;)
                        if _os.path.exists(_p):
                            decisions_chain_df = _pd.read_csv(_p)
                            st.write(&#34;🔗 Loaded decisions from gui_outputs/branch_decisions_chain.csv (fallback)&#34;)
                    except Exception:
                        pass
                norm_df, norm_report = normalize_assignments(
                    chain_df_fixed,
                    trajectories=trajectories_for_analysis,
                    junctions=[junction],
                    current_junction_idx=current_junction_idx,
                    decisions_df=decisions_chain_df,
                    prefer_decisions=True,
                    include_outliers=False,
                )
                chain_df_call = norm_df  # override with normalized assignments
                st.write(&#34;🧭 Assignments normalization report:&#34;)
                st.write(norm_report)

                physio_data = analyze_physiological_at_junctions(
                    trajectories=trajectories_for_analysis,
                    junctions=[junction],
                    assignments_df=chain_df_call,  # Use the assignments with merged decision points
                    decision_mode=discover_decision_mode,  # Use discover decision mode
                    r_outer_list=r_outer_list,
                    path_length=discover_path_length,  # Use discover path length
                    epsilon=discover_epsilon,  # Use discover epsilon
                    linger_delta=discover_linger_delta,  # Use discover linger delta
                    physio_window=3.0
                )

                # DEBUG: Check what was passed to physiological analysis
                st.write(f&#34;🔍 **DEBUG: Physiological Analysis Input Check:**&#34;)
                st.write(f&#34;- Trajectories passed: {len(trajectories_for_analysis)}&#34;)
                st.write(f&#34;- First trajectory ID: {trajectories_for_analysis[0].tid} (type: {type(trajectories_for_analysis[0].tid)})&#34;)
                st.write(f&#34;- Assignments DataFrame shape: {chain_df_fixed.shape}&#34;)
                st.write(f&#34;- Assignments trajectory column dtype: {chain_df_fixed[&#39;trajectory&#39;].dtype}&#34;)
                st.write(f&#34;- Assignments trajectory sample: {chain_df_fixed[&#39;trajectory&#39;].head().tolist()}&#34;)
                st.write(f&#34;- Junction: {junction}&#34;)
                st.write(f&#34;- Junction index: {current_junction_idx}&#34;)

            st.success(&#34;✅ Physiological analysis completed&#34;)
            st.write(f&#34;🔍 **Physiological Results:** {len(physio_data) if physio_data is not None else 0} rows&#34;)

            # Debug: Show physiological results details
            if physio_data is not None and len(physio_data) &gt; 0:
                st.write(f&#34;🔍 **Physiological Results Debug:**&#34;)
                st.write(f&#34;- Rows: {len(physio_data)}&#34;)
                st.write(f&#34;- Columns: {list(physio_data.columns)}&#34;)
                if &#39;heart_rate_change&#39; in physio_data.columns:
                    hr_change_count = physio_data[&#39;heart_rate_change&#39;].notna().sum()
                    st.write(f&#34;- Heart rate change values: {hr_change_count}&#34;)
                else:
                    st.write(f&#34;- ❌ &#39;heart_rate_change&#39; column missing!&#34;)
            else:
                st.write(f&#34;🔍 **Physiological Results Debug:** No data returned&#34;)

            # Analyze pupil dilation trajectories
            st.info(&#34;👁️ Analyzing pupil dilation trajectories...&#34;)
            with st.spinner(&#34;Processing pupil dilation data...&#34;):
                # Ensure expected branch column exists for pupil analysis
                chain_df_call = chain_df_fixed.copy()

                # Ensure the junction-specific branch column exists
                if current_branch_col not in chain_df_call.columns:
                    st.error(f&#34;❌ **Missing branch column**: {current_branch_col} not found for Junction {current_junction_idx}&#34;)
                else:
                    st.write(f&#34;🔧 **Using junction-specific column**: {current_branch_col} for Junction {current_junction_idx}&#34;)

                # CRITICAL FIX: Ensure we&#39;re using the correct junction-specific assignments
                # If we have a single-junction assignment (single &#39;branch&#39; column),
                # we need to make sure the decision points are calculated for THIS junction,
                # not some other junction&#39;s decision points.
                st.write(f&#34;🔍 **Pupil Analysis - Junction-Specific Assignment Debug:**&#34;)
                st.write(f&#34;- Current junction index: {current_junction_idx}&#34;)
                st.write(f&#34;- Current branch column: {current_branch_col}&#34;)
                st.write(f&#34;- Available columns: {list(chain_df_call.columns)}&#34;)
                if current_branch_col in chain_df_call.columns:
                    st.write(f&#34;- {current_branch_col} values: {chain_df_call[current_branch_col].value_counts().to_dict()}&#34;)
                else:
                    st.write(f&#34;- ❌ {current_branch_col} column not found!&#34;)

                # Create copies of trajectories and remap their IDs to match the assignments DataFrame
                # This ensures the analysis functions can find the correct assignments
                import copy as _copy
                trajectories_for_analysis = [_copy.copy(tr) for tr in processed_trajectories]

                # CRITICAL FIX: Update trajectory IDs to match the assignments DataFrame
                # The assignments DataFrame has original trajectory IDs, so we need to match them
                assignment_ids = chain_df_fixed[&#39;trajectory&#39;].unique()

                # Update trajectory IDs to match assignments DataFrame IDs
                for i, _tr in enumerate(trajectories_for_analysis):
                    if i &lt; len(assignment_ids):
                        _tr.tid = assignment_ids[i]
                    else:
                        _tr.tid = i

                # Normalize assignments using shared consistency layer (replaces ad-hoc fixes)
                try:
                    from .verta_consistency import normalize_assignments
                except Exception:
                    from verta.verta_consistency import normalize_assignments

                decisions_chain_df = st.session_state.analysis_results.get(&#34;branches&#34;, {}).get(&#34;decision_points&#34;)
                if decisions_chain_df is None:
                    # Fallback: try to load from default GUI outputs dir
                    try:
                        import os as _os
                        import pandas as _pd
                        _p = _os.path.join(&#34;gui_outputs&#34;, &#34;branch_decisions_chain.csv&#34;)
                        if _os.path.exists(_p):
                            decisions_chain_df = _pd.read_csv(_p)
                            st.write(&#34;🔗 Loaded decisions from gui_outputs/branch_decisions_chain.csv (fallback)&#34;)
                    except Exception:
                        pass
                norm_df, norm_report = normalize_assignments(
                    chain_df_fixed,
                    trajectories=trajectories_for_analysis,
                    junctions=[junction],
                    current_junction_idx=current_junction_idx,
                    decisions_df=decisions_chain_df,
                    prefer_decisions=True,
                    include_outliers=False,
                )
                chain_df_call = norm_df  # override with normalized assignments
                st.write(&#34;🧭 Assignments normalization report:&#34;)
                st.write(norm_report)
                # Debug: Check if normalize_assignments already merged decision points
                precomputed_count = chain_df_call[&#39;decision_idx&#39;].notna().sum() if &#39;decision_idx&#39; in chain_df_call.columns else 0
                st.write(f&#34;🔍 Decision points after normalize_assignments (pupil): {precomputed_count} trajectories have precomputed decision points&#34;)
                if precomputed_count &gt; 0:
                    st.write(f&#34;🔍 Sample decision points: {chain_df_call[[&#39;trajectory&#39;, &#39;decision_idx&#39;, &#39;intercept_x&#39;, &#39;intercept_z&#39;]].head(3).to_dict(&#39;records&#39;)}&#34;)
                else:
                    st.write(&#34;⚠️ No decision points found - this may cause analysis to fail&#34;)

                pupil_data = analyze_pupil_dilation_trajectory(
                    trajectories=trajectories_for_analysis,
                    junctions=[junction],
                    assignments_df=chain_df_call,  # Use the assignments with merged decision points
                    decision_mode=discover_decision_mode,  # Use discover decision mode
                    r_outer_list=r_outer_list,
                    path_length=discover_path_length,  # Use discover path length
                    epsilon=discover_epsilon,  # Use discover epsilon
                    linger_delta=discover_linger_delta,  # Use discover linger delta
                    physio_window=3.0
                )

            st.success(&#34;✅ Pupil dilation analysis completed&#34;)
            st.write(f&#34;🔍 **Pupil Results:** {len(pupil_data) if pupil_data is not None else 0} rows&#34;)

            # Debug: Show pupil results details
            if pupil_data is not None and len(pupil_data) &gt; 0:
                st.write(f&#34;🔍 **Pupil Results Debug:**&#34;)
                st.write(f&#34;- Rows: {len(pupil_data)}&#34;)
                st.write(f&#34;- Columns: {list(pupil_data.columns)}&#34;)
                if &#39;pupil_change&#39; in pupil_data.columns:
                    pupil_change_count = pupil_data[&#39;pupil_change&#39;].notna().sum()
                    st.write(f&#34;- Pupil change values: {pupil_change_count}&#34;)
                else:
                    st.write(f&#34;- ❌ &#39;pupil_change&#39; column missing!&#34;)
            else:
                st.write(f&#34;🔍 **Pupil Results Debug:** No data returned&#34;)

            # Analyze head yaw at decisions
            st.info(&#34;🧭 Analyzing head yaw at decisions...&#34;)
            with st.spinner(&#34;Processing head yaw data...&#34;):
                # Ensure expected branch column exists for head yaw analysis
                chain_df_call = chain_df_fixed.copy()

                # Ensure the junction-specific branch column exists
                if current_branch_col not in chain_df_call.columns:
                    st.error(f&#34;❌ **Missing branch column**: {current_branch_col} not found for Junction {current_junction_idx}&#34;)
                else:
                    st.write(f&#34;🔧 **Using junction-specific column**: {current_branch_col} for Junction {current_junction_idx}&#34;)

                # CRITICAL FIX: Ensure we&#39;re using the correct junction-specific assignments
                # If we have a single-junction assignment (single &#39;branch&#39; column),
                # we need to make sure the decision points are calculated for THIS junction,
                # not some other junction&#39;s decision points.
                st.write(f&#34;🔍 **Head Yaw Analysis - Junction-Specific Assignment Debug:**&#34;)
                st.write(f&#34;- Current junction index: {current_junction_idx}&#34;)
                st.write(f&#34;- Current branch column: {current_branch_col}&#34;)
                st.write(f&#34;- Available columns: {list(chain_df_call.columns)}&#34;)
                if current_branch_col in chain_df_call.columns:
                    st.write(f&#34;- {current_branch_col} values: {chain_df_call[current_branch_col].value_counts().to_dict()}&#34;)
                else:
                    st.write(f&#34;- ❌ {current_branch_col} column not found!&#34;)

                # Create copies of trajectories and remap their IDs to match the assignments DataFrame
                # This ensures the analysis functions can find the correct assignments
                import copy as _copy
                trajectories_for_analysis = [_copy.copy(tr) for tr in processed_trajectories]

                # CRITICAL FIX: Update trajectory IDs to match the assignments DataFrame
                # The assignments DataFrame has original trajectory IDs, so we need to match them
                assignment_ids = chain_df_fixed[&#39;trajectory&#39;].unique()

                # Update trajectory IDs to match assignments DataFrame IDs
                for i, _tr in enumerate(trajectories_for_analysis):
                    if i &lt; len(assignment_ids):
                        _tr.tid = assignment_ids[i]
                    else:
                        _tr.tid = i

                # Normalize assignments using shared consistency layer (replaces ad-hoc fixes)
                try:
                    from .verta_consistency import normalize_assignments
                except Exception:
                    from verta.verta_consistency import normalize_assignments

                decisions_chain_df = st.session_state.analysis_results.get(&#34;branches&#34;, {}).get(&#34;decision_points&#34;)
                if decisions_chain_df is None:
                    # Fallback: try to load from default GUI outputs dir
                    try:
                        import os as _os
                        import pandas as _pd
                        _p = _os.path.join(&#34;gui_outputs&#34;, &#34;branch_decisions_chain.csv&#34;)
                        if _os.path.exists(_p):
                            decisions_chain_df = _pd.read_csv(_p)
                            st.write(&#34;🔗 Loaded decisions from gui_outputs/branch_decisions_chain.csv (fallback)&#34;)
                    except Exception:
                        pass
                norm_df, norm_report = normalize_assignments(
                    chain_df_fixed,
                    trajectories=trajectories_for_analysis,
                    junctions=[junction],
                    current_junction_idx=current_junction_idx,
                    decisions_df=decisions_chain_df,
                    prefer_decisions=True,
                    include_outliers=False,
                )
                chain_df_call = norm_df  # override with normalized assignments
                st.write(&#34;🧭 Assignments normalization report:&#34;)
                st.write(norm_report)
                # Debug: Check if normalize_assignments already merged decision points
                precomputed_count = chain_df_call[&#39;decision_idx&#39;].notna().sum() if &#39;decision_idx&#39; in chain_df_call.columns else 0
                st.write(f&#34;🔍 Decision points after normalize_assignments: {precomputed_count} trajectories have precomputed decision points&#34;)
                if precomputed_count &gt; 0:
                    st.write(f&#34;🔍 Sample decision points: {chain_df_call[[&#39;trajectory&#39;, &#39;decision_idx&#39;, &#39;intercept_x&#39;, &#39;intercept_z&#39;]].head(3).to_dict(&#39;records&#39;)}&#34;)
                else:
                    st.write(&#34;⚠️ No decision points found - this may cause analysis to fail&#34;)

                # Get the decision mode used by the discover analysis
                discover_decision_mode = &#34;pathlen&#34;  # Default to pathlen since that&#39;s what you&#39;re using
                if existing_assignments is not None:
                    # Try to get the decision mode from the junction data
                    discover_junction_key = f&#34;junction_{current_junction_idx}&#34;
                    if discover_junction_key in st.session_state.analysis_results.get(&#34;branches&#34;, {}):
                        junction_data = st.session_state.analysis_results[&#34;branches&#34;][discover_junction_key]
                        discover_decision_mode = junction_data.get(&#34;decision_mode&#34;, &#34;pathlen&#34;)

                st.write(f&#34;🔧 **Using discover decision mode**: {discover_decision_mode} (same as discover analysis)&#34;)

                # Debug: Check what DataFrame is being passed to gaze analysis
                st.write(f&#34;🔍 **DataFrame being passed to gaze analysis:**&#34;)
                st.write(f&#34;- Shape: {chain_df_call.shape}&#34;)
                st.write(f&#34;- Columns: {list(chain_df_call.columns)}&#34;)
                st.write(f&#34;- Has decision_idx: {&#39;decision_idx&#39; in chain_df_call.columns}&#34;)
                st.write(f&#34;- Has intercept_x: {&#39;intercept_x&#39; in chain_df_call.columns}&#34;)
                st.write(f&#34;- Has intercept_z: {&#39;intercept_z&#39; in chain_df_call.columns}&#34;)
                if &#39;decision_idx&#39; in chain_df_call.columns:
                    precomputed_count = chain_df_call[&#39;decision_idx&#39;].notna().sum()
                    st.write(f&#34;- Precomputed decision points: {precomputed_count} out of {len(chain_df_call)}&#34;)
                    st.write(f&#34;- Sample precomputed data: {chain_df_call[[&#39;trajectory&#39;, &#39;decision_idx&#39;, &#39;intercept_x&#39;, &#39;intercept_z&#39;]].head(3).to_dict(&#39;records&#39;)}&#34;)

                head_yaw_data = compute_head_yaw_at_decisions(
                    trajectories=trajectories_for_analysis,
                    junctions=[junction],
                    assignments_df=chain_df_call,  # Use the assignments with merged decision points
                    decision_mode=discover_decision_mode,  # Use the same decision mode as discover analysis
                    r_outer_list=r_outer_list,
                    path_length=path_length,
                    epsilon=epsilon,
                    linger_delta=linger_delta,  # Use the same linger_delta as discover analysis
                    base_index=current_junction_idx if current_junction_idx is not None else 0,
                )

            st.success(&#34;✅ Head yaw analysis completed&#34;)
            st.write(f&#34;🔍 **Head Yaw Results:** {len(head_yaw_data) if head_yaw_data is not None else 0} rows&#34;)

            # Debug: Show head yaw results details
            if head_yaw_data is not None and len(head_yaw_data) &gt; 0:
                st.write(f&#34;🔍 **Head Yaw Results Debug:**&#34;)
                st.write(f&#34;- Rows: {len(head_yaw_data)}&#34;)
                st.write(f&#34;- Columns: {list(head_yaw_data.columns)}&#34;)
                if &#39;head_yaw&#39; in head_yaw_data.columns:
                    head_yaw_count = head_yaw_data[&#39;head_yaw&#39;].notna().sum()
                    st.write(f&#34;- Head yaw values: {head_yaw_count}&#34;)
                else:
                    st.write(f&#34;- ❌ &#39;head_yaw&#39; column missing!&#34;)
                if &#39;yaw_difference&#39; in head_yaw_data.columns:
                    yaw_diff_count = head_yaw_data[&#39;yaw_difference&#39;].notna().sum()
                    st.write(f&#34;- Yaw difference values: {yaw_diff_count}&#34;)
                else:
                    st.write(f&#34;- ❌ &#39;yaw_difference&#39; column missing!&#34;)
            else:
                st.write(f&#34;🔍 **Head Yaw Results Debug:** No data returned&#34;)

            # Create per-junction pupil dilation heatmap
            st.info(&#34;🗺️ Creating junction-specific pupil dilation heatmap...&#34;)
            with st.spinner(&#34;Generating junction heatmap...&#34;):
                # Get heatmap settings from session state
                cell_size = st.session_state.get(&#39;pupil_heatmap_cell_size&#39;, 50.0)
                normalization = st.session_state.get(&#39;pupil_heatmap_normalization&#39;, &#39;relative&#39;)

                # Create per-junction heatmap (focused on current junction)
                # Get junction index from junction_key in session state
                junction_idx = None
                for idx, j in enumerate(st.session_state.junctions):
                    if j.cx == junction.cx and j.cz == junction.cz and j.r == junction.r:
                        junction_idx = idx
                        break

                junction_heatmaps = create_per_junction_pupil_heatmap(
                    trajectories=processed_trajectories,
                    junctions=[junction],
                    r_outer_list=[r_outer],
                    cell_size=cell_size,
                    normalization=normalization,
                    base_index=junction_idx if junction_idx is not None else 0
                )

                # Fix the junction index in the heatmap data
                # The function returns a dict with key=0 (local index), but we need the global index
                # Re-key the dictionary to use the global junction index
                if junction_idx is not None and 0 in junction_heatmaps:
                    junction_heatmaps = {junction_idx: junction_heatmaps[0]}


            # Update debug status to completed
            st.session_state[&#39;gaze_debug_info&#39;][junction_key][&#39;status&#39;] = &#39;completed&#39;
            st.session_state[&#39;gaze_debug_info&#39;][junction_key][&#39;physio_rows&#39;] = len(physio_data) if physio_data is not None else 0
            st.session_state[&#39;gaze_debug_info&#39;][junction_key][&#39;pupil_rows&#39;] = len(pupil_data) if pupil_data is not None else 0
            st.session_state[&#39;gaze_debug_info&#39;][junction_key][&#39;head_yaw_rows&#39;] = len(head_yaw_data) if head_yaw_data is not None else 0

            # Save CSV files to gui_outputs
            try:
                import os
                gaze_data_dir = os.path.join(&#34;gui_outputs&#34;, &#34;gaze_data&#34;)
                os.makedirs(gaze_data_dir, exist_ok=True)

                # Get junction index for file naming
                junction_idx = None
                for idx, j in enumerate(st.session_state.junctions):
                    if j.cx == junction.cx and j.cz == junction.cz and j.r == junction.r:
                        junction_idx = idx
                        break

                if junction_idx is not None:
                    junction_prefix = f&#34;junction_{junction_idx}&#34;

                    # Save physiological analysis data
                    if physio_data is not None and len(physio_data) &gt; 0:
                        # Ensure all numpy arrays are converted to lists for CSV compatibility
                        physio_data_clean = physio_data.copy()
                        for col in physio_data_clean.columns:
                            if physio_data_clean[col].dtype == &#39;object&#39;:
                                # Check if column contains numpy arrays
                                physio_data_clean[col] = physio_data_clean[col].apply(
                                    lambda x: x.tolist() if hasattr(x, &#39;tolist&#39;) else x
                                )

                        physio_file = os.path.join(gaze_data_dir, f&#34;{junction_prefix}_physiological_analysis.csv&#34;)
                        physio_data_clean.to_csv(physio_file, index=False)
                        st.info(f&#34;📁 Physiological data saved to: {physio_file}&#34;)

                    # Save pupil dilation data
                    if pupil_data is not None and len(pupil_data) &gt; 0:
                        # Ensure all numpy arrays are converted to lists for CSV compatibility
                        pupil_data_clean = pupil_data.copy()
                        for col in pupil_data_clean.columns:
                            if pupil_data_clean[col].dtype == &#39;object&#39;:
                                # Check if column contains numpy arrays
                                pupil_data_clean[col] = pupil_data_clean[col].apply(
                                    lambda x: x.tolist() if hasattr(x, &#39;tolist&#39;) else x
                                )

                        pupil_file = os.path.join(gaze_data_dir, f&#34;{junction_prefix}_pupil_trajectory_analysis.csv&#34;)
                        pupil_data_clean.to_csv(pupil_file, index=False)
                        st.info(f&#34;📁 Pupil trajectory data saved to: {pupil_file}&#34;)

                    # Save head yaw data
                    if head_yaw_data is not None and len(head_yaw_data) &gt; 0:
                        # Ensure all numpy arrays are converted to lists for CSV compatibility
                        head_yaw_data_clean = head_yaw_data.copy()
                        for col in head_yaw_data_clean.columns:
                            if head_yaw_data_clean[col].dtype == &#39;object&#39;:
                                # Check if column contains numpy arrays
                                head_yaw_data_clean[col] = head_yaw_data_clean[col].apply(
                                    lambda x: x.tolist() if hasattr(x, &#39;tolist&#39;) else x
                                )

                        gaze_file = os.path.join(gaze_data_dir, f&#34;{junction_prefix}_gaze_analysis.csv&#34;)
                        head_yaw_data_clean.to_csv(gaze_file, index=False)
                        st.info(f&#34;📁 Gaze analysis data saved to: {gaze_file}&#34;)

                    # Save pupil heatmap data as JSON
                    if junction_heatmaps:
                        heatmap_file = os.path.join(gaze_data_dir, f&#34;{junction_prefix}_pupil_heatmap.json&#34;)
                        import json
                        # Convert numpy arrays to lists for JSON serialization
                        def convert_numpy_to_list(obj):
                            if hasattr(obj, &#39;tolist&#39;):
                                return obj.tolist()
                            elif isinstance(obj, dict):
                                return {k: convert_numpy_to_list(v) for k, v in obj.items()}
                            elif isinstance(obj, list):
                                return [convert_numpy_to_list(item) for item in obj]
                            elif hasattr(obj, &#39;to_dict&#39;):  # Handle pandas DataFrames
                                return obj.to_dict(&#39;records&#39;)
                            elif hasattr(obj, &#39;__dict__&#39;):  # Handle dataclass objects (like Circle)
                                return {k: convert_numpy_to_list(v) for k, v in obj.__dict__.items()}
                            else:
                                return obj

                        # Use the converted version for JSON serialization
                        heatmap_data = convert_numpy_to_list(junction_heatmaps)
                        with open(heatmap_file, &#39;w&#39;) as f:
                            json.dump(heatmap_data, f, indent=2)
                        st.info(f&#34;📁 Pupil heatmap data saved to: {heatmap_file}&#34;)

            except Exception as e:
                st.warning(f&#34;⚠️ Could not save gaze data files: {e}&#34;)

            # Create comprehensive results
            results = {
                &#39;physiological&#39;: physio_data,
                &#39;pupil_dilation&#39;: pupil_data,
                &#39;head_yaw&#39;: head_yaw_data,
                &#39;pupil_heatmap_junction&#39;: junction_heatmaps,
                &#39;junction&#39;: junction,
                &#39;r_outer&#39;: r_outer
            }

            # Convert any numpy arrays to lists for JSON serialization
            def convert_numpy_to_list(obj):
                if hasattr(obj, &#39;tolist&#39;):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {k: convert_numpy_to_list(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy_to_list(item) for item in obj]
                elif hasattr(obj, &#39;to_dict&#39;):  # Handle pandas DataFrames
                    return obj.to_dict(&#39;records&#39;)
                elif hasattr(obj, &#39;__dict__&#39;):  # Handle dataclass objects (like Circle)
                    return {k: convert_numpy_to_list(v) for k, v in obj.__dict__.items()}
                else:
                    return obj

            # Apply conversion to results for JSON serialization
            results_for_json = convert_numpy_to_list(results)

            # Return original results (with DataFrames and Circle objects) for plotting functions
            return results

        except Exception as e:
            # Update debug status to error
            st.session_state[&#39;gaze_debug_info&#39;][junction_key][&#39;status&#39;] = &#39;error&#39;
            st.session_state[&#39;gaze_debug_info&#39;][junction_key][&#39;error&#39;] = str(e)

            # More detailed error information
            import traceback
            st.error(f&#34;❌ Comprehensive gaze analysis failed: {e}&#34;)
            st.error(f&#34;**Error details:** {str(e)}&#34;)
            st.error(f&#34;**Error type:** {type(e).__name__}&#34;)

            # Show traceback for debugging
            st.code(traceback.format_exc())

            if processed_trajectories:
                sample_traj = processed_trajectories[0]
                st.write(f&#34;- Sample trajectory type: {type(sample_traj)}&#34;)
                st.write(f&#34;- Sample trajectory attributes: {[attr for attr in dir(sample_traj) if not attr.startswith(&#39;_&#39;)]}&#34;)

            st.info(&#34;🔄 Falling back to movement pattern analysis...&#34;)
            return self._analyze_movement_patterns_optimized(
                trajectories=trajectories,
                junction=junction,
                r_outer=r_outer,
                decision_mode=decision_mode,
                path_length=path_length,
                epsilon=epsilon
            )

    def _get_filtered_trajectories_for_junction(self, trajectories, junction, r_outer):
        &#34;&#34;&#34;Get trajectories that pass through a specific junction area, with clipped coordinates.&#34;&#34;&#34;
        import numpy as np

        filtered_trajs = []

        for traj in trajectories:
            # Calculate distances from junction center
            rx = traj.x - junction.cx
            rz = traj.z - junction.cz
            r = np.hypot(rx, rz)

            # Keep trajectories that pass through the junction area
            if np.any(r &lt;= r_outer):
                # Create a clipped version that only shows the junction-relevant portion
                # This helps with zoom by reducing the data extent
                junction_mask = r &lt;= (r_outer * 1.5)  # Slightly larger than r_outer for context

                if np.any(junction_mask):
                    # Create a simple object to hold the clipped data
                    class ClippedTraj:
                        def __init__(self, tid, x, z):
                            self.tid = tid
                            self.x = x
                            self.z = z

                    clipped_traj = ClippedTraj(
                        tid=traj.tid,
                        x=traj.x[junction_mask],
                        z=traj.z[junction_mask]
                    )
                    filtered_trajs.append(clipped_traj)

        return filtered_trajs

    def _safe_get_time_value(self, trajectory, decision_idx):
        &#34;&#34;&#34;Safely get time value from trajectory, handling different data types.&#34;&#34;&#34;
        import numpy as np

        if not hasattr(trajectory, &#39;t&#39;) or trajectory.t is None or decision_idx &gt;= len(trajectory.t):
            return np.nan

        try:
            time_value = trajectory.t[decision_idx]

            # Handle string time values
            if isinstance(time_value, str):
                try:
                    import pandas as pd
                    return pd.to_timedelta(time_value).total_seconds()
                except:
                    return np.nan

            # Handle numeric values
            if isinstance(time_value, (int, float)) and not np.isnan(time_value):
                return float(time_value)

            return np.nan
        except:
            return np.nan

    def _add_pupil_dilation_analysis(self, gaze_data, trajectories, junction, r_outer):
        &#34;&#34;&#34;Add simplified pupil dilation analysis for regular trajectories.&#34;&#34;&#34;
        import numpy as np

        # Check if any trajectory has time data
        has_time_data = any(hasattr(traj, &#39;t&#39;) and traj.t is not None for traj in trajectories)

        if not has_time_data:
            # Add placeholder columns
            gaze_data[&#39;pupil_baseline&#39;] = np.nan
            gaze_data[&#39;pupil_decision&#39;] = np.nan
            gaze_data[&#39;pupil_dilation&#39;] = np.nan
            gaze_data[&#39;pupil_analysis_available&#39;] = False
            return gaze_data

        # Add pupil analysis columns
        gaze_data[&#39;pupil_baseline&#39;] = np.nan
        gaze_data[&#39;pupil_decision&#39;] = np.nan
        gaze_data[&#39;pupil_dilation&#39;] = np.nan
        gaze_data[&#39;pupil_analysis_available&#39;] = True

        # For regular trajectories, we can&#39;t do real pupil analysis, but we can analyze timing patterns
        for idx, row in gaze_data.iterrows():
            traj_idx = int(row[&#39;trajectory&#39;])
            decision_idx = int(row[&#39;decision_idx&#39;])

            if traj_idx &lt; len(trajectories):
                trajectory = trajectories[traj_idx]

                if hasattr(trajectory, &#39;t&#39;) and trajectory.t is not None and decision_idx &lt; len(trajectory.t):
                    try:
                        decision_time = self._safe_get_time_value(trajectory, decision_idx)

                        if np.isnan(decision_time):
                            continue

                        # Analyze timing patterns around decision point
                        time_window = 3.0  # 3 second window
                        time_mask = (trajectory.t &gt;= decision_time - time_window) &amp; (trajectory.t &lt;= decision_time + time_window)

                        if np.any(time_mask):
                            # Calculate timing-based metrics as proxy for physiological analysis
                            pre_decision_times = trajectory.t[(trajectory.t &gt;= decision_time - time_window) &amp; (trajectory.t &lt; decision_time)]
                            post_decision_times = trajectory.t[(trajectory.t &gt; decision_time) &amp; (trajectory.t &lt;= decision_time + time_window)]

                            # Use timing patterns as proxy for pupil analysis
                            gaze_data.loc[idx, &#39;pupil_baseline&#39;] = len(pre_decision_times) if len(pre_decision_times) &gt; 0 else 0
                            gaze_data.loc[idx, &#39;pupil_decision&#39;] = len(post_decision_times) if len(post_decision_times) &gt; 0 else 0
                            gaze_data.loc[idx, &#39;pupil_dilation&#39;] = len(post_decision_times) - len(pre_decision_times)
                    except Exception as e:
                        # Skip this trajectory if there&#39;s any error with time data
                        continue

        return gaze_data

    def _analyze_movement_patterns_all_junctions(self, trajectories, junctions, r_outer_list, decision_mode, path_length, epsilon):
        &#34;&#34;&#34;Analyze movement patterns across all junctions in temporal order.&#34;&#34;&#34;
        import numpy as np
        import pandas as pd

        results = []

        for traj_idx, trajectory in enumerate(trajectories):
            # Find all junction visits in temporal order for this trajectory
            junction_sequence = self._find_junction_sequence(trajectory, junctions, r_outer_list)

            # Analyze each junction visit in the sequence
            for visit_idx, junction_idx in enumerate(junction_sequence):
                junction = junctions[junction_idx]
                r_outer = r_outer_list[junction_idx]

                # Find decision point for this specific junction visit
                decision_idx = self._find_decision_point_for_junction_visit(
                    trajectory, junction, r_outer, decision_mode, path_length, epsilon, junction_sequence, visit_idx
                )

                if decision_idx is not None and decision_idx &lt; len(trajectory.x):
                    # Calculate movement metrics for this junction visit
                    movement_yaw = np.nan
                    if decision_idx &gt; 0 and decision_idx &lt; len(trajectory.x) - 1:
                        dx = trajectory.x[decision_idx + 1] - trajectory.x[decision_idx - 1]
                        dz = trajectory.z[decision_idx + 1] - trajectory.z[decision_idx - 1]
                        movement_magnitude = np.hypot(dx, dz)
                        if movement_magnitude &gt; 1e-3:
                            movement_yaw = np.degrees(np.arctan2(dx, dz))

                    # Calculate approach and exit directions
                    approach_yaw = np.nan
                    if decision_idx &gt; 0:
                        dx_approach = trajectory.x[decision_idx] - trajectory.x[decision_idx - 1]
                        dz_approach = trajectory.z[decision_idx] - trajectory.z[decision_idx - 1]
                        approach_magnitude = np.hypot(dx_approach, dz_approach)
                        if approach_magnitude &gt; 1e-3:
                            approach_yaw = np.degrees(np.arctan2(dx_approach, dz_approach))

                    exit_yaw = np.nan
                    if decision_idx &lt; len(trajectory.x) - 1:
                        dx_exit = trajectory.x[decision_idx + 1] - trajectory.x[decision_idx]
                        dz_exit = trajectory.z[decision_idx + 1] - trajectory.z[decision_idx]
                        exit_magnitude = np.hypot(dx_exit, dz_exit)
                        if exit_magnitude &gt; 1e-3:
                            exit_yaw = np.degrees(np.arctan2(dx_exit, dz_exit))

                    # Calculate distance from junction center
                    distance_from_center = np.sqrt(
                        (trajectory.x[decision_idx] - junction.cx)**2 +
                        (trajectory.z[decision_idx] - junction.cz)**2
                    )

                    # Calculate trajectory position metrics
                    trajectory_length = len(trajectory.x)
                    decision_ratio = decision_idx / trajectory_length if trajectory_length &gt; 0 else 0

                    results.append({
                        &#34;trajectory&#34;: traj_idx,
                        &#34;junction&#34;: junction_idx,
                        &#34;visit_order&#34;: visit_idx,  # Order of this junction in the trajectory
                        &#34;decision_idx&#34;: decision_idx,
                        &#34;trajectory_length&#34;: trajectory_length,
                        &#34;decision_ratio&#34;: decision_ratio,
                        &#34;movement_yaw&#34;: movement_yaw,
                        &#34;approach_yaw&#34;: approach_yaw,
                        &#34;exit_yaw&#34;: exit_yaw,
                        &#34;distance_from_center&#34;: distance_from_center,
                        &#34;decision_x&#34;: trajectory.x[decision_idx],
                        &#34;decision_z&#34;: trajectory.z[decision_idx],
                        &#34;time_at_decision&#34;: self._safe_get_time_value(trajectory, decision_idx)
                    })

        return pd.DataFrame(results)

    def _find_junction_sequence(self, trajectory, junctions, r_outer_list):
        &#34;&#34;&#34;Find the temporal sequence of junction visits for a trajectory.&#34;&#34;&#34;
        import numpy as np

        sequence = []
        current_junction = None

        for point_idx in range(len(trajectory.x)):
            x, z = trajectory.x[point_idx], trajectory.z[point_idx]

            # Check if we&#39;re at a new junction
            for junction_idx, (junction, r_outer) in enumerate(zip(junctions, r_outer_list)):
                distance = np.sqrt((x - junction.cx)**2 + (z - junction.cz)**2)

                if distance &lt;= r_outer:
                    if current_junction != junction_idx:
                        sequence.append(junction_idx)
                        current_junction = junction_idx
                    break
            else:
                # Not at any junction
                current_junction = None

        return sequence

    def _find_decision_point_for_junction_visit(self, trajectory, junction, r_outer, decision_mode, path_length, epsilon, junction_sequence, visit_idx):
        &#34;&#34;&#34;Find decision point for a specific junction visit in the sequence.&#34;&#34;&#34;
        import numpy as np

        # Find the range of points where this junction was visited
        junction_points = []
        for point_idx in range(len(trajectory.x)):
            x, z = trajectory.x[point_idx], trajectory.z[point_idx]
            distance = np.sqrt((x - junction.cx)**2 + (z - junction.cz)**2)
            if distance &lt;= r_outer:
                junction_points.append(point_idx)

        if not junction_points:
            return None

        # Use the first point of junction visit as decision point
        # This represents when the user first entered the junction
        return junction_points[0]

    def _find_radial_decision_point(self, trajectory, junction, r_outer):
        &#34;&#34;&#34;Find decision point using radial method.&#34;&#34;&#34;
        import numpy as np

        # Find the first point that enters the junction
        for i in range(len(trajectory.x)):
            distance = np.sqrt((trajectory.x[i] - junction.cx)**2 + (trajectory.z[i] - junction.cz)**2)
            if distance &lt;= r_outer:
                return i
        return None

    def _find_path_length_decision_point(self, trajectory, junction, path_length, epsilon):
        &#34;&#34;&#34;Find decision point using path length method.&#34;&#34;&#34;
        import numpy as np

        # Find closest point to junction center
        distances = np.sqrt((trajectory.x - junction.cx)**2 + (trajectory.z - junction.cz)**2)
        closest_idx = np.argmin(distances)

        # Use a more reasonable search window
        search_window = min(int(path_length), len(trajectory.x) // 10, 50)  # Smaller, more reasonable window
        start_idx = max(0, closest_idx - search_window)
        end_idx = min(len(trajectory.x), closest_idx + search_window)

        # Look for decision point within the search window
        for i in range(start_idx, end_idx):
            distance = np.sqrt((trajectory.x[i] - junction.cx)**2 + (trajectory.z[i] - junction.cz)**2)
            if distance &lt;= junction.r + epsilon:
                return i

        # If no point found within junction radius, return closest point
        return closest_idx

    def _find_nearest_to_center(self, trajectory, junction):
        &#34;&#34;&#34;Find nearest point to junction center.&#34;&#34;&#34;
        import numpy as np

        distances = np.sqrt((trajectory.x - junction.cx)**2 + (trajectory.z - junction.cz)**2)
        return np.argmin(distances)

    def render_gaze_visualizations(self):
        &#34;&#34;&#34;Render gaze analysis visualizations&#34;&#34;&#34;
        st.markdown(&#34;### Gaze and Physiological Analysis Results&#34;)

        # Check if analysis results and gaze_results exist
        if (st.session_state.analysis_results is None or
            &#34;gaze_results&#34; not in st.session_state.analysis_results):
            st.info(&#34;No gaze analysis results available. Run gaze analysis first.&#34;)
            return

        # Display gaze results for each junction
        for junction_key, gaze_data in st.session_state.analysis_results[&#34;gaze_results&#34;].items():
            st.markdown(f&#34;#### {junction_key.replace(&#39;_&#39;, &#39; &#39;).title()}&#34;)

            if gaze_data is None:
                st.info(&#34;No gaze analysis data available for this junction&#34;)
                continue

            # Check if we have comprehensive gaze data or fallback data
            if isinstance(gaze_data, dict):
                if &#39;error&#39; in gaze_data:
                    # Show error information
                    st.error(f&#34;❌ **Gaze Analysis Failed for {junction_key}**&#34;)
                    st.write(f&#34;**Error:** {gaze_data[&#39;error&#39;]}&#34;)
                    st.write(f&#34;**Error Type:** {gaze_data[&#39;error_type&#39;]}&#34;)

                    # Show suggestions based on error type
                    if &#34;No assignments found&#34; in gaze_data[&#39;error&#39;]:
                        st.info(&#34;💡 **Solution:** Run &#39;🔍 Discover Branches&#39; analysis first to create proper assignments&#34;)
                    elif &#34;trajectory&#34; in gaze_data[&#39;error&#39;].lower():
                        st.info(&#34;💡 **Solution:** Check if trajectories actually pass through this junction&#34;)
                    elif &#34;column&#34; in gaze_data[&#39;error&#39;].lower():
                        st.info(&#34;💡 **Solution:** Check your gaze column mappings in the Data tab&#34;)

                    # Show empty plots with error messages
                    self._render_error_gaze_results(gaze_data, junction_key)
                elif &#39;physiological&#39; in gaze_data:
                    # Comprehensive gaze analysis results
                    self._render_comprehensive_gaze_results(gaze_data, junction_key)
                else:
                    # Fallback movement pattern results
                    st.warning(&#34;⚠️ Using fallback visualization&#34;)
                    self._render_fallback_gaze_results(gaze_data, junction_key)
            else:
                # Fallback movement pattern results
                st.warning(&#34;⚠️ Using fallback visualization&#34;)
                self._render_fallback_gaze_results(gaze_data, junction_key)

    def _render_fallback_gaze_results(self, gaze_data, junction_key):
        &#34;&#34;&#34;Render fallback gaze results when no proper gaze analysis was performed.&#34;&#34;&#34;
        st.markdown(&#34;**Gaze Analysis Results:**&#34;)

        if gaze_data is not None:
            # Check if this is actual gaze data DataFrame or movement data
            if hasattr(gaze_data, &#39;columns&#39;) and len(gaze_data) &gt; 0:
                if &#39;analysis_type&#39; in gaze_data.columns and gaze_data[&#39;analysis_type&#39;].iloc[0] == &#39;gaze&#39;:
                    # This is actual gaze data
                    st.success(&#34;✅ Gaze analysis completed successfully!&#34;)

                    # Display gaze-specific metrics
                    col1, col2, col3, col4 = st.columns(4)

                    with col1:
                        st.metric(&#34;Total Trajectories&#34;, len(gaze_data))

                    with col2:
                        valid_head_yaw = gaze_data[&#39;head_yaw&#39;].dropna()
                        if len(valid_head_yaw) &gt; 0:
                            st.metric(&#34;Valid Head Directions&#34;, len(valid_head_yaw))
                        else:
                            st.metric(&#34;Valid Head Directions&#34;, 0)

                    with col3:
                        valid_pupil = gaze_data[[&#39;pupil_l&#39;, &#39;pupil_r&#39;]].dropna()
                        if len(valid_pupil) &gt; 0:
                            st.metric(&#34;Valid Pupil Data&#34;, len(valid_pupil))
                        else:
                            st.metric(&#34;Valid Pupil Data&#34;, 0)

                    with col4:
                        valid_hr = gaze_data[&#39;heart_rate&#39;].dropna()
                        if len(valid_hr) &gt; 0:
                            st.metric(&#34;Valid Heart Rate&#34;, len(valid_hr))
                        else:
                            st.metric(&#34;Valid Heart Rate&#34;, 0)

                    # Show gaze data table
                    st.dataframe(gaze_data.head(20), width=&#39;stretch&#39;)

                    if len(gaze_data) &gt; 20:
                        st.info(f&#34;Showing first 20 of {len(gaze_data)} gaze records&#34;)

                    # Debug: Show what type of trajectory objects we have
                    st.markdown(&#34;**🔍 Debug Information:**&#34;)
                    sample_traj = st.session_state.trajectories[0] if st.session_state.trajectories else None
                    if sample_traj:
                        st.write(f&#34;- Trajectory type: {type(sample_traj).__name__}&#34;)
                        st.write(f&#34;- Available attributes: {[attr for attr in dir(sample_traj) if not attr.startswith(&#39;_&#39;)]}&#34;)
                        if hasattr(sample_traj, &#39;headset_gaze_x&#39;):
                            st.write(f&#34;- Has gaze data: ✅&#34;)
                        else:
                            st.write(f&#34;- Has gaze data: ❌&#34;)

                    # Create gaze-specific visualizations
                    self._create_gaze_visualizations(gaze_data, junction_key)

                else:
                    # This is movement data - shouldn&#39;t happen in gaze analysis
                    st.warning(&#34;⚠️ **No Gaze Data Available**&#34;)
                    st.info(&#34;&#34;&#34;
                    **Gaze analysis requires:**
                    - Eye tracking data (`Headset.Gaze.X`, `Headset.Gaze.Y`)
                    - Head orientation data (`Headset.Head.Forward.X`, `Headset.Head.Forward.Z`)
                    - Physiological data (`Headset.PupilDilation.L`, `Headset.PupilDilation.R`, `Headset.HeartRate`)

                    **Current data only contains position information (x, z, t).**

                    **To perform gaze analysis:**
                    1. Load data with eye tracking sensors
                    2. Ensure column mappings are correct
                    3. Use VR headset with gaze tracking capabilities
                    &#34;&#34;&#34;)
        else:
            st.warning(&#34;⚠️ **No Gaze Analysis Performed**&#34;)
            st.info(&#34;&#34;&#34;
            **Gaze analysis was not performed because:**
            - No eye tracking data detected
            - Column mappings not properly configured
            - Data doesn&#39;t contain required gaze/physiological fields

            **Please check:**
            1. Your data contains gaze tracking columns
            2. Column mappings are correctly specified
            3. Data format matches VR headset export format
            &#34;&#34;&#34;)

    def _render_error_gaze_results(self, gaze_data, junction_key):
        &#34;&#34;&#34;Render error gaze results with informative empty plots.&#34;&#34;&#34;
        import matplotlib.pyplot as plt
        import numpy as np

        st.markdown(&#34;**Gaze Analysis Results:**&#34;)

        # Create empty plots with error messages
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        fig.suptitle(f&#39;Comprehensive Gaze Analysis - {junction_key}&#39;, fontsize=16, fontweight=&#39;bold&#39;)

        # 1. Heart Rate Change Distribution
        ax1 = axes[0, 0]
        ax1.text(0.5, 0.5, f&#39;Analysis Failed\n{gaze_data[&#34;error&#34;]}&#39;,
                ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax1.transAxes, fontsize=10)
        ax1.set_title(&#39;Heart Rate Change Distribution&#39;)
        ax1.set_xlim(0, 1)
        ax1.set_ylim(0, 1)

        # 2. Pupil Dilation Change Distribution
        ax2 = axes[0, 1]
        ax2.text(0.5, 0.5, f&#39;Analysis Failed\n{gaze_data[&#34;error&#34;]}&#39;,
                ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax2.transAxes, fontsize=10)
        ax2.set_title(&#39;Pupil Dilation Change Distribution&#39;)
        ax2.set_xlim(0, 1)
        ax2.set_ylim(0, 1)

        # 3. Head Yaw Distribution
        ax3 = axes[1, 0]
        ax3.text(0.5, 0.5, f&#39;Analysis Failed\n{gaze_data[&#34;error&#34;]}&#39;,
                ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax3.transAxes, fontsize=10)
        ax3.set_title(&#39;Head Yaw Distribution&#39;)
        ax3.set_xlim(0, 1)
        ax3.set_ylim(0, 1)

        # 4. Gaze-Movement Difference Distribution
        ax4 = axes[1, 1]
        ax4.text(0.5, 0.5, f&#39;Analysis Failed\n{gaze_data[&#34;error&#34;]}&#39;,
                ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax4.transAxes, fontsize=10)
        ax4.set_title(&#39;Gaze-Movement Difference Distribution&#39;)
        ax4.set_xlim(0, 1)
        ax4.set_ylim(0, 1)

        plt.tight_layout()
        st.pyplot(fig)
        plt.close()

    def _render_comprehensive_gaze_results(self, gaze_data, junction_key):
        &#34;&#34;&#34;Render comprehensive gaze analysis results (dictionary format).&#34;&#34;&#34;
        import pandas as pd

        # Display physiological data
        if &#39;physiological&#39; in gaze_data and gaze_data[&#39;physiological&#39;] is not None:
            physio_df = gaze_data[&#39;physiological&#39;]
            if len(physio_df) &gt; 0:
                st.markdown(&#34;**Physiological Analysis:**&#34;)

                col1, col2, col3 = st.columns(3)
                with col1:
                    if &#39;heart_rate_change&#39; in physio_df.columns:
                        valid_hr = physio_df[&#39;heart_rate_change&#39;].dropna()
                        if len(valid_hr) &gt; 0:
                            st.metric(&#34;Avg Heart Rate Change&#34;, f&#34;{valid_hr.mean():.1f} bpm&#34;)
                        else:
                            st.metric(&#34;Avg Heart Rate Change&#34;, &#34;N/A&#34;)
                    else:
                        st.metric(&#34;Avg Heart Rate Change&#34;, &#34;Column not found&#34;)

                with col2:
                    if &#39;pupil_change&#39; in physio_df.columns:
                        valid_pupil = physio_df[&#39;pupil_change&#39;].dropna()
                        if len(valid_pupil) &gt; 0:
                            st.metric(&#34;Avg Pupil Change&#34;, f&#34;{valid_pupil.mean():.2f}&#34;)
                        else:
                            st.metric(&#34;Avg Pupil Change&#34;, &#34;N/A&#34;)
                    else:
                        st.metric(&#34;Avg Pupil Change&#34;, &#34;Column not found&#34;)

                with col3:
                    st.metric(&#34;Total Measurements&#34;, len(physio_df))

                st.dataframe(physio_df.head(10), width=&#39;stretch&#39;)

        # Display pupil dilation data
        if &#39;pupil_dilation&#39; in gaze_data and gaze_data[&#39;pupil_dilation&#39;] is not None:
            pupil_df = gaze_data[&#39;pupil_dilation&#39;]
            if len(pupil_df) &gt; 0:
                st.markdown(&#34;**Pupil Dilation Analysis:**&#34;)

                col1, col2, col3 = st.columns(3)
                with col1:
                    valid_baseline = pupil_df[&#39;pupil_baseline&#39;].dropna()
                    if len(valid_baseline) &gt; 0:
                        st.metric(&#34;Avg Pupil Baseline&#34;, f&#34;{valid_baseline.mean():.2f}&#34;)

                with col2:
                    valid_decision = pupil_df[&#39;pupil_decision&#39;].dropna()
                    if len(valid_decision) &gt; 0:
                        st.metric(&#34;Avg Pupil at Decision&#34;, f&#34;{valid_decision.mean():.2f}&#34;)

                with col3:
                    valid_change = pupil_df[&#39;pupil_change&#39;].dropna()
                    if len(valid_change) &gt; 0:
                        st.metric(&#34;Avg Pupil Change&#34;, f&#34;{valid_change.mean():.2f}&#34;)

                st.dataframe(pupil_df.head(10), width=&#39;stretch&#39;)

        # Display head yaw data
        if &#39;head_yaw&#39; in gaze_data and gaze_data[&#39;head_yaw&#39;] is not None:
            head_df = gaze_data[&#39;head_yaw&#39;]
            if len(head_df) &gt; 0:
                st.markdown(&#34;**Head Yaw Analysis:**&#34;)

                col1, col2, col3 = st.columns(3)
                with col1:
                    valid_yaw = head_df[&#39;head_yaw&#39;].dropna()
                    if len(valid_yaw) &gt; 0:
                        st.metric(&#34;Avg Head Yaw&#34;, f&#34;{valid_yaw.mean():.1f}°&#34;)

                with col2:
                    valid_diff = head_df[&#39;yaw_difference&#39;].dropna()
                    if len(valid_diff) &gt; 0:
                        st.metric(&#34;Avg Gaze-Movement Diff&#34;, f&#34;{valid_diff.mean():.1f}°&#34;)

                with col3:
                    st.metric(&#34;Total Measurements&#34;, len(head_df))

                st.dataframe(head_df.head(10), width=&#39;stretch&#39;)

        # Create visualizations for comprehensive gaze data
        self._create_comprehensive_gaze_visualizations(gaze_data, junction_key)

        # Add advanced gaze plotting features from CLI version
        self._create_advanced_gaze_plots(gaze_data, junction_key)

        # Display pupil dilation heatmaps
        has_global_heatmap = st.session_state.analysis_results and &#39;pupil_heatmap_global&#39; in st.session_state.analysis_results

        # Check if any junction has heatmap data
        has_junction_heatmaps = False
        if st.session_state.analysis_results and &#39;gaze_results&#39; in st.session_state.analysis_results:
            gaze_results = st.session_state.analysis_results[&#39;gaze_results&#39;]
            has_junction_heatmaps = any(
                isinstance(gaze_data_item, dict) and &#39;pupil_heatmap_junction&#39; in gaze_data_item
                for gaze_data_item in gaze_results.values()
            )

        if has_global_heatmap or has_junction_heatmaps:
            st.markdown(&#34;---&#34;)
            st.markdown(&#34;### 🗺️ Pupil Dilation Spatial Heatmaps&#34;)
            st.info(&#34;Spatial distribution of pupil dilation changes across the map&#34;)

            # Calculate consistent scaling across all heatmaps
            from verta.verta_gaze import get_consistent_pupil_scaling
            heatmap_data_list = []
            if has_global_heatmap:
                heatmap_data_list.append(st.session_state.analysis_results[&#39;pupil_heatmap_global&#39;])
            if has_junction_heatmaps:
                gaze_results = st.session_state.analysis_results[&#39;gaze_results&#39;]
                for gaze_data_item in gaze_results.values():
                    if isinstance(gaze_data_item, dict) and &#39;pupil_heatmap_junction&#39; in gaze_data_item:
                        heatmap_data_list.extend(gaze_data_item[&#39;pupil_heatmap_junction&#39;].values())

            normalization = st.session_state.get(&#39;pupil_heatmap_normalization&#39;, &#39;relative&#39;)
            vmin, vmax = get_consistent_pupil_scaling(heatmap_data_list, normalization)

            st.write(f&#34;🎨 **Consistent Color Scaling:** {vmin:.1f}% to {vmax:.1f}% (realistic pupil dilation range)&#34;)

            # Create tabs for global and per-junction views
            tab_junction, tab_global = st.tabs([&#34;🎯 Junction Heatmap&#34;, &#34;🌍 Global Heatmap&#34;])

            with tab_junction:
                if has_junction_heatmaps:
                    st.markdown(&#34;#### Junction-Specific Pupil Patterns&#34;)
                    st.caption(&#34;Focused view of pupil changes at each junction (includes approach paths)&#34;)

                    # Display heatmap for current junction only
                    current_junction_heatmaps = {}
                    if isinstance(gaze_data, dict) and &#39;pupil_heatmap_junction&#39; in gaze_data:
                        current_junction_heatmaps = gaze_data[&#39;pupil_heatmap_junction&#39;]

                    if len(current_junction_heatmaps) == 0:
                        st.info(&#34;ℹ️ No junction heatmaps available&#34;)
                    else:
                        for junction_idx, heatmap_data in current_junction_heatmaps.items():
                            with st.expander(f&#34;**Junction {junction_idx}**&#34;, expanded=True):
                                # Check for errors
                                if heatmap_data.get(&#39;error&#39;):
                                    st.warning(f&#34;⚠️ {heatmap_data[&#39;error&#39;]}&#34;)
                                else:
                                    # Get junction info
                                    junction = heatmap_data.get(&#39;junction&#39;)
                                    r_outer = heatmap_data.get(&#39;r_outer&#39;, &#39;N/A&#39;)

                                    col_info1, col_info2 = st.columns(2)
                                    with col_info1:
                                        st.caption(f&#34;📍 Center: ({junction.cx:.1f}, {junction.cz:.1f})&#34;)
                                    with col_info2:
                                        st.caption(f&#34;📏 Analysis radius (r_outer): {r_outer}&#34;)

                                    # Check if pre-generated plot exists first
                                    junction_key = f&#34;junction_{junction_idx}&#34;
                                    pre_generated_plot_path = os.path.join(&#34;gui_outputs&#34;, f&#34;junction_{junction_idx}&#34;, &#34;gaze_plots&#34;, f&#34;{junction_key}_pupil_heatmap.png&#34;)

                                    # Debug: Show the path being checked
                                    st.write(f&#34;🔍 **Debug:** Looking for junction heatmap at: `{pre_generated_plot_path}`&#34;)
                                    st.write(f&#34;🔍 **Debug:** File exists: {os.path.exists(pre_generated_plot_path)}&#34;)

                                    if os.path.exists(pre_generated_plot_path):
                                        # Load and display pre-generated plot
                                        st.image(pre_generated_plot_path, caption=f&#34;Junction {junction_idx} Pupil Dilation (Pre-generated)&#34;)
                                        st.caption(&#34;📁 Plot generated during analysis&#34;)
                                    else:
                                        # Generate plot on-demand (fallback)
                                        st.caption(&#34;🔄 Generating plot on-demand...&#34;)
                                        st.write(f&#34;⚠️ **Debug:** Pre-generated plot not found at `{pre_generated_plot_path}`&#34;)

                                        # Get filtered trajectories for this junction (only those passing through)
                                        filtered_trajs_for_plot = self._get_filtered_trajectories_for_junction(
                                            st.session_state.trajectories, junction, r_outer
                                        )

                                        # Plot junction heatmap (with minimap, with filtered trajectories only)
                                        fig_junction = plot_pupil_dilation_heatmap(
                                            heatmap_data=heatmap_data,
                                            junctions=[junction] if junction else None,
                                            trajectories=filtered_trajs_for_plot,
                                            all_trajectories=st.session_state.trajectories,  # Pass all trajectories for minimap
                                            title=f&#34;Junction {junction_idx} Pupil Dilation&#34;,
                                            show_sample_counts=False,
                                            show_minimap=True,
                                            vmin=vmin,
                                            vmax=vmax
                                        )
                                        st.pyplot(fig_junction)
                                        import matplotlib.pyplot as plt
                                        plt.close(fig_junction)

                                    # Show junction statistics
                                    heatmap = heatmap_data[&#39;heatmap&#39;]
                                    # Ensure heatmap is a numpy array
                                    if not isinstance(heatmap, np.ndarray):
                                        heatmap = np.array(heatmap)
                                    valid_bins = heatmap[~np.isnan(heatmap)]
                                    norm_method = heatmap_data[&#39;normalization_used&#39;]

                                    if len(valid_bins) &gt; 0:
                                        col1, col2, col3 = st.columns(3)

                                        with col1:
                                            if norm_method == &#34;relative&#34;:
                                                st.metric(&#34;Mean Change&#34;, f&#34;{np.mean(valid_bins):.2f}%&#34;)
                                            else:
                                                st.metric(&#34;Mean Z-score&#34;, f&#34;{np.mean(valid_bins):.2f}&#34;)

                                        with col2:
                                            if norm_method == &#34;relative&#34;:
                                                st.metric(&#34;Max Absolute Change&#34;, f&#34;±{np.max(np.abs(valid_bins)):.2f}%&#34;)
                                            else:
                                                st.metric(&#34;Max Absolute Z-score&#34;, f&#34;{np.max(np.abs(valid_bins)):.2f}&#34;)

                                        with col3:
                                            st.metric(&#34;Trajectories&#34;, f&#34;{heatmap_data[&#39;valid_trajectories&#39;]}&#34;)
                else:
                    st.info(&#34;ℹ️ No junction heatmaps available&#34;)

            with tab_global:
                if has_global_heatmap:
                    st.markdown(&#34;#### Global Pupil Dilation Patterns&#34;)
                    st.caption(&#34;Shows pupil changes across the entire environment&#34;)

                    global_heatmap_data = st.session_state.analysis_results[&#39;pupil_heatmap_global&#39;]

                    # Check for errors
                    if global_heatmap_data.get(&#39;error&#39;):
                        st.warning(f&#34;⚠️ {global_heatmap_data[&#39;error&#39;]}&#34;)
                    else:
                        # Check if pre-generated global plot exists first
                        pre_generated_global_path = os.path.join(&#34;gui_outputs&#34;, &#34;gaze_plots&#34;, &#34;global_pupil_heatmap.png&#34;)

                        # Debug: Show the path being checked
                        st.write(f&#34;🔍 **Debug:** Looking for global heatmap at: `{pre_generated_global_path}`&#34;)
                        st.write(f&#34;🔍 **Debug:** File exists: {os.path.exists(pre_generated_global_path)}&#34;)

                        if os.path.exists(pre_generated_global_path):
                            # Load and display pre-generated plot
                            st.image(pre_generated_global_path, caption=&#34;Global Pupil Dilation Heatmap (Pre-generated)&#34;)
                            st.caption(&#34;📁 Plot generated during analysis&#34;)
                        else:
                            # Generate plot on-demand (fallback)
                            st.caption(&#34;🔄 Generating global heatmap on-demand...&#34;)
                            st.write(f&#34;⚠️ **Debug:** Pre-generated plot not found at `{pre_generated_global_path}`&#34;)

                            # Plot global heatmap (only once, without minimap)
                            fig_global = plot_pupil_dilation_heatmap(
                                heatmap_data=global_heatmap_data,
                                junctions=st.session_state.junctions,
                                trajectories=st.session_state.trajectories,
                                title=&#34;Global Pupil Dilation Heatmap&#34;,
                                show_sample_counts=False,
                                show_minimap=False,
                                vmin=vmin,
                                vmax=vmax
                            )
                            st.pyplot(fig_global)
                            import matplotlib.pyplot as plt
                            plt.close(fig_global)

                        # Show statistics
                        heatmap = global_heatmap_data[&#39;heatmap&#39;]
                        # Ensure heatmap is a numpy array
                        if not isinstance(heatmap, np.ndarray):
                            heatmap = np.array(heatmap)
                        valid_bins = heatmap[~np.isnan(heatmap)]
                        norm_method = global_heatmap_data[&#39;normalization_used&#39;]

                        if len(valid_bins) &gt; 0:
                            col1, col2, col3, col4 = st.columns(4)

                            with col1:
                                if norm_method == &#34;relative&#34;:
                                    st.metric(&#34;Mean Change&#34;, f&#34;{np.mean(valid_bins):.2f}%&#34;)
                                else:
                                    st.metric(&#34;Mean Z-score&#34;, f&#34;{np.mean(valid_bins):.2f}&#34;)

                            with col2:
                                if norm_method == &#34;relative&#34;:
                                    st.metric(&#34;Max Dilation&#34;, f&#34;+{np.max(valid_bins):.2f}%&#34;)
                                else:
                                    st.metric(&#34;Max Z-score&#34;, f&#34;{np.max(valid_bins):.2f}&#34;)

                            with col3:
                                if norm_method == &#34;relative&#34;:
                                    st.metric(&#34;Max Constriction&#34;, f&#34;{np.min(valid_bins):.2f}%&#34;)
                                else:
                                    st.metric(&#34;Min Z-score&#34;, f&#34;{np.min(valid_bins):.2f}&#34;)

                            with col4:
                                st.metric(&#34;Valid Bins&#34;, f&#34;{len(valid_bins)}/{heatmap.size}&#34;)
                else:
                    st.info(&#34;ℹ️ Global heatmap not available&#34;)

    def _create_comprehensive_gaze_visualizations(self, gaze_data, junction_key):
        &#34;&#34;&#34;Create visualizations for comprehensive gaze analysis results.&#34;&#34;&#34;
        import matplotlib.pyplot as plt
        import numpy as np

        # Ensure expected column names exist for plotting
        if isinstance(gaze_data, dict):
            gaze_data = self._normalize_gaze_result_frames(gaze_data)

        # Create figure with subplots
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle(f&#39;Comprehensive Gaze Analysis - {junction_key.replace(&#34;_&#34;, &#34; &#34;).title()}&#39;, fontsize=14)

        # 1. Heart Rate Distribution
        ax1 = axes[0, 0]
        if &#39;physiological&#39; in gaze_data and gaze_data[&#39;physiological&#39;] is not None:
            physio_data = gaze_data[&#39;physiological&#39;]

            # Handle both DataFrame and list formats
            if isinstance(physio_data, list):
                import pandas as pd
                physio_df = pd.DataFrame(physio_data)
            else:
                physio_df = physio_data

            if &#39;heart_rate_change&#39; in physio_df.columns:
                hr_data = physio_df[&#39;heart_rate_change&#39;].dropna()
                if len(hr_data) &gt; 0:
                    ax1.hist(hr_data, bins=15, alpha=0.7, color=&#39;red&#39;, edgecolor=&#39;black&#39;)
                    ax1.set_title(&#39;Heart Rate Change Distribution\n(Baseline: Normal navigation 2-5s before junction entry)&#39;)
                    ax1.set_xlabel(&#39;Heart Rate Change (bpm)&#39;)
                    ax1.set_ylabel(&#39;Frequency&#39;)
                    ax1.grid(True, alpha=0.3)
                else:
                    ax1.text(0.5, 0.5, &#39;No heart rate data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax1.transAxes)
                    ax1.set_title(&#39;Heart Rate Change Distribution\n(Baseline: Normal navigation 2-5s before junction entry)&#39;)
            else:
                ax1.text(0.5, 0.5, &#39;heart_rate_change column not found&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax1.transAxes)
                ax1.set_title(&#39;Heart Rate Change Distribution&#39;)
        else:
            ax1.text(0.5, 0.5, &#39;No physiological data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax1.transAxes)
            ax1.set_title(&#39;Heart Rate Change Distribution&#39;)

        # 2. Pupil Dilation Change
        ax2 = axes[0, 1]
        if &#39;pupil_dilation&#39; in gaze_data and gaze_data[&#39;pupil_dilation&#39;] is not None:
            pupil_data = gaze_data[&#39;pupil_dilation&#39;]

            # Handle both DataFrame and list formats
            if isinstance(pupil_data, list):
                import pandas as pd
                pupil_df = pd.DataFrame(pupil_data)
            else:
                pupil_df = pupil_data

            if &#39;pupil_change&#39; in pupil_df.columns:
                pupil_values = pupil_df[&#39;pupil_change&#39;].dropna()
                if len(pupil_values) &gt; 0:
                    ax2.hist(pupil_values, bins=15, alpha=0.7, color=&#39;green&#39;, edgecolor=&#39;black&#39;)
                    ax2.set_title(&#39;Pupil Dilation Change Distribution\n(Baseline: Normal navigation 2-5s before junction entry)&#39;)
                    ax2.set_xlabel(&#39;Pupil Change (mm)&#39;)
                    ax2.set_ylabel(&#39;Frequency&#39;)
                    ax2.grid(True, alpha=0.3)
                else:
                    ax2.text(0.5, 0.5, &#39;No pupil data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax2.transAxes)
                    ax2.set_title(&#39;Pupil Dilation Change Distribution\n(Baseline: Normal navigation 2-5s before junction entry)&#39;)
            else:
                ax2.text(0.5, 0.5, &#39;pupil_change column not found&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax2.transAxes)
                ax2.set_title(&#39;Pupil Dilation Change Distribution&#39;)
        else:
            ax2.text(0.5, 0.5, &#39;No pupil dilation data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax2.transAxes)
            ax2.set_title(&#39;Pupil Dilation Change Distribution&#39;)

        # 3. Head Yaw Distribution
        ax3 = axes[1, 0]
        if &#39;head_yaw&#39; in gaze_data and gaze_data[&#39;head_yaw&#39;] is not None:
            yaw_data = gaze_data[&#39;head_yaw&#39;]

            # Handle both DataFrame and list formats
            if isinstance(yaw_data, list):
                import pandas as pd
                yaw_df = pd.DataFrame(yaw_data)
            else:
                yaw_df = yaw_data

            if &#39;head_yaw&#39; in yaw_df.columns:
                yaw_values = yaw_df[&#39;head_yaw&#39;].dropna()
                if len(yaw_values) &gt; 0:
                    ax3.hist(yaw_values, bins=20, alpha=0.7, color=&#39;blue&#39;, edgecolor=&#39;black&#39;)
                    ax3.set_title(&#39;Head Yaw Distribution&#39;)
                    ax3.set_xlabel(&#39;Head Yaw (degrees)&#39;)
                    ax3.set_ylabel(&#39;Frequency&#39;)
                    ax3.grid(True, alpha=0.3)
                else:
                    ax3.text(0.5, 0.5, &#39;No head yaw data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax3.transAxes)
                    ax3.set_title(&#39;Head Yaw Distribution&#39;)
            else:
                ax3.text(0.5, 0.5, &#39;head_yaw column not found&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax3.transAxes)
                ax3.set_title(&#39;Head Yaw Distribution&#39;)
        else:
            ax3.text(0.5, 0.5, &#39;No head yaw data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax3.transAxes)
            ax3.set_title(&#39;Head Yaw Distribution&#39;)

        # 4. Gaze-Movement Difference
        ax4 = axes[1, 1]
        if &#39;head_yaw&#39; in gaze_data and gaze_data[&#39;head_yaw&#39;] is not None:
            yaw_data = gaze_data[&#39;head_yaw&#39;]

            # Handle both DataFrame and list formats
            if isinstance(yaw_data, list):
                import pandas as pd
                yaw_df = pd.DataFrame(yaw_data)
            else:
                yaw_df = yaw_data

            if &#39;yaw_difference&#39; in yaw_df.columns:
                diff_values = yaw_df[&#39;yaw_difference&#39;].dropna()
                if len(diff_values) &gt; 0:
                    ax4.hist(diff_values, bins=20, alpha=0.7, color=&#39;purple&#39;, edgecolor=&#39;black&#39;)
                    ax4.set_title(&#39;Gaze-Movement Difference Distribution&#39;)
                    ax4.set_xlabel(&#39;Difference (degrees)&#39;)
                    ax4.set_ylabel(&#39;Frequency&#39;)
                    ax4.grid(True, alpha=0.3)
                else:
                    ax4.text(0.5, 0.5, &#39;No gaze-movement data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax4.transAxes)
                    ax4.set_title(&#39;Gaze-Movement Difference Distribution&#39;)
            else:
                ax4.text(0.5, 0.5, &#39;yaw_difference column not found&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax4.transAxes)
                ax4.set_title(&#39;Gaze-Movement Difference Distribution&#39;)
        else:
            ax4.text(0.5, 0.5, &#39;No gaze-movement data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax4.transAxes)
            ax4.set_title(&#39;Gaze-Movement Difference Distribution&#39;)

        plt.tight_layout()
        st.pyplot(fig)
        plt.close()

    def _generate_gaze_plots_during_analysis(self, gaze_data, junction_key, out_dir):
        &#34;&#34;&#34;Generate gaze plots during analysis (not just in visualization tab).&#34;&#34;&#34;
        import os
        import matplotlib.pyplot as plt

        # Create gaze plots directory
        gaze_plots_dir = os.path.join(&#34;gui_outputs&#34;, f&#34;junction_{junction_key.split(&#39;_&#39;)[1]}&#34;, &#34;gaze_plots&#34;)
        os.makedirs(gaze_plots_dir, exist_ok=True)

        # Normalize result frames so downstream plotting functions find expected columns
        if isinstance(gaze_data, dict):
            gaze_data = self._normalize_gaze_result_frames(gaze_data)

        # Get trajectories from session state for plotting
        trajectories = st.session_state.get(&#39;trajectories&#39;, [])
        if not trajectories:
            return

        # Get junction information
        junction = gaze_data.get(&#39;junction&#39;)
        r_outer = gaze_data.get(&#39;r_outer&#39;)

        if junction is None:
            return

        # Generate gaze directions plot
        try:
            from verta.verta_gaze import plot_gaze_directions_at_junctions

            plot_path = os.path.join(gaze_plots_dir, f&#34;{junction_key}_gaze_directions.png&#34;)

            # Use fresh head_yaw data from gaze analysis (not old cached data)
            gaze_df = gaze_data.get(&#39;head_yaw&#39;)
            if gaze_df is not None and not gaze_df.empty:
                # Debug: Show branch information in the fresh data
                unique_branches = sorted(gaze_df[&#39;branch&#39;].unique()) if &#39;branch&#39; in gaze_df.columns else []
                print(f&#34;🔍 **Fresh head_yaw data branches**: {unique_branches}&#34;)
                print(f&#34;🔍 **Fresh head_yaw data shape**: {gaze_df.shape}&#34;)
                print(f&#34;🔍 **Fresh head_yaw data columns**: {list(gaze_df.columns)}&#34;)

                # Additional debugging for J3+ junctions
                junction_num = junction_key.split(&#39;_&#39;)[1] if &#39;_&#39; in junction_key else &#39;0&#39;
                if int(junction_num) &gt;= 3:
                    print(f&#34;🔍 **JUNCTION {junction_num} DEBUG**:&#34;)
                    print(f&#34;- Junction: {junction}&#34;)
                    print(f&#34;- R_outer: {r_outer}&#34;)
                    print(f&#34;- Trajectories passed to plotting: {len(trajectories)}&#34;)
                    print(f&#34;- Head_yaw data rows: {len(gaze_df)}&#34;)
                    print(f&#34;- Branch distribution: {gaze_df[&#39;branch&#39;].value_counts().to_dict() if &#39;branch&#39; in gaze_df.columns else &#39;No branch column&#39;}&#34;)

                # Show sample of the fresh data
                if len(gaze_df) &gt; 0:
                    print(f&#34;🔍 **Sample fresh head_yaw data**:&#34;)
                    print(gaze_df.head())

                plot_gaze_directions_at_junctions(
                    trajectories=trajectories,
                    junctions=[junction],
                    gaze_df=gaze_df,
                    out_path=plot_path,
                    r_outer_list=[r_outer] if r_outer else [None],
                    junction_labels=[f&#34;Junction {junction_key.split(&#39;_&#39;)[1]}&#34;],
                )
        except Exception as e:
            print(f&#34;Could not generate gaze directions plot: {e}&#34;)

        # Generate physiological analysis plot
        try:
            from verta.verta_gaze import plot_physiological_by_branch

            plot_path = os.path.join(gaze_plots_dir, f&#34;{junction_key}_physiological_analysis.png&#34;)
            physio_df = gaze_data.get(&#39;physiological&#39;)

            if physio_df is not None and not physio_df.empty:
                # Debug: Show branch information in the fresh physiological data
                unique_branches = sorted(physio_df[&#39;branch&#39;].unique()) if &#39;branch&#39; in physio_df.columns else []
                print(f&#34;**Fresh physiological data branches**: {unique_branches}&#34;)
                print(f&#34;**Fresh physiological data shape**: {physio_df.shape}&#34;)

                # Additional debugging for J3+ junctions
                junction_num = junction_key.split(&#39;_&#39;)[1] if &#39;_&#39; in junction_key else &#39;0&#39;
                if int(junction_num) &gt;= 3:
                    print(f&#34;🔍 **JUNCTION {junction_num} PHYSIO DEBUG**:&#34;)
                    print(f&#34;- Physiological data rows: {len(physio_df)}&#34;)
                    print(f&#34;- Branch distribution: {physio_df[&#39;branch&#39;].value_counts().to_dict() if &#39;branch&#39; in physio_df.columns else &#39;No branch column&#39;}&#34;)
                    print(f&#34;- Sample physio data:&#34;)
                    print(physio_df.head())

                plot_physiological_by_branch(
                    physio_df=physio_df,
                    out_path=plot_path,
                )
        except Exception as e:
            print(f&#34;Could not generate physiological analysis plot: {e}&#34;)

        # Generate pupil trajectory analysis plot
        try:
            from verta.verta_gaze import plot_pupil_trajectory_analysis

            plot_path = os.path.join(gaze_plots_dir, f&#34;{junction_key}_pupil_trajectory.png&#34;)
            pupil_df = gaze_data.get(&#39;pupil&#39;)

            if pupil_df is not None and not pupil_df.empty:
                plot_pupil_trajectory_analysis(
                    pupil_traj_df=pupil_df,
                    out_path=plot_path,
                )
        except Exception as e:
            print(f&#34;Could not generate pupil trajectory plot: {e}&#34;)

        # Generate junction-specific heatmap plot
        try:
            from verta.verta_gaze import plot_pupil_dilation_heatmap
            import numpy as np
            import matplotlib.pyplot as plt

            # Get heatmap data from gaze_data
            junction_heatmaps = gaze_data.get(&#39;pupil_heatmap_junction&#39;)
            st.write(f&#34;🔍 **Debug:** Junction heatmaps available: {junction_heatmaps is not None}&#34;)
            if junction_heatmaps:
                st.write(f&#34;🔍 **Debug:** Junction heatmaps length: {len(junction_heatmaps)}&#34;)
                st.write(f&#34;🔍 **Debug:** Junction heatmaps keys: {list(junction_heatmaps.keys())}&#34;)
            else:
                st.write(f&#34;🔍 **Debug:** No junction heatmaps found in gaze_data&#34;)

            if junction_heatmaps and len(junction_heatmaps) &gt; 0:
                # Get the heatmap data for this junction
                junction_idx = None
                st.write(f&#34;🔍 **Debug:** Looking for junction: ({junction.cx}, {junction.cz}, r={junction.r})&#34;)
                for idx, junc in enumerate(st.session_state.junctions):
                    st.write(f&#34;🔍 **Debug:** Checking junction {idx}: ({junc.cx}, {junc.cz}, r={junc.r})&#34;)
                    if (junc.cx == junction.cx and junc.cz == junction.cz and junc.r == junction.r):
                        junction_idx = idx
                        st.write(f&#34;🔍 **Debug:** Found matching junction at index {junction_idx}&#34;)
                        break

                st.write(f&#34;🔍 **Debug:** Junction index found: {junction_idx}&#34;)

                if junction_idx is not None and junction_idx in junction_heatmaps:
                    heatmap_data = junction_heatmaps[junction_idx]
                    st.write(f&#34;🔍 **Debug:** Found heatmap data for junction {junction_idx}&#34;)

                    # Create heatmap plot
                    plot_path = os.path.join(gaze_plots_dir, f&#34;{junction_key}_pupil_heatmap.png&#34;)
                    st.write(f&#34;🔍 **Debug:** Plot path: {plot_path}&#34;)

                    # Filter trajectories for this junction (same logic as visualizations tab)
                    filtered_trajs_for_plot = []
                    for traj in trajectories:
                        # Check if trajectory passes through this junction
                        rx = traj.x - junction.cx
                        rz = traj.z - junction.cz
                        r = np.hypot(rx, rz)
                        if np.any(r &lt;= junction.r):
                            filtered_trajs_for_plot.append(traj)

                    st.write(f&#34;🔍 **Debug:** Filtered trajectories for plot: {len(filtered_trajs_for_plot)}&#34;)

                    # Create the heatmap plot
                    st.write(f&#34;🔍 **Debug:** Creating heatmap plot...&#34;)
                    try:
                        fig = plot_pupil_dilation_heatmap(
                            heatmap_data=heatmap_data,
                            junctions=[junction],
                            trajectories=filtered_trajs_for_plot,
                            all_trajectories=trajectories,  # Pass all trajectories for minimap
                            title=f&#34;Junction {junction_idx} Pupil Dilation&#34;,
                            show_sample_counts=False,
                            show_minimap=True,
                            vmin=None,  # Let the function determine scaling
                            vmax=None
                        )

                        st.write(f&#34;🔍 **Debug:** Heatmap plot created successfully&#34;)

                        # Save the plot
                        st.write(f&#34;🔍 **Debug:** Saving plot to {plot_path}...&#34;)
                        fig.savefig(plot_path, dpi=150, bbox_inches=&#34;tight&#34;)
                        plt.close(fig)

                        print(f&#34;Junction heatmap plot saved to: {plot_path}&#34;)
                        st.write(f&#34;🔍 **Debug:** Junction plot saved to: `{plot_path}`&#34;)
                        st.write(f&#34;🔍 **Debug:** File exists after save: {os.path.exists(plot_path)}&#34;)
                    except Exception as plot_error:
                        st.write(f&#34;❌ **Debug:** Error creating heatmap plot: {plot_error}&#34;)
                        st.write(f&#34;❌ **Debug:** Error type: {type(plot_error)}&#34;)
                        import traceback
                        st.write(f&#34;❌ **Debug:** Traceback: {traceback.format_exc()}&#34;)
                        print(f&#34;Error creating junction heatmap plot: {plot_error}&#34;)
                        print(f&#34;Traceback: {traceback.format_exc()}&#34;)
                else:
                    st.write(f&#34;🔍 **Debug:** No heatmap data found for junction {junction_idx}&#34;)
                    print(f&#34;No heatmap data found for junction {junction_idx}&#34;)
            else:
                print(f&#34;No junction heatmaps available for {junction_key}&#34;)

        except Exception as e:
            print(f&#34;Could not generate junction heatmap plot: {e}&#34;)

    def _create_advanced_gaze_plots(self, gaze_data, junction_key):
        &#34;&#34;&#34;Create advanced gaze plots using CLI plotting functions.&#34;&#34;&#34;
        import matplotlib.pyplot as plt
        import tempfile
        import os

        # Normalize result frames so downstream plotting functions find expected columns
        if isinstance(gaze_data, dict):
            gaze_data = self._normalize_gaze_result_frames(gaze_data)

        st.markdown(&#34;### Advanced Gaze Analysis Plots&#34;)

        # Get trajectories from session state for plotting
        trajectories = st.session_state.get(&#39;trajectories&#39;, [])
        if not trajectories:
            st.warning(&#34;⚠️ No trajectories available for advanced plotting&#34;)
            return

        # Get junction information
        junction = gaze_data.get(&#39;junction&#39;)
        r_outer = gaze_data.get(&#39;r_outer&#39;)

        if junction is None:
            st.warning(&#34;⚠️ No junction information available for plotting&#34;)
            return

        # Check if plots were already generated during analysis
        junction_num = junction_key.split(&#39;_&#39;)[1] if &#39;_&#39; in junction_key else &#39;0&#39;
        analysis_plots_dir = os.path.join(&#34;gui_outputs&#34;, f&#34;junction_{junction_num}&#34;, &#34;gaze_plots&#34;)

        # Define plot paths
        gaze_directions_path = os.path.join(analysis_plots_dir, f&#34;{junction_key}_gaze_directions.png&#34;)
        physio_path = os.path.join(analysis_plots_dir, f&#34;{junction_key}_physiological_analysis.png&#34;)
        pupil_path = os.path.join(analysis_plots_dir, f&#34;{junction_key}_pupil_trajectory.png&#34;)

        # If plots exist from analysis, display them instead of regenerating
        if os.path.exists(analysis_plots_dir):
            st.info(f&#34;📊 **Displaying plots generated during analysis**&#34;)


            if os.path.exists(gaze_directions_path):
                st.markdown(&#34;#### Gaze Directions at Junction&#34;)
                st.image(gaze_directions_path, caption=&#34;👁️ Gaze directions at decision points&#34;)

            if os.path.exists(physio_path):
                st.markdown(&#34;#### Physiological Analysis&#34;)
                st.image(physio_path, caption=&#34;📊 Decision Point Analysis: Physiological changes during junction approach&#34;)

            if os.path.exists(pupil_path):
                st.markdown(&#34;#### Pupil Trajectory Analysis&#34;)
                st.image(pupil_path, caption=&#34;🗺️ Junction Area Analysis: Pupil changes across entire junction region&#34;)

            return

        # Fallback: Generate plots if they don&#39;t exist (for backward compatibility)
        st.info(f&#34;📊 **Generating plots on-demand**&#34;)

        # Create gaze plots directory in gui_outputs
        gaze_plots_dir = os.path.join(&#34;gui_outputs&#34;, &#34;gaze_plots&#34;)
        os.makedirs(gaze_plots_dir, exist_ok=True)

        # Only generate consistency report (plots are now generated during analysis)
        try:
            # Gaze Consistency Report
            if &#39;head_yaw&#39; in gaze_data and gaze_data[&#39;head_yaw&#39;] is not None:
                head_yaw_df = gaze_data[&#39;head_yaw&#39;]
                if len(head_yaw_df) &gt; 0:
                    st.markdown(&#34;#### Gaze-Movement Consistency Report&#34;)

                    # Import the reporting function
                    from verta.verta_gaze import gaze_movement_consistency_report

                    consistency_report = gaze_movement_consistency_report(head_yaw_df)

                    # Display the report
                    if &#39;error&#39; not in consistency_report:
                        col1, col2, col3 = st.columns(3)

                        with col1:
                            st.metric(
                                &#34;Mean Absolute Yaw Difference&#34;,
                                f&#34;{consistency_report.get(&#39;mean_absolute_yaw_difference&#39;, 0):.1f}°&#34;
                            )

                        with col2:
                            st.metric(
                                &#34;Aligned Percentage&#34;,
                                f&#34;{consistency_report.get(&#39;aligned_percentage&#39;, 0):.1f}%&#34;
                            )

                        with col3:
                            st.metric(
                                &#34;Total Decisions&#34;,
                                consistency_report.get(&#39;total_decisions&#39;, 0)
                            )

                        # Show branch-specific alignment
                        branch_metrics = {k: v for k, v in consistency_report.items()
                                       if k.startswith(&#39;branch_&#39;) and k.endswith(&#39;_alignment&#39;)}

                        if branch_metrics:
                            st.markdown(&#34;**Branch-Specific Alignment:**&#34;)
                            for branch_key, alignment in branch_metrics.items():
                                branch_num = branch_key.replace(&#39;branch_&#39;, &#39;&#39;).replace(&#39;_alignment&#39;, &#39;&#39;)
                                st.write(f&#34;- Branch {branch_num}: {alignment:.1f}° average difference&#34;)

                        # Save consistency report to file
                        import json
                        consistency_file = os.path.join(gaze_plots_dir, f&#34;{junction_key}_gaze_consistency_report.json&#34;)
                        with open(consistency_file, &#39;w&#39;) as f:
                            json.dump(consistency_report, f, indent=2)
                        st.info(f&#34;📁 Consistency report saved to: {consistency_file}&#34;)

                    else:
                        st.warning(f&#34;⚠️ {consistency_report[&#39;error&#39;]}&#34;)

        except Exception as e:
            st.error(f&#34;❌ Error creating advanced gaze plots: {e}&#34;)
            import traceback
            st.error(f&#34;**Error details:** {traceback.format_exc()}&#34;)

    def _create_gaze_visualizations(self, gaze_data, junction_key):
        &#34;&#34;&#34;Create gaze-specific visualizations.&#34;&#34;&#34;
        import matplotlib.pyplot as plt
        import numpy as np

        if len(gaze_data) == 0:
            st.info(&#34;No gaze data available for visualization&#34;)
            return

        # Create figure with subplots
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle(f&#39;Gaze Analysis Visualizations - {junction_key.replace(&#34;_&#34;, &#34; &#34;).title()}&#39;, fontsize=14)

        # 1. Head Direction Distribution
        ax1 = axes[0, 0]
        head_yaw_data = gaze_data[&#39;head_yaw&#39;].dropna()
        if len(head_yaw_data) &gt; 0:
            ax1.hist(head_yaw_data, bins=20, alpha=0.7, color=&#39;skyblue&#39;, edgecolor=&#39;black&#39;)
            ax1.set_title(&#39;Head Direction Distribution&#39;)
            ax1.set_xlabel(&#39;Head Yaw (degrees)&#39;)
            ax1.set_ylabel(&#39;Frequency&#39;)
            ax1.grid(True, alpha=0.3)
        else:
            ax1.text(0.5, 0.5, &#39;No head direction data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax1.transAxes)
            ax1.set_title(&#39;Head Direction Distribution&#39;)

        # 2. Pupil Dilation Comparison
        ax2 = axes[0, 1]
        pupil_l_data = gaze_data[&#39;pupil_l&#39;].dropna()
        pupil_r_data = gaze_data[&#39;pupil_r&#39;].dropna()
        if len(pupil_l_data) &gt; 0 and len(pupil_r_data) &gt; 0:
            ax2.scatter(pupil_l_data, pupil_r_data, alpha=0.6, color=&#39;green&#39;)
            ax2.set_title(&#39;Left vs Right Pupil Dilation&#39;)
            ax2.set_xlabel(&#39;Left Pupil Size&#39;)
            ax2.set_ylabel(&#39;Right Pupil Size&#39;)
            ax2.grid(True, alpha=0.3)
        else:
            ax2.text(0.5, 0.5, &#39;No pupil data available&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax2.transAxes)
            ax2.set_title(&#39;Left vs Right Pupil Dilation&#39;)

        # 3. Heart Rate Distribution
        ax3 = axes[1, 0]
        hr_data = gaze_data[&#39;heart_rate&#39;].dropna()
        if len(hr_data) &gt; 0:
            ax3.hist(hr_data, bins=15, alpha=0.7, color=&#39;red&#39;, edgecolor=&#39;black&#39;)
            ax3.set_title(&#39;Heart Rate Distribution&#39;)
            ax3.set_xlabel(&#39;Heart Rate (bpm)&#39;)
            ax3.set_ylabel(&#39;Frequency&#39;)
            ax3.grid(True, alpha=0.3)
        else:
            ax3.text(0.5, 0.5, &#39;No heart rate data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax3.transAxes)
            ax3.set_title(&#39;Heart Rate Distribution&#39;)

        # 4. Gaze Direction Scatter
        ax4 = axes[1, 1]
        gaze_x_data = gaze_data[&#39;gaze_x&#39;].dropna()
        gaze_y_data = gaze_data[&#39;gaze_y&#39;].dropna()
        if len(gaze_x_data) &gt; 0 and len(gaze_y_data) &gt; 0:
            ax4.scatter(gaze_x_data, gaze_y_data, alpha=0.6, color=&#39;purple&#39;)
            ax4.set_title(&#39;Gaze Direction Scatter&#39;)
            ax4.set_xlabel(&#39;Gaze X&#39;)
            ax4.set_ylabel(&#39;Gaze Y&#39;)
            ax4.grid(True, alpha=0.3)
        else:
            ax4.text(0.5, 0.5, &#39;No gaze direction data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax4.transAxes)
            ax4.set_title(&#39;Gaze Direction Scatter&#39;)

        plt.tight_layout()
        st.pyplot(fig)
        plt.close()

    def _create_movement_visualizations(self, gaze_data, junction_key):
        &#34;&#34;&#34;Create simple movement pattern visualizations.&#34;&#34;&#34;
        import matplotlib.pyplot as plt
        import numpy as np

        if len(gaze_data) == 0:
            return

        # Create figure with subplots
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))

        # 1. Movement direction histogram
        valid_movements = gaze_data[&#39;movement_yaw&#39;].dropna()
        if len(valid_movements) &gt; 0:
            ax1.hist(valid_movements, bins=20, alpha=0.7, edgecolor=&#39;black&#39;)
            ax1.set_title(&#39;Movement Direction Distribution&#39;)
            ax1.set_xlabel(&#39;Movement Yaw (degrees)&#39;)
            ax1.set_ylabel(&#39;Frequency&#39;)
            ax1.grid(True, alpha=0.3)
        else:
            ax1.text(0.5, 0.5, &#39;No movement data available&#39;,
                    ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax1.transAxes)
            ax1.set_title(&#39;Movement Direction Distribution&#39;)

        # 2. Junction visit order distribution (new diagnostic plot)
        if &#39;visit_order&#39; in gaze_data.columns:
            visit_orders = gaze_data[&#39;visit_order&#39;].dropna()
            if len(visit_orders) &gt; 0:
                ax2.hist(visit_orders, bins=range(int(visit_orders.max()) + 2), alpha=0.7, edgecolor=&#39;black&#39;, color=&#39;orange&#39;)
                ax2.set_title(&#39;Junction Visit Order Distribution&#39;)
                ax2.set_xlabel(&#39;Visit Order (0=first junction, 1=second junction, etc.)&#39;)
                ax2.set_ylabel(&#39;Frequency&#39;)
                ax2.grid(True, alpha=0.3)
            else:
                ax2.text(0.5, 0.5, &#39;No visit order data&#39;,
                        ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax2.transAxes)
                ax2.set_title(&#39;Junction Visit Order Distribution&#39;)
        else:
            ax2.text(0.5, 0.5, &#39;No visit order data available&#39;,
                    ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax2.transAxes)
            ax2.set_title(&#39;Junction Visit Order Distribution&#39;)

        # 3. Distance from center distribution
        distances = gaze_data[&#39;distance_from_center&#39;].dropna()
        if len(distances) &gt; 0:
            ax3.hist(distances, bins=20, alpha=0.7, edgecolor=&#39;black&#39;, color=&#39;green&#39;)
            ax3.set_title(&#39;Distance from Junction Center&#39;)
            ax3.set_xlabel(&#39;Distance (units)&#39;)
            ax3.set_ylabel(&#39;Frequency&#39;)
            ax3.grid(True, alpha=0.3)
        else:
            ax3.text(0.5, 0.5, &#39;No distance data available&#39;,
                    ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax3.transAxes)
            ax3.set_title(&#39;Distance from Junction Center&#39;)

        # 4. Decision points scatter plot
        ax4.scatter(gaze_data[&#39;decision_x&#39;], gaze_data[&#39;decision_z&#39;], alpha=0.6, s=20)
        ax4.set_title(&#39;Decision Points Location&#39;)
        ax4.set_xlabel(&#39;X Position&#39;)
        ax4.set_ylabel(&#39;Z Position&#39;)
        ax4.grid(True, alpha=0.3)

        plt.tight_layout()

        # Save the plot
        import os
        junction_num = junction_key.split(&#39;_&#39;)[1]
        junction_dir = os.path.join(&#34;gui_outputs&#34;, f&#34;junction_{junction_num}&#34;)
        os.makedirs(junction_dir, exist_ok=True)

        plot_path = os.path.join(junction_dir, &#34;Movement_Patterns.png&#34;)
        plt.savefig(plot_path, dpi=300, bbox_inches=&#39;tight&#39;)
        plt.close()

        # Display the plot
        st.image(plot_path, caption=f&#34;Movement Pattern Analysis - {junction_key}&#34;, width=&#39;stretch&#39;)

    def generate_cli_command(self, analysis_type: str, results: dict, cluster_method: str = &#34;dbscan&#34;, cluster_params: dict = None, decision_mode: str = &#34;hybrid&#34;, decision_params: dict = None):
        &#34;&#34;&#34;Generate CLI command for easy copying&#34;&#34;&#34;
        st.markdown(&#34;### 📋 Command Line Output&#34;)
        st.markdown(&#34;Copy and paste this command to run the same analysis in the terminal:&#34;)

        if analysis_type == &#34;discover&#34;:
            # Generate discover commands for each junction
            for junction_key, branch_data in results.items():
                # Skip non-junction keys like &#34;chain_decisions&#34;
                if junction_key == &#34;chain_decisions&#34; or not junction_key.startswith(&#34;junction_&#34;):
                    continue

                junction_num = junction_key.split(&#39;_&#39;)[1]
                junction = st.session_state.junctions[int(junction_num)]
                r_outer = st.session_state.junction_r_outer.get(int(junction_num), 50.0)

                st.markdown(f&#34;#### {junction_key.replace(&#39;_&#39;, &#39; &#39;).title()}&#34;)

                # Generate the CLI command with current cluster method and parameters
                cli_command = f&#34;&#34;&#34;route-analyzer discover \\
  --input ./data \\
  --columns x=Headset.Head.Position.X,z=Headset.Head.Position.Z,t=Time \\
  --scale 0.2 \\
  --junction {junction.cx:.1f} {junction.cz:.1f} \\
  --radius {junction.r:.1f} \\
  --r_outer {r_outer:.1f} \\
  --distance 100.0 \\
  --epsilon 0.05 \\
  --k 3 \\
  --decision_mode {decision_mode} \\
  --cluster_method {cluster_method}&#34;&#34;&#34;

                # Add cluster method specific parameters
                if cluster_method == &#34;dbscan&#34; and cluster_params:
                    cli_command += f&#34; \\\n  --min_samples {cluster_params.get(&#39;min_samples&#39;, 5)} \\\n  --angle_eps {cluster_params.get(&#39;angle_eps&#39;, 15.0)}&#34;
                elif cluster_method == &#34;kmeans&#34; and cluster_params:
                    cli_command += f&#34; \\\n  --k_min {cluster_params.get(&#39;k_min&#39;, 2)} \\\n  --k_max {cluster_params.get(&#39;k_max&#39;, 6)}&#34;
                elif cluster_method == &#34;auto&#34; and cluster_params:
                    cli_command += f&#34; \\\n  --k_min {cluster_params.get(&#39;k_min&#39;, 2)} \\\n  --k_max {cluster_params.get(&#39;k_max&#39;, 6)} \\\n  --min_sep_deg {cluster_params.get(&#39;min_sep_deg&#39;, 12.0)} \\\n  --angle_eps {cluster_params.get(&#39;angle_eps&#39;, 15.0)}&#34;

                # Add decision mode specific parameters
                if decision_mode == &#34;radial&#34; and decision_params:
                    cli_command += f&#34; \\\n  --r_outer {decision_params.get(&#39;r_outer&#39;, 50.0)} \\\n  --epsilon {decision_params.get(&#39;epsilon&#39;, 0.05)}&#34;
                elif decision_mode == &#34;pathlen&#34; and decision_params:
                    cli_command += f&#34; \\\n  --distance {decision_params.get(&#39;path_length&#39;, 100.0)} \\\n  --linger_delta {decision_params.get(&#39;linger_delta&#39;, 0.0)}&#34;
                elif decision_mode == &#34;hybrid&#34; and decision_params:
                    cli_command += f&#34; \\\n  --r_outer {decision_params.get(&#39;r_outer&#39;, 50.0)} \\\n  --distance {decision_params.get(&#39;path_length&#39;, 100.0)}&#34;

                cli_command += f&#34; \\\n  --out ./outputs/{junction_key}&#34;

                st.code(cli_command, language=&#34;bash&#34;)

                # Show branch statistics
                if &#34;summary&#34; in branch_data and branch_data[&#34;summary&#34;] is not None:
                    st.markdown(&#34;**Branch Statistics:**&#34;)
                    summary_df = branch_data[&#34;summary&#34;]
                    for _, row in summary_df.iterrows():
                        st.write(f&#34;- Branch {int(row[&#39;branch&#39;])}: {int(row[&#39;count&#39;])} trajectories ({row[&#39;percent&#39;]:.1f}%)&#34;)

        elif analysis_type == &#34;assign&#34;:
            # Generate assign commands for each junction
            for junction_key, assignment_data in results.items():
                junction_num = junction_key.split(&#39;_&#39;)[1]

                # Get junction info from assignment data or session state
                if &#34;junction&#34; in assignment_data:
                    junction = assignment_data[&#34;junction&#34;]
                    # Try to get r_outer from assignment data, then session state, then default
                    r_outer = assignment_data.get(&#34;r_outer&#34;,
                                                st.session_state.junction_r_outer.get(int(junction_num), 50.0))
                else:
                    junction = st.session_state.junctions[int(junction_num)]
                    r_outer = st.session_state.junction_r_outer.get(int(junction_num), 50.0)

                st.markdown(f&#34;#### {junction_key.replace(&#39;_&#39;, &#39; &#39;).title()}&#34;)

                # Get parameters from assignment data
                path_length = assignment_data.get(&#34;path_length&#34;, 100.0)
                epsilon = assignment_data.get(&#34;epsilon&#34;, 0.05)
                assign_scale = assignment_data.get(&#34;assign_scale&#34;, 0.2)  # Get assign-specific scale factor

                # Generate the CLI command (requires centers file from discover)
                cli_command = f&#34;&#34;&#34;route-analyzer assign \\
  --input ./data \\
  --columns x=Headset.Head.Position.X,z=Headset.Head.Position.Z,t=Time \\
  --scale {assign_scale:.1f} \\
  --junction {junction.cx:.1f} {junction.cz:.1f} \\
  --radius {junction.r:.1f} \\
  --r_outer {r_outer:.1f} \\
  --distance {path_length:.1f} \\
  --epsilon {epsilon:.3f} \\
  --decision_mode pathlen \\
  --centers ./outputs/{junction_key}/branch_centers.npy \\
  --out ./outputs/{junction_key}_assign&#34;&#34;&#34;

                st.code(cli_command, language=&#34;bash&#34;)

                # Show assignment statistics
                if &#34;assignments&#34; in assignment_data:
                    assignments_df = assignment_data[&#34;assignments&#34;]
                    st.markdown(&#34;**Assignment Statistics:**&#34;)
                    branch_counts = assignments_df[&#39;branch&#39;].value_counts().sort_index()
                    total = len(assignments_df)
                    for branch, count in branch_counts.items():
                        percentage = (count / total * 100) if total &gt; 0 else 0
                        st.write(f&#34;- Branch {int(branch)}: {int(count)} trajectories ({percentage:.1f}%)&#34;)

        elif analysis_type == &#34;predict&#34;:
            # Generate predict command for all junctions
            junctions_str = &#34; &#34;.join([f&#34;{j.cx:.1f} {j.cz:.1f} {j.r:.1f}&#34; for j in st.session_state.junctions])
            r_outer_str = &#34; &#34;.join([str(st.session_state.junction_r_outer.get(i, 50.0)) for i in range(len(st.session_state.junctions))])

            cli_command = f&#34;&#34;&#34;route-analyzer predict \\
  --input ./data \\
  --columns x=Headset.Head.Position.X,z=Headset.Head.Position.Z,t=Time \\
  --scale 0.2 \\
  --junctions {junctions_str} \\
  --r_outer_list {r_outer_str} \\
  --distance 100.0 \\
  --decision_mode {decision_mode} \\
  --cluster_method {cluster_method}&#34;&#34;&#34;

            # Add cluster method specific parameters
            if cluster_method == &#34;dbscan&#34; and cluster_params:
                cli_command += f&#34; \\\n  --min_samples {cluster_params.get(&#39;min_samples&#39;, 5)} \\\n  --angle_eps {cluster_params.get(&#39;angle_eps&#39;, 15.0)}&#34;
            elif cluster_method == &#34;kmeans&#34; and cluster_params:
                cli_command += f&#34; \\\n  --k_min {cluster_params.get(&#39;k_min&#39;, 2)} \\\n  --k_max {cluster_params.get(&#39;k_max&#39;, 6)}&#34;
            elif cluster_method == &#34;auto&#34; and cluster_params:
                cli_command += f&#34; \\\n  --k_min {cluster_params.get(&#39;k_min&#39;, 2)} \\\n  --k_max {cluster_params.get(&#39;k_max&#39;, 6)} \\\n  --min_sep_deg {cluster_params.get(&#39;min_sep_deg&#39;, 12.0)} \\\n  --angle_eps {cluster_params.get(&#39;angle_eps&#39;, 15.0)}&#34;

            # Add decision mode specific parameters
            if decision_mode == &#34;radial&#34; and decision_params:
                cli_command += f&#34; \\\n  --r_outer_list {decision_params.get(&#39;r_outer&#39;, 50.0)} \\\n  --epsilon {decision_params.get(&#39;epsilon&#39;, 0.05)}&#34;
            elif decision_mode == &#34;pathlen&#34; and decision_params:
                cli_command += f&#34; \\\n  --distance {decision_params.get(&#39;path_length&#39;, 100.0)} \\\n  --linger_delta {decision_params.get(&#39;linger_delta&#39;, 0.0)}&#34;
            elif decision_mode == &#34;hybrid&#34; and decision_params:
                cli_command += f&#34; \\\n  --r_outer_list {decision_params.get(&#39;r_outer&#39;, 50.0)} \\\n  --distance {decision_params.get(&#39;path_length&#39;, 100.0)}&#34;

            cli_command += f&#34; \\\n  --out ./outputs/prediction&#34;

            st.code(cli_command, language=&#34;bash&#34;)

        elif analysis_type == &#34;metrics&#34;:
            # Generate metrics commands for each junction
            for junction_key, metrics_data in results.items():
                if not junction_key.startswith(&#34;junction_&#34;):
                    continue

                junction_num = junction_key.split(&#39;_&#39;)[1]
                junction = metrics_data.get(&#34;junction&#34;)
                if junction is None:
                    continue

                r_outer = metrics_data.get(&#34;r_outer_value&#34;, metrics_data.get(&#34;r_outer&#34;, 50.0))
                decision_mode = metrics_data.get(&#34;decision_mode&#34;, &#34;pathlen&#34;)
                distance = metrics_data.get(&#34;distance&#34;, 100.0)
                trend_window = metrics_data.get(&#34;trend_window&#34;, 5)
                min_outward = metrics_data.get(&#34;min_outward&#34;, 0.0)

                st.markdown(f&#34;#### {junction_key.replace(&#39;_&#39;, &#39; &#39;).title()}&#34;)

                # Generate the CLI command
                cli_command = f&#34;&#34;&#34;route-analyzer metrics \\
  --input ./data \\
  --columns x=Headset.Head.Position.X,z=Headset.Head.Position.Z,t=Time \\
  --scale 0.2 \\
  --junction {junction.cx:.1f} {junction.cz:.1f} \\
  --radius {junction.r:.1f} \\
  --decision_mode {decision_mode} \\
  --distance {distance:.1f}&#34;&#34;&#34;

                if decision_mode in [&#34;radial&#34;, &#34;hybrid&#34;]:
                    cli_command += f&#34; \\\n  --r_outer {r_outer:.1f}&#34;

                if decision_mode == &#34;radial&#34;:
                    cli_command += f&#34; \\\n  --trend_window {trend_window} \\\n  --min_outward {min_outward:.1f}&#34;

                cli_command += f&#34; \\\n  --out ./outputs/{junction_key}_metrics&#34;

                st.code(cli_command, language=&#34;bash&#34;)

        elif analysis_type == &#34;gaze&#34;:
            # Generate gaze command for all junctions
            if len(st.session_state.junctions) == 1:
                # Single junction
                junction = st.session_state.junctions[0]
                r_outer = st.session_state.junction_r_outer.get(0, 50.0)
                gaze_data = list(results.values())[0] if results else {}
                decision_mode = gaze_data.get(&#34;decision_mode&#34;, &#34;hybrid&#34;)
                path_length = gaze_data.get(&#34;path_length&#34;, 100.0)
                epsilon = gaze_data.get(&#34;epsilon&#34;, 0.05)
                linger_delta = gaze_data.get(&#34;linger_delta&#34;, 5.0)

                cli_command = f&#34;&#34;&#34;route-analyzer gaze \\
  --input ./data \\
  --columns x=Headset.Head.Position.X,z=Headset.Head.Position.Z,t=Time \\
  --scale 0.2 \\
  --junction {junction.cx:.1f} {junction.cz:.1f} \\
  --radius {junction.r:.1f} \\
  --r_outer {r_outer:.1f} \\
  --distance {path_length:.1f} \\
  --epsilon {epsilon:.3f} \\
  --decision_mode {decision_mode} \\
  --linger_delta {linger_delta:.1f} \\
  --cluster_method {cluster_method}&#34;&#34;&#34;
            else:
                # Multiple junctions
                junctions_str = &#34; &#34;.join([f&#34;{j.cx:.1f} {j.cz:.1f} {j.r:.1f}&#34; for j in st.session_state.junctions])
                r_outer_list = [st.session_state.junction_r_outer.get(i, 50.0) for i in range(len(st.session_state.junctions))]
                r_outer_str = &#34; &#34;.join([str(r) for r in r_outer_list])
                gaze_data = list(results.values())[0] if results else {}
                decision_mode = gaze_data.get(&#34;decision_mode&#34;, &#34;hybrid&#34;)
                path_length = gaze_data.get(&#34;path_length&#34;, 100.0)
                epsilon = gaze_data.get(&#34;epsilon&#34;, 0.05)
                linger_delta = gaze_data.get(&#34;linger_delta&#34;, 5.0)

                cli_command = f&#34;&#34;&#34;route-analyzer gaze \\
  --input ./data \\
  --columns x=Headset.Head.Position.X,z=Headset.Head.Position.Z,t=Time \\
  --scale 0.2 \\
  --junctions {junctions_str} \\
  --r_outer_list {r_outer_str} \\
  --distance {path_length:.1f} \\
  --epsilon {epsilon:.3f} \\
  --decision_mode {decision_mode} \\
  --linger_delta {linger_delta:.1f} \\
  --cluster_method {cluster_method}&#34;&#34;&#34;

            # Add cluster method specific parameters
            if cluster_method == &#34;dbscan&#34; and cluster_params:
                cli_command += f&#34; \\\n  --min_samples {cluster_params.get(&#39;min_samples&#39;, 5)} \\\n  --angle_eps {cluster_params.get(&#39;angle_eps&#39;, 15.0)}&#34;
            elif cluster_method == &#34;kmeans&#34; and cluster_params:
                cli_command += f&#34; \\\n  --k {cluster_params.get(&#39;k&#39;, 3)}&#34;
            elif cluster_method == &#34;auto&#34; and cluster_params:
                cli_command += f&#34; \\\n  --k_min {cluster_params.get(&#39;k_min&#39;, 2)} \\\n  --k_max {cluster_params.get(&#39;k_max&#39;, 6)} \\\n  --min_sep_deg {cluster_params.get(&#39;min_sep_deg&#39;, 12.0)} \\\n  --angle_eps {cluster_params.get(&#39;angle_eps&#39;, 15.0)}&#34;

            cli_command += f&#34; \\\n  --out ./outputs/gaze_analysis&#34;

            st.code(cli_command, language=&#34;bash&#34;)

        elif analysis_type == &#34;intent&#34;:
            # Generate intent command for all junctions
            if len(st.session_state.junctions) == 1:
                # Single junction
                junction = st.session_state.junctions[0]
                intent_data = list(results.values())[0] if results else {}
                decision_mode = intent_data.get(&#34;decision_mode&#34;, &#34;hybrid&#34;)
                path_length = intent_data.get(&#34;path_length&#34;, 100.0)
                epsilon = intent_data.get(&#34;epsilon&#34;, 0.05)
                linger_delta = intent_data.get(&#34;linger_delta&#34;, 5.0)
                prediction_distances = intent_data.get(&#34;prediction_distances&#34;, [100.0, 75.0, 50.0, 25.0])
                model_type = intent_data.get(&#34;model_type&#34;, &#34;random_forest&#34;)
                cv_folds = intent_data.get(&#34;cv_folds&#34;, 5)
                test_split = intent_data.get(&#34;test_split&#34;, 0.2)

                cli_command = f&#34;&#34;&#34;route-analyzer intent \\
  --input ./data \\
  --columns x=Headset.Head.Position.X,z=Headset.Head.Position.Z,t=Time \\
  --scale 0.2 \\
  --junction {junction.cx:.1f} {junction.cz:.1f} {junction.r:.1f} \\
  --distance {path_length:.1f} \\
  --epsilon {epsilon:.3f} \\
  --decision_mode {decision_mode} \\
  --linger_delta {linger_delta:.1f} \\
  --cluster_method {cluster_method}&#34;&#34;&#34;
            else:
                # Multiple junctions
                junctions_str = &#34; &#34;.join([f&#34;{j.cx:.1f} {j.cz:.1f} {j.r:.1f}&#34; for j in st.session_state.junctions])
                intent_data = list(results.values())[0] if results else {}
                decision_mode = intent_data.get(&#34;decision_mode&#34;, &#34;hybrid&#34;)
                path_length = intent_data.get(&#34;path_length&#34;, 100.0)
                epsilon = intent_data.get(&#34;epsilon&#34;, 0.05)
                linger_delta = intent_data.get(&#34;linger_delta&#34;, 5.0)
                prediction_distances = intent_data.get(&#34;prediction_distances&#34;, [100.0, 75.0, 50.0, 25.0])
                model_type = intent_data.get(&#34;model_type&#34;, &#34;random_forest&#34;)
                cv_folds = intent_data.get(&#34;cv_folds&#34;, 5)
                test_split = intent_data.get(&#34;test_split&#34;, 0.2)

                cli_command = f&#34;&#34;&#34;route-analyzer intent \\
  --input ./data \\
  --columns x=Headset.Head.Position.X,z=Headset.Head.Position.Z,t=Time \\
  --scale 0.2 \\
  --junctions {junctions_str} \\
  --distance {path_length:.1f} \\
  --epsilon {epsilon:.3f} \\
  --decision_mode {decision_mode} \\
  --linger_delta {linger_delta:.1f} \\
  --cluster_method {cluster_method}&#34;&#34;&#34;

            # Add cluster method specific parameters
            if cluster_method == &#34;dbscan&#34; and cluster_params:
                cli_command += f&#34; \\\n  --min_samples {cluster_params.get(&#39;min_samples&#39;, 5)} \\\n  --angle_eps {cluster_params.get(&#39;angle_eps&#39;, 15.0)}&#34;
            elif cluster_method == &#34;kmeans&#34; and cluster_params:
                cli_command += f&#34; \\\n  --k {cluster_params.get(&#39;k&#39;, 3)}&#34;
            elif cluster_method == &#34;auto&#34; and cluster_params:
                cli_command += f&#34; \\\n  --k_min {cluster_params.get(&#39;k_min&#39;, 2)} \\\n  --k_max {cluster_params.get(&#39;k_max&#39;, 6)} \\\n  --min_sep_deg {cluster_params.get(&#39;min_sep_deg&#39;, 12.0)} \\\n  --angle_eps {cluster_params.get(&#39;angle_eps&#39;, 15.0)}&#34;

            # Add intent-specific parameters
            prediction_distances_str = &#34; &#34;.join([str(d) for d in prediction_distances])
            cli_command += f&#34; \\\n  --prediction_distances {prediction_distances_str} \\\n  --model_type {model_type} \\\n  --cv_folds {cv_folds} \\\n  --test_split {test_split}&#34;

            cli_command += f&#34; \\\n  --out ./outputs/intent_recognition&#34;

            st.code(cli_command, language=&#34;bash&#34;)

    def render_conditional_probabilities(self):
        &#34;&#34;&#34;Render conditional probabilities&#34;&#34;&#34;
        st.markdown(&#34;### Conditional Probabilities&#34;)

        if &#34;conditional_probabilities&#34; in st.session_state.analysis_results:
            cond_probs = st.session_state.analysis_results[&#34;conditional_probabilities&#34;]

            # Create a DataFrame for better display
            df_data = []
            for junction_key, probs in cond_probs.items():
                junction_num = junction_key.split(&#39;_&#39;)[1] if &#39;_&#39; in junction_key else junction_key[1:]
                for origin, dest_probs in probs.items():
                    for dest, prob in dest_probs.items():
                        df_data.append({
                            &#39;Junction&#39;: f&#39;J{junction_num}&#39;,
                            &#39;From&#39;: origin,
                            &#39;To&#39;: dest,
                            &#39;Probability&#39;: f&#34;{prob:.1%}&#34;
                        })

            if df_data:
                df = pd.DataFrame(df_data)
                st.dataframe(df, width=&#39;stretch&#39;)
            else:
                st.info(&#34;No conditional probabilities available&#34;)

    def render_pattern_analysis(self):
        &#34;&#34;&#34;Render pattern analysis results&#34;&#34;&#34;
        st.markdown(&#34;### Pattern Analysis&#34;)

        if &#34;choice_patterns&#34; in st.session_state.analysis_results:
            patterns = st.session_state.analysis_results[&#34;choice_patterns&#34;]

            # Display pattern statistics
            st.markdown(&#34;#### Pattern Statistics&#34;)
            for junction_key, pattern_data in patterns.items():
                junction_num = junction_key.split(&#39;_&#39;)[1] if &#39;_&#39; in junction_key else junction_key[1:]
                st.markdown(f&#34;**Junction {junction_num}:**&#34;)

                if &#34;total_trajectories&#34; in pattern_data:
                    st.write(f&#34;- Total trajectories: {pattern_data[&#39;total_trajectories&#39;]}&#34;)

                if &#34;choice_counts&#34; in pattern_data:
                    st.write(f&#34;- Choice counts: {pattern_data[&#39;choice_counts&#39;]}&#34;)

    def render_intent_visualizations(self):
        &#34;&#34;&#34;Render intent recognition analysis visualizations&#34;&#34;&#34;
        st.markdown(&#34;### 🧠 Intent Recognition Results&#34;)

        if (st.session_state.analysis_results is None or
            &#34;intent_recognition&#34; not in st.session_state.analysis_results):
            st.info(&#34;No intent recognition results available.&#34;)
            return

        intent_data = st.session_state.analysis_results[&#34;intent_recognition&#34;]

        # Get successful junctions
        successful_junctions = {k: v for k, v in intent_data.items() if &#39;error&#39; not in v}

        if not successful_junctions:
            st.warning(&#34;⚠️ No successful intent recognition results to visualize&#34;)
            return

        # Junction selector
        junction_keys = list(successful_junctions.keys())
        if len(junction_keys) &gt; 1:
            selected_junction = st.selectbox(
                &#34;Select Junction:&#34;,
                junction_keys,
                format_func=lambda x: f&#34;Junction {x.replace(&#39;junction_&#39;, &#39;&#39;)}&#34;
            )
        else:
            selected_junction = junction_keys[0]

        junction_results = successful_junctions[selected_junction]
        junction_num = selected_junction.replace(&#39;junction_&#39;, &#39;&#39;)

        # Summary metrics
        st.markdown(f&#34;#### Junction {junction_num} Summary&#34;)

        models_trained = junction_results[&#39;training_results&#39;].get(&#39;models_trained&#39;, {})

        if models_trained:
            # Create metrics row
            cols = st.columns(len(models_trained))
            for idx, (dist, model_info) in enumerate(sorted(models_trained.items())):
                with cols[idx]:
                    st.metric(
                        f&#34;{dist} units&#34;,
                        f&#34;{model_info[&#39;cv_mean_accuracy&#39;]:.1%}&#34;,
                        f&#34;n={model_info[&#39;n_samples&#39;]}&#34;
                    )

            # Overall accuracy
            avg_acc = np.mean([m[&#39;cv_mean_accuracy&#39;] for m in models_trained.values()])
            st.markdown(f&#34;**Average Accuracy:** {avg_acc:.1%}&#34;)

            # Interpretation
            if avg_acc &gt; 0.85:
                st.success(&#34;🟢 Excellent Predictability&#34;)
            elif avg_acc &gt; 0.70:
                st.info(&#34;🟡 Good Predictability&#34;)
            else:
                st.warning(&#34;🔴 Moderate Predictability&#34;)

        # Feature Importance Plot
        st.markdown(&#34;#### Feature Importance&#34;)
        feature_importance_path = os.path.join(&#34;gui_outputs&#34;, &#34;intent_recognition&#34;,
                                               f&#34;junction_{junction_num}&#34;,
                                               &#34;intent_feature_importance.png&#34;)
        if os.path.exists(feature_importance_path):
            st.image(feature_importance_path, width=&#39;stretch&#39;)
        else:
            st.info(&#34;Feature importance plot not available&#34;)

        # Accuracy Analysis Plot
        st.markdown(&#34;#### Prediction Accuracy vs Distance&#34;)
        accuracy_path = os.path.join(&#34;gui_outputs&#34;, &#34;intent_recognition&#34;,
                                     f&#34;junction_{junction_num}&#34;,
                                     &#34;intent_accuracy_analysis.png&#34;)
        if os.path.exists(accuracy_path):
            st.image(accuracy_path, width=&#39;stretch&#39;)
            st.caption(&#34;This shows how prediction accuracy improves as users get closer to the junction&#34;)
        else:
            st.info(&#34;Accuracy analysis plot not available&#34;)

        # Test Predictions
        if &#39;test_predictions&#39; in junction_results:
            st.markdown(&#34;#### Sample Predictions&#34;)

            test_preds = junction_results[&#39;test_predictions&#39;]

            # Show a few example predictions
            example_count = min(5, len(test_preds))

            for traj_id in list(test_preds.keys())[:example_count]:
                pred_info = test_preds[traj_id]
                actual = pred_info[&#39;actual_branch&#39;]

                with st.expander(f&#34;Trajectory: {traj_id} (Actual: Branch {actual})&#34;):
                    predictions = pred_info[&#39;predictions_by_distance&#39;]

                    # Create visualization
                    distances = []
                    predicted_branches = []
                    confidences = []
                    correct_flags = []

                    for dist in sorted(predictions.keys(), reverse=True):
                        p = predictions[dist]
                        distances.append(f&#34;{dist}u&#34;)
                        predicted_branches.append(f&#34;Branch {p[&#39;predicted_branch&#39;]}&#34;)
                        confidences.append(p[&#39;confidence&#39;])
                        correct_flags.append(&#34;✓&#34; if p[&#39;correct&#39;] else &#34;✗&#34;)

                    # Create DataFrame
                    pred_df = pd.DataFrame({
                        &#39;Distance Before&#39;: distances,
                        &#39;Predicted&#39;: predicted_branches,
                        &#39;Confidence&#39;: [f&#34;{c:.1%}&#34; for c in confidences],
                        &#39;Correct&#39;: correct_flags
                    })

                    st.dataframe(pred_df, width=&#39;stretch&#39;)

                    # Confidence chart
                    import plotly.graph_objects as go

                    fig = go.Figure()
                    fig.add_trace(go.Scatter(
                        x=[float(d.replace(&#39;u&#39;, &#39;&#39;)) for d in distances],
                        y=confidences,
                        mode=&#39;lines+markers&#39;,
                        name=&#39;Confidence&#39;,
                        line=dict(color=&#39;blue&#39;, width=3),
                        marker=dict(size=10)
                    ))
                    fig.update_layout(
                        title=&#34;Prediction Confidence Over Distance&#34;,
                        xaxis_title=&#34;Distance to Junction (units)&#34;,
                        yaxis_title=&#34;Confidence&#34;,
                        yaxis_range=[0, 1],
                        height=300
                    )
                    st.plotly_chart(fig, width=&#39;stretch&#39;, key=f&#34;intent_confidence_{junction_num}_{traj_id}&#34;)

        # Feature importance table
        if &#39;feature_importance&#39; in junction_results[&#39;training_results&#39;]:
            st.markdown(&#34;#### Feature Importance (Detailed)&#34;)

            with st.expander(&#34;View Feature Importance by Distance&#34;):
                feature_imp = junction_results[&#39;training_results&#39;][&#39;feature_importance&#39;]

                for dist in sorted(feature_imp.keys()):
                    st.markdown(f&#34;**{dist} units before junction:**&#34;)

                    importance_dict = feature_imp[dist]
                    sorted_features = sorted(importance_dict.items(),
                                           key=lambda x: x[1], reverse=True)

                    feat_df = pd.DataFrame(sorted_features[:10],
                                          columns=[&#39;Feature&#39;, &#39;Importance&#39;])
                    feat_df[&#39;Importance&#39;] = feat_df[&#39;Importance&#39;].apply(lambda x: f&#34;{x:.3f}&#34;)

                    st.dataframe(feat_df, width=&#39;stretch&#39;)
                    st.markdown(&#34;---&#34;)

        # Download results
        st.markdown(&#34;#### Download Results&#34;)

        results_path = os.path.join(&#34;gui_outputs&#34;, &#34;intent_recognition&#34;,
                                    f&#34;junction_{junction_num}&#34;,
                                    &#34;intent_training_results.json&#34;)

        if os.path.exists(results_path):
            with open(results_path, &#39;r&#39;) as f:
                results_json = f.read()

            st.download_button(
                label=&#34;📥 Download Training Results (JSON)&#34;,
                data=results_json,
                file_name=f&#34;intent_recognition_junction_{junction_num}.json&#34;,
                mime=&#34;application/json&#34;
            )

        # Explanation
        with st.expander(&#34;ℹ️ Understanding Intent Recognition&#34;):
            st.markdown(&#34;&#34;&#34;
            **Intent Recognition** predicts which route users will choose **before** they reach decision points.

            **Key Insights:**
            - **Higher accuracy at closer distances**: Predictions improve as users approach junctions
            - **Feature importance**: Shows which trajectory features best predict choices
            - **Early prediction**: Enables proactive systems that respond before users act

            **Applications:**
            - 🗺️ Proactive wayfinding and navigation hints
            - 🎨 Adaptive UI that highlights likely options
            - 🚦 Congestion prediction and traffic management
            - ⚠️ Anomaly detection (unexpected behavior)
            - ⚡ Performance optimization (asset preloading)

            **Accuracy Interpretation:**
            - **&gt;85%**: Excellent - Highly predictable behavior
            - **70-85%**: Good - Clear patterns exist
            - **&lt;70%**: Moderate - Variable or exploratory behavior
            &#34;&#34;&#34;)

    def render_enhanced_visualizations(self):
        &#34;&#34;&#34;Render enhanced analysis visualizations&#34;&#34;&#34;
        st.markdown(&#34;### 🚨 Enhanced Analysis Results&#34;)

        # Check if enhanced analysis results exist
        if (st.session_state.analysis_results is None or
            &#34;enhanced&#34; not in st.session_state.analysis_results):
            st.info(&#34;No enhanced analysis results available. Run enhanced analysis first.&#34;)
            return

        enhanced_data = st.session_state.analysis_results[&#34;enhanced&#34;]

        # Create tabs for different analysis components
        tab1, tab2, tab3, tab4 = st.tabs([&#34;🚨 Evacuation Analysis&#34;, &#34;💡 Recommendations&#34;, &#34;⚠️ Risk Assessment&#34;, &#34;📊 Efficiency Metrics&#34;])

        with tab1:
            self._render_evacuation_analysis(enhanced_data[&#34;evacuation_analysis&#34;])

        with tab2:
            self._render_recommendations(enhanced_data[&#34;recommendations&#34;])

        with tab3:
            self._render_risk_assessment(enhanced_data[&#34;risk_assessment&#34;])

        with tab4:
            self._render_efficiency_metrics(enhanced_data[&#34;efficiency_metrics&#34;])

    def _render_evacuation_analysis(self, evacuation_data):
        &#34;&#34;&#34;Render evacuation analysis visualizations&#34;&#34;&#34;
        st.markdown(&#34;#### Evacuation Flow Analysis&#34;)

        # Add explanation
        st.info(&#34;&#34;&#34;
        **🚨 Evacuation Analysis Explanation:**
        - **Bottlenecks**: Junctions where &gt;60% of traffic uses the same route (HIGH risk: &gt;80%)
        - **Optimal Routes**: Junctions with balanced traffic distribution (balance ratio &gt;0.7)
        - **Balance Ratio**: Measures how evenly traffic is distributed across branches (0.0=all traffic in one route, 1.0=perfectly balanced)
        - **Entropy**: Information theory measure of traffic distribution diversity
        &#34;&#34;&#34;)

        # Bottlenecks
        if evacuation_data[&#34;bottlenecks&#34;]:
            st.markdown(&#34;##### 🚧 Identified Bottlenecks&#34;)
            for bottleneck in evacuation_data[&#34;bottlenecks&#34;]:
                risk_color = &#34;🔴&#34; if bottleneck[&#34;risk_level&#34;] == &#34;HIGH&#34; else &#34;🟡&#34;
                st.markdown(f&#34;&#34;&#34;
                {risk_color} **Junction {bottleneck[&#39;junction&#39;]}, Branch {int(bottleneck[&#39;branch&#39;])}**
                - Concentration: {bottleneck[&#39;concentration&#39;]:.1%}
                - Trajectories: {bottleneck[&#39;trajectory_count&#39;]}
                - Risk Level: {bottleneck[&#39;risk_level&#39;]}
                &#34;&#34;&#34;)
        else:
            st.success(&#34;✅ No significant bottlenecks detected&#34;)

        # Optimal routes
        if evacuation_data[&#34;optimal_routes&#34;]:
            st.markdown(&#34;##### ✅ Optimal Routes&#34;)
            st.info(&#34;**Optimal Routes**: Junctions with well-balanced traffic distribution (balance ratio &gt;0.7) - these are good for evacuation as traffic spreads evenly across multiple routes.&#34;)
            for route in evacuation_data[&#34;optimal_routes&#34;]:
                st.markdown(f&#34;&#34;&#34;
                **Junction {route[&#39;junction&#39;]}**
                - Balance Ratio: {route[&#39;balance_ratio&#39;]:.2f} (higher = more balanced)
                - Entropy: {route[&#39;entropy&#39;]:.2f} (higher = more diverse routes)
                - Branch Count: {route[&#39;branch_count&#39;]} (number of available routes)
                &#34;&#34;&#34;)
        else:
            st.info(&#34;ℹ️ No optimal routes identified (all junctions have concentrated traffic)&#34;)

        # Flow analysis chart
        if evacuation_data[&#34;flow_analysis&#34;]:
            st.markdown(&#34;##### 📊 Flow Distribution&#34;)
            import pandas as pd
            import plotly.express as px

            flow_data = []
            for junction_key, data in evacuation_data[&#34;flow_analysis&#34;].items():
                junction_num = junction_key.split(&#39;_&#39;)[1]
                for branch, count in data[&#34;branch_distribution&#34;].items():
                    flow_data.append({
                        &#34;Junction&#34;: f&#34;J{junction_num}&#34;,
                        &#34;Branch&#34;: f&#34;Branch {int(branch)}&#34;,
                        &#34;Trajectory Count&#34;: count,
                        &#34;Percentage&#34;: count / data[&#34;total_trajectories&#34;] * 100
                    })

            if flow_data:
                df = pd.DataFrame(flow_data)
                fig = px.bar(df, x=&#34;Junction&#34;, y=&#34;Trajectory Count&#34;, color=&#34;Branch&#34;,
                           title=&#34;Trajectory Distribution by Junction and Branch&#34;,
                           hover_data=[&#34;Percentage&#34;])
                st.plotly_chart(fig, width=&#39;stretch&#39;)

    def _render_recommendations(self, recommendations):
        &#34;&#34;&#34;Render recommendations&#34;&#34;&#34;
        st.markdown(&#34;#### 💡 Actionable Recommendations&#34;)

        # Add explanation
        st.info(&#34;&#34;&#34;
        **💡 Recommendations Explanation:**
        - **HIGH Priority**: Critical issues requiring immediate attention (bottlenecks &gt;80% concentration)
        - **MEDIUM Priority**: System-wide issues or moderate bottlenecks (60-80% concentration)
        - **LOW Priority**: Maintenance recommendations for well-performing junctions
        - **Signage**: Directional signs to distribute traffic away from bottlenecks
        - **Route Modification**: Physical changes like widening or adding alternative routes
        &#34;&#34;&#34;)

        if not recommendations:
            st.info(&#34;No specific recommendations generated&#34;)
            return

        # Group by priority
        high_priority = [r for r in recommendations if r[&#34;priority&#34;] == &#34;HIGH&#34;]
        medium_priority = [r for r in recommendations if r[&#34;priority&#34;] == &#34;MEDIUM&#34;]
        low_priority = [r for r in recommendations if r[&#34;priority&#34;] == &#34;LOW&#34;]

        if high_priority:
            st.markdown(&#34;##### 🔴 High Priority&#34;)
            for rec in high_priority:
                st.markdown(f&#34;&#34;&#34;
                **{rec[&#39;type&#39;]}** - Junction {rec[&#39;junction&#39;]}
                {rec[&#39;message&#39;]}
                &#34;&#34;&#34;)

        if medium_priority:
            st.markdown(&#34;##### 🟡 Medium Priority&#34;)
            for rec in medium_priority:
                st.markdown(f&#34;&#34;&#34;
                **{rec[&#39;type&#39;]}** - Junction {rec[&#39;junction&#39;]}
                {rec[&#39;message&#39;]}
                &#34;&#34;&#34;)

        if low_priority:
            st.markdown(&#34;##### 🟢 Low Priority&#34;)
            for rec in low_priority:
                st.markdown(f&#34;&#34;&#34;
                **{rec[&#39;type&#39;]}** - Junction {rec[&#39;junction&#39;]}
                {rec[&#39;message&#39;]}
                &#34;&#34;&#34;)

    def _render_risk_assessment(self, risk_data):
        &#34;&#34;&#34;Render risk assessment visualizations&#34;&#34;&#34;
        st.markdown(&#34;#### ⚠️ Risk Assessment&#34;)

        # Add explanation
        st.info(&#34;&#34;&#34;
        **⚠️ Unified Risk Assessment Explanation:**
        - **Overall Risk Score**: 0.0-1.0 scale (0.0=Low Risk, 1.0=High Risk) - normalized across all junctions
        - **Risk Factors**: Each junction assessed on 3 dimensions:
          • **Concentration Risk**: Traffic concentration in single route (&gt;70% = risk)
          • **Diversity Risk**: Number of available routes (&lt;2 routes = high risk, 2 routes = moderate risk)
          • **Crowding Risk**: Traffic volume (&gt;50 trajectories = moderate, &gt;100 = high)
        - **Risk Levels**: HIGH (≥0.7), MEDIUM (≥0.4), LOW (&lt;0.4)
        - **Unified Score**: All risk factors combined and normalized to 0-1 scale
        &#34;&#34;&#34;)

        # Overall risk score
        overall_score = risk_data[&#34;overall_risk_score&#34;]
        risk_level = &#34;HIGH&#34; if overall_score &gt; 0.7 else &#34;MEDIUM&#34; if overall_score &gt; 0.3 else &#34;LOW&#34;
        risk_color = &#34;🔴&#34; if risk_level == &#34;HIGH&#34; else &#34;🟡&#34; if risk_level == &#34;MEDIUM&#34; else &#34;🟢&#34;

        st.markdown(f&#34;&#34;&#34;
        ##### Overall Risk Score: {risk_color} {risk_level}
        **Score: {overall_score:.2f}** (0.0 = Low Risk, 1.0 = High Risk)
        &#34;&#34;&#34;)

        # High risk junctions (now includes all risk levels)
        if risk_data[&#34;high_risk_junctions&#34;]:
            st.markdown(&#34;##### 🚨 Risk Assessment by Junction&#34;)

            # Group by risk level
            high_risk = [j for j in risk_data[&#34;high_risk_junctions&#34;] if j[&#34;risk_level&#34;] == &#34;HIGH&#34;]
            medium_risk = [j for j in risk_data[&#34;high_risk_junctions&#34;] if j[&#34;risk_level&#34;] == &#34;MEDIUM&#34;]

            if high_risk:
                st.markdown(&#34;###### 🔴 HIGH Risk Junctions&#34;)
                for junction in high_risk:
                    st.markdown(f&#34;&#34;&#34;
                    **Junction {junction[&#39;junction&#39;]}**
                    - Risk Score: {junction[&#39;risk_score&#39;]:.2f}
                    - Trajectory Count: {junction[&#39;trajectory_count&#39;]}
                    - Concentration: {junction[&#39;concentration&#39;]:.1%}
                    - Route Count: {junction[&#39;route_count&#39;]}
                    - Risk Factors: {&#39;, &#39;.join([f[0] for f in junction[&#39;risk_factors&#39;]])}
                    &#34;&#34;&#34;)

            if medium_risk:
                st.markdown(&#34;###### 🟡 MEDIUM Risk Junctions&#34;)
                for junction in medium_risk:
                    st.markdown(f&#34;&#34;&#34;
                    **Junction {junction[&#39;junction&#39;]}**
                    - Risk Score: {junction[&#39;risk_score&#39;]:.2f}
                    - Trajectory Count: {junction[&#39;trajectory_count&#39;]}
                    - Concentration: {junction[&#39;concentration&#39;]:.1%}
                    - Route Count: {junction[&#39;route_count&#39;]}
                    - Risk Factors: {&#39;, &#39;.join([f[0] for f in junction[&#39;risk_factors&#39;]])}
                    &#34;&#34;&#34;)
        else:
            st.success(&#34;✅ No significant risks identified&#34;)

        # Risk visualization
        if risk_data[&#34;high_risk_junctions&#34;]:
            import plotly.express as px
            import pandas as pd

            risk_chart_data = []
            for junction in risk_data[&#34;high_risk_junctions&#34;]:
                risk_chart_data.append({
                    &#34;Junction&#34;: f&#34;J{junction[&#39;junction&#39;]}&#34;,
                    &#34;Risk Score&#34;: junction[&#39;risk_score&#39;],
                    &#34;Risk Level&#34;: junction[&#39;risk_level&#39;],
                    &#34;Trajectory Count&#34;: junction[&#39;trajectory_count&#39;],
                    &#34;Concentration&#34;: junction[&#39;concentration&#39;],
                    &#34;Route Count&#34;: junction[&#39;route_count&#39;]
                })

            if risk_chart_data:
                df = pd.DataFrame(risk_chart_data)
                fig = px.bar(df, x=&#34;Junction&#34;, y=&#34;Risk Score&#34;, color=&#34;Risk Level&#34;,
                           title=&#34;Unified Risk Assessment by Junction&#34;,
                           color_discrete_map={&#34;HIGH&#34;: &#34;red&#34;, &#34;MEDIUM&#34;: &#34;orange&#34;, &#34;LOW&#34;: &#34;green&#34;},
                           hover_data=[&#34;Trajectory Count&#34;, &#34;Concentration&#34;, &#34;Route Count&#34;])
                st.plotly_chart(fig, width=&#39;stretch&#39;)

    def _render_efficiency_metrics(self, efficiency_data):
        &#34;&#34;&#34;Render efficiency metrics visualizations&#34;&#34;&#34;
        st.markdown(&#34;#### 📊 Efficiency Metrics&#34;)

        # Add explanation
        st.info(&#34;&#34;&#34;
        **📊 Efficiency Metrics Explanation:**
        - **Route Efficiency**: Entropy-based measure of traffic distribution quality (0.0=all traffic in one route, 1.0=perfectly distributed)
        - **Capacity Utilization**: How well junctions handle their traffic load (trajectories/100, capped at 100%)
        - **Overall Efficiency**: Average route efficiency across all junctions
        - **Higher values = better evacuation performance**
        &#34;&#34;&#34;)

        # Overall efficiency
        overall_efficiency = efficiency_data[&#34;overall_efficiency&#34;]
        efficiency_level = &#34;HIGH&#34; if overall_efficiency &gt; 0.7 else &#34;MEDIUM&#34; if overall_efficiency &gt; 0.4 else &#34;LOW&#34;
        efficiency_color = &#34;🟢&#34; if efficiency_level == &#34;HIGH&#34; else &#34;🟡&#34; if efficiency_level == &#34;MEDIUM&#34; else &#34;🔴&#34;

        st.markdown(f&#34;&#34;&#34;
        ##### Overall Efficiency: {efficiency_color} {efficiency_level}
        **Score: {overall_efficiency:.2f}** (0.0 = Low Efficiency, 1.0 = High Efficiency)
        &#34;&#34;&#34;)

        # Route efficiency by junction
        if efficiency_data[&#34;route_efficiency&#34;]:
            st.markdown(&#34;##### 🛣️ Route Efficiency by Junction&#34;)
            import plotly.express as px
            import pandas as pd

            efficiency_chart_data = []
            for junction_key, efficiency in efficiency_data[&#34;route_efficiency&#34;].items():
                junction_num = junction_key.split(&#39;_&#39;)[1]
                efficiency_chart_data.append({
                    &#34;Junction&#34;: f&#34;J{junction_num}&#34;,
                    &#34;Route Efficiency&#34;: efficiency,
                    &#34;Capacity Utilization&#34;: efficiency_data[&#34;capacity_utilization&#34;].get(junction_key, 0)
                })

            if efficiency_chart_data:
                df = pd.DataFrame(efficiency_chart_data)

                # Route efficiency chart
                fig1 = px.bar(df, x=&#34;Junction&#34;, y=&#34;Route Efficiency&#34;,
                            title=&#34;Route Efficiency by Junction&#34;,
                            color=&#34;Route Efficiency&#34;,
                            color_continuous_scale=&#34;RdYlGn&#34;)
                st.plotly_chart(fig1, width=&#39;stretch&#39;)

                # Capacity utilization chart
                fig2 = px.bar(df, x=&#34;Junction&#34;, y=&#34;Capacity Utilization&#34;,
                            title=&#34;Capacity Utilization by Junction&#34;,
                            color=&#34;Capacity Utilization&#34;,
                            color_continuous_scale=&#34;RdYlGn&#34;)
                st.plotly_chart(fig2, width=&#39;stretch&#39;)

        # Efficiency summary
        st.markdown(&#34;##### 📈 Efficiency Summary&#34;)
        col1, col2 = st.columns(2)

        with col1:
            st.metric(&#34;Overall Route Efficiency&#34;, f&#34;{overall_efficiency:.2f}&#34;)

        with col2:
            avg_capacity = sum(efficiency_data[&#34;capacity_utilization&#34;].values()) / len(efficiency_data[&#34;capacity_utilization&#34;]) if efficiency_data[&#34;capacity_utilization&#34;] else 0
            st.metric(&#34;Average Capacity Utilization&#34;, f&#34;{avg_capacity:.2f}&#34;)

    def render_export(self):
        &#34;&#34;&#34;Render the export interface&#34;&#34;&#34;
        st.markdown(&#39;&lt;h2 class=&#34;section-header&#34;&gt;💾 Export Results&lt;/h2&gt;&#39;, unsafe_allow_html=True)

        if not st.session_state.analysis_results:
            st.warning(&#34;⚠️ Please run an analysis first&#34;)
            return

        st.markdown(&#34;### Export Options&#34;)

        # Export format selection
        export_format = st.selectbox(
            &#34;Export Format:&#34;,
            [&#34;JSON&#34;, &#34;CSV&#34;, &#34;ZIP Archive&#34;],
            help=&#34;Select the format for exporting results&#34;
        )

        if st.button(&#34;📥 Export Results&#34;):
            self.export_results(export_format)

    def export_results(self, format: str):
        &#34;&#34;&#34;Export analysis results&#34;&#34;&#34;
        try:
            if format == &#34;JSON&#34;:
                # Export as JSON
                json_str = json.dumps(st.session_state.analysis_results, indent=2, default=str)
                st.download_button(
                    label=&#34;Download JSON&#34;,
                    data=json_str,
                    file_name=&#34;analysis_results.json&#34;,
                    mime=&#34;application/json&#34;
                )

            elif format == &#34;CSV&#34;:
                # Export as CSV (if applicable)
                if &#34;metrics&#34; in st.session_state.analysis_results:
                    # Export metrics as CSV
                    import pandas as pd
                    df = pd.DataFrame(st.session_state.analysis_results[&#34;metrics&#34;])
                    csv_data = df.to_csv(index=False)
                    st.download_button(
                        label=&#34;Download Metrics CSV&#34;,
                        data=csv_data,
                        file_name=&#34;metrics_results.csv&#34;,
                        mime=&#34;text/csv&#34;
                    )
                else:
                    st.info(&#34;CSV export available for metrics data&#34;)

            elif format == &#34;ZIP Archive&#34;:
                # Create comprehensive ZIP archive with all files from gui_outputs
                with tempfile.TemporaryDirectory() as temp_dir:
                    # Save analysis results JSON
                    results_file = os.path.join(temp_dir, &#34;analysis_results.json&#34;)
                    with open(results_file, &#39;w&#39;) as f:
                        json.dump(st.session_state.analysis_results, f, indent=2, default=str)

                    # Create ZIP
                    zip_path = os.path.join(temp_dir, &#34;analysis_results.zip&#34;)
                    with zipfile.ZipFile(zip_path, &#39;w&#39;) as zipf:
                        # Add analysis results JSON
                        zipf.write(results_file, &#34;analysis_results.json&#34;)

                        # Add all files from gui_outputs directory
                        gui_outputs_dir = &#34;gui_outputs&#34;
                        if os.path.exists(gui_outputs_dir):
                            for root, dirs, files in os.walk(gui_outputs_dir):
                                for file in files:
                                    file_path = os.path.join(root, file)
                                    # Create relative path within ZIP
                                    rel_path = os.path.relpath(file_path, gui_outputs_dir)
                                    zipf.write(file_path, rel_path)

                    # Download ZIP
                    with open(zip_path, &#39;rb&#39;) as f:
                        zip_data = f.read()

                    st.download_button(
                        label=&#34;Download Complete Analysis Package&#34;,
                        data=zip_data,
                        file_name=&#34;complete_analysis_results.zip&#34;,
                        mime=&#34;application/zip&#34;
                    )

                    # Show what&#39;s included in the ZIP
                    st.info(&#34;📦 **Complete Analysis Package includes:**&#34;)
                    st.write(&#34;• Analysis results (JSON)&#34;)
                    st.write(&#34;• All visualizations (PNG files)&#34;)
                    st.write(&#34;• All data tables (CSV files)&#34;)
                    st.write(&#34;• Gaze analysis results&#34;)
                    st.write(&#34;• Physiological analysis data&#34;)
                    st.write(&#34;• Pupil trajectory data&#34;)
                    st.write(&#34;• Consistency reports&#34;)
                    st.write(&#34;• Branch assignments&#34;)
                    st.write(&#34;• Decision points&#34;)
                    st.write(&#34;• Metrics results&#34;)
                    st.write(&#34;• Enhanced analysis results&#34;)
                    st.write(&#34;• Risk assessment data&#34;)
                    st.write(&#34;• Efficiency metrics&#34;)
                    st.write(&#34;• Evacuation analysis&#34;)
                    st.write(&#34;• Recommendations&#34;)
                    st.write(&#34;• Intent recognition models&#34;)
                    st.write(&#34;• Feature importance analysis&#34;)
                    st.write(&#34;• ML prediction results&#34;)

            st.success(&#34;✅ Export ready!&#34;)

        except Exception as e:
            st.error(f&#34;❌ Export failed: {str(e)}&#34;)

    def run_quick_analysis(self):
        &#34;&#34;&#34;Run a quick analysis with default parameters&#34;&#34;&#34;
        if not st.session_state.trajectories or not st.session_state.junctions:
            st.warning(&#34;⚠️ Please load data and define junctions first&#34;)
            return

        st.session_state.current_step = &#34;analysis&#34;
        st.rerun()

    def clear_all_data(self):
        &#34;&#34;&#34;Clear all data and reset the application&#34;&#34;&#34;
        st.session_state.trajectories = []
        st.session_state.junctions = []
        st.session_state.junction_r_outer = {}
        st.session_state.analysis_results = None
        st.session_state.current_step = &#34;data_upload&#34;
        st.success(&#34;✅ All data cleared!&#34;)
        st.rerun()

    def run(self):
        &#34;&#34;&#34;Main GUI run method&#34;&#34;&#34;
        # Add custom CSS for image aspect ratio preservation
        st.markdown(&#34;&#34;&#34;
        &lt;style&gt;
        .stImage &gt; img {
            object-fit: contain !important;
            max-width: 100% !important;
            height: auto !important;
        }
        .stImage &gt; div {
            display: flex !important;
            justify-content: center !important;
        }
        &lt;/style&gt;
        &#34;&#34;&#34;, unsafe_allow_html=True)

        self.render_header()
        self.render_navigation()

        # Render current step
        if st.session_state.current_step == &#34;data_upload&#34;:
            self.render_data_upload()
        elif st.session_state.current_step == &#34;junction_editor&#34;:
            self.render_junction_editor()
        elif st.session_state.current_step == &#34;analysis&#34;:
            self.render_analysis()
        elif st.session_state.current_step == &#34;visualization&#34;:
            self.render_visualization()
        elif st.session_state.current_step == &#34;export&#34;:
            self.render_export()

def main():
    &#34;&#34;&#34;Main entry point&#34;&#34;&#34;
    gui = RouteAnalyzerGUI()
    gui.run()

if __name__ == &#34;__main__&#34;:
    main()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="verta.verta_gui.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Main entry point</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def main():
    &#34;&#34;&#34;Main entry point&#34;&#34;&#34;
    gui = RouteAnalyzerGUI()
    gui.run()</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="verta.verta_gui.RouteAnalyzerGUI"><code class="flex name class">
<span>class <span class="ident">RouteAnalyzerGUI</span></span>
</code></dt>
<dd>
<div class="desc"><p>Main GUI class for VERTA</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RouteAnalyzerGUI:
    &#34;&#34;&#34;Main GUI class for VERTA&#34;&#34;&#34;

    def __init__(self):
        self.initialize_session_state()
        self.logger = get_logger()

    def initialize_session_state(self):
        &#34;&#34;&#34;Initialize Streamlit session state variables&#34;&#34;&#34;
        if &#39;junctions&#39; not in st.session_state:
            st.session_state.junctions = []
        if &#39;junction_r_outer&#39; not in st.session_state:
            st.session_state.junction_r_outer = {}  # Store r_outer for each junction
        if &#39;trajectories&#39; not in st.session_state:
            st.session_state.trajectories = []
        # Unified model: trajectories may include optional gaze/physio fields
        if &#39;gaze_column_mappings&#39; not in st.session_state:
            st.session_state.gaze_column_mappings = {}
        if &#39;analysis_results&#39; not in st.session_state:
            st.session_state.analysis_results = None
        if &#39;current_step&#39; not in st.session_state:
            st.session_state.current_step = &#34;data_upload&#34;
        if &#39;scale_factor&#39; not in st.session_state:
            st.session_state.scale_factor = 0.2  # Default scale factor
        if &#39;data_loaded&#39; not in st.session_state:
            st.session_state.data_loaded = False
        # Flash message shown after reruns (tuple: (level, text))
        if &#39;flash_message&#39; not in st.session_state:
            st.session_state.flash_message = None

        # Track junction state for UI refresh
        if &#39;junction_state_hash&#39; not in st.session_state:
            st.session_state.junction_state_hash = 0

        # Debug: Track session state changes
        if &#39;debug_session_state&#39; not in st.session_state:
            st.session_state.debug_session_state = {
                &#39;trajectories_count&#39;: len(st.session_state.trajectories) if st.session_state.trajectories else 0,
                &#39;gaze_trajectories_count&#39;: 0,
                &#39;last_modified&#39;: &#39;initialize&#39;
            }

    def render_header(self):
        &#34;&#34;&#34;Render the main header&#34;&#34;&#34;
        st.markdown(&#39;&lt;h1 class=&#34;main-header&#34;&gt;🗺️ VERTA&lt;/h1&gt;&#39;, unsafe_allow_html=True)
        st.markdown(&#34;&#34;&#34;
        &lt;div style=&#34;text-align: center; color: #666; margin-bottom: 2rem;&#34;&gt;
            Interactive analysis tool for VR trajectory data and junction-based choice prediction
        &lt;/div&gt;
        &#34;&#34;&#34;, unsafe_allow_html=True)
        # Show any pending flash message at the very top
        self._show_flash()

    def _show_flash(self) -&gt; None:
        &#34;&#34;&#34;Display and clear one-time flash message stored in session state.&#34;&#34;&#34;
        msg = st.session_state.get(&#39;flash_message&#39;)
        if not msg:
            return
        # msg can be a tuple (level, text) or just a string
        if isinstance(msg, tuple) and len(msg) &gt;= 2:
            level, text = msg[0], msg[1]
        else:
            level, text = &#39;success&#39;, str(msg)
        if level == &#39;warning&#39;:
            st.warning(text)
        elif level == &#39;error&#39;:
            st.error(text)
        elif level == &#39;info&#39;:
            st.info(text)
        else:
            st.success(text)
        # Clear after showing once
        st.session_state.flash_message = None

    def render_navigation(self):
        &#34;&#34;&#34;Render the navigation sidebar&#34;&#34;&#34;
        st.sidebar.title(&#34;Navigation&#34;)

        steps = {
            &#34;data_upload&#34;: &#34;📁 Data Upload&#34;,
            &#34;junction_editor&#34;: &#34;🎯 Junction Editor&#34;,
            &#34;analysis&#34;: &#34;📊 Analysis&#34;,
            &#34;visualization&#34;: &#34;📈 Visualization&#34;,
            &#34;export&#34;: &#34;💾 Export Results&#34;
        }

        for step_key, step_name in steps.items():
            if st.sidebar.button(step_name, key=f&#34;nav_{step_key}&#34;):
                st.session_state.current_step = step_key
                st.rerun()

        st.sidebar.markdown(&#34;---&#34;)
        st.sidebar.markdown(&#34;### Current Status&#34;)

        # Status indicators
        data_status = &#34;✅&#34; if (st.session_state.trajectories and getattr(st.session_state, &#39;data_loaded&#39;, False)) else &#34;❌&#34;
        junction_status = &#34;✅&#34; if st.session_state.junctions else &#34;❌&#34;

        st.sidebar.markdown(f&#34;{data_status} Data Loaded&#34;)
        st.sidebar.markdown(f&#34;{junction_status} Junctions Defined&#34;)

        if st.session_state.trajectories and st.session_state.junctions:
            st.sidebar.markdown(&#34;✅ Ready for Analysis&#34;)
        else:
            st.sidebar.markdown(&#34;⚠️ Complete setup steps&#34;)

    def render_data_upload(self):
        &#34;&#34;&#34;Render the data upload interface&#34;&#34;&#34;
        st.markdown(&#39;&lt;h2 class=&#34;section-header&#34;&gt;📁 Data Upload&lt;/h2&gt;&#39;, unsafe_allow_html=True)

        col1, col2 = st.columns([2, 1])

        with col1:
            st.markdown(&#34;### Upload Trajectory Data&#34;)

            # File upload
            uploaded_files = st.file_uploader(
                &#34;Choose CSV files&#34;,
                type=[&#39;csv&#39;],
                accept_multiple_files=True,
                help=&#34;Upload one or more CSV files containing trajectory data&#34;
            )

            # Folder path input
            st.markdown(&#34;### Or specify folder path&#34;)
            folder_path = st.text_input(
                &#34;Folder path:&#34;,
                value=&#34;&#34;,
                help=&#34;Path to folder containing CSV files&#34;
            )

            # Column mapping
            st.markdown(&#34;### Column Mapping&#34;)
            col_x, col_z, col_t = st.columns(3)

            with col_x:
                x_col = st.text_input(&#34;X Column:&#34;, value=&#34;Headset.Head.Position.X&#34;)
            with col_z:
                z_col = st.text_input(&#34;Z Column:&#34;, value=&#34;Headset.Head.Position.Z&#34;)
            with col_t:
                t_col = st.text_input(&#34;Time Column:&#34;, value=&#34;Time&#34;)

            # New: Gaze/Physiology column mapping now lives here
            with st.expander(&#34;🔧 Gaze/Physiology Column Mapping&#34;, expanded=False):
                col_g1, col_g2 = st.columns(2)
                with col_g1:
                    head_forward_x_col = st.text_input(&#34;Head Forward X&#34;, value=st.session_state.gaze_column_mappings.get(&#39;head_forward_x&#39;, &#39;Headset.Head.Forward.X&#39;))
                    head_forward_z_col = st.text_input(&#34;Head Forward Z&#34;, value=st.session_state.gaze_column_mappings.get(&#39;head_forward_z&#39;, &#39;Headset.Head.Forward.Z&#39;))
                    gaze_x_map = st.text_input(&#34;Gaze X&#34;, value=st.session_state.gaze_column_mappings.get(&#39;gaze_x&#39;, &#39;Headset.Gaze.X&#39;))
                    gaze_y_map = st.text_input(&#34;Gaze Y&#34;, value=st.session_state.gaze_column_mappings.get(&#39;gaze_y&#39;, &#39;Headset.Gaze.Y&#39;))
                with col_g2:
                    pupil_l_map = st.text_input(&#34;Pupil Left&#34;, value=st.session_state.gaze_column_mappings.get(&#39;pupil_l&#39;, &#39;Headset.PupilDilation.L&#39;))
                    pupil_r_map = st.text_input(&#34;Pupil Right&#34;, value=st.session_state.gaze_column_mappings.get(&#39;pupil_r&#39;, &#39;Headset.PupilDilation.R&#39;))
                    heart_rate_map = st.text_input(&#34;Heart Rate&#34;, value=st.session_state.gaze_column_mappings.get(&#39;heart_rate&#39;, &#39;Headset.HeartRate&#39;))

                st.session_state.gaze_column_mappings = {
                    &#39;head_forward_x&#39;: head_forward_x_col.strip(),
                    &#39;head_forward_z&#39;: head_forward_z_col.strip(),
                    &#39;gaze_x&#39;: gaze_x_map.strip(),
                    &#39;gaze_y&#39;: gaze_y_map.strip(),
                    &#39;pupil_l&#39;: pupil_l_map.strip(),
                    &#39;pupil_r&#39;: pupil_r_map.strip(),
                    &#39;heart_rate&#39;: heart_rate_map.strip(),
                }

            # Analysis parameters
            st.markdown(&#34;### Analysis Parameters&#34;)
            col_scale, col_threshold = st.columns(2)

            with col_scale:
                scale = st.number_input(&#34;Scale Factor:&#34;, value=st.session_state.get(&#34;scale_factor&#34;, 0.2), min_value=0.01, max_value=1.0, step=0.01)
                st.session_state.scale_factor = scale  # Store scale factor in session state
            with col_threshold:
                motion_threshold = st.number_input(&#34;Motion Threshold:&#34;, value=0.1, min_value=0.01, max_value=1.0, step=0.01)

        with col2:
            st.markdown(&#34;### Quick Actions&#34;)

            if st.button(&#34;🔄 Load Data&#34;, type=&#34;primary&#34;):
                if uploaded_files and folder_path.strip():
                    # Both provided - show warning and ask user to choose
                    st.warning(&#34;⚠️ **Both file uploads and folder path are specified.**&#34;)
                    st.info(&#34;**Current behavior:** File uploads will be processed (folder path will be ignored).&#34;)
                    st.info(&#34;**To use folder path instead:** Clear the file uploads and click &#39;Load Data&#39; again.&#34;)

                    # Process uploaded files (current behavior)
                    self.load_uploaded_files(uploaded_files, x_col, z_col, t_col, scale, motion_threshold)
                elif uploaded_files:
                    # Process uploaded files
                    self.load_uploaded_files(uploaded_files, x_col, z_col, t_col, scale, motion_threshold)
                elif folder_path.strip():
                    # Process folder path
                    self.load_trajectory_data(folder_path, x_col, z_col, t_col, scale, motion_threshold)
                else:
                    st.warning(&#34;⚠️ Please upload files or specify a folder path&#34;)

            if st.button(&#34;📋 Load Sample Data&#34;):
                self.load_sample_data()

            if st.session_state.trajectories:
                st.markdown(&#34;### Data Summary&#34;)
                st.write(f&#34;**Trajectories loaded:** {len(st.session_state.trajectories)}&#34;)

                if len(st.session_state.trajectories) &gt; 0:
                    sample_traj = st.session_state.trajectories[0]
                    st.write(f&#34;**Sample trajectory points:** {len(sample_traj.x)}&#34;)
                    st.write(f&#34;**X range:** {min(sample_traj.x):.1f} to {max(sample_traj.x):.1f}&#34;)
                    st.write(f&#34;**Z range:** {min(sample_traj.z):.1f} to {max(sample_traj.z):.1f}&#34;)

    def load_trajectory_data(self, folder_path: str, x_col: str, z_col: str, t_col: str, scale: float, motion_threshold: float):
        &#34;&#34;&#34;Load trajectory data from folder using unified model&#34;&#34;&#34;
        try:
            # Create progress bar
            progress_bar = st.progress(0)
            status_text = st.empty()

            status_text.text(&#34;🔄 Initializing data loading...&#34;)
            progress_bar.progress(10)

            # Build comprehensive column mapping for VR headset data
            column_mapping = {
                &#39;x&#39;: x_col,
                &#39;z&#39;: z_col,
                &#39;t&#39;: t_col,
                # VR headset gaze/physio columns
                &#39;head_forward_x&#39;: st.session_state.gaze_column_mappings.get(&#39;head_forward_x&#39;, &#39;Headset.Head.Forward.X&#39;),
                &#39;head_forward_y&#39;: st.session_state.gaze_column_mappings.get(&#39;head_forward_y&#39;, &#39;Headset.Head.Forward.Y&#39;),
                &#39;head_forward_z&#39;: st.session_state.gaze_column_mappings.get(&#39;head_forward_z&#39;, &#39;Headset.Head.Forward.Z&#39;),
                &#39;head_up_x&#39;: st.session_state.gaze_column_mappings.get(&#39;head_up_x&#39;, &#39;Headset.Head.Up.X&#39;),
                &#39;head_up_y&#39;: st.session_state.gaze_column_mappings.get(&#39;head_up_y&#39;, &#39;Headset.Head.Up.Y&#39;),
                &#39;head_up_z&#39;: st.session_state.gaze_column_mappings.get(&#39;head_up_z&#39;, &#39;Headset.Head.Up.Z&#39;),
                &#39;gaze_x&#39;: st.session_state.gaze_column_mappings.get(&#39;gaze_x&#39;, &#39;Headset.Gaze.X&#39;),
                &#39;gaze_y&#39;: st.session_state.gaze_column_mappings.get(&#39;gaze_y&#39;, &#39;Headset.Gaze.Y&#39;),
                &#39;pupil_l&#39;: st.session_state.gaze_column_mappings.get(&#39;pupil_l&#39;, &#39;Headset.PupilDilation.L&#39;),
                &#39;pupil_r&#39;: st.session_state.gaze_column_mappings.get(&#39;pupil_r&#39;, &#39;Headset.PupilDilation.R&#39;),
                &#39;heart_rate&#39;: st.session_state.gaze_column_mappings.get(&#39;heart_rate&#39;, &#39;Headset.HeartRate&#39;),
            }

            status_text.text(&#34;🔍 Scanning folder for CSV files...&#34;)
            progress_bar.progress(30)

            # Add a small delay to ensure progress bar is visible
            import time
            time.sleep(0.1)

            # Check what columns are available in the first CSV file and auto-detect gaze columns
            import glob
            import pandas as pd
            csv_files = glob.glob(os.path.join(folder_path, &#34;*.csv&#34;))
            if csv_files:
                sample_df = pd.read_csv(csv_files[0])

                # Auto-detect gaze columns if mappings are empty
                if not st.session_state.gaze_column_mappings:
                    st.info(&#34;🔍 **Auto-detecting gaze columns from CSV file...**&#34;)

                    # Try to find gaze columns by common patterns
                    detected_mappings = {}

                    # Look for pupil dilation columns
                    pupil_cols = [col for col in sample_df.columns if &#39;pupil&#39; in col.lower() and (&#39;l&#39; in col.lower() or &#39;left&#39; in col.lower())]
                    if pupil_cols:
                        detected_mappings[&#39;pupil_l&#39;] = pupil_cols[0]

                    pupil_cols = [col for col in sample_df.columns if &#39;pupil&#39; in col.lower() and (&#39;r&#39; in col.lower() or &#39;right&#39; in col.lower())]
                    if pupil_cols:
                        detected_mappings[&#39;pupil_r&#39;] = pupil_cols[0]

                    # Look for heart rate columns
                    hr_cols = [col for col in sample_df.columns if &#39;heart&#39; in col.lower() or &#39;hr&#39; in col.lower()]
                    if hr_cols:
                        detected_mappings[&#39;heart_rate&#39;] = hr_cols[0]

                    # Look for gaze columns
                    gaze_x_cols = [col for col in sample_df.columns if &#39;gaze&#39; in col.lower() and (&#39;x&#39; in col.lower() or &#39;horizontal&#39; in col.lower())]
                    if gaze_x_cols:
                        detected_mappings[&#39;gaze_x&#39;] = gaze_x_cols[0]

                    gaze_y_cols = [col for col in sample_df.columns if &#39;gaze&#39; in col.lower() and (&#39;y&#39; in col.lower() or &#39;vertical&#39; in col.lower())]
                    if gaze_y_cols:
                        detected_mappings[&#39;gaze_y&#39;] = gaze_y_cols[0]

                    # Look for head forward columns
                    head_fwd_x_cols = [col for col in sample_df.columns if &#39;head&#39; in col.lower() and &#39;forward&#39; in col.lower() and &#39;x&#39; in col.lower()]
                    if head_fwd_x_cols:
                        detected_mappings[&#39;head_forward_x&#39;] = head_fwd_x_cols[0]

                    head_fwd_z_cols = [col for col in sample_df.columns if &#39;head&#39; in col.lower() and &#39;forward&#39; in col.lower() and (&#39;z&#39; in col.lower() or &#39;depth&#39; in col.lower())]
                    if head_fwd_z_cols:
                        detected_mappings[&#39;head_forward_z&#39;] = head_fwd_z_cols[0]

                    # Update session state with detected mappings
                    if detected_mappings:
                        st.session_state.gaze_column_mappings.update(detected_mappings)
                        st.success(f&#34;✅ **Auto-detected gaze columns:** {detected_mappings}&#34;)

                        # Update column mapping with detected values
                        column_mapping.update(detected_mappings)
                    else:
                        st.warning(&#34;⚠️ **No gaze columns auto-detected**&#34;)

                # Check if gaze columns exist (using current mappings)
                gaze_columns = [&#39;Headset.Head.Forward.X&#39;, &#39;Headset.Head.Forward.Z&#39;, &#39;Headset.Gaze.X&#39;,
                              &#39;Headset.Gaze.Y&#39;, &#39;Headset.PupilDilation.L&#39;, &#39;Headset.PupilDilation.R&#39;, &#39;Headset.HeartRate&#39;]

                # Use detected mappings if available
                actual_gaze_columns = []
                for gaze_type in [&#39;head_forward_x&#39;, &#39;head_forward_z&#39;, &#39;gaze_x&#39;, &#39;gaze_y&#39;, &#39;pupil_l&#39;, &#39;pupil_r&#39;, &#39;heart_rate&#39;]:
                    col_name = st.session_state.gaze_column_mappings.get(gaze_type, gaze_columns[[&#39;head_forward_x&#39;, &#39;head_forward_z&#39;, &#39;gaze_x&#39;, &#39;gaze_y&#39;, &#39;pupil_l&#39;, &#39;pupil_r&#39;, &#39;heart_rate&#39;].index(gaze_type)])
                    actual_gaze_columns.append(col_name)

                missing_gaze_cols = [col for col in actual_gaze_columns if col not in sample_df.columns]
                if missing_gaze_cols:
                    st.warning(f&#34;⚠️ **Missing gaze columns:** {missing_gaze_cols}&#34;)
                else:
                    st.success(&#34;✅ **All gaze columns found in CSV!**&#34;)

            status_text.text(&#34;📊 Loading trajectory data...&#34;)
            progress_bar.progress(60)

            # Add a small delay to ensure progress bar is visible
            import time
            time.sleep(0.1)

            # Create progress callback function
            def update_progress(current, total, message):
                progress_percent = int(60 + (current / total) * 30)  # 60-90% range
                progress_bar.progress(progress_percent)
                status_text.text(message)

            # Use unified loader - always returns Trajectory objects with optional gaze fields
            trajectories = load_folder(
                folder=folder_path,
                pattern=&#34;*.csv&#34;,
                columns=column_mapping,
                require_time=False,
                scale=scale,
                motion_threshold=motion_threshold,
                progress_callback=update_progress
            )

            status_text.text(&#34;💾 Storing trajectories in session...&#34;)
            progress_bar.progress(90)

            # Add another small delay to show progress
            time.sleep(0.1)

            # Store unified trajectories
            st.session_state.trajectories = trajectories

            # Update status display
            st.session_state.data_loaded = True

            progress_bar.progress(100)
            status_text.text(&#34;✅ Data loading completed!&#34;)

            # Clear progress elements first
            progress_bar.empty()
            status_text.empty()

            # Queue success flash for top-of-page after rerun
            st.session_state.flash_message = (&#39;success&#39;, f&#34;🎉 Successfully loaded {len(trajectories)} trajectories!&#34;)

            # Force rerun to update status display and show flash at top
            st.rerun()

        except Exception as e:
            st.error(f&#34;❌ Error loading data: {str(e)}&#34;)
            if &#39;progress_bar&#39; in locals():
                progress_bar.empty()
            if &#39;status_text&#39; in locals():
                status_text.empty()

    def load_uploaded_files(self, uploaded_files, x_col: str, z_col: str, t_col: str, scale: float, motion_threshold: float):
        &#34;&#34;&#34;Load trajectory data from uploaded files using unified model&#34;&#34;&#34;
        try:
            with st.spinner(&#34;Loading uploaded files...&#34;):
                import pandas as pd
                import io
                import numpy as np
                import tempfile
                import os
                from verta.verta_data_loader import Trajectory, TrajectoryLoader, ColumnMapping

                # Create progress bar
                progress_bar = st.progress(0)
                status_text = st.empty()

                status_text.text(&#34;🔄 Initializing file processing...&#34;)
                progress_bar.progress(5)

                # Build comprehensive column mapping for VR headset data (same as folder loading)
                column_mapping = {
                    &#39;x&#39;: x_col,
                    &#39;z&#39;: z_col,
                    &#39;t&#39;: t_col,
                    # VR headset gaze/physio columns
                    &#39;head_forward_x&#39;: st.session_state.gaze_column_mappings.get(&#39;head_forward_x&#39;, &#39;Headset.Head.Forward.X&#39;),
                    &#39;head_forward_y&#39;: st.session_state.gaze_column_mappings.get(&#39;head_forward_y&#39;, &#39;Headset.Head.Forward.Y&#39;),
                    &#39;head_forward_z&#39;: st.session_state.gaze_column_mappings.get(&#39;head_forward_z&#39;, &#39;Headset.Head.Forward.Z&#39;),
                    &#39;head_up_x&#39;: st.session_state.gaze_column_mappings.get(&#39;head_up_x&#39;, &#39;Headset.Head.Up.X&#39;),
                    &#39;head_up_y&#39;: st.session_state.gaze_column_mappings.get(&#39;head_up_y&#39;, &#39;Headset.Head.Up.Y&#39;),
                    &#39;head_up_z&#39;: st.session_state.gaze_column_mappings.get(&#39;head_up_z&#39;, &#39;Headset.Head.Up.Z&#39;),
                    &#39;gaze_x&#39;: st.session_state.gaze_column_mappings.get(&#39;gaze_x&#39;, &#39;Headset.Gaze.X&#39;),
                    &#39;gaze_y&#39;: st.session_state.gaze_column_mappings.get(&#39;gaze_y&#39;, &#39;Headset.Gaze.Y&#39;),
                    &#39;pupil_l&#39;: st.session_state.gaze_column_mappings.get(&#39;pupil_l&#39;, &#39;Headset.PupilDilation.L&#39;),
                    &#39;pupil_r&#39;: st.session_state.gaze_column_mappings.get(&#39;pupil_r&#39;, &#39;Headset.PupilDilation.R&#39;),
                    &#39;heart_rate&#39;: st.session_state.gaze_column_mappings.get(&#39;heart_rate&#39;, &#39;Headset.HeartRate&#39;),
                }

                # Auto-detect gaze columns from first uploaded file (same as folder loading)
                if uploaded_files and not st.session_state.gaze_column_mappings:
                    st.info(&#34;🔍 **Auto-detecting gaze columns from uploaded file...**&#34;)

                    # Read first file to detect columns
                    first_file = uploaded_files[0]
                    df_sample = pd.read_csv(io.StringIO(first_file.read().decode(&#39;utf-8&#39;)))
                    first_file.seek(0)  # Reset file pointer

                    # Try to find gaze columns by common patterns (same logic as folder loading)
                    detected_mappings = {}

                    # Look for pupil dilation columns
                    pupil_cols = [col for col in df_sample.columns if &#39;pupil&#39; in col.lower() and (&#39;l&#39; in col.lower() or &#39;left&#39; in col.lower())]
                    if pupil_cols:
                        detected_mappings[&#39;pupil_l&#39;] = pupil_cols[0]

                    pupil_cols = [col for col in df_sample.columns if &#39;pupil&#39; in col.lower() and (&#39;r&#39; in col.lower() or &#39;right&#39; in col.lower())]
                    if pupil_cols:
                        detected_mappings[&#39;pupil_r&#39;] = pupil_cols[0]

                    # Look for heart rate columns
                    hr_cols = [col for col in df_sample.columns if &#39;heart&#39; in col.lower() or &#39;hr&#39; in col.lower()]
                    if hr_cols:
                        detected_mappings[&#39;heart_rate&#39;] = hr_cols[0]

                    # Look for gaze columns
                    gaze_x_cols = [col for col in df_sample.columns if &#39;gaze&#39; in col.lower() and (&#39;x&#39; in col.lower() or &#39;horizontal&#39; in col.lower())]
                    if gaze_x_cols:
                        detected_mappings[&#39;gaze_x&#39;] = gaze_x_cols[0]

                    gaze_y_cols = [col for col in df_sample.columns if &#39;gaze&#39; in col.lower() and (&#39;y&#39; in col.lower() or &#39;vertical&#39; in col.lower())]
                    if gaze_y_cols:
                        detected_mappings[&#39;gaze_y&#39;] = gaze_y_cols[0]

                    # Look for head forward columns
                    head_fwd_x_cols = [col for col in df_sample.columns if &#39;head&#39; in col.lower() and &#39;forward&#39; in col.lower() and &#39;x&#39; in col.lower()]
                    if head_fwd_x_cols:
                        detected_mappings[&#39;head_forward_x&#39;] = head_fwd_x_cols[0]

                    head_fwd_z_cols = [col for col in df_sample.columns if &#39;head&#39; in col.lower() and &#39;forward&#39; in col.lower() and (&#39;z&#39; in col.lower() or &#39;depth&#39; in col.lower())]
                    if head_fwd_z_cols:
                        detected_mappings[&#39;head_forward_z&#39;] = head_fwd_z_cols[0]

                    # Update session state with detected mappings
                    if detected_mappings:
                        st.session_state.gaze_column_mappings.update(detected_mappings)
                        st.success(f&#34;✅ **Auto-detected gaze columns:** {detected_mappings}&#34;)

                        # Update column mapping with detected values
                        column_mapping.update(detected_mappings)
                    else:
                        st.warning(&#34;⚠️ **No gaze columns auto-detected**&#34;)

                # Create temporary directory to store uploaded files
                with tempfile.TemporaryDirectory() as temp_dir:
                    status_text.text(&#34;📁 Saving uploaded files to temporary directory...&#34;)
                    progress_bar.progress(10)

                    # Save uploaded files to temporary directory
                    temp_files = []
                    for i, uploaded_file in enumerate(uploaded_files):
                        temp_file_path = os.path.join(temp_dir, uploaded_file.name)
                        with open(temp_file_path, &#39;wb&#39;) as f:
                            f.write(uploaded_file.getvalue())
                        temp_files.append(temp_file_path)

                    status_text.text(&#34;📊 Loading trajectories using unified loader...&#34;)
                    progress_bar.progress(20)

                    # Use the same unified loader as folder loading
                    loader = TrajectoryLoader(ColumnMapping.from_dict(column_mapping))

                    # Create progress callback function
                    def update_progress(current, total, message):
                        progress_percent = int(20 + (current / total) * 70)  # 20-90% range
                        progress_bar.progress(progress_percent)
                        status_text.text(message)

                    # Load trajectories using unified loader
                    trajectories = loader.load_folder(
                        folder=temp_dir,
                        pattern=&#34;*.csv&#34;,
                        trajectory_class=Trajectory,
                        require_time=False,
                        scale=scale,
                        motion_threshold=motion_threshold,
                        progress_callback=update_progress
                    )

                if trajectories:
                    status_text.text(&#34;💾 Storing trajectories in session...&#34;)
                    progress_bar.progress(90)

                    st.session_state.trajectories = trajectories

                    # Update status display
                    st.session_state.data_loaded = True

                    progress_bar.progress(100)
                    status_text.text(&#34;✅ File processing completed!&#34;)

                    # Clear progress elements first
                    progress_bar.empty()
                    status_text.empty()

                    # Queue success flash for top-of-page after rerun
                    st.session_state.flash_message = (
                        &#39;success&#39;,
                        f&#34;🎉 Successfully loaded {len(trajectories)} trajectories from {len(uploaded_files)} files!&#34;
                    )

                    # Force rerun to update status display and show flash at top
                    st.rerun()
                else:
                    st.error(&#34;❌ No valid trajectories could be loaded from uploaded files&#34;)

        except Exception as e:
            st.error(f&#34;❌ Error loading uploaded files: {str(e)}&#34;)

    def load_sample_data(self):
        &#34;&#34;&#34;Load sample data for demonstration&#34;&#34;&#34;
        try:
            with st.spinner(&#34;Loading sample data...&#34;):
                # Create sample trajectories
                np.random.seed(42)
                trajectories = []

                for i in range(10):
                    # Create a sample trajectory
                    n_points = np.random.randint(1000, 5000)
                    t = np.linspace(0, 100, n_points)
                    x = np.cumsum(np.random.normal(0, 0.5, n_points)) + 500
                    z = np.cumsum(np.random.normal(0, 0.5, n_points)) + 200

                    trajectory = Trajectory(
                        tid=str(i),
                        x=x,
                        z=z,
                        t=t
                    )
                    trajectories.append(trajectory)

                st.session_state.trajectories = trajectories
                st.success(f&#34;✅ Loaded {len(trajectories)} sample trajectories!&#34;)

        except Exception as e:
            st.error(f&#34;❌ Error loading sample data: {str(e)}&#34;)

    def load_assign_trajectories(self, assign_params: dict = None):
        &#34;&#34;&#34;Load trajectories for assign function based on trajectory option&#34;&#34;&#34;
        try:
            # Get scale factor from assign parameters
            assign_scale = assign_params.get(&#34;assign_scale&#34;, 0.2) if assign_params else 0.2
            trajectory_option = assign_params.get(&#34;trajectory_option&#34;, &#34;Upload files&#34;) if assign_params else &#34;Upload files&#34;

            if trajectory_option == &#34;Upload files&#34;:
                trajectory_files = assign_params.get(&#34;trajectory_files&#34;) if assign_params else None
                if not trajectory_files:
                    st.error(&#34;❌ No trajectory files uploaded&#34;)
                    return None

                # Process uploaded files
                trajectories = []
                for uploaded_file in trajectory_files:
                    # Save uploaded file to temporary location
                    with tempfile.NamedTemporaryFile(delete=False, suffix=&#39;.csv&#39;) as tmp_file:
                        tmp_file.write(uploaded_file.getvalue())
                        tmp_path = tmp_file.name

                    # Load trajectory data
                    df = pd.read_csv(tmp_path)

                    # Use default column names if not specified
                    x_col = &#34;Headset.Head.Position.X&#34; if &#34;Headset.Head.Position.X&#34; in df.columns else df.columns[0]
                    z_col = &#34;Headset.Head.Position.Z&#34; if &#34;Headset.Head.Position.Z&#34; in df.columns else df.columns[1]
                    t_col = &#34;Time&#34; if &#34;Time&#34; in df.columns else df.columns[2] if len(df.columns) &gt; 2 else None

                    # Filter out NaN values in coordinate columns
                    coord_mask = df[[x_col, z_col]].notnull().all(axis=1)
                    df_clean = df[coord_mask].copy()

                    if len(df_clean) == 0:
                        st.warning(f&#34;⚠️ Skipping {uploaded_file.name}: All coordinates are NaN&#34;)
                        continue

                    if len(df_clean) &lt; len(df):
                        st.info(f&#34;ℹ️ Cleaned {uploaded_file.name}: Removed {len(df) - len(df_clean)} rows with NaN coordinates&#34;)

                    # Create trajectory with scale factor applied
                    trajectory = Trajectory(
                        tid=uploaded_file.name,
                        x=df_clean[x_col].values * assign_scale,  # Apply scale factor
                        z=df_clean[z_col].values * assign_scale,  # Apply scale factor
                        t=df_clean[t_col].values if t_col else np.arange(len(df_clean))
                    )
                    trajectories.append(trajectory)

                    # Clean up temporary file
                    os.unlink(tmp_path)

                return trajectories

            else:  # Select folder
                trajectory_folder = assign_params.get(&#34;trajectory_folder&#34;) if assign_params else None
                if not trajectory_folder or not trajectory_folder.strip():
                    st.error(&#34;❌ No trajectory folder path specified&#34;)
                    return None

                # Load from folder
                column_mapping = {
                    &#34;x&#34;: &#34;Headset.Head.Position.X&#34;,
                    &#34;z&#34;: &#34;Headset.Head.Position.Z&#34;,
                    &#34;t&#34;: &#34;Time&#34;
                }

                trajectories = load_folder(
                    folder=trajectory_folder,
                    pattern=&#34;*.csv&#34;,
                    columns=column_mapping,
                    scale=assign_scale,  # Use assign-specific scale factor
                    motion_threshold=0.1
                )

                if not trajectories:
                    st.error(f&#34;❌ No trajectories found in folder: {trajectory_folder}&#34;)
                    return None

                # Show data cleaning summary
                total_trajectories = len(trajectories)
                st.info(f&#34;ℹ️ Loaded {total_trajectories} trajectories from folder&#34;)
                st.info(f&#34;ℹ️ NaN filtering applied during loading (built into load_folder function)&#34;)

                return trajectories

        except Exception as e:
            st.error(f&#34;❌ Failed to load assign trajectories: {str(e)}&#34;)
            return None

    def load_assign_centers(self, assign_params: dict = None):
        &#34;&#34;&#34;Load junction centers for assign function based on centers option&#34;&#34;&#34;
        try:
            centers_option = assign_params.get(&#34;centers_option&#34;, &#34;Use session centers&#34;) if assign_params else &#34;Use session centers&#34;

            if centers_option == &#34;Use session centers&#34;:
                # Get centers from previous discover analysis
                if &#34;branches&#34; not in st.session_state.analysis_results:
                    st.error(&#34;❌ No centers found from previous discover analysis&#34;)
                    return None

                centers_dict = {}
                for junction_key, branch_data in st.session_state.analysis_results[&#34;branches&#34;].items():
                    if &#34;centers&#34; in branch_data:
                        centers_dict[junction_key] = branch_data[&#34;centers&#34;]

                return centers_dict

            elif centers_option == &#34;Upload files&#34;:
                centers_files = assign_params.get(&#34;centers_files&#34;) if assign_params else None
                if not centers_files:
                    st.error(&#34;❌ No centers files uploaded&#34;)
                    return None

                centers_dict = {}
                for i, centers_file in enumerate(centers_files):
                    # Save uploaded file to temporary location
                    with tempfile.NamedTemporaryFile(delete=False, suffix=&#39;.npy&#39;) as tmp_file:
                        tmp_file.write(centers_file.getvalue())
                        tmp_path = tmp_file.name

                    # Load centers
                    centers = np.load(tmp_path)
                    centers_dict[f&#34;junction_{i}&#34;] = centers

                    # Clean up temporary file
                    os.unlink(tmp_path)

                return centers_dict

            else:  # Select folder
                centers_folder = assign_params.get(&#34;centers_folder&#34;) if assign_params else None
                if not centers_folder or not centers_folder.strip():
                    st.error(&#34;❌ No centers folder path specified&#34;)
                    return None

                # Load centers from folder (search subfolders)
                centers_dict = {}
                for root, dirs, files in os.walk(centers_folder):
                    for file in files:
                        if file.startswith(&#34;branch_centers_j&#34;) and file.endswith(&#34;.npy&#34;):
                            # Extract junction number from filename
                            junction_num = file.split(&#34;_&#34;)[-1].split(&#34;.&#34;)[0]
                            centers_path = os.path.join(root, file)
                            centers = np.load(centers_path)
                            centers_dict[f&#34;junction_{junction_num}&#34;] = centers

                if not centers_dict:
                    st.error(f&#34;❌ No center files found in folder: {centers_folder}&#34;)
                    st.info(&#34;💡 Looking for files named: branch_centers_j*.npy&#34;)
                    return None

                return centers_dict

        except Exception as e:
            st.error(f&#34;❌ Failed to load assign centers: {str(e)}&#34;)
            return None

    def render_junction_editor(self):
        &#34;&#34;&#34;Render the interactive junction editor&#34;&#34;&#34;
        st.markdown(&#39;&lt;h2 class=&#34;section-header&#34;&gt;🎯 Junction Editor&lt;/h2&gt;&#39;, unsafe_allow_html=True)

        col1, col2 = st.columns([2, 1])

        with col1:
            st.markdown(&#34;### Interactive Junction Management&#34;)

            # Instructions
            st.info(&#34;&#34;&#34;
            **How to use:**
            1. **Add junctions** using the controls on the right
            2. **Edit existing junctions** by changing position, radius, or r_outer values
            3. **Hover over junctions** on the map to see their properties
            4. **Delete junctions** using the 🗑️ button
            &#34;&#34;&#34;)

            # Create interactive plot for junction editing
            if st.session_state.trajectories:
                self.render_junction_plot()
            else:
                st.warning(&#34;⚠️ Please load trajectory data first&#34;)

        with col2:
            st.markdown(&#34;### Junction Controls&#34;)

            # Add new junction section (always visible)
            with st.container():
                st.markdown(&#34;#### Add New Junction&#34;)
                col_x, col_z = st.columns(2)

                with col_x:
                    new_x = st.number_input(&#34;X Position:&#34;, value=500.0, step=10.0)
                with col_z:
                    new_z = st.number_input(&#34;Z Position:&#34;, value=300.0, step=10.0)

                col_radius, col_r_outer = st.columns(2)
                with col_radius:
                    new_radius = st.number_input(&#34;Radius:&#34;, value=30.0, min_value=5.0, max_value=100.0, step=5.0)
                with col_r_outer:
                    new_r_outer = st.number_input(&#34;R Outer:&#34;, value=50.0, min_value=10.0, max_value=200.0, step=5.0)

                if st.button(&#34;➕ Add Junction&#34;):
                    new_junction = Circle(cx=new_x, cz=new_z, r=new_radius)
                    st.session_state.junctions.append(new_junction)
                    st.session_state.junction_r_outer[len(st.session_state.junctions)-1] = new_r_outer

                    # Update junction state hash to force UI refresh
                    st.session_state.junction_state_hash += 1

                    st.success(f&#34;Added junction at ({new_x}, {new_z}) with r_outer={new_r_outer}&#34;)
                    st.rerun()

            st.markdown(&#34;---&#34;)

            # Bulk operations (always visible)
            st.markdown(&#34;#### Bulk Operations&#34;)
            col_clear, col_sample = st.columns(2)

            with col_clear:
                if st.button(&#34;🗑️ Clear All&#34;):
                    st.session_state.junctions = []
                    st.session_state.junction_r_outer = {}

                    # Update junction state hash to force UI refresh
                    st.session_state.junction_state_hash += 1

                    st.rerun()

            with col_sample:
                if st.button(&#34;📋 Load Sample&#34;):
                    self.load_sample_junctions()

            st.markdown(&#34;---&#34;)

            # Scrollable junction list
            st.markdown(&#34;#### Current Junctions&#34;)
            if st.session_state.junctions:
                # Create a scrollable container for the junction list
                st.markdown(f&#34;**Total Junctions: {len(st.session_state.junctions)}**&#34;)
                with st.container():
                    st.markdown(&#39;&lt;div class=&#34;junction-list-container&#34;&gt;&#39;, unsafe_allow_html=True)

                    # Use a scrollable area for the junction list
                    for i, junction in enumerate(st.session_state.junctions):
                        with st.expander(f&#34;Junction {i} - ({junction.cx:.1f}, {junction.cz:.1f})&#34;, expanded=False):
                            # Junction info and delete button
                            col_del, col_info = st.columns([1, 4])

                            with col_del:
                                if st.button(&#34;🗑️&#34;, key=f&#34;del_{i}&#34;, help=&#34;Delete this junction&#34;):
                                    # Store the deleted junction info for debugging
                                    deleted_junction = st.session_state.junctions[i]

                                    # Remove the junction from the list
                                    st.session_state.junctions.pop(i)

                                    # Remove the corresponding r_outer entry
                                    if i in st.session_state.junction_r_outer:
                                        del st.session_state.junction_r_outer[i]

                                    # Reindex remaining junctions and r_outer values
                                    new_r_outer = {}
                                    for j, junction in enumerate(st.session_state.junctions):
                                        old_idx = j + (1 if j &gt;= i else 0)
                                        if old_idx in st.session_state.junction_r_outer:
                                            new_r_outer[j] = st.session_state.junction_r_outer[old_idx]
                                    st.session_state.junction_r_outer = new_r_outer

                                    # Show success message
                                    st.success(f&#34;Deleted Junction {i} at ({deleted_junction.cx:.1f}, {deleted_junction.cz:.1f})&#34;)

                                    # Update junction state hash to force UI refresh
                                    st.session_state.junction_state_hash += 1

                                    # Force a complete rerun to refresh all UI elements
                                    st.rerun()

                            with col_info:
                                st.write(f&#34;Position: ({junction.cx:.1f}, {junction.cz:.1f})&#34;)
                                st.write(f&#34;Radius: {junction.r}&#34;)
                                st.write(f&#34;R_outer: {st.session_state.junction_r_outer.get(i, 50.0)}&#34;)

                            # Position editing
                            st.markdown(&#34;**Edit Position:**&#34;)
                            col_x_edit, col_z_edit = st.columns(2)

                            with col_x_edit:
                                new_x = st.number_input(
                                    f&#34;X:&#34;,
                                    value=float(junction.cx),
                                    step=1.0,
                                    key=f&#34;x_edit_{i}&#34;
                                )

                            with col_z_edit:
                                new_z = st.number_input(
                                    f&#34;Z:&#34;,
                                    value=float(junction.cz),
                                    step=1.0,
                                    key=f&#34;z_edit_{i}&#34;
                                )

                            # Radius editing
                            new_radius = st.number_input(
                                f&#34;Radius:&#34;,
                                value=float(junction.r),
                                min_value=5.0,
                                max_value=100.0,
                                step=1.0,
                                key=f&#34;radius_edit_{i}&#34;
                            )

                            # R_outer control
                            current_r_outer = st.session_state.junction_r_outer.get(i, 50.0)
                            new_r_outer = st.number_input(
                                f&#34;R Outer:&#34;,
                                value=current_r_outer,
                                min_value=10.0,
                                max_value=200.0,
                                step=5.0,
                                key=f&#34;r_outer_{i}&#34;
                            )

                            # Update junction if any values changed
                            if (new_x != junction.cx or new_z != junction.cz or
                                new_radius != junction.r or new_r_outer != current_r_outer):

                                # Update junction
                                st.session_state.junctions[i] = Circle(cx=new_x, cz=new_z, r=new_radius)
                                st.session_state.junction_r_outer[i] = new_r_outer

                                # Update junction state hash to force UI refresh
                                st.session_state.junction_state_hash += 1

                                st.rerun()

                    st.markdown(&#39;&lt;/div&gt;&#39;, unsafe_allow_html=True)
            else:
                st.info(&#34;No junctions defined yet&#34;)

    def render_junction_plot(self):
        &#34;&#34;&#34;Render interactive plot for junction editing&#34;&#34;&#34;
        if not st.session_state.trajectories:
            return

        # Create plot
        fig = go.Figure()

        # Add ALL trajectories (with sampling for performance)
        all_trajectories = st.session_state.trajectories
        for i, traj in enumerate(all_trajectories):
            # Sample every 20th point for performance with many trajectories
            sample_rate = max(1, len(traj.x) // 1000)  # Adaptive sampling
            fig.add_trace(go.Scatter(
                x=traj.x[::sample_rate],
                y=traj.z[::sample_rate],
                mode=&#39;lines&#39;,
                line=dict(color=&#39;lightgray&#39;, width=0.5),
                name=f&#39;Trajectory {i}&#39;,
                showlegend=False,
                opacity=0.6
            ))

        # Add junctions with r_outer circles
        for i, junction in enumerate(st.session_state.junctions):
            # Get r_outer for this junction
            r_outer = st.session_state.junction_r_outer.get(i, 50.0)

            # Junction center with hover info
            fig.add_trace(go.Scatter(
                x=[junction.cx],
                y=[junction.cz],
                mode=&#39;markers&#39;,
                marker=dict(size=20, color=&#39;red&#39;, symbol=&#39;circle&#39;),
                name=f&#39;J{i}&#39;,
                text=f&#39;J{i}&lt;br&gt;Pos: ({junction.cx:.1f}, {junction.cz:.1f})&lt;br&gt;Radius: {junction.r}&lt;br&gt;R_outer: {r_outer}&#39;,
                textposition=&#39;middle center&#39;,
                textfont=dict(color=&#39;white&#39;, size=14, family=&#34;Arial Black&#34;),
                hovertemplate=f&#39;&lt;b&gt;Junction {i}&lt;/b&gt;&lt;br&gt;&#39; +
                             f&#39;Position: ({junction.cx:.1f}, {junction.cz:.1f})&lt;br&gt;&#39; +
                             f&#39;Radius: {junction.r}&lt;br&gt;&#39; +
                             f&#39;R_outer: {r_outer}&lt;br&gt;&#39; +
                             &#39;&lt;extra&gt;&lt;/extra&gt;&#39;
            ))

            # Junction radius circle (decision radius)
            theta = np.linspace(0, 2*np.pi, 100)
            circle_x = junction.cx + junction.r * np.cos(theta)
            circle_z = junction.cz + junction.r * np.sin(theta)

            fig.add_trace(go.Scatter(
                x=circle_x,
                y=circle_z,
                mode=&#39;lines&#39;,
                line=dict(color=&#39;orange&#39;, width=3),
                showlegend=False,
                hovertemplate=f&#39;&lt;b&gt;Junction {i} - Decision Radius&lt;/b&gt;&lt;br&gt;&#39; +
                             f&#39;Position: ({junction.cx:.1f}, {junction.cz:.1f})&lt;br&gt;&#39; +
                             f&#39;Decision Radius: {junction.r}&lt;br&gt;&#39; +
                             f&#39;R_outer: {r_outer}&lt;br&gt;&#39; +
                             &#39;&lt;extra&gt;&lt;/extra&gt;&#39;,
                name=f&#39;J{i} Decision Radius&#39;
            ))

            # R_outer circle (analysis radius)
            r_outer_x = junction.cx + r_outer * np.cos(theta)
            r_outer_z = junction.cz + r_outer * np.sin(theta)

            fig.add_trace(go.Scatter(
                x=r_outer_x,
                y=r_outer_z,
                mode=&#39;lines&#39;,
                line=dict(color=&#39;blue&#39;, width=2, dash=&#39;dash&#39;),
                showlegend=False,
                hovertemplate=f&#39;&lt;b&gt;Junction {i} - Analysis Radius&lt;/b&gt;&lt;br&gt;&#39; +
                             f&#39;Position: ({junction.cx:.1f}, {junction.cz:.1f})&lt;br&gt;&#39; +
                             f&#39;Decision Radius: {junction.r}&lt;br&gt;&#39; +
                             f&#39;R_outer: {r_outer}&lt;br&gt;&#39; +
                             &#39;&lt;extra&gt;&lt;/extra&gt;&#39;,
                name=f&#39;J{i} R_outer&#39;
            ))

            # Add invisible junction area for better hover capture
            # Create a filled circle area that will capture hover events
            theta_dense = np.linspace(0, 2*np.pi, 200)
            area_x = junction.cx + r_outer * np.cos(theta_dense)
            area_z = junction.cz + r_outer * np.sin(theta_dense)

            fig.add_trace(go.Scatter(
                x=area_x,
                y=area_z,
                mode=&#39;lines&#39;,
                fill=&#39;toself&#39;,
                fillcolor=&#39;rgba(255, 0, 0, 0.05)&#39;,  # Very light red fill
                line=dict(width=0),  # No visible line
                showlegend=False,
                hovertemplate=f&#39;&lt;b&gt;Junction {i} Area&lt;/b&gt;&lt;br&gt;&#39; +
                             f&#39;Position: ({junction.cx:.1f}, {junction.cz:.1f})&lt;br&gt;&#39; +
                             f&#39;Decision Radius: {junction.r}&lt;br&gt;&#39; +
                             f&#39;R_outer: {r_outer}&lt;br&gt;&#39; +
                             &#39;&lt;extra&gt;&lt;/extra&gt;&#39;,
                name=f&#39;J{i} Area&#39;,
                hoveron=&#39;fills&#39;  # Only show hover on filled area
            ))

        # Update layout
        fig.update_layout(
            title=&#34;Interactive Junction Editor - All Trajectories&#34;,
            xaxis_title=&#34;X Position&#34;,
            yaxis_title=&#34;Z Position&#34;,
            hovermode=&#39;closest&#39;,
            showlegend=False
        )

        # Set equal aspect ratio for both axes
        fig.update_xaxes(scaleanchor=&#34;y&#34;, scaleratio=1)
        fig.update_yaxes(scaleanchor=&#34;x&#34;, scaleratio=1)

        # Add legend manually
        fig.add_trace(go.Scatter(
            x=[None], y=[None],
            mode=&#39;markers&#39;,
            marker=dict(size=10, color=&#39;red&#39;),
            name=&#39;Junction Center&#39;,
            showlegend=True
        ))
        fig.add_trace(go.Scatter(
            x=[None], y=[None],
            mode=&#39;lines&#39;,
            line=dict(color=&#39;orange&#39;, width=3),
            name=&#39;Decision Radius&#39;,
            showlegend=True
        ))
        fig.add_trace(go.Scatter(
            x=[None], y=[None],
            mode=&#39;lines&#39;,
            line=dict(color=&#39;blue&#39;, width=2, dash=&#39;dash&#39;),
            name=&#39;R_outer (Analysis Radius)&#39;,
            showlegend=True
        ))
        fig.add_trace(go.Scatter(
            x=[None], y=[None],
            mode=&#39;lines&#39;,
            line=dict(color=&#39;lightgray&#39;, width=1),
            name=&#39;Trajectories&#39;,
            showlegend=True
        ))

        st.plotly_chart(fig, config={&#39;displayModeBar&#39;: True}, width=&#39;stretch&#39;)

    def load_sample_junctions(self):
        &#34;&#34;&#34;Load sample junctions&#34;&#34;&#34;
        sample_junctions = [
            Circle(cx=685, cz=170, r=30),
            Circle(cx=550, cz=-90, r=30),
            Circle(cx=730, cz=440, r=20),
            Circle(cx=520, cz=340, r=40),
            Circle(cx=500, cz=515, r=20),
            Circle(cx=575, cz=430, r=15),
            Circle(cx=500, cz=205, r=20)
        ]

        # Sample r_outer values
        sample_r_outer = [100.0, 50.0, 45.0, 45.0, 45.0, 30.0, 45.0]

        st.session_state.junctions = sample_junctions
        st.session_state.junction_r_outer = {i: r_outer for i, r_outer in enumerate(sample_r_outer)}
        st.success(&#34;✅ Loaded sample junctions with r_outer values!&#34;)
        st.rerun()

    def render_analysis(self):
        &#34;&#34;&#34;Render the analysis interface&#34;&#34;&#34;
        st.markdown(&#39;&lt;h2 class=&#34;section-header&#34;&gt;📊 Analysis&lt;/h2&gt;&#39;, unsafe_allow_html=True)

        if not st.session_state.trajectories:
            st.warning(&#34;⚠️ Please load trajectory data first&#34;)
            return

        col1, col2 = st.columns([3, 1])

        with col1:
            st.markdown(&#34;### Analysis Configuration&#34;)

            # Analysis type selection
            analysis_type_options = {
                &#34;discover&#34;: &#34;🔍 Discover Branches - Find decision branches at junctions&#34;,
                &#34;assign&#34;: &#34;📊 Assign Trajectories - Assign trajectories to discovered branches&#34;,
                &#34;metrics&#34;: &#34;📈 Movement Metrics - Calculate trajectory movement patterns and timing&#34;,
                &#34;gaze&#34;: &#34;👁️ Gaze &amp; Physiology - Analyze eye tracking and physiological data&#34;,
                &#34;predict&#34;: &#34;🔮 Predict Choices - Predict junction choice patterns&#34;,
                &#34;intent&#34;: &#34;🧠 Intent Recognition - Predict route choices BEFORE decision points (ML)&#34;,
                &#34;enhanced&#34;: &#34;🚨 Enhanced Analysis - Evacuation planning, risk assessment, and efficiency metrics&#34;
            }

            # Reset analysis type if we&#39;re coming from another tab
            if st.session_state.current_step == &#34;analysis&#34;:
                # Clear any cached analysis type to ensure fresh selection
                if &#34;cached_analysis_type&#34; in st.session_state:
                    del st.session_state.cached_analysis_type

            analysis_type = st.selectbox(
                &#34;Analysis Type:&#34;,
                list(analysis_type_options.keys()),
                format_func=lambda x: analysis_type_options[x],
                help=&#34;Select the type of analysis to perform&#34;,
                key=f&#34;analysis_type_select_{st.session_state.current_step}&#34;
            )

            # Store the selected analysis type
            st.session_state.cached_analysis_type = analysis_type

            # Initialize default values
            decision_mode = &#34;pathlen&#34;
            cluster_method = &#34;dbscan&#34;
            seed = 42

            # Initialize cluster parameters with defaults
            dbscan_eps = 0.5
            dbscan_min_samples = 5
            dbscan_angle_eps = 15.0
            kmeans_k = 3
            kmeans_k_min = 2
            kmeans_k_max = 6
            auto_k_min = 2
            auto_k_max = 6
            auto_min_sep_deg = 12.0
            auto_angle_eps = 15.0

            # Initialize decision mode parameters with defaults
            radial_r_outer = 50.0
            radial_epsilon = 0.05
            pathlen_path_length = 100.0
            pathlen_linger_delta = 0.0
            hybrid_r_outer = 50.0
            hybrid_path_length = 100.0
            hybrid_linger_delta = 0.0

            # Initialize metrics parameters with defaults
            metrics_decision_mode = &#34;pathlen&#34;
            metrics_distance = 100.0
            metrics_r_outer = 50.0
            metrics_trend_window = 5
            metrics_min_outward = 0.0

            # Initialize discover parameters with defaults
            discover_decision_mode = &#34;hybrid&#34;
            discover_r_outer = 50.0
            discover_epsilon = 0.05
            discover_path_length = 100.0
            discover_linger_delta = 0.0
            discover_hybrid_r_outer = 50.0
            discover_hybrid_path_length = 100.0

            # Show parameters based on analysis type
            if analysis_type == &#34;predict&#34;:
                # Predict analysis uses only spatial tracking - no parameters needed
                st.info(&#34;ℹ️ Predict analysis uses spatial tracking only. No additional parameters required.&#34;)

                # Set default values for compatibility (not used in analysis)
                cluster_method = &#34;kmeans&#34;
                seed = 42
                decision_mode = &#34;hybrid&#34;

                # REMOVED: All cluster method parameters - not needed for spatial tracking only

            elif analysis_type == &#34;intent&#34;:
                # Intent Recognition - ML-based early prediction
                st.markdown(&#34;#### 🧠 Intent Recognition Configuration&#34;)
                st.info(&#34;🤖 Machine Learning-based prediction of route choices **before** users reach decision points.&#34;)

                # Check if Discover has been run
                has_discover_results = (st.session_state.analysis_results and
                                       &#39;branches&#39; in st.session_state.analysis_results)

                if has_discover_results:
                    st.success(&#34;✅ Will use branch assignments from your previous &#39;Discover Branches&#39; analysis&#34;)
                else:
                    st.warning(&#34;⚠️ No &#39;Discover Branches&#39; results found. Will use default clustering parameters.&#34;)
                    st.info(&#34;💡 **Recommended:** Run &#39;Discover Branches&#39; analysis first to control clustering settings!&#34;)

                # Prediction distances
                st.markdown(&#34;##### Prediction Distances&#34;)
                dist_col1, dist_col2, dist_col3, dist_col4 = st.columns(4)

                with dist_col1:
                    intent_dist_100 = st.checkbox(&#34;100 units&#34;, value=True, help=&#34;Predict 100 units before junction&#34;)
                with dist_col2:
                    intent_dist_75 = st.checkbox(&#34;75 units&#34;, value=True, help=&#34;Predict 75 units before junction&#34;)
                with dist_col3:
                    intent_dist_50 = st.checkbox(&#34;50 units&#34;, value=True, help=&#34;Predict 50 units before junction&#34;)
                with dist_col4:
                    intent_dist_25 = st.checkbox(&#34;25 units&#34;, value=True, help=&#34;Predict 25 units before junction&#34;)

                # Build prediction distances list
                intent_prediction_distances = []
                if intent_dist_100:
                    intent_prediction_distances.append(100.0)
                if intent_dist_75:
                    intent_prediction_distances.append(75.0)
                if intent_dist_50:
                    intent_prediction_distances.append(50.0)
                if intent_dist_25:
                    intent_prediction_distances.append(25.0)

                if not intent_prediction_distances:
                    st.warning(&#34;⚠️ Select at least one prediction distance!&#34;)
                    intent_prediction_distances = [50.0]  # Default

                # Model configuration
                st.markdown(&#34;##### Model Configuration&#34;)
                model_col1, model_col2 = st.columns(2)

                with model_col1:
                    intent_model_type = st.selectbox(
                        &#34;ML Model:&#34;,
                        [&#34;random_forest&#34;, &#34;gradient_boosting&#34;],
                        index=0,
                        help=&#34;Random Forest: Fast, robust | Gradient Boosting: More accurate but slower&#34;
                    )

                with model_col2:
                    intent_cv_folds = st.number_input(
                        &#34;Cross-validation Folds:&#34;,
                        value=5,
                        min_value=2,
                        max_value=10,
                        help=&#34;Number of folds for cross-validation&#34;
                    )

                # Feature configuration
                with st.expander(&#34;🔧 Advanced: Feature Configuration&#34;):
                    st.markdown(&#34;**Features Used:**&#34;)
                    st.markdown(&#34;&#34;&#34;
                    - ✅ **Spatial**: Distance, approach angle, lateral offset
                    - ✅ **Kinematic**: Speed, acceleration, curvature, sinuosity
                    - ✅ **Gaze** (if available): Gaze angle, alignment, head rotation
                    - ✅ **Physiological** (if available): Heart rate, pupil dilation
                    - ✅ **Contextual**: Previous junction choices
                    &#34;&#34;&#34;)

                    intent_test_split = st.slider(
                        &#34;Test Set Size (%):&#34;,
                        min_value=10,
                        max_value=40,
                        value=20,
                        help=&#34;Percentage of data reserved for testing&#34;
                    )

                # Store intent parameters in session state
                if &#39;intent_params&#39; not in st.session_state:
                    st.session_state.intent_params = {}

                st.session_state.intent_params = {
                    &#39;prediction_distances&#39;: intent_prediction_distances,
                    &#39;model_type&#39;: intent_model_type,
                    &#39;cv_folds&#39;: intent_cv_folds,
                    &#39;test_split&#39;: intent_test_split / 100.0
                }

                # Set default values for compatibility
                cluster_method = &#34;kmeans&#34;
                seed = 42
                decision_mode = &#34;hybrid&#34;

            elif analysis_type == &#34;discover&#34;:
                # Clustering parameters (used by discover analysis)
                st.markdown(&#34;#### Clustering Parameters&#34;)
                col_method, col_seed = st.columns(2)

                with col_method:
                    cluster_method = st.selectbox(
                        &#34;Cluster Method:&#34;,
                        [&#34;dbscan&#34;, &#34;kmeans&#34;, &#34;auto&#34;],
                        index=0,  # Default to DBSCAN
                        help=&#34;Clustering method for discover analysis&#34;
                    )

                with col_seed:
                    seed = st.number_input(
                        &#34;Random Seed:&#34;,
                        value=42,
                        min_value=0,
                        max_value=10000,
                        step=1,
                        help=&#34;Random seed for reproducibility&#34;
                    )

                # Decision mode parameters (needed for discover analysis)
                st.markdown(&#34;#### Decision Mode Parameters&#34;)
                col_decision_mode, col_decision_param = st.columns(2)

                with col_decision_mode:
                    discover_decision_mode = st.selectbox(
                        &#34;Decision Mode:&#34;,
                        [&#34;radial&#34;, &#34;pathlen&#34;, &#34;hybrid&#34;],
                        index=2,  # Default to hybrid
                        help=&#34;Decision mode for discover analysis&#34;
                    )

                with col_decision_param:
                    if discover_decision_mode == &#34;radial&#34;:
                        st.info(&#34;ℹ️ Using junction-specific r_outer values from the Junctions tab&#34;)
                        # r_outer will be overridden by junction-specific values
                        discover_r_outer = 50.0
                        discover_epsilon = st.number_input(
                            &#34;Epsilon:&#34;,
                            value=0.05,
                            min_value=0.01,
                            max_value=1.0,
                            step=0.01,
                            help=&#34;Epsilon parameter&#34;
                        )
                    elif discover_decision_mode == &#34;pathlen&#34;:
                        discover_path_length = st.number_input(
                            &#34;Path Length:&#34;,
                            value=100.0,
                            min_value=10.0,
                            max_value=500.0,
                            step=10.0,
                            help=&#34;Path length for pathlen mode&#34;
                        )
                        discover_linger_delta = st.number_input(
                            &#34;Linger Delta:&#34;,
                            value=0.0,
                            min_value=0.0,
                            max_value=50.0,
                            step=1.0,
                            help=&#34;Linger distance beyond junction&#34;
                        )
                    elif discover_decision_mode == &#34;hybrid&#34;:
                        st.info(&#34;ℹ️ Using junction-specific r_outer values from the Junctions tab&#34;)
                        # r_outer will be overridden by junction-specific values
                        discover_hybrid_r_outer = 50.0
                        discover_hybrid_path_length = st.number_input(
                            &#34;Hybrid Path Length:&#34;,
                            value=100.0,
                            min_value=10.0,
                            max_value=500.0,
                            step=10.0,
                            help=&#34;Path length for hybrid mode&#34;
                        )
                        discover_hybrid_linger_delta = st.number_input(
                            &#34;Hybrid Linger Delta:&#34;,
                            value=0.0,
                            min_value=0.0,
                            max_value=50.0,
                            step=1.0,
                            help=&#34;Linger distance beyond junction for hybrid mode&#34;
                        )

                # Dynamic parameters based on cluster method (for discover analysis)
                st.markdown(&#34;#### Cluster Method Parameters&#34;)
                if cluster_method == &#34;dbscan&#34;:
                    col_eps, col_min_samples = st.columns(2)
                    with col_eps:
                        dbscan_eps = st.number_input(
                            &#34;DBSCAN Epsilon (eps):&#34;,
                            value=0.5,
                            min_value=0.1,
                            max_value=10.0,
                            step=0.1,
                            help=&#34;Maximum distance between samples in the same neighborhood&#34;
                        )
                    with col_min_samples:
                        dbscan_min_samples = st.number_input(
                            &#34;DBSCAN Min Samples:&#34;,
                            value=5,
                            min_value=1,
                            max_value=50,
                            step=1,
                            help=&#34;Minimum number of samples in a neighborhood&#34;
                        )

                    # Add angle_eps parameter for DBSCAN
                    dbscan_angle_eps = st.number_input(
                        &#34;DBSCAN Angle Epsilon (degrees):&#34;,
                        value=11.0,
                        min_value=1.0,
                        max_value=90.0,
                        step=1.0,
                        help=&#34;Angle epsilon for DBSCAN clustering (angular separation between clusters)&#34;
                    )
                elif cluster_method == &#34;kmeans&#34;:
                    col_k, col_k_range = st.columns(2)
                    with col_k:
                        kmeans_k = st.number_input(
                            &#34;K-Means K (number of clusters):&#34;,
                            value=3,
                            min_value=2,
                            max_value=20,
                            step=1,
                            help=&#34;Number of clusters to form&#34;
                        )
                    with col_k_range:
                        kmeans_k_min = st.number_input(
                            &#34;K-Means K Min:&#34;,
                            value=2,
                            min_value=2,
                            max_value=10,
                            step=1,
                            help=&#34;Minimum number of clusters for auto selection&#34;
                        )
                        kmeans_k_max = st.number_input(
                            &#34;K-Means K Max:&#34;,
                            value=6,
                            min_value=3,
                            max_value=20,
                            step=1,
                            help=&#34;Maximum number of clusters for auto selection&#34;
                        )
                elif cluster_method == &#34;auto&#34;:
                    col_k_range, col_separation = st.columns(2)
                    with col_k_range:
                        auto_k_min = st.number_input(
                            &#34;Auto K Min:&#34;,
                            value=2,
                            min_value=2,
                            max_value=10,
                            step=1,
                            help=&#34;Minimum number of clusters for auto selection&#34;
                        )
                        auto_k_max = st.number_input(
                            &#34;Auto K Max:&#34;,
                            value=6,
                            min_value=3,
                            max_value=20,
                            step=1,
                            help=&#34;Maximum number of clusters for auto selection&#34;
                        )
                    with col_separation:
                        auto_min_sep_deg = st.number_input(
                            &#34;Auto Min Separation (degrees):&#34;,
                            value=12.0,
                            min_value=1.0,
                            max_value=90.0,
                            step=1.0,
                            help=&#34;Minimum separation in degrees between clusters&#34;
                        )
                        auto_angle_eps = st.number_input(
                            &#34;Auto Angle Epsilon (degrees):&#34;,
                            value=11.0,
                            min_value=1.0,
                            max_value=90.0,
                            step=1.0,
                            help=&#34;Angle epsilon for auto clustering&#34;
                        )

            elif analysis_type == &#34;enhanced&#34;:
                # Enhanced analysis parameters (same as discover since it uses discover_decision_chain)
                st.markdown(&#34;#### Enhanced Analysis Parameters&#34;)
                st.info(&#34;🚨 Enhanced analysis uses the same clustering and decision parameters as discover analysis, then performs evacuation planning, risk assessment, and efficiency analysis.&#34;)

                # Clustering parameters (same as discover)
                st.markdown(&#34;##### Clustering Parameters&#34;)
                col_method, col_seed = st.columns(2)

                with col_method:
                    cluster_method = st.selectbox(
                        &#34;Cluster Method:&#34;,
                        [&#34;dbscan&#34;, &#34;kmeans&#34;, &#34;auto&#34;],
                        index=0,  # Default to DBSCAN
                        help=&#34;Clustering method for enhanced analysis&#34;
                    )

                with col_seed:
                    seed = st.number_input(
                        &#34;Random Seed:&#34;,
                        value=42,
                        min_value=0,
                        max_value=10000,
                        step=1,
                        help=&#34;Random seed for reproducibility&#34;
                    )

                # Decision mode parameters (same as discover)
                st.markdown(&#34;##### Decision Mode Parameters&#34;)
                col_decision_mode, col_decision_param = st.columns(2)

                with col_decision_mode:
                    discover_decision_mode = st.selectbox(
                        &#34;Decision Mode:&#34;,
                        [&#34;radial&#34;, &#34;pathlen&#34;, &#34;hybrid&#34;],
                        index=2,  # Default to hybrid
                        help=&#34;Decision mode for enhanced analysis&#34;
                    )

                with col_decision_param:
                    if discover_decision_mode == &#34;radial&#34;:
                        st.info(&#34;ℹ️ Using junction-specific r_outer values from the Junctions tab&#34;)
                        # r_outer will be overridden by junction-specific values
                        discover_r_outer = 50.0
                        discover_epsilon = st.number_input(
                            &#34;Epsilon:&#34;,
                            value=0.05,
                            min_value=0.01,
                            max_value=1.0,
                            step=0.01,
                            help=&#34;Epsilon parameter&#34;
                        )
                    elif discover_decision_mode == &#34;pathlen&#34;:
                        discover_path_length = st.number_input(
                            &#34;Path Length:&#34;,
                            value=100.0,
                            min_value=10.0,
                            max_value=500.0,
                            step=10.0,
                            help=&#34;Path length for pathlen mode&#34;
                        )
                        discover_linger_delta = st.number_input(
                            &#34;Linger Delta:&#34;,
                            value=0.0,
                            min_value=0.0,
                            max_value=50.0,
                            step=1.0,
                            help=&#34;Linger distance beyond junction&#34;
                        )
                    elif discover_decision_mode == &#34;hybrid&#34;:
                        st.info(&#34;ℹ️ Using junction-specific r_outer values from the Junctions tab&#34;)
                        # r_outer will be overridden by junction-specific values
                        discover_hybrid_r_outer = 50.0
                        discover_hybrid_path_length = st.number_input(
                            &#34;Hybrid Path Length:&#34;,
                            value=100.0,
                            min_value=10.0,
                            max_value=500.0,
                            step=10.0,
                            help=&#34;Path length for hybrid mode&#34;
                        )
                        discover_hybrid_linger_delta = st.number_input(
                            &#34;Hybrid Linger Delta:&#34;,
                            value=0.0,
                            min_value=0.0,
                            max_value=50.0,
                            step=1.0,
                            help=&#34;Linger distance beyond junction for hybrid mode&#34;
                        )

                # Dynamic parameters based on cluster method (same as discover)
                st.markdown(&#34;##### Cluster Method Parameters&#34;)
                if cluster_method == &#34;dbscan&#34;:
                    col_eps, col_min_samples = st.columns(2)
                    with col_eps:
                        dbscan_eps = st.number_input(
                            &#34;DBSCAN Epsilon (eps):&#34;,
                            value=0.5,
                            min_value=0.1,
                            max_value=10.0,
                            step=0.1,
                            help=&#34;Maximum distance between samples in the same neighborhood&#34;
                        )
                    with col_min_samples:
                        dbscan_min_samples = st.number_input(
                            &#34;DBSCAN Min Samples:&#34;,
                            value=5,
                            min_value=1,
                            max_value=50,
                            step=1,
                            help=&#34;Minimum number of samples in a neighborhood&#34;
                        )

                    # Add angle_eps parameter for DBSCAN
                    dbscan_angle_eps = st.number_input(
                        &#34;DBSCAN Angle Epsilon (degrees):&#34;,
                        value=11.0,
                        min_value=1.0,
                        max_value=90.0,
                        step=1.0,
                        help=&#34;Angle epsilon for DBSCAN clustering (angular separation between clusters)&#34;
                    )
                elif cluster_method == &#34;kmeans&#34;:
                    col_k, col_k_range = st.columns(2)
                    with col_k:
                        kmeans_k = st.number_input(
                            &#34;K-Means K (number of clusters):&#34;,
                            value=3,
                            min_value=2,
                            max_value=20,
                            step=1,
                            help=&#34;Number of clusters to form&#34;
                        )
                    with col_k_range:
                        kmeans_k_min = st.number_input(
                            &#34;K-Means K Min:&#34;,
                            value=2,
                            min_value=2,
                            max_value=10,
                            step=1,
                            help=&#34;Minimum number of clusters for auto selection&#34;
                        )
                        kmeans_k_max = st.number_input(
                            &#34;K-Means K Max:&#34;,
                            value=6,
                            min_value=3,
                            max_value=20,
                            step=1,
                            help=&#34;Maximum number of clusters for auto selection&#34;
                        )
                elif cluster_method == &#34;auto&#34;:
                    col_k_range, col_separation = st.columns(2)
                    with col_k_range:
                        auto_k_min = st.number_input(
                            &#34;Auto K Min:&#34;,
                            value=2,
                            min_value=2,
                            max_value=10,
                            step=1,
                            help=&#34;Minimum number of clusters for auto selection&#34;
                        )
                        auto_k_max = st.number_input(
                            &#34;Auto K Max:&#34;,
                            value=6,
                            min_value=3,
                            max_value=20,
                            step=1,
                            help=&#34;Maximum number of clusters for auto selection&#34;
                        )
                    with col_separation:
                        auto_min_sep_deg = st.number_input(
                            &#34;Auto Min Separation (degrees):&#34;,
                            value=12.0,
                            min_value=1.0,
                            max_value=90.0,
                            step=1.0,
                            help=&#34;Minimum separation in degrees between clusters&#34;
                        )
                        auto_angle_eps = st.number_input(
                            &#34;Auto Angle Epsilon (degrees):&#34;,
                            value=11.0,
                            min_value=1.0,
                            max_value=90.0,
                            step=1.0,
                            help=&#34;Angle epsilon for auto clustering&#34;
                        )

            elif analysis_type == &#34;metrics&#34;:
                # Metrics-specific parameters
                st.markdown(&#34;#### Metrics Parameters&#34;)
                st.info(&#34;Metrics analysis computes timing and distance metrics for trajectories. The decision mode determines how junction timing is calculated - &#39;pathlen&#39; measures time to reach a distance threshold, &#39;radial&#39; measures time to exit a radius, and &#39;hybrid&#39; tries radial first then falls back to pathlen.&#34;)

                col_metrics_mode, col_metrics_distance = st.columns(2)

                with col_metrics_mode:
                    st.session_state.metrics_decision_mode = st.selectbox(
                        &#34;Decision Mode:&#34;,
                        [&#34;pathlen&#34;, &#34;radial&#34;, &#34;hybrid&#34;],
                        index=2,  # Default to hybrid
                        help=&#34;Decision mode for junction timing analysis&#34;
                    )

                with col_metrics_distance:
                    st.session_state.metrics_distance = st.number_input(
                        &#34;Distance Threshold:&#34;,
                        value=100.0,
                        min_value=10.0,
                        max_value=500.0,
                        step=10.0,
                        help=&#34;Path length for decision timing (pathlen mode) or outer radius (radial mode)&#34;
                    )

                # Additional metrics parameters
                col_trend_window, col_min_outward = st.columns(2)

                with col_trend_window:
                    st.session_state.metrics_trend_window = st.number_input(
                        &#34;Metrics Trend Window:&#34;,
                        value=5,
                        min_value=1,
                        max_value=20,
                        step=1,
                        help=&#34;Trend window for radial mode&#34;
                    )

                st.session_state.metrics_min_outward = st.number_input(
                    &#34;Metrics Min Outward:&#34;,
                    value=0.0,
                    min_value=0.0,
                    max_value=10.0,
                    step=0.1,
                    help=&#34;Minimum outward movement for radial mode&#34;
                )

                # Show info about using junction-specific r_outer values
                if st.session_state.metrics_decision_mode in [&#34;radial&#34;, &#34;hybrid&#34;]:
                    st.info(&#34;ℹ️ Using junction-specific r_outer values from the Junctions tab&#34;)

            elif analysis_type == &#34;gaze&#34;:
                # Gaze analysis parameters
                st.markdown(&#34;#### Gaze &amp; Physiological Analysis Parameters&#34;)

                # Prefer gaze trajectories if available
                active_trajs = st.session_state.trajectories

                # Check if we have proper gaze trajectory data
                has_gaze_data = self._check_for_gaze_data(active_trajs)

                # Show simple status message
                if active_trajs:
                    from verta.verta_data_loader import has_vr_headset_data
                    has_vr = any(has_vr_headset_data(traj) for traj in active_trajs)

                    if has_vr:
                        st.success(&#34;✅ This dataset contains VR headset data!&#34;)

                # Show analysis options
                if has_gaze_data:

                    # Check if we have existing branch assignments
                    has_existing_assignments = (st.session_state.analysis_results is not None and
                                             &#34;branches&#34; in st.session_state.analysis_results)

                    if has_existing_assignments:
                        st.success(&#34;✅ **Existing branch assignments found!**&#34;)
                        st.info(&#34;🔍 Gaze analysis will use existing branch assignments from previous discover analysis.&#34;)
                        st.write(&#34;**💡 To create new assignments:** Run &#39;🔍 Discover Branches&#39; analysis first, then return here for gaze analysis.&#34;)

                        # Always use existing assignments
                        st.session_state.use_existing_assignments = True
                        st.session_state.run_custom_discover = False

                    else:
                        st.warning(&#34;⚠️ **No existing branch assignments found!**&#34;)
                        st.error(&#34;**Prerequisite:** You must run &#39;🔍 Discover Branches&#39; analysis first to create branch assignments.&#34;)
                        st.write(&#34;**Steps:**&#34;)
                        st.write(&#34;1. Go to the &#39;🔍 Discover Branches&#39; analysis&#34;)
                        st.write(&#34;2. Run the discover analysis to create branch assignments&#34;)
                        st.write(&#34;3. Return here to run gaze analysis&#34;)

                        # Disable gaze analysis if no assignments
                        st.session_state.use_existing_assignments = False
                        st.session_state.run_custom_discover = False

                        # Show a disabled button
                        st.button(&#34;Run Analysis&#34;, disabled=True, help=&#34;Run &#39;🔍 Discover Branches&#39; analysis first to create branch assignments&#34;)
                        return

                    # Pupil Dilation Heatmap Settings
                    st.markdown(&#34;---&#34;)
                    st.markdown(&#34;#### 🗺️ Pupil Dilation Heatmap Settings&#34;)
                    st.info(&#34;Configure spatial heatmap visualization of pupil dilation patterns&#34;)

                    col_grid, col_norm = st.columns(2)

                    with col_grid:
                        # Initialize session state with default value if not set
                        if &#39;pupil_heatmap_cell_size&#39; not in st.session_state:
                            st.session_state.pupil_heatmap_cell_size = 10.0

                        st.session_state.pupil_heatmap_cell_size = st.slider(
                            &#34;Cell Size (coordinate units):&#34;,
                            min_value=1.0,
                            max_value=200.0,
                            value=float(st.session_state.pupil_heatmap_cell_size),
                            step=1.0,
                            help=&#34;Size of each grid cell in coordinate units (smaller = finer resolution)&#34;
                        )

                    with col_norm:
                        # Initialize session state with default value if not set
                        if &#39;pupil_heatmap_normalization&#39; not in st.session_state:
                            st.session_state.pupil_heatmap_normalization = &#39;relative&#39;

                        st.session_state.pupil_heatmap_normalization = st.selectbox(
                            &#34;Normalization:&#34;,
                            [&#34;relative&#34;, &#34;zscore&#34;],
                            index=0 if st.session_state.pupil_heatmap_normalization == &#39;relative&#39; else 1,
                            help=&#34;Relative: % change from baseline. Z-score: standard deviations from mean&#34;
                        )

                    # Show expected grid dimensions
                    if st.session_state.trajectories:
                        # Calculate approximate grid dimensions
                        all_x = np.concatenate([t.x for t in st.session_state.trajectories[:10] if hasattr(t, &#39;x&#39;)])
                        all_z = np.concatenate([t.z for t in st.session_state.trajectories[:10] if hasattr(t, &#39;z&#39;)])
                        x_range = np.max(all_x) - np.min(all_x)
                        z_range = np.max(all_z) - np.min(all_z)
                        cell_size = st.session_state.pupil_heatmap_cell_size
                        grid_x = int(np.ceil(x_range / cell_size))
                        grid_z = int(np.ceil(z_range / cell_size))

                        st.info(f&#34;📏 **Expected grid:** {grid_x} × {grid_z} cells ({cell_size}×{cell_size} units each)&#34;)

                    # Run Analysis button
                    if st.button(&#34;Run Analysis&#34;, type=&#34;primary&#34;):
                        # Run gaze analysis
                        self.run_analysis(&#34;gaze&#34;, &#34;hybrid&#34;, &#34;dbscan&#34;, 42)

                else:
                    st.info(&#34;ℹ️ Gaze/Physiological analysis requires VR headset data with eye tracking.&#34;)

            elif analysis_type == &#34;assign&#34;:
                st.markdown(&#34;#### Assign Parameters&#34;)

                # Add scaling warning
                st.info(&#34;💡 **Important**: Ensure the scale factor used for trajectories matches the scale factor used during discover analysis. Mismatched scaling will cause assignment failures.&#34;)

                # Scale factor input for assign analysis
                st.markdown(&#34;**📏 Scale Factor for Assignment:**&#34;)
                col_scale_assign, col_scale_info = st.columns([1, 2])

                with col_scale_assign:
                    assign_scale = st.number_input(
                        &#34;Scale Factor:&#34;,
                        value=st.session_state.get(&#34;scale_factor&#34;, 0.2),
                        min_value=0.01,
                        max_value=1.0,
                        step=0.01,
                        key=&#34;assign_scale_factor&#34;,
                        help=&#34;Scale factor to apply to trajectory coordinates&#34;
                    )

                with col_scale_info:
                    # Show scale factor from discover analysis if available
                    analysis_results = st.session_state.get(&#34;analysis_results&#34;)
                    if analysis_results and &#34;branches&#34; in analysis_results:
                        discover_scale = None
                        for junction_key, branch_data in analysis_results[&#34;branches&#34;].items():
                            if &#34;scale&#34; in branch_data:
                                discover_scale = branch_data[&#34;scale&#34;]
                                break

                        if discover_scale is not None:
                            st.info(f&#34;🔍 **Discover used scale**: {discover_scale:.2f}&#34;)
                            if abs(assign_scale - discover_scale) &gt; 0.01:
                                st.warning(f&#34;⚠️ **Scale mismatch detected!** Consider using {discover_scale:.2f} for consistency.&#34;)
                        else:
                            st.info(&#34;🔍 No discover scale factor found&#34;)
                    else:
                        st.info(&#34;🔍 No discover analysis found&#34;)

                # Simplified data input options
                st.markdown(&#34;**📁 Data Input Options:**&#34;)

                # Trajectories input
                st.markdown(&#34;**Trajectories:**&#34;)
                trajectory_option = st.radio(
                    &#34;Trajectory Source:&#34;,
                    [&#34;Upload files&#34;, &#34;Select folder&#34;],
                    key=&#34;assign_trajectory_option&#34;,
                    help=&#34;Upload new trajectories to be assigned to existing branches&#34;
                )

                # Centers input
                st.markdown(&#34;**Junction Centers:**&#34;)
                centers_option = st.radio(
                    &#34;Centers Source:&#34;,
                    [&#34;Use session centers&#34;, &#34;Upload files&#34;, &#34;Select folder&#34;],
                    key=&#34;assign_centers_option&#34;,
                    help=&#34;Choose how to provide junction center data&#34;
                )

                # File upload and folder selection based on options
                if trajectory_option == &#34;Upload files&#34;:
                    st.markdown(&#34;**📤 Upload Trajectory Files:**&#34;)
                    trajectory_files = st.file_uploader(
                        &#34;Choose trajectory CSV files:&#34;,
                        type=[&#39;csv&#39;],
                        accept_multiple_files=True,
                        key=&#34;assign_trajectory_files&#34;,
                        help=&#34;Upload CSV files containing trajectory data to be assigned to existing branches&#34;
                    )
                else:  # Select folder
                    st.markdown(&#34;**📁 Select Trajectory Folder:**&#34;)
                    trajectory_folder = st.text_input(
                        &#34;Trajectory folder path:&#34;,
                        key=&#34;assign_trajectory_folder&#34;,
                        help=&#34;Enter the path to the folder containing trajectory CSV files to be assigned to existing branches&#34;
                    )

                if centers_option == &#34;Upload files&#34;:
                    st.markdown(&#34;**📤 Upload Center Files:**&#34;)
                    centers_files = st.file_uploader(
                        &#34;Choose center files (.npy or .zip):&#34;,
                        type=[&#39;npy&#39;, &#39;zip&#39;],
                        accept_multiple_files=True,
                        key=&#34;assign_centers_files&#34;,
                        help=&#34;Upload .npy files containing junction centers or .zip files with multiple centers&#34;
                    )
                elif centers_option == &#34;Select folder&#34;:
                    st.markdown(&#34;**📁 Select Centers Folder:**&#34;)
                    centers_folder = st.text_input(
                        &#34;Centers folder path:&#34;,
                        key=&#34;assign_centers_folder&#34;,
                        help=&#34;Enter the path to the folder containing junction centers (will search subfolders for branch_centers_j*.npy files)&#34;
                    )

                # Assignment parameters
                st.markdown(&#34;**⚙️ Assignment Parameters:**&#34;)
                # Decision mode selector (mirror discover logic with selectbox)
                # Default from discover if available
                default_mode = &#34;hybrid&#34;
                if centers_option == &#34;Use session centers&#34; and st.session_state.get(&#34;analysis_results&#34;) and &#34;branches&#34; in st.session_state.analysis_results:
                    # Try to fetch from first matching junction block
                    for _jk, _bd in st.session_state.analysis_results[&#34;branches&#34;].items():
                        if isinstance(_bd, dict) and &#34;decision_mode&#34; in _bd:
                            default_mode = _bd.get(&#34;decision_mode&#34;, default_mode)
                            break

                col_decision_mode, col_decision_param = st.columns(2)
                with col_decision_mode:
                    assign_decision_mode = st.selectbox(
                        &#34;Decision Mode:&#34;,
                        [&#34;pathlen&#34;, &#34;radial&#34;, &#34;hybrid&#34;],
                        index=[&#34;pathlen&#34;,&#34;radial&#34;,&#34;hybrid&#34;].index(default_mode) if default_mode in [&#34;pathlen&#34;,&#34;radial&#34;,&#34;hybrid&#34;] else 2,
                        key=&#34;assign_decision_mode&#34;,
                        help=&#34;How to compute initial direction vectors for assignment&#34;
                    )

                # Auto-rediscovery (always available when uploading new trajectories)
                st.markdown(&#34;**🧭 Auto-Discover New Branches (optional):**&#34;)
                auto_col1, auto_col2, auto_col3 = st.columns(3)
                with auto_col1:
                    st.checkbox(
                        &#34;Enable auto-rediscover&#34;,
                        value=False,
                        key=&#34;assign_auto_rediscover&#34;,
                        help=&#34;If outlier assignments form a dense region of size ≥ min samples, rerun discovery for this junction using all trajectories (existing + newly uploaded).&#34;
                    )
                with auto_col2:
                    st.number_input(
                        &#34;Min samples for new branch&#34;,
                        value=5,
                        min_value=2,
                        max_value=100,
                        step=1,
                        key=&#34;assign_auto_min_samples&#34;,
                        help=&#34;Minimum outlier vectors required to trigger rediscovery.&#34;
                    )
                with auto_col3:
                    st.number_input(
                        &#34;Angle eps (deg)&#34;,
                        value=11.0,
                        min_value=1.0,
                        max_value=90.0,
                        step=1.0,
                        key=&#34;assign_auto_angle_eps&#34;,
                        help=&#34;Angular neighborhood size for detecting a dense outlier region.&#34;
                    )

                # Decision mode parameters in second column (mirror discover UI)
                with col_decision_param:
                    # Fetch defaults from discover if using session centers
                    pref_path_length = 100.0
                    pref_linger = 0.0
                    pref_epsilon = 0.05
                    if centers_option == &#34;Use session centers&#34; and st.session_state.get(&#34;analysis_results&#34;) and &#34;branches&#34; in st.session_state.analysis_results:
                        for _jk, _bd in st.session_state.analysis_results[&#34;branches&#34;].items():
                            if isinstance(_bd, dict):
                                if &#34;path_length&#34; in _bd:
                                    pref_path_length = float(_bd.get(&#34;path_length&#34;, pref_path_length))
                                if &#34;linger_delta&#34; in _bd:
                                    pref_linger = float(_bd.get(&#34;linger_delta&#34;, pref_linger))
                                if &#34;epsilon&#34; in _bd:
                                    pref_epsilon = float(_bd.get(&#34;epsilon&#34;, pref_epsilon))
                                break

                    if assign_decision_mode == &#34;radial&#34;:
                        st.info(&#34;ℹ️ Using r_outer from junctions (or stored discover results)&#34;)
                        assign_r_outer = None  # Will be fetched per junction
                        assign_epsilon = st.number_input(
                            &#34;Epsilon:&#34;,
                            value=pref_epsilon,
                            min_value=0.001,
                            max_value=1.0,
                            step=0.001,
                            format=&#34;%.3f&#34;,
                            key=&#34;assign_epsilon&#34;,
                            help=&#34;Minimum movement threshold&#34;
                        )
                        assign_path_length = 100.0
                        assign_linger_delta = 0.0
                    elif assign_decision_mode == &#34;pathlen&#34;:
                        assign_path_length = st.number_input(
                            &#34;Path Length:&#34;,
                            value=pref_path_length,
                            min_value=10.0,
                            max_value=500.0,
                            step=10.0,
                            key=&#34;assign_path_length&#34;,
                            help=&#34;Path length for decision point&#34;
                        )
                        assign_linger_delta = st.number_input(
                            &#34;Linger Delta:&#34;,
                            value=pref_linger,
                            min_value=0.0,
                            max_value=200.0,
                            step=1.0,
                            key=&#34;assign_linger_delta&#34;,
                            help=&#34;Linger distance beyond junction&#34;
                        )
                        assign_epsilon = st.number_input(
                            &#34;Epsilon:&#34;,
                            value=pref_epsilon,
                            min_value=0.001,
                            max_value=1.0,
                            step=0.001,
                            format=&#34;%.3f&#34;,
                            key=&#34;assign_epsilon&#34;,
                            help=&#34;Minimum movement threshold&#34;
                        )
                        assign_r_outer = None
                    elif assign_decision_mode == &#34;hybrid&#34;:
                        st.info(&#34;ℹ️ Using r_outer from junctions (or stored discover results)&#34;)
                        assign_r_outer = None  # Will be fetched per junction
                        assign_path_length = st.number_input(
                            &#34;Hybrid Path Length:&#34;,
                            value=pref_path_length,
                            min_value=10.0,
                            max_value=500.0,
                            step=10.0,
                            key=&#34;assign_path_length&#34;,
                            help=&#34;Path length for hybrid mode&#34;
                        )
                        assign_linger_delta = st.number_input(
                            &#34;Hybrid Linger Delta:&#34;,
                            value=pref_linger,
                            min_value=0.0,
                            max_value=200.0,
                            step=1.0,
                            key=&#34;assign_linger_delta&#34;,
                            help=&#34;Linger distance for hybrid mode&#34;
                        )
                        assign_epsilon = st.number_input(
                            &#34;Epsilon:&#34;,
                            value=pref_epsilon,
                            min_value=0.001,
                            max_value=1.0,
                            step=0.001,
                            format=&#34;%.3f&#34;,
                            key=&#34;assign_epsilon&#34;,
                            help=&#34;Minimum movement threshold&#34;
                        )

                # Junction parameters for external data (only show if not using session centers)
                if centers_option != &#34;Use session centers&#34;:
                    st.markdown(&#34;**🎯 Junction Parameters (for external data):**&#34;)
                    st.info(&#34;If using external trajectory data, you may need to specify junction parameters manually.&#34;)

                    col_junction_cx, col_junction_cz, col_junction_r = st.columns(3)

                    with col_junction_cx:
                        assign_junction_cx = st.number_input(
                            &#34;Junction Center X:&#34;,
                            value=0.0,
                            key=&#34;assign_junction_cx&#34;,
                            help=&#34;X coordinate of junction center&#34;
                        )

                    with col_junction_cz:
                        assign_junction_cz = st.number_input(
                            &#34;Junction Center Z:&#34;,
                            value=0.0,
                            key=&#34;assign_junction_cz&#34;,
                            help=&#34;Z coordinate of junction center&#34;
                        )

                    with col_junction_r:
                        assign_junction_r = st.number_input(
                            &#34;Junction Radius:&#34;,
                            value=50.0,
                            min_value=1.0,
                            max_value=200.0,
                            step=1.0,
                            key=&#34;assign_junction_r&#34;,
                            help=&#34;Radius of the junction area&#34;
                        )

                # Legacy code removed - using simplified interface above

        with col2:
            st.markdown(&#34;### Run Analysis&#34;)

            # Check if junctions are defined for analysis
            has_junctions = bool(st.session_state.junctions)

            if not has_junctions:
                st.warning(&#34;⚠️ **No junctions defined!** Please define junctions in the Junction Editor before running analysis.&#34;)
                st.info(&#34;💡 **Tip:** Go to the Junction Editor tab to define junctions for your analysis.&#34;)

            if st.button(&#34;🚀 Run Analysis&#34;, type=&#34;primary&#34;, disabled=not has_junctions):
                # Collect cluster method parameters
                cluster_params = {}
                if analysis_type == &#34;discover&#34; or analysis_type == &#34;enhanced&#34;:
                    if cluster_method == &#34;dbscan&#34;:
                        cluster_params = {&#34;eps&#34;: dbscan_eps, &#34;min_samples&#34;: dbscan_min_samples, &#34;angle_eps&#34;: dbscan_angle_eps}
                    elif cluster_method == &#34;kmeans&#34;:
                        cluster_params = {&#34;k&#34;: kmeans_k, &#34;k_min&#34;: kmeans_k_min, &#34;k_max&#34;: kmeans_k_max}
                    elif cluster_method == &#34;auto&#34;:
                        cluster_params = {&#34;k_min&#34;: auto_k_min, &#34;k_max&#34;: auto_k_max, &#34;min_sep_deg&#34;: auto_min_sep_deg, &#34;angle_eps&#34;: auto_angle_eps}

                # Collect decision mode parameters
                decision_params = {}
                if analysis_type == &#34;predict&#34;:
                    if decision_mode == &#34;radial&#34;:
                        decision_params = {&#34;r_outer&#34;: radial_r_outer, &#34;epsilon&#34;: radial_epsilon}
                    elif decision_mode == &#34;pathlen&#34;:
                        decision_params = {&#34;path_length&#34;: pathlen_path_length, &#34;linger_delta&#34;: pathlen_linger_delta}
                    elif decision_mode == &#34;hybrid&#34;:
                        decision_params = {&#34;r_outer&#34;: hybrid_r_outer, &#34;path_length&#34;: hybrid_path_length, &#34;linger_delta&#34;: hybrid_linger_delta}
                elif analysis_type == &#34;discover&#34; or analysis_type == &#34;enhanced&#34;:
                    if discover_decision_mode == &#34;radial&#34;:
                        decision_params = {&#34;r_outer&#34;: discover_r_outer, &#34;epsilon&#34;: discover_epsilon}
                    elif discover_decision_mode == &#34;pathlen&#34;:
                        decision_params = {&#34;path_length&#34;: discover_path_length, &#34;linger_delta&#34;: discover_linger_delta}
                    elif discover_decision_mode == &#34;hybrid&#34;:
                        decision_params = {&#34;r_outer&#34;: discover_hybrid_r_outer, &#34;path_length&#34;: discover_hybrid_path_length, &#34;linger_delta&#34;: discover_hybrid_linger_delta}

                # Add metrics-specific parameters if analysis type is metrics
                if analysis_type == &#34;metrics&#34;:
                    decision_params.update({
                        &#34;decision_mode&#34;: st.session_state.get(&#34;metrics_decision_mode&#34;, &#34;pathlen&#34;),
                        &#34;distance&#34;: st.session_state.get(&#34;metrics_distance&#34;, 100.0),
                        &#34;r_outer&#34;: st.session_state.get(&#34;metrics_r_outer&#34;, 50.0),
                        &#34;trend_window&#34;: st.session_state.get(&#34;metrics_trend_window&#34;, 5),
                        &#34;min_outward&#34;: st.session_state.get(&#34;metrics_min_outward&#34;, 0.0)
                    })

                # Collect assign parameters if needed
                assign_params = {}
                if analysis_type == &#34;assign&#34;:
                    # Get assignment parameters
                    assign_path_length = st.session_state.get(&#34;assign_path_length&#34;, 100.0)
                    assign_epsilon = st.session_state.get(&#34;assign_epsilon&#34;, 0.05)
                    # Auto-rediscover controls
                    assign_auto_rediscover = st.session_state.get(&#34;assign_auto_rediscover&#34;, False)
                    assign_auto_min_samples = st.session_state.get(&#34;assign_auto_min_samples&#34;, 5)
                    assign_auto_angle_eps = st.session_state.get(&#34;assign_auto_angle_eps&#34;, 15.0)

                    # Get trajectory and centers options
                    trajectory_option = st.session_state.get(&#34;assign_trajectory_option&#34;, &#34;Upload files&#34;)
                    centers_option = st.session_state.get(&#34;assign_centers_option&#34;, &#34;Use session centers&#34;)

                    # Collect trajectory data
                    trajectory_files = None
                    trajectory_folder = None
                    if trajectory_option == &#34;Upload files&#34;:
                        trajectory_files = st.session_state.get(&#34;assign_trajectory_files&#34;)
                    else:  # Select folder
                        trajectory_folder = st.session_state.get(&#34;assign_trajectory_folder&#34;)

                    # Collect centers data
                    centers_files = None
                    centers_folder = None
                    if centers_option == &#34;Upload files&#34;:
                        centers_files = st.session_state.get(&#34;assign_centers_files&#34;)
                    elif centers_option == &#34;Select folder&#34;:
                        centers_folder = st.session_state.get(&#34;assign_centers_folder&#34;)

                    assign_params = {
                        &#34;path_length&#34;: assign_path_length,
                        &#34;epsilon&#34;: assign_epsilon,
                        &#34;assign_scale&#34;: assign_scale,  # Add assign-specific scale factor
                        &#34;decision_mode&#34;: assign_decision_mode,
                        &#34;r_outer&#34;: assign_r_outer,
                        &#34;linger_delta&#34;: assign_linger_delta,
                        &#34;auto_rediscover&#34;: assign_auto_rediscover,
                        &#34;auto_min_samples&#34;: assign_auto_min_samples,
                        &#34;auto_angle_eps&#34;: assign_auto_angle_eps,
                        &#34;trajectory_option&#34;: trajectory_option,
                        &#34;centers_option&#34;: centers_option,
                        &#34;trajectory_files&#34;: trajectory_files,
                        &#34;trajectory_folder&#34;: trajectory_folder,
                        &#34;centers_files&#34;: centers_files,
                        &#34;centers_folder&#34;: centers_folder,
                        &#34;junction_cx&#34;: st.session_state.get(&#34;assign_junction_cx&#34;, 0.0),
                        &#34;junction_cz&#34;: st.session_state.get(&#34;assign_junction_cz&#34;, 0.0),
                        &#34;junction_r&#34;: st.session_state.get(&#34;assign_junction_r&#34;, 50.0)
                    }

                self.run_analysis(analysis_type, decision_mode, cluster_method, seed, cluster_params, decision_params, assign_params, discover_decision_mode)

            if st.session_state.analysis_results:
                st.markdown(&#34;### Analysis Results&#34;)
                st.success(&#34;✅ Analysis completed successfully!&#34;)


                # Show summary based on analysis type
                if analysis_type == &#34;discover&#34;:
                    if &#34;branches&#34; in st.session_state.analysis_results:
                        st.write(f&#34;**Branches discovered:** {len(st.session_state.analysis_results[&#39;branches&#39;])}&#34;)

                elif analysis_type == &#34;assign&#34;:
                    if &#34;assignments&#34; in st.session_state.analysis_results:
                        st.write(f&#34;**Trajectories assigned:** {len(st.session_state.analysis_results[&#39;assignments&#39;])}&#34;)

                        # Show debug information if available
                        if &#34;assign_debug_info&#34; in st.session_state and st.session_state.assign_debug_info:
                            st.markdown(&#34;### 🔍 Debug Information&#34;)
                            for junction_key, debug_info in st.session_state.assign_debug_info.items():
                                with st.expander(f&#34;Debug Info for {junction_key}&#34;, expanded=False):
                                    st.write(&#34;**Junction Parameters:**&#34;)
                                    st.write(f&#34;- Center: {debug_info[&#39;junction_params&#39;][&#39;center&#39;]}&#34;)
                                    st.write(f&#34;- Radius: {debug_info[&#39;junction_params&#39;][&#39;radius&#39;]}&#34;)
                                    st.write(f&#34;- R_outer: {debug_info[&#39;junction_params&#39;][&#39;r_outer&#39;]}&#34;)

                                    st.write(&#34;**Assignment Parameters:**&#34;)
                                    st.write(f&#34;- Path length: {debug_info[&#39;assignment_params&#39;][&#39;path_length&#39;]}&#34;)
                                    st.write(f&#34;- Epsilon: {debug_info[&#39;assignment_params&#39;][&#39;epsilon&#39;]}&#34;)

                                    st.write(&#34;**Data Info:**&#34;)
                                    st.write(f&#34;- Centers shape: {debug_info[&#39;data_info&#39;][&#39;centers_shape&#39;]}&#34;)
                                    st.write(f&#34;- Trajectories: {debug_info[&#39;data_info&#39;][&#39;trajectories&#39;]}&#34;)

                                    st.write(&#34;**Assignment Distribution:**&#34;)
                                    total_trajectories = sum(debug_info[&#39;assignment_distribution&#39;].values())
                                    for branch, count in debug_info[&#39;assignment_distribution&#39;].items():
                                        percentage = (count / total_trajectories) * 100
                                        st.write(f&#34;- Branch {branch}: {count} trajectories ({percentage:.1f}%)&#34;)

                                    # Add troubleshooting info if most trajectories are -2/-1
                                    neg2_count = debug_info[&#39;assignment_distribution&#39;].get(-2, 0)
                                    neg1_count = debug_info[&#39;assignment_distribution&#39;].get(-1, 0)

                                    if (neg2_count + neg1_count) / total_trajectories &gt; 0.8:  # More than 80% are -2/-1
                                        st.warning(&#34;⚠️ **Troubleshooting:** Most trajectories are getting -2/-1 assignments!&#34;)
                                        st.write(&#34;**Possible solutions:**&#34;)
                                        st.write(&#34;1. **Increase junction radius** - Current radius might be too small&#34;)
                                        st.write(&#34;2. **Adjust junction center** - Center might not match trajectory paths&#34;)
                                        st.write(&#34;3. **Check trajectory data** - Ensure trajectories actually pass through junction area&#34;)
                                        st.write(&#34;4. **Use manual junction parameters** - Try different center coordinates and radius&#34;)

                                    st.write(&#34;**First 10 Assignments:**&#34;)
                                    if debug_info[&#39;assignments_sample&#39;]:
                                        st.dataframe(pd.DataFrame(debug_info[&#39;assignments_sample&#39;]), width=&#39;stretch&#39;)

                elif analysis_type == &#34;metrics&#34;:
                    if &#34;metrics&#34; in st.session_state.analysis_results:
                        st.write(f&#34;**Metrics computed:** {len(st.session_state.analysis_results[&#39;metrics&#39;])}&#34;)

                        # Show debug information for metrics
                        st.markdown(&#34;---&#34;)
                        st.markdown(&#34;### 🔍 Debug Information&#34;)

                        # Get debug information from the first few trajectories
                        if st.session_state.trajectories:
                            st.write(&#34;**Debug Status:**&#34;)
                            st.write(f&#34;- Total trajectories: {len(st.session_state.trajectories)}&#34;)

                            # Sample first 5 trajectories for debug
                            time_data_debug = []
                            for i, traj in enumerate(st.session_state.trajectories[:5]):
                                time_debug = {
                                    &#34;trajectory_id&#34;: i,
                                    &#34;time_data_type&#34;: str(type(traj.t)),
                                    &#34;time_data_shape&#34;: traj.t.shape if traj.t is not None else None,
                                    &#34;time_data_sample&#34;: traj.t[:3].tolist() if traj.t is not None and len(traj.t) &gt; 0 else None,
                                    &#34;time_data_dtype&#34;: str(traj.t.dtype) if traj.t is not None else None,
                                    &#34;time_is_none&#34;: traj.t is None,
                                    &#34;time_length&#34;: len(traj.t) if traj.t is not None else 0,
                                    # Add position data diagnostics
                                    &#34;x_data_type&#34;: str(type(traj.x)),
                                    &#34;x_data_shape&#34;: traj.x.shape if traj.x is not None else None,
                                    &#34;x_data_sample&#34;: traj.x[:3].tolist() if traj.x is not None and len(traj.x) &gt; 0 else None,
                                    &#34;x_data_dtype&#34;: str(traj.x.dtype) if traj.x is not None else None,
                                    &#34;x_is_none&#34;: traj.x is None,
                                    &#34;x_length&#34;: len(traj.x) if traj.x is not None else 0,
                                    &#34;z_data_type&#34;: str(type(traj.z)),
                                    &#34;z_data_shape&#34;: traj.z.shape if traj.z is not None else None,
                                    &#34;z_data_sample&#34;: traj.z[:3].tolist() if traj.z is not None and len(traj.z) &gt; 0 else None,
                                    &#34;z_data_dtype&#34;: str(traj.z.dtype) if traj.z is not None else None,
                                    &#34;z_is_none&#34;: traj.z is None,
                                    &#34;z_length&#34;: len(traj.z) if traj.z is not None else 0
                                }
                                time_data_debug.append(time_debug)

                            st.write(f&#34;- time_data_debug length: {len(time_data_debug)}&#34;)

                            if time_data_debug:
                                with st.expander(&#34;🔍 Trajectory Data Debug Information&#34;, expanded=True):
                                    st.write(&#34;**First 5 trajectories data analysis:**&#34;)
                                    for debug_info in time_data_debug:
                                        st.write(f&#34;**Trajectory {debug_info[&#39;trajectory_id&#39;]}:**&#34;)

                                        # Time data
                                        st.write(&#34;**Time Data:**&#34;)
                                        st.write(f&#34;- Is None: {debug_info[&#39;time_is_none&#39;]}&#34;)
                                        st.write(f&#34;- Length: {debug_info[&#39;time_length&#39;]}&#34;)
                                        st.write(f&#34;- Type: {debug_info[&#39;time_data_type&#39;]}&#34;)
                                        st.write(f&#34;- Shape: {debug_info[&#39;time_data_shape&#39;]}&#34;)
                                        st.write(f&#34;- Dtype: {debug_info[&#39;time_data_dtype&#39;]}&#34;)
                                        st.write(f&#34;- Sample: {debug_info[&#39;time_data_sample&#39;]}&#34;)

                                        # Position data
                                        st.write(&#34;**Position Data (X):**&#34;)
                                        st.write(f&#34;- Is None: {debug_info[&#39;x_is_none&#39;]}&#34;)
                                        st.write(f&#34;- Length: {debug_info[&#39;x_length&#39;]}&#34;)
                                        st.write(f&#34;- Type: {debug_info[&#39;x_data_type&#39;]}&#34;)
                                        st.write(f&#34;- Shape: {debug_info[&#39;x_data_shape&#39;]}&#34;)
                                        st.write(f&#34;- Dtype: {debug_info[&#39;x_data_dtype&#39;]}&#34;)
                                        st.write(f&#34;- Sample: {debug_info[&#39;x_data_sample&#39;]}&#34;)

                                        st.write(&#34;**Position Data (Z):**&#34;)
                                        st.write(f&#34;- Is None: {debug_info[&#39;z_is_none&#39;]}&#34;)
                                        st.write(f&#34;- Length: {debug_info[&#39;z_length&#39;]}&#34;)
                                        st.write(f&#34;- Type: {debug_info[&#39;z_data_type&#39;]}&#34;)
                                        st.write(f&#34;- Shape: {debug_info[&#39;z_data_shape&#39;]}&#34;)
                                        st.write(f&#34;- Dtype: {debug_info[&#39;z_data_dtype&#39;]}&#34;)
                                        st.write(f&#34;- Sample: {debug_info[&#39;z_data_sample&#39;]}&#34;)

                                        st.write(&#34;---&#34;)
                            else:
                                st.info(&#34;No time data debug information available&#34;)

                elif analysis_type == &#34;gaze&#34;:
                    if &#34;gaze_results&#34; in st.session_state.analysis_results:
                        st.write(f&#34;**Gaze analysis completed**&#34;)

                elif analysis_type == &#34;predict&#34;:
                    if &#34;choice_patterns&#34; in st.session_state.analysis_results:
                        st.write(f&#34;**Choice patterns analyzed**&#34;)

    def run_analysis(self, analysis_type: str, decision_mode: str, cluster_method: str, seed: int, cluster_params: dict = None, decision_params: dict = None, assign_params: dict = None, discover_decision_mode: str = &#34;hybrid&#34;):
        &#34;&#34;&#34;Run the selected analysis&#34;&#34;&#34;
        try:
            with st.spinner(f&#34;Running {analysis_type} analysis...&#34;):

                if analysis_type == &#34;discover&#34;:
                    # Unified multi-junction discovery for consistent decisions and assignments
                    import os
                    output_dir = &#34;gui_outputs&#34;
                    os.makedirs(output_dir, exist_ok=True)

                    # Cluster/decision parameters
                    k_value = cluster_params.get(&#34;k&#34;, 3) if cluster_params else 3
                    min_samples = cluster_params.get(&#34;min_samples&#34;, 5) if cluster_params else 5
                    k_min = cluster_params.get(&#34;k_min&#34;, 2) if cluster_params else 2
                    k_max = cluster_params.get(&#34;k_max&#34;, 6) if cluster_params else 6
                    min_sep_deg = cluster_params.get(&#34;min_sep_deg&#34;, 12.0) if cluster_params else 12.0
                    angle_eps = cluster_params.get(&#34;angle_eps&#34;, 15.0) if cluster_params else 15.0

                    path_length = decision_params.get(&#34;path_length&#34;, 100.0) if decision_params else 100.0
                    epsilon = decision_params.get(&#34;epsilon&#34;, 0.05) if decision_params else 0.05
                    linger_delta = decision_params.get(&#34;linger_delta&#34;, 5.0) if decision_params else 5.0
                    r_outer_list = [st.session_state.junction_r_outer.get(i, 50.0) for i in range(len(st.session_state.junctions))]

                    # Run consolidated discovery
                    chain_df, centers_list, decisions_chain_df = discover_decision_chain(
                        trajectories=st.session_state.trajectories,
                        junctions=st.session_state.junctions,
                        path_length=path_length,
                        epsilon=epsilon,
                        seed=seed,
                        decision_mode=discover_decision_mode,
                        r_outer_list=r_outer_list,
                        linger_delta=linger_delta,
                        out_dir=output_dir,
                        cluster_method=cluster_method,
                        k=k_value,
                        k_min=k_min,
                        k_max=k_max,
                        min_sep_deg=min_sep_deg,
                        angle_eps=angle_eps,
                        min_samples=min_samples,
                    )

                    # Build per-junction results view from chain_df/centers_list
                    results = {}
                    for i, junction in enumerate(st.session_state.junctions):
                        junction_key = f&#34;junction_{i}&#34;
                        col = f&#34;branch_j{i}&#34;
                        if col in chain_df.columns:
                            df_i = chain_df[[&#34;trajectory&#34;, col]].copy()
                            df_i = df_i.rename(columns={col: &#34;branch&#34;})
                            # summary counts for main branches (&gt;=0)
                            vc = df_i[df_i[&#34;branch&#34;] &gt;= 0][&#34;branch&#34;].value_counts().sort_index()
                            summary_i = vc.rename_axis(&#34;branch&#34;).to_frame(&#34;count&#34;).reset_index()
                            total_i = int(summary_i[&#34;count&#34;].sum()) if len(summary_i) else 0
                            summary_i[&#34;percent&#34;] = summary_i[&#34;count&#34;] / max(1, total_i) * 100.0
                        else:
                            import pandas as _pd
                            df_i = _pd.DataFrame(columns=[&#34;trajectory&#34;,&#34;branch&#34;])  # empty
                            summary_i = _pd.DataFrame(columns=[&#34;branch&#34;,&#34;count&#34;,&#34;percent&#34;])  # empty

                        results[junction_key] = {
                            &#34;assignments&#34;: df_i,
                            &#34;summary&#34;: summary_i,
                            &#34;centers&#34;: centers_list[i] if i &lt; len(centers_list) else None,
                            &#34;junction&#34;: junction,
                            &#34;r_outer&#34;: r_outer_list[i] if i &lt; len(r_outer_list) else None,
                            &#34;path_length&#34;: path_length,
                            &#34;epsilon&#34;: epsilon,
                            &#34;linger_delta&#34;: linger_delta,  # Store linger_delta for gaze analysis
                            &#34;decision_mode&#34;: discover_decision_mode,
                            &#34;scale&#34;: st.session_state.get(&#34;scale_factor&#34;, 1.0),
                        }

                    # Flow graph generation removed - discover should only do discovery, not flow analysis

                    # Persist results and decisions for gaze reuse
                    if st.session_state.analysis_results is None:
                        st.session_state.analysis_results = {}
                    st.session_state.analysis_results[&#34;branches&#34;] = results

                    # Debug: Check chain_decisions DataFrame
                    st.write(f&#34;🔍 **Chain Decisions Debug:**&#34;)
                    st.write(f&#34;- decisions_chain_df is not None: {decisions_chain_df is not None}&#34;)
                    if decisions_chain_df is not None:
                        st.write(f&#34;- decisions_chain_df length: {len(decisions_chain_df)}&#34;)
                        st.write(f&#34;- decisions_chain_df columns: {list(decisions_chain_df.columns)}&#34;)
                        if not decisions_chain_df.empty:
                            st.write(f&#34;- Junction indices in decisions_chain_df: {sorted(decisions_chain_df[&#39;junction_index&#39;].unique())}&#34;)
                        else:
                            st.write(&#34;- decisions_chain_df is empty!&#34;)
                    else:
                        st.write(&#34;- decisions_chain_df is None!&#34;)

                    # Store branch assignments (chain_df) as chain_decisions for gaze analysis
                    if chain_df is not None and len(chain_df) &gt; 0:
                        st.session_state.analysis_results.setdefault(&#34;branches&#34;, {})
                        st.session_state.analysis_results[&#34;branches&#34;][&#34;chain_decisions&#34;] = chain_df
                        st.write(f&#34;✅ **Stored branch assignments (chain_df) with {len(chain_df)} rows in session state**&#34;)
                        st.write(f&#34;🔍 **Branch assignment columns:** {list(chain_df.columns)}&#34;)

                        # Debug: Check for branch_jX columns specifically
                        branch_cols = [col for col in chain_df.columns if col.startswith(&#39;branch_j&#39;)]
                        st.write(f&#34;🔍 **Branch columns found:** {branch_cols}&#34;)
                        if len(branch_cols) &gt; 0:
                            st.write(f&#34;🔍 **Sample branch data:** {chain_df[branch_cols].head()}&#34;)
                        else:
                            st.error(&#34;❌ **No branch_jX columns found in chain_df!**&#34;)
                    else:
                        st.write(f&#34;❌ **Not storing branch assignments - chain_df is None or empty**&#34;)

                    # Also store decision points separately for reference
                    if decisions_chain_df is not None and len(decisions_chain_df) &gt; 0:
                        st.session_state.analysis_results.setdefault(&#34;branches&#34;, {})
                        st.session_state.analysis_results[&#34;branches&#34;][&#34;decision_points&#34;] = decisions_chain_df
                        st.write(f&#34;✅ **Stored decision points with {len(decisions_chain_df)} rows in session state**&#34;)
                    else:
                        st.write(f&#34;❌ **Not storing decision points - DataFrame is None or empty**&#34;)

                    # Add debugging information for flow analysis
                    try:
                        st.markdown(&#34;#### 🔍 Flow Analysis Debug&#34;)

                        # Count trajectories that visit multiple junctions
                        multi_junction_trajectories = 0
                        junction_visit_counts = {}

                        for i, junction in enumerate(st.session_state.junctions):
                            junction_key = f&#34;junction_{i}&#34;
                            if junction_key in results and &#34;assignments&#34; in results[junction_key]:
                                assignments = results[junction_key][&#34;assignments&#34;]
                                if not assignments.empty:
                                    visited_trajectories = set(assignments[&#34;trajectory&#34;].unique())
                                    junction_visit_counts[i] = visited_trajectories

                        # Find trajectories that visit multiple junctions
                        all_trajectories = set()
                        for trajectories in junction_visit_counts.values():
                            all_trajectories.update(trajectories)

                        for traj_id in all_trajectories:
                            visited_junctions = [i for i, trajs in junction_visit_counts.items() if traj_id in trajs]
                            if len(visited_junctions) &gt; 1:
                                multi_junction_trajectories += 1

                        st.info(f&#34;📊 **Flow Analysis Summary:**&#34;)
                        st.write(f&#34;- Total trajectories: {len(st.session_state.trajectories)}&#34;)
                        st.write(f&#34;- Trajectories visiting multiple junctions: {multi_junction_trajectories}&#34;)
                        st.write(f&#34;- Junction visit counts: {[len(trajs) for trajs in junction_visit_counts.values()]}&#34;)

                        if multi_junction_trajectories == 0:
                            st.warning(&#34;⚠️ **No trajectories visit multiple junctions!** This explains the zero flow matrix.&#34;)
                            st.write(&#34;**Possible causes:**&#34;)
                            st.write(&#34;1. Trajectories are too short to reach multiple junctions&#34;)
                            st.write(&#34;2. Junction r_outer values are too small&#34;)
                            st.write(&#34;3. Junctions are too far apart&#34;)
                            st.write(&#34;4. Trajectory data needs different scaling&#34;)

                    except Exception as e:
                        st.warning(f&#34;Debug analysis failed: {str(e)}&#34;)

                    #st.success(f&#34;✅ Discover analysis completed successfully for all {len(st.session_state.junctions)} junctions!&#34;)
                    self.generate_cli_command(&#34;discover&#34;, results, cluster_method, cluster_params, decision_mode, decision_params)

                elif analysis_type == &#34;assign&#34;:
                    # Assign trajectories to branches using simplified interface
                    import numpy as np

                    # Load trajectories based on trajectory option
                    trajectories = self.load_assign_trajectories(assign_params)
                    if trajectories is None:
                        return

                    # Load centers based on centers option
                    centers_dict = self.load_assign_centers(assign_params)
                    if centers_dict is None:
                        return

                    # Validate that we have compatible data
                    if not trajectories:
                        st.error(&#34;❌ No trajectories loaded for assignment&#34;)
                        return

                    if not centers_dict:
                        st.error(&#34;❌ No junction centers loaded for assignment&#34;)
                        return

                    # Get assignment parameters
                    path_length = assign_params.get(&#34;path_length&#34;, 100.0) if assign_params else 100.0
                    epsilon = assign_params.get(&#34;epsilon&#34;, 0.05) if assign_params else 0.05

                    results = {}
                    successful_assignments = 0

                    # Process each junction
                    for junction_key, centers in centers_dict.items():
                        try:
                            # Extract junction number from key (e.g., &#34;junction_0&#34; -&gt; 0)
                            junction_num = int(junction_key.split(&#39;_&#39;)[1])

                            # Get assignment parameters - use stored values from discover analysis if available
                            centers_option = assign_params.get(&#34;centers_option&#34;, &#34;Use session centers&#34;) if assign_params else &#34;Use session centers&#34;
                            if centers_option == &#34;Use session centers&#34;:
                                if junction_key in st.session_state.analysis_results[&#34;branches&#34;]:
                                    branch_data = st.session_state.analysis_results[&#34;branches&#34;][junction_key]
                                    # Use stored parameters from discover analysis
                                    stored_path_length = branch_data.get(&#34;path_length&#34;, path_length)
                                    stored_epsilon = branch_data.get(&#34;epsilon&#34;, epsilon)
                                    stored_scale = branch_data.get(&#34;scale&#34;, 1.0)
                                    st.info(f&#34;📊 Using assignment parameters from discover analysis: path_length={stored_path_length:.1f}, epsilon={stored_epsilon:.3f}, scale={stored_scale:.1f}&#34;)
                                else:
                                    stored_path_length = path_length
                                    stored_epsilon = epsilon
                                    stored_scale = 1.0
                            else:
                                stored_path_length = path_length
                                stored_epsilon = epsilon
                                stored_scale = 1.0

                            # Get junction and r_outer - prioritize stored parameters from discover analysis
                            if centers_option == &#34;Use session centers&#34;:
                                # Try to get junction parameters from discover analysis first
                                if junction_key in st.session_state.analysis_results[&#34;branches&#34;]:
                                    branch_data = st.session_state.analysis_results[&#34;branches&#34;][junction_key]
                                    if &#34;junction&#34; in branch_data and &#34;r_outer&#34; in branch_data:
                                        junction = branch_data[&#34;junction&#34;]
                                        r_outer = branch_data[&#34;r_outer&#34;]
                                        stored_scale = branch_data.get(&#34;scale&#34;, 1.0)
                                        st.info(f&#34;📊 Using junction parameters from discover analysis: center=({junction.cx:.1f}, {junction.cz:.1f}), radius={junction.r:.1f}, r_outer={r_outer:.1f}&#34;)
                                        st.info(f&#34;📊 Scale factor from discover analysis: {stored_scale:.1f}&#34;)

                                        # Check if current trajectories use different scale factor
                                        if hasattr(st.session_state, &#39;trajectories&#39;) and st.session_state.trajectories:
                                            # Estimate scale factor from trajectory coordinates
                                            sample_traj = st.session_state.trajectories[0]
                                            if hasattr(sample_traj, &#39;x&#39;) and len(sample_traj.x) &gt; 0:
                                                # Simple heuristic: if coordinates are much larger than expected, scale might be different
                                                max_coord = max(abs(sample_traj.x.max()), abs(sample_traj.z.max()))
                                                if max_coord &gt; 1000 and stored_scale &lt; 0.5:
                                                    st.warning(f&#34;⚠️ Scale factor mismatch detected! Discover used {stored_scale:.1f}, but current trajectories appear to use a different scale.&#34;)
                                                    st.warning(f&#34;⚠️ This may cause assignment failures. Consider using the same scale factor as discover analysis.&#34;)
                                    else:
                                        # Fallback to session state junctions
                                        if junction_num &lt; len(st.session_state.junctions):
                                            junction = st.session_state.junctions[junction_num]
                                            r_outer = st.session_state.junction_r_outer.get(junction_num, 50.0)
                                        else:
                                            st.error(f&#34;❌ No junction parameters found for {junction_key}&#34;)
                                            continue
                                else:
                                    st.error(f&#34;❌ No discover analysis data found for {junction_key}&#34;)
                                    continue
                            elif junction_num &lt; len(st.session_state.junctions):
                                junction = st.session_state.junctions[junction_num]
                                r_outer = st.session_state.junction_r_outer.get(junction_num, 50.0)
                            else:
                                # If using external data, use manual parameters or estimate from trajectory data
                                manual_cx = assign_params.get(&#34;junction_cx&#34;, 0.0)
                                manual_cz = assign_params.get(&#34;junction_cz&#34;, 0.0)
                                manual_r = assign_params.get(&#34;junction_r&#34;, 50.0)

                                if manual_cx != 0.0 or manual_cz != 0.0 or manual_r != 50.0:
                                    # Use manual parameters
                                    junction = Circle(cx=manual_cx, cz=manual_cz, r=manual_r)
                                    r_outer = manual_r * 2.0
                                    st.info(f&#34;📊 Using manual junction: center=({manual_cx:.1f}, {manual_cz:.1f}), radius={manual_r:.1f}&#34;)
                                else:
                                    # Estimate junction from trajectory data
                                    st.warning(f&#34;⚠️ No junction defined for {junction_key}. Attempting to estimate from trajectory data...&#34;)

                                    # Estimate junction center from trajectory data
                                    all_x = np.concatenate([tr.x for tr in trajectories])
                                    all_z = np.concatenate([tr.z for tr in trajectories])

                                    # Show trajectory data range for debugging
                                    st.info(f&#34;📊 Trajectory data range:&#34;)
                                    st.write(f&#34;- X range: {np.min(all_x):.1f} to {np.max(all_x):.1f}&#34;)
                                    st.write(f&#34;- Z range: {np.min(all_z):.1f} to {np.max(all_z):.1f}&#34;)

                                    # Use median as center (more robust than mean)
                                    estimated_cx = float(np.median(all_x))
                                    estimated_cz = float(np.median(all_z))

                                    # Estimate radius based on data spread - use a more conservative approach
                                    distances = np.sqrt((all_x - estimated_cx)**2 + (all_z - estimated_cz)**2)
                                    estimated_r = float(np.percentile(distances, 75))  # Use 75th percentile for radius

                                    junction = Circle(cx=estimated_cx, cz=estimated_cz, r=max(estimated_r, 20.0))
                                    r_outer = estimated_r * 3.0  # Make r_outer much larger than junction radius

                                    st.info(f&#34;📊 Estimated junction: center=({estimated_cx:.1f}, {estimated_cz:.1f}), radius={estimated_r:.1f}&#34;)
                                    st.warning(f&#34;⚠️ Using estimated junction for {junction_key}. Consider defining junctions manually for better results.&#34;)
                                    st.info(f&#34;💡 Tip: If trajectories still get -2/-1, try increasing the junction radius or adjusting the center coordinates.&#34;)

                            # Create output directory for this junction
                            import os
                            out_dir = os.path.join(&#34;gui_outputs&#34;, f&#34;junction_{junction_num}&#34;)
                            os.makedirs(out_dir, exist_ok=True)

                            # Run assignment
                            # Determine decision parameters to use
                            dm = assign_params.get(&#34;decision_mode&#34;, &#34;pathlen&#34;)
                            ld = assign_params.get(&#34;linger_delta&#34;, 0.0)
                            dm_r_outer = assign_params.get(&#34;r_outer&#34;, r_outer)

                            assignments = assign_branches(
                                trajectories=trajectories,
                                centers=centers,
                                junction=junction,
                                path_length=stored_path_length,
                                epsilon=stored_epsilon,
                                decision_mode=dm,
                                r_outer=dm_r_outer,
                                linger_delta=ld,
                                out_dir=out_dir
                            )

                            # Optional: auto-rediscover if outlier cluster among new assignments is large enough
                            try:
                                auto_flag = st.session_state.get(&#34;assign_auto_rediscover&#34;, False)
                                min_samples_new = int(st.session_state.get(&#34;assign_auto_min_samples&#34;, 5))
                                angle_eps_new = float(st.session_state.get(&#34;assign_auto_angle_eps&#34;, 15.0))
                                # Auto-rediscover: detect dense outlier regions among newly uploaded trajectories
                                if auto_flag and len(assignments) &gt; 0:
                                    # Identify outliers (-1) that entered junction and have usable vectors
                                    from verta.verta_decisions import compute_assignment_vectors
                                    from verta.verta_clustering import cluster_angles_dbscan
                                    # Compute vectors for these trajectories with same decision params
                                    vec_df = compute_assignment_vectors(
                                        trajectories=trajectories,
                                        junction=junction,
                                        path_length=stored_path_length,
                                        decision_mode=dm,
                                        r_outer=dm_r_outer,
                                        epsilon=stored_epsilon,
                                    )
                                    # Merge to filter to current outliers only
                                    outlier_ids = set(assignments[assignments[&#34;branch&#34;] == -1][&#34;trajectory&#34;].tolist())
                                    if outlier_ids:
                                        use = vec_df[(vec_df[&#34;trajectory&#34;].isin(outlier_ids)) &amp; (vec_df[&#34;entered&#34;]) &amp; (vec_df[&#34;usable&#34;])].copy()
                                        if len(use) &gt;= min_samples_new:
                                            V = np.vstack([use[[&#34;vx&#34;,&#34;vz&#34;]].to_numpy()]) if len(use) else np.zeros((0,2))
                                            if V.size:
                                                labels_o, centers_o = cluster_angles_dbscan(V, eps_deg=angle_eps_new, min_samples=min_samples_new)
                                                # If any valid cluster exists (label &gt;=0), trigger rediscovery using all available trajectories
                                                if (labels_o &gt;= 0).any():
                                                    st.info(f&#34;🔄 Auto-rediscover triggered for {junction_key}: detected dense outlier region (min_samples={min_samples_new}, angle_eps={angle_eps_new}°)&#34;)
                                                    # Build combined trajectory set: existing session trajectories + newly provided
                                                    all_trajs = []
                                                    if hasattr(st.session_state, &#39;trajectories&#39;) and st.session_state.trajectories:
                                                        all_trajs.extend(st.session_state.trajectories)
                                                    all_trajs.extend([t for t in trajectories if t not in all_trajs])
                                                    # Rerun discovery for this junction
                                                    from verta.verta_decisions import discover_branches
                                                    new_assign, _sum, new_centers = discover_branches(
                                                        trajectories=all_trajs,
                                                        junction=junction,
                                                        k=centers.shape[0] if centers is not None and centers.size else 3,
                                                        path_length=stored_path_length,
                                                        epsilon=stored_epsilon,
                                                        seed=seed,
                                                        decision_mode=dm,
                                                        r_outer=dm_r_outer,
                                                        out_dir=out_dir,
                                                        cluster_method=cluster_method,
                                                        k_min=st.session_state.get(&#34;discover_k_min&#34;, 2),
                                                        k_max=st.session_state.get(&#34;discover_k_max&#34;, 6),
                                                        min_sep_deg=st.session_state.get(&#34;discover_min_sep_deg&#34;, 12.0),
                                                        angle_eps=st.session_state.get(&#34;discover_angle_eps&#34;, 15.0),
                                                        min_samples=min_samples_new,
                                                        junction_number=junction_num,
                                                        all_junctions=[junction]
                                                    )
                                                    # Update centers to the rediscovered ones and reassign current trajectories for display
                                                    centers = new_centers
                                                    assignments = assign_branches(
                                                        trajectories=trajectories,
                                                        centers=centers,
                                                        junction=junction,
                                                        path_length=stored_path_length,
                                                        epsilon=stored_epsilon,
                                                        decision_mode=dm,
                                                        r_outer=dm_r_outer,
                                                        linger_delta=ld,
                                                        out_dir=out_dir
                                                    )
                                                    st.warning(&#34;⚠️ Branch IDs may have been renumbered due to rediscovery.&#34;)
                            except Exception as _e:
                                # Keep assignment results even if auto-rediscover path fails
                                pass

                            # Enhanced debugging for assignment issues
                            if assignments is not None and len(assignments) &gt; 0:
                                # Check if assignments is a string (error message) or pandas DataFrame
                                if isinstance(assignments, str):
                                    st.error(f&#34;🚨 **Assignment Error:** {assignments}&#34;)
                                    st.error(&#34;This indicates the assign_branches function returned an error message instead of assignment results.&#34;)
                                    st.error(&#34;Check the assign_branches function implementation or input parameters.&#34;)
                                elif hasattr(assignments, &#39;iterrows&#39;):  # pandas DataFrame
                                    # Count assignment types from DataFrame
                                    assignment_counts = assignments[&#39;branch&#39;].value_counts().to_dict()

                                    total_trajectories = len(assignments)
                                    neg2_count = assignment_counts.get(-2, 0)
                                    neg1_count = assignment_counts.get(-1, 0)

                                    # If most trajectories are -2/-1, provide enhanced debugging
                                    if (neg2_count + neg1_count) / total_trajectories &gt; 0.8:
                                        st.error(f&#34;🚨 **Assignment Issue Detected for {junction_key}:**&#34;)
                                        st.error(f&#34;   -2 (never entered): {neg2_count} trajectories ({neg2_count/total_trajectories*100:.1f}%)&#34;)
                                        st.error(f&#34;   -1 (no usable vector): {neg1_count} trajectories ({neg1_count/total_trajectories*100:.1f}%)&#34;)

                                        # Enhanced debugging analysis
                                        st.info(f&#34;🔍 **Enhanced Debug Analysis:**&#34;)

                                        # Show trajectory data ranges
                                        all_x = []
                                        all_z = []
                                        for traj in trajectories:
                                            all_x.extend(traj.x)
                                            all_z.extend(traj.z)

                                        if all_x and all_z:
                                            st.info(f&#34;📊 **Trajectory Data Ranges:**&#34;)
                                            st.info(f&#34;   X: {min(all_x):.1f} to {max(all_x):.1f} (range: {max(all_x)-min(all_x):.1f})&#34;)
                                            st.info(f&#34;   Z: {min(all_z):.1f} to {max(all_z):.1f} (range: {max(all_z)-min(all_z):.1f})&#34;)
                                            st.info(f&#34;   Total points: {len(all_x)}&#34;)

                                            # Show how many trajectories actually pass through the junction area
                                            trajectories_in_junction = 0
                                            trajectories_with_usable_vectors = 0

                                            for traj in trajectories:
                                                # Check if trajectory passes through junction area
                                                distances = np.sqrt((traj.x - junction.cx)**2 + (traj.z - junction.cz)**2)
                                                if np.any(distances &lt;= junction.r):
                                                    trajectories_in_junction += 1

                                                    # Check if trajectory has usable vectors (length &gt; epsilon)
                                                    if len(traj.x) &gt; 1:
                                                        dx = np.diff(traj.x)
                                                        dz = np.diff(traj.z)
                                                        movement = np.sqrt(dx**2 + dz**2)
                                                        if np.any(movement &gt; stored_epsilon):
                                                            trajectories_with_usable_vectors += 1

                                            st.info(f&#34;📊 **Junction Analysis:**&#34;)
                                            st.info(f&#34;   Trajectories passing through junction: {trajectories_in_junction}/{len(trajectories)}&#34;)
                                            st.info(f&#34;   Trajectories with usable vectors: {trajectories_with_usable_vectors}/{len(trajectories)}&#34;)

                                            # Analyze movement patterns for -1 assignments
                                            if neg1_count &gt; neg2_count:  # More -1 than -2 assignments
                                                st.error(f&#34;🚨 **Critical Issue: Most trajectories are -1 (entered junction but no usable vectors)!**&#34;)

                                                # Analyze movement patterns
                                                st.info(f&#34;🔍 **Movement Analysis:**&#34;)
                                                all_movements = []
                                                nan_trajectories = 0
                                                for traj in trajectories:
                                                    if len(traj.x) &gt; 1:
                                                        # Check for NaN values
                                                        if np.any(np.isnan(traj.x)) or np.any(np.isnan(traj.z)):
                                                            nan_trajectories += 1
                                                            continue

                                                        dx = np.diff(traj.x)
                                                        dz = np.diff(traj.z)
                                                        movement = np.sqrt(dx**2 + dz**2)
                                                        # Filter out NaN movements
                                                        valid_movements = movement[~np.isnan(movement)]
                                                        all_movements.extend(valid_movements)

                                                if nan_trajectories &gt; 0:
                                                    st.error(f&#34;🚨 **CRITICAL: {nan_trajectories} trajectories contain NaN coordinates!**&#34;)
                                                    st.error(&#34;This will cause assignment failures. Check your trajectory data for missing/invalid coordinates.&#34;)

                                                if all_movements:
                                                    percentile_5 = np.percentile(all_movements, 5)
                                                    percentile_10 = np.percentile(all_movements, 10)
                                                    percentile_25 = np.percentile(all_movements, 25)
                                                    mean_movement = np.mean(all_movements)

                                                    st.info(f&#34;📊 **Movement Statistics:**&#34;)
                                                    st.info(f&#34;   Mean movement: {mean_movement:.4f}&#34;)
                                                    st.info(f&#34;   5th percentile: {percentile_5:.4f}&#34;)
                                                    st.info(f&#34;   10th percentile: {percentile_10:.4f}&#34;)
                                                    st.info(f&#34;   25th percentile: {percentile_25:.4f}&#34;)
                                                    st.info(f&#34;   Current epsilon: {stored_epsilon:.3f}&#34;)

                                                    # Suggest epsilon adjustment
                                                    if stored_epsilon &gt; percentile_25:
                                                        suggested_epsilon = percentile_10
                                                        st.warning(f&#34;⚠️ **Epsilon too high!** Try: {suggested_epsilon:.4f} (current: {stored_epsilon:.3f})&#34;)
                                                    elif stored_epsilon &lt; percentile_5:
                                                        suggested_epsilon = percentile_10
                                                        st.warning(f&#34;⚠️ **Epsilon too low!** Try: {suggested_epsilon:.4f} (current: {stored_epsilon:.3f})&#34;)
                                                    else:
                                                        suggested_epsilon = percentile_5
                                                        st.warning(f&#34;⚠️ **Try lower epsilon:** {suggested_epsilon:.4f} (current: {stored_epsilon:.3f})&#34;)
                                                else:
                                                    st.error(f&#34;🚨 **NO VALID MOVEMENTS FOUND!**&#34;)
                                                    st.error(&#34;All trajectories have NaN coordinates or invalid movement data.&#34;)
                                                    st.error(&#34;This explains why all trajectories get -1 assignments.&#34;)
                                                    st.error(&#34;**SOLUTION**: Check your trajectory data for missing/invalid coordinates.&#34;)

                                                    # Show sample trajectory analysis
                                                    st.info(f&#34;🔍 **Sample Trajectory Analysis (first 3):**&#34;)
                                                    for i, traj in enumerate(trajectories[:3]):
                                                        if len(traj.x) &gt; 1:
                                                            # Check for NaN values
                                                            has_nan = np.any(np.isnan(traj.x)) or np.any(np.isnan(traj.z))
                                                            if has_nan:
                                                                st.error(f&#34;   Trajectory {i}: ⚠️ CONTAINS NaN COORDINATES!&#34;)
                                                                continue

                                                            dx = np.diff(traj.x)
                                                            dz = np.diff(traj.z)
                                                            movement = np.sqrt(dx**2 + dz**2)
                                                            max_movement = np.max(movement) if len(movement) &gt; 0 else 0
                                                            mean_movement = np.mean(movement) if len(movement) &gt; 0 else 0

                                                            # Check if trajectory passes through junction
                                                            distances = np.sqrt((traj.x - junction.cx)**2 + (traj.z - junction.cz)**2)
                                                            in_junction = np.any(distances &lt;= junction.r)
                                                            min_distance = np.min(distances)

                                                            st.info(f&#34;   Trajectory {i}: max_movement={max_movement:.3f}, mean_movement={mean_movement:.3f}, in_junction={in_junction}, min_distance={min_distance:.1f}&#34;)

                                            # Suggest junction radius adjustment
                                            if trajectories_in_junction &lt; len(trajectories) * 0.5:
                                                st.warning(f&#34;⚠️ **Low junction coverage!** Only {trajectories_in_junction}/{len(trajectories)} trajectories pass through the junction area.&#34;)

                                                # Calculate suggested radius to cover more trajectories
                                                all_distances = []
                                                for traj in trajectories:
                                                    distances = np.sqrt((traj.x - junction.cx)**2 + (traj.z - junction.cz)**2)
                                                    all_distances.extend(distances)

                                                suggested_radius = np.percentile(all_distances, 80)  # Cover 80% of trajectory points
                                                st.warning(f&#34;⚠️ **Suggested radius:** {suggested_radius:.1f} (current: {junction.r:.1f})&#34;)
                                                st.warning(f&#34;⚠️ **Suggested center:** ({junction.cx:.1f}, {junction.cz:.1f}) - verify this matches your junction location&#34;)
                                else:
                                    st.warning(f&#34;⚠️ Unexpected assignment format: {type(assignments)} - {assignments}&#34;)
                                    st.warning(&#34;Expected pandas DataFrame or string, but got something else.&#34;)

                            results[junction_key] = {
                                &#34;assignments&#34;: assignments,
                                &#34;centers&#34;: centers,
                                &#34;junction&#34;: junction,
                                &#34;path_length&#34;: path_length,
                                &#34;epsilon&#34;: epsilon
                            }

                            successful_assignments += 1
                            st.success(f&#34;✅ Completed assignment for {junction_key} ({len(assignments)} trajectories)&#34;)

                            # Store debug information in session state
                            branch_counts = assignments[&#39;branch&#39;].value_counts().sort_index()
                            debug_info = {
                                &#34;junction_params&#34;: {
                                    &#34;center&#34;: f&#34;({junction.cx:.1f}, {junction.cz:.1f})&#34;,
                                    &#34;radius&#34;: f&#34;{junction.r:.1f}&#34;,
                                    &#34;r_outer&#34;: f&#34;{r_outer:.1f}&#34;
                                },
                                &#34;assignment_params&#34;: {
                                    &#34;path_length&#34;: f&#34;{path_length:.1f}&#34;,
                                    &#34;epsilon&#34;: f&#34;{epsilon:.3f}&#34;
                                },
                                &#34;data_info&#34;: {
                                    &#34;centers_shape&#34;: str(centers.shape),
                                    &#34;trajectories&#34;: len(trajectories)
                                },
                                &#34;assignment_distribution&#34;: dict(branch_counts),
                                &#34;assignments_sample&#34;: assignments.head(10).to_dict(&#39;records&#39;)
                            }

                            # Store in session state
                            if &#34;assign_debug_info&#34; not in st.session_state:
                                st.session_state.assign_debug_info = {}
                            st.session_state.assign_debug_info[junction_key] = debug_info

                        except Exception as e:
                            st.error(f&#34;❌ Assignment failed for {junction_key}: {str(e)}&#34;)
                            continue

                    # Store results - preserve existing analysis results
                    if st.session_state.analysis_results is None:
                        st.session_state.analysis_results = {}
                    st.session_state.analysis_results[&#34;assignments&#34;] = results

                    # Show summary
                    total_junctions = len(centers_dict)
                    if successful_assignments == total_junctions:
                        st.success(f&#34;✅ Assign analysis completed successfully for all {total_junctions} junctions!&#34;)
                    else:
                        st.warning(f&#34;⚠️ Assign analysis completed for {successful_assignments}/{total_junctions} junctions&#34;)

                    # Show assignment statistics
                    if results:
                        st.markdown(&#34;### Assignment Summary&#34;)
                        total_trajectories = len(trajectories)
                        st.write(f&#34;**Total trajectories processed:** {total_trajectories}&#34;)
                        st.write(f&#34;**Junctions processed:** {successful_assignments}&#34;)

                        # Show branch distribution for first junction as example
                        first_junction = list(results.keys())[0]
                        assignments_df = results[first_junction][&#34;assignments&#34;]
                        branch_counts = assignments_df[&#34;branch&#34;].value_counts().sort_index()

                        st.markdown(f&#34;**Branch distribution for {first_junction}:**&#34;)
                        for branch, count in branch_counts.items():
                            percentage = (count / len(assignments_df)) * 100
                            st.write(f&#34;- Branch {branch}: {count} trajectories ({percentage:.1f}%)&#34;)

                    # Generate CLI command for easy copying
                    self.generate_cli_command(&#34;assign&#34;, results, cluster_method, cluster_params, decision_mode, decision_params)

                elif analysis_type == &#34;metrics&#34;:
                    # Compute timing metrics
                    metrics = []
                    trajectories_with_time_data = 0
                    trajectories_without_time_data = 0
                    time_data_debug = []

                    # Get junctions from session state or estimate from data (ONCE, before the loop)
                    junctions_to_use = st.session_state.junctions if st.session_state.junctions else []

                    # If no junctions defined, estimate from trajectory data
                    if not junctions_to_use:
                        st.info(&#34;📊 No junctions defined. Estimating junctions from trajectory data...&#34;)

                        # Estimate junction from trajectory data (similar to assign analysis)
                        all_x = np.concatenate([tr.x for tr in st.session_state.trajectories])
                        all_z = np.concatenate([tr.z for tr in st.session_state.trajectories])

                        # Use median as center (more robust than mean)
                        estimated_cx = float(np.median(all_x))
                        estimated_cz = float(np.median(all_z))

                        # Estimate radius based on data spread
                        distances = np.sqrt((all_x - estimated_cx)**2 + (all_z - estimated_cz)**2)
                        estimated_r = float(np.percentile(distances, 75))  # Use 75th percentile for radius

                        # Create estimated junction
                        estimated_junction = Circle(cx=estimated_cx, cz=estimated_cz, r=max(estimated_r, 20.0))
                        junctions_to_use = [estimated_junction]

                        st.info(f&#34;📊 Estimated junction: center=({estimated_cx:.1f}, {estimated_cz:.1f}), radius={estimated_r:.1f}&#34;)

                        # Debug: Show trajectory data range
                        st.info(f&#34;📊 Trajectory data range:&#34;)
                        st.write(f&#34;- X range: {np.min(all_x):.1f} to {np.max(all_x):.1f}&#34;)
                        st.write(f&#34;- Z range: {np.min(all_z):.1f} to {np.max(all_z):.1f}&#34;)
                        st.write(f&#34;- Total trajectories: {len(st.session_state.trajectories)}&#34;)
                        st.write(f&#34;- Total coordinate points: {len(all_x)}&#34;)

                    # Debug: Count how many trajectories pass through the estimated junction
                    if junctions_to_use:
                        junction = junctions_to_use[0]
                        trajectories_through_junction = 0
                        for traj in st.session_state.trajectories:
                            dist = np.hypot(traj.x - junction.cx, traj.z - junction.cz)
                            if np.any(dist &lt;= junction.r):
                                trajectories_through_junction += 1

                        st.info(f&#34;📊 Junction Analysis:&#34;)
                        st.write(f&#34;- Trajectories passing through estimated junction: {trajectories_through_junction}/{len(st.session_state.trajectories)} ({trajectories_through_junction/len(st.session_state.trajectories)*100:.1f}%)&#34;)

                    for i, traj in enumerate(st.session_state.trajectories):
                        try:
                            # Debug time data for first few trajectories
                            if i &lt; 5:  # Debug first 5 trajectories
                                time_debug = {
                            &#34;trajectory_id&#34;: i,
                                    &#34;time_data_type&#34;: str(type(traj.t)),
                                    &#34;time_data_shape&#34;: traj.t.shape if traj.t is not None else None,
                                    &#34;time_data_sample&#34;: traj.t[:3].tolist() if traj.t is not None and len(traj.t) &gt; 0 else None,
                                    &#34;time_data_dtype&#34;: str(traj.t.dtype) if traj.t is not None else None
                                }
                                time_data_debug.append(time_debug)

                            # Compute basic trajectory metrics
                            basic_metrics = compute_basic_trajectory_metrics(traj)

                            # Track time data availability
                            if basic_metrics[&#34;total_time&#34;] &gt; 0:
                                trajectories_with_time_data += 1
                            else:
                                trajectories_without_time_data += 1

                            # Compute junction-specific timing metrics
                            junction_metrics = {}

                            # Compute timing for each junction
                            for j, junction in enumerate(junctions_to_use):
                                try:
                                    # First check if trajectory actually passes through this junction
                                    entered, _ = entered_junction_idx(traj.x, traj.z, junction)

                                    if not entered:
                                        # Trajectory doesn&#39;t pass through this junction, set NaN
                                        junction_metrics[f&#34;junction_{j}_time&#34;] = float(&#39;nan&#39;)
                                        junction_metrics[f&#34;junction_{j}_mode&#34;] = &#34;no_entry&#34;
                                        # Set speed metrics to NaN as well
                                        junction_metrics[f&#34;junction_{j}_speed&#34;] = float(&#39;nan&#39;)
                                        junction_metrics[f&#34;junction_{j}_speed_mode&#34;] = &#34;no_entry&#34;
                                        junction_metrics[f&#34;junction_{j}_entry_speed&#34;] = float(&#39;nan&#39;)
                                        junction_metrics[f&#34;junction_{j}_exit_speed&#34;] = float(&#39;nan&#39;)
                                        junction_metrics[f&#34;junction_{j}_avg_transit_speed&#34;] = float(&#39;nan&#39;)
                                        continue

                                    # Use the selected decision mode from GUI (with defaults)
                                    decision_mode = getattr(st.session_state, &#39;metrics_decision_mode&#39;, &#39;pathlen&#39;)
                                    distance = getattr(st.session_state, &#39;metrics_distance&#39;, 100.0)
                                    # Use r_outer from junction definitions like other functions do
                                    r_outer_list = [st.session_state.junction_r_outer.get(i, 50.0) for i in range(len(st.session_state.junctions))]
                                    r_outer = r_outer_list[j] if j &lt; len(r_outer_list) else 50.0
                                    trend_window = getattr(st.session_state, &#39;metrics_trend_window&#39;, 5)
                                    min_outward = getattr(st.session_state, &#39;metrics_min_outward&#39;, 0.0)

                                    # Dynamic path length adjustment for all modes
                                    if len(traj.x) &gt; 1:
                                        dx = np.diff(traj.x)
                                        dz = np.diff(traj.z)
                                        segments = np.hypot(dx, dz)
                                        total_distance = float(np.sum(segments))

                                        # Apply dynamic adjustment based on mode
                                        if decision_mode == &#34;pathlen&#34;:
                                            # Use 5% of total distance, but at least 0.1 and at most 10.0
                                            dynamic_distance = max(0.1, min(10.0, total_distance * 0.05))
                                            distance = dynamic_distance
                                        elif decision_mode == &#34;radial&#34;:
                                            # For radial mode, use a smaller r_outer based on trajectory data
                                            dynamic_r_outer = max(5.0, min(50.0, total_distance * 0.1))
                                            r_outer = dynamic_r_outer

                                        # Debug: Show dynamic parameters for first few trajectories
                                        if i &lt; 5:
                                            if decision_mode == &#34;pathlen&#34;:
                                                st.write(f&#34;🔍 Trajectory {i}: total_distance={total_distance:.2f}, dynamic_distance={dynamic_distance:.2f}&#34;)
                                            elif decision_mode == &#34;radial&#34;:
                                                st.write(f&#34;🔍 Trajectory {i}: total_distance={total_distance:.2f}, dynamic_r_outer={dynamic_r_outer:.2f}&#34;)

                                    # Try the timing calculation with the dynamic parameters
                                    t_val, mode_used = _timing_for_traj(
                                        tr=traj,
                                        junction=junction,
                                        decision_mode=decision_mode,
                                        distance=distance,
                                        r_outer=r_outer,
                                        trend_window=trend_window,
                                        min_outward=min_outward,
                                    )

                                    # Compute speed analysis for this junction
                                    speed_val, speed_mode = speed_through_junction(
                                        tr=traj,
                                        junction=junction,
                                        decision_mode=decision_mode,
                                        path_length=distance,
                                        r_outer=r_outer,
                                        window=trend_window,
                                        min_outward=min_outward,
                                    )

                                    # Compute junction transit speeds
                                    entry_speed, exit_speed, avg_transit_speed = junction_transit_speed(traj, junction)

                                    # If still NaN and we have some movement, try with even smaller parameters
                                    if np.isnan(t_val) and len(traj.x) &gt; 1:
                                        if decision_mode == &#34;pathlen&#34;:
                                            # Try with just 1% of total distance, minimum 0.01
                                            fallback_distance = max(0.01, total_distance * 0.01)
                                            t_val, mode_used = _timing_for_traj(
                                                tr=traj,
                                                junction=junction,
                                                decision_mode=decision_mode,
                                                distance=fallback_distance,
                                                r_outer=r_outer,
                                                trend_window=trend_window,
                                                min_outward=min_outward,
                                            )
                                        elif decision_mode == &#34;radial&#34;:
                                            # Try with even smaller r_outer
                                            fallback_r_outer = max(2.0, total_distance * 0.05)
                                            t_val, mode_used = _timing_for_traj(
                                                tr=traj,
                                                junction=junction,
                                                decision_mode=decision_mode,
                                                distance=distance,
                                                r_outer=fallback_r_outer,
                                                trend_window=trend_window,
                                                min_outward=min_outward,
                                            )

                                        if i &lt; 5 and not np.isnan(t_val):
                                            if decision_mode == &#34;pathlen&#34;:
                                                st.write(f&#34;🔍 Trajectory {i}: Fallback worked! fallback_distance={fallback_distance:.3f}&#34;)
                                            elif decision_mode == &#34;radial&#34;:
                                                st.write(f&#34;🔍 Trajectory {i}: Fallback worked! fallback_r_outer={fallback_r_outer:.3f}&#34;)

                                    junction_metrics[f&#34;junction_{j}_time&#34;] = t_val
                                    junction_metrics[f&#34;junction_{j}_mode&#34;] = mode_used
                                    # Add speed analysis metrics
                                    junction_metrics[f&#34;junction_{j}_speed&#34;] = speed_val
                                    junction_metrics[f&#34;junction_{j}_speed_mode&#34;] = speed_mode
                                    junction_metrics[f&#34;junction_{j}_entry_speed&#34;] = entry_speed
                                    junction_metrics[f&#34;junction_{j}_exit_speed&#34;] = exit_speed
                                    junction_metrics[f&#34;junction_{j}_avg_transit_speed&#34;] = avg_transit_speed
                                except Exception as e:
                                    st.warning(f&#34;⚠️ Junction {j} timing failed for trajectory {i}: {e}&#34;)
                                    junction_metrics[f&#34;junction_{j}_time&#34;] = float(&#39;nan&#39;)
                                    junction_metrics[f&#34;junction_{j}_mode&#34;] = &#34;error&#34;

                            # Combine basic and junction metrics
                            combined_metrics = {
                                &#34;trajectory_id&#34;: i,
                                &#34;trajectory_tid&#34;: traj.tid,
                                **basic_metrics,
                                **junction_metrics
                            }
                            metrics.append(combined_metrics)

                        except Exception as e:
                            st.error(f&#34;❌ Failed to compute metrics for trajectory {i} ({traj.tid}): {e}&#34;)
                            # Add error entry to maintain consistency
                            error_metrics = {
                                &#34;trajectory_id&#34;: i,
                                &#34;trajectory_tid&#34;: traj.tid,
                                &#34;total_time&#34;: 0.0,
                                &#34;total_distance&#34;: 0.0,
                                &#34;average_speed&#34;: 0.0,
                                &#34;error&#34;: str(e)
                            }
                            metrics.append(error_metrics)

                    # Show time data debug information (always show for metrics analysis)
                    st.markdown(&#34;---&#34;)
                    st.markdown(&#34;### 🔍 Debug Information&#34;)

                    # Debug: Show what we have
                    st.write(f&#34;**Debug Status:**&#34;)
                    st.write(f&#34;- time_data_debug length: {len(time_data_debug)}&#34;)
                    st.write(f&#34;- trajectories_without_time_data: {trajectories_without_time_data}&#34;)
                    st.write(f&#34;- trajectories_with_time_data: {trajectories_with_time_data}&#34;)

                    if time_data_debug:
                        with st.expander(&#34;🔍 Time Data Debug Information&#34;, expanded=True):
                            st.write(&#34;**First 5 trajectories time data analysis:**&#34;)
                            for debug_info in time_data_debug:
                                st.write(f&#34;**Trajectory {debug_info[&#39;trajectory_id&#39;]}:**&#34;)
                                st.write(f&#34;- Type: {debug_info[&#39;time_data_type&#39;]}&#34;)
                                st.write(f&#34;- Shape: {debug_info[&#39;time_data_shape&#39;]}&#34;)
                                st.write(f&#34;- Dtype: {debug_info[&#39;time_data_dtype&#39;]}&#34;)
                                st.write(f&#34;- Sample: {debug_info[&#39;time_data_sample&#39;]}&#34;)
                                st.write(&#34;---&#34;)
                    else:
                        st.info(&#34;No time data debug information available&#34;)

                    # Show detailed analysis of failing trajectories
                    if trajectories_without_time_data &gt; 0:
                        with st.expander(&#34;🔍 Detailed Time Data Analysis&#34;, expanded=True):
                            st.write(f&#34;**Analysis of {trajectories_without_time_data} trajectories with invalid time data:**&#34;)

                            # Sample a few failing trajectories for detailed analysis
                            failing_samples = []
                            for i, traj in enumerate(st.session_state.trajectories):
                                if i &gt;= 10:  # Limit to first 10 for performance
                                    break
                                try:
                                    basic_metrics = compute_basic_trajectory_metrics(traj)
                                    if basic_metrics[&#34;total_time&#34;] == 0:
                                        failing_samples.append({
                                            &#34;id&#34;: i,
                                            &#34;tid&#34;: traj.tid,
                                            &#34;time_data&#34;: traj.t[:5].tolist() if traj.t is not None and len(traj.t) &gt; 0 else None,
                                            &#34;time_dtype&#34;: str(traj.t.dtype) if traj.t is not None else None,
                                            &#34;time_shape&#34;: traj.t.shape if traj.t is not None else None,
                                            &#34;time_is_none&#34;: traj.t is None,
                                            &#34;time_length&#34;: len(traj.t) if traj.t is not None else 0
                                        })
                                except Exception as e:
                                    failing_samples.append({
                                        &#34;id&#34;: i,
                                        &#34;tid&#34;: traj.tid,
                                        &#34;error&#34;: str(e)
                                    })

                            if failing_samples:
                                st.write(&#34;**Sample failing trajectories:**&#34;)
                                for sample in failing_samples:
                                    st.write(f&#34;**Trajectory {sample[&#39;id&#39;]} ({sample[&#39;tid&#39;]}):**&#34;)
                                    if &#39;error&#39; in sample:
                                        st.write(f&#34;- Error: {sample[&#39;error&#39;]}&#34;)
                                    else:
                                        st.write(f&#34;- Time data is None: {sample[&#39;time_is_none&#39;]}&#34;)
                                        st.write(f&#34;- Time data length: {sample[&#39;time_length&#39;]}&#34;)
                                        st.write(f&#34;- Time data sample: {sample[&#39;time_data&#39;]}&#34;)
                                        st.write(f&#34;- Time dtype: {sample[&#39;time_dtype&#39;]}&#34;)
                                        st.write(f&#34;- Time shape: {sample[&#39;time_shape&#39;]}&#34;)
                                    st.write(&#34;---&#34;)

                            # Check if time data is completely missing
                            none_count = sum(1 for traj in st.session_state.trajectories[:10] if traj.t is None)
                            if none_count &gt; 0:
                                st.warning(f&#34;⚠️ {none_count} out of 10 sample trajectories have NO time data (t=None)&#34;)
                                st.write(&#34;**This suggests:**&#34;)
                                st.write(&#34;- Time column mapping is incorrect&#34;)
                                st.write(&#34;- Time column doesn&#39;t exist in CSV files&#34;)
                                st.write(&#34;- Time column is completely empty&#34;)

                                # Try to diagnose column mapping issue
                                st.write(&#34;**Column Mapping Diagnosis:**&#34;)
                                st.write(&#34;The GUI is looking for a time column, but it might not exist or be named differently.&#34;)
                                st.write(&#34;**Common time column names:**&#34;)
                                st.write(&#34;- &#39;Time&#39; (most common)&#34;)
                                st.write(&#34;- &#39;time&#39;&#34;)
                                st.write(&#34;- &#39;t&#39;&#34;)
                                st.write(&#34;- &#39;timestamp&#39;&#34;)
                                st.write(&#34;- &#39;Timestamp&#39;&#34;)
                                st.write(&#34;- &#39;TIME&#39;&#34;)
                                st.write(&#34;&#34;)
                                st.write(&#34;**To fix this:**&#34;)
                                st.write(&#34;1. Check the Data Upload tab&#34;)
                                st.write(&#34;2. Look at the &#39;Time Column&#39; field&#34;)
                                st.write(&#34;3. Make sure it matches a column name in your CSV files&#34;)
                                st.write(&#34;4. If unsure, try common names like &#39;Time&#39;, &#39;time&#39;, or &#39;t&#39;&#34;)

                            st.write(&#34;**Common time data issues:**&#34;)
                            st.write(&#34;- Empty strings or null values&#34;)
                            st.write(&#34;- Non-numeric text (e.g., &#39;Time: 1.23&#39;, &#39;1.23s&#39;)&#34;)
                            st.write(&#34;- Mixed data types in the same column&#34;)
                            st.write(&#34;- Missing or corrupted time data&#34;)
                            st.write(&#34;- Incorrect column mapping&#34;)

                    # Store results - preserve existing analysis results
                    if st.session_state.analysis_results is None:
                        st.session_state.analysis_results = {}
                    st.session_state.analysis_results[&#34;metrics&#34;] = metrics

                    # Save metrics to CSV file
                    try:
                        import os
                        import pandas as pd
                        os.makedirs(&#34;gui_outputs&#34;, exist_ok=True)
                        df = pd.DataFrame(metrics)
                        csv_path = os.path.join(&#34;gui_outputs&#34;, &#34;metrics_results.csv&#34;)
                        df.to_csv(csv_path, index=False)
                        st.info(f&#34;📁 Metrics saved to: {csv_path}&#34;)
                    except Exception as e:
                        st.warning(f&#34;⚠️ Could not save metrics to file: {e}&#34;)

                    # Generate and save metrics plots for export and visualization reuse
                    try:
                        import os
                        import glob
                        import pandas as pd
                        import matplotlib.pyplot as plt
                        import math

                        metrics_dir = os.path.join(&#34;gui_outputs&#34;, &#34;metrics&#34;)
                        os.makedirs(metrics_dir, exist_ok=True)

                        metrics_df = pd.DataFrame(metrics)

                        # Helper to safely save and close figures
                        def _save_fig(path):
                            plt.tight_layout()
                            plt.savefig(path, dpi=150)
                            plt.close()

                        # ---- Utilities for KDE and distribution fitting ----
                        def _kde_curve(values):
                            arr = np.asarray(values, dtype=float)
                            arr = arr[np.isfinite(arr)]
                            if len(arr) &lt; 2:
                                return None
                            std = np.std(arr)
                            if std == 0:
                                return None
                            n = len(arr)
                            # Silverman&#39;s rule of thumb
                            h = 1.06 * std * (n ** (-1.0 / 5.0))
                            xs = np.linspace(np.percentile(arr, 1), np.percentile(arr, 99), 200)
                            diffs = (xs[:, None] - arr[None, :]) / h
                            kernel = np.exp(-0.5 * diffs * diffs) / (math.sqrt(2.0 * math.pi))
                            density = np.sum(kernel, axis=1) / (n * h)
                            return xs, density

                        def _loglik_normal(arr, mu, sigma):
                            if sigma &lt;= 0:
                                return -np.inf
                            return np.sum(-0.5 * np.log(2 * np.pi) - np.log(sigma) - 0.5 * ((arr - mu) / sigma) ** 2)

                        def _loglik_lognormal(arr, mu_log, sigma_log):
                            if sigma_log &lt;= 0:
                                return -np.inf
                            if np.any(arr &lt;= 0):
                                return -np.inf
                            z = (np.log(arr) - mu_log) / sigma_log
                            return np.sum(-0.5 * np.log(2 * np.pi) - np.log(sigma_log) - np.log(arr) - 0.5 * z * z)

                        def _loglik_gamma(arr, k, theta):
                            if k &lt;= 0 or theta &lt;= 0:
                                return -np.inf
                            if np.any(arr &lt;= 0):
                                return -np.inf
                            # log pdf = (k-1)ln x - x/theta - k ln theta - lgamma(k)
                            return np.sum((k - 1) * np.log(arr) - arr / theta - k * np.log(theta) - math.lgamma(k))

                        def _fit_and_plot_overlays(ax, arr, xlabel, base_color, alt_color):
                            # KDE overlay
                            kde = _kde_curve(arr)
                            if kde is not None:
                                xs_kde, dens_kde = kde
                                ax.plot(xs_kde, dens_kde, color=alt_color, linewidth=2, alpha=0.9, label=&#34;KDE&#34;)

                            # Candidate distributions and AIC
                            candidates = []
                            n = len(arr)
                            if n &gt;= 2:
                                # Normal
                                mu = float(np.mean(arr))
                                sigma = float(np.std(arr))
                                ll_n = _loglik_normal(arr, mu, sigma) if sigma &gt; 0 else -np.inf
                                aic_n = 2 * 2 - 2 * ll_n  # 2 params
                                candidates.append({
                                    &#34;name&#34;: &#34;Normal&#34;,
                                    &#34;params&#34;: (mu, sigma),
                                    &#34;aic&#34;: aic_n,
                                    &#34;pdf&#34;: lambda x: (1.0 / (sigma * math.sqrt(2 * math.pi))) * np.exp(-0.5 * ((x - mu) / sigma) ** 2),
                                    &#34;label&#34;: f&#34;Normal (μ={mu:.2f}, σ={sigma:.2f})&#34;
                                })

                                # Log-normal (positive values)
                                pos = arr[arr &gt; 0]
                                if len(pos) &gt;= 2 and np.std(np.log(pos)) &gt; 0:
                                    mu_l = float(np.mean(np.log(pos)))
                                    sigma_l = float(np.std(np.log(pos)))
                                    ll_l = _loglik_lognormal(pos, mu_l, sigma_l)
                                    aic_l = 2 * 2 - 2 * ll_l  # 2 params
                                    candidates.append({
                                        &#34;name&#34;: &#34;LogNormal&#34;,
                                        &#34;params&#34;: (mu_l, sigma_l),
                                        &#34;aic&#34;: aic_l,
                                        &#34;pdf&#34;: lambda x: np.where(x &gt; 0, (1.0 / (x * sigma_l * math.sqrt(2 * math.pi))) * np.exp(-0.5 * ((np.log(x) - mu_l) / sigma_l) ** 2), 0.0),
                                        &#34;label&#34;: f&#34;LogNormal (μlog={mu_l:.2f}, σlog={sigma_l:.2f})&#34;
                                    })

                                # Gamma (method of moments)
                                if np.all(arr &gt; 0) and np.var(arr) &gt; 0:
                                    mean = float(np.mean(arr))
                                    var = float(np.var(arr))
                                    k = mean * mean / var
                                    theta = var / mean
                                    ll_g = _loglik_gamma(arr, k, theta)
                                    aic_g = 2 * 2 - 2 * ll_g  # 2 params (k, theta)
                                    candidates.append({
                                        &#34;name&#34;: &#34;Gamma&#34;,
                                        &#34;params&#34;: (k, theta),
                                        &#34;aic&#34;: aic_g,
                                        &#34;pdf&#34;: lambda x: np.where(x &gt; 0, (x ** (k - 1)) * np.exp(-x / theta) / (math.gamma(k) * (theta ** k)), 0.0),
                                        &#34;label&#34;: f&#34;Gamma (k={k:.2f}, θ={theta:.2f})&#34;
                                    })

                            if candidates:
                                best = min(candidates, key=lambda c: c[&#34;aic&#34;])
                                xs = np.linspace(np.percentile(arr, 1), np.percentile(arr, 99), 200)
                                ax.plot(xs, best[&#34;pdf&#34;](xs), color=base_color, linewidth=2.5, label=f&#34;Best: {best[&#39;label&#39;]} (AIC {best[&#39;aic&#39;]:.1f})&#34;)
                                ax.legend()

                        # 1) Total Time Distribution (+ KDE and best-fit distribution)
                        if &#34;total_time&#34; in metrics_df.columns and metrics_df[&#34;total_time&#34;].notna().any():
                            plt.figure(figsize=(8, 4))
                            vals = metrics_df[&#34;total_time&#34;].dropna().to_numpy()
                            ax = plt.gca()
                            ax.hist(vals, bins=30, color=&#34;#4C78A8&#34;, alpha=0.55, density=True, edgecolor=&#34;none&#34;)
                            _fit_and_plot_overlays(ax, vals, xlabel=&#34;Seconds&#34;, base_color=&#34;#1F77B4&#34;, alt_color=&#34;#4C78A8&#34;)
                            ax.set_title(&#34;Total Time Distribution (s)&#34;)
                            ax.set_xlabel(&#34;Seconds&#34;)
                            ax.set_ylabel(&#34;Density&#34;)
                            _save_fig(os.path.join(metrics_dir, &#34;total_time_distribution.png&#34;))

                        # 2) Average Speed Distribution (+ KDE and best-fit distribution)
                        if &#34;average_speed&#34; in metrics_df.columns and metrics_df[&#34;average_speed&#34;].notna().any():
                            plt.figure(figsize=(8, 4))
                            vals = metrics_df[&#34;average_speed&#34;].dropna().to_numpy()
                            ax = plt.gca()
                            ax.hist(vals, bins=30, color=&#34;#F58518&#34;, alpha=0.55, density=True, edgecolor=&#34;none&#34;)
                            _fit_and_plot_overlays(ax, vals, xlabel=&#34;Speed&#34;, base_color=&#34;#DD8452&#34;, alt_color=&#34;#F58518&#34;)
                            ax.set_title(&#34;Average Speed Distribution&#34;)
                            ax.set_xlabel(&#34;Speed&#34;)
                            ax.set_ylabel(&#34;Density&#34;)
                            _save_fig(os.path.join(metrics_dir, &#34;average_speed_distribution.png&#34;))

                        # 3) Total Distance Distribution (+ KDE and best-fit distribution)
                        if &#34;total_distance&#34; in metrics_df.columns and metrics_df[&#34;total_distance&#34;].notna().any():
                            plt.figure(figsize=(8, 4))
                            vals = metrics_df[&#34;total_distance&#34;].dropna().to_numpy()
                            ax = plt.gca()
                            ax.hist(vals, bins=30, color=&#34;#54A24B&#34;, alpha=0.55, density=True, edgecolor=&#34;none&#34;)
                            _fit_and_plot_overlays(ax, vals, xlabel=&#34;Distance&#34;, base_color=&#34;#2CA02C&#34;, alt_color=&#34;#54A24B&#34;)
                            ax.set_title(&#34;Total Distance Distribution&#34;)
                            ax.set_xlabel(&#34;Distance&#34;)
                            ax.set_ylabel(&#34;Density&#34;)
                            _save_fig(os.path.join(metrics_dir, &#34;total_distance_distribution.png&#34;))

                        # Discover junction-related columns
                        junction_time_cols = [c for c in metrics_df.columns if c.startswith(&#34;junction_&#34;) and c.endswith(&#34;_time&#34;)]
                        speed_cols = [c for c in metrics_df.columns if c.startswith(&#34;junction_&#34;) and c.endswith(&#34;_speed&#34;)]

                        # 4) Speed vs Time Correlation (means per junction for selected speed metrics)
                        # Use average transit speed if available; else fall back to generic _speed
                        suffix_candidates = [&#34;_avg_transit_speed&#34;, &#34;_speed&#34;]
                        chosen_speed_cols = []
                        for sfx in suffix_candidates:
                            candidate = [c for c in speed_cols if c.endswith(sfx)]
                            if candidate:
                                chosen_speed_cols = candidate
                                break
                        if chosen_speed_cols and junction_time_cols:
                            means_speed = []
                            means_time = []
                            labels = []
                            for sc in chosen_speed_cols:
                                jn = sc.split(&#34;_&#34;)[1]
                                tc = f&#34;junction_{jn}_time&#34;
                                if tc in metrics_df.columns:
                                    df_pair = metrics_df[[sc, tc]].dropna()
                                    if len(df_pair) &gt; 0:
                                        means_speed.append(df_pair[sc].mean())
                                        means_time.append(df_pair[tc].mean())
                                        labels.append(f&#34;J{jn}&#34;)
                            if means_speed and means_time:
                                plt.figure(figsize=(6, 6))
                                plt.scatter(means_time, means_speed, c=&#34;#4C78A8&#34;)
                                for x, y, lab in zip(means_time, means_speed, labels):
                                    plt.annotate(lab, (x, y), xytext=(5, 5), textcoords=&#39;offset points&#39;, fontsize=8)
                                plt.title(&#34;Speed vs Time (means per junction)&#34;)
                                plt.xlabel(&#34;Time (s)&#34;)
                                plt.ylabel(&#34;Speed&#34;)
                                _save_fig(os.path.join(metrics_dir, &#34;speed_vs_time_correlation.png&#34;))

                        # 5) Entry vs Exit Speed by Junction (grouped bars)
                        entry_cols = [c for c in metrics_df.columns if c.endswith(&#34;_entry_speed&#34;)]
                        exit_cols = [c for c in metrics_df.columns if c.endswith(&#34;_exit_speed&#34;)]
                        if entry_cols and exit_cols:
                            data = []
                            labels_j = []
                            for ec in entry_cols:
                                jn = ec.split(&#34;_&#34;)[1]
                                xc = f&#34;junction_{jn}_exit_speed&#34;
                                if xc in metrics_df.columns:
                                    e_vals = metrics_df[ec].dropna()
                                    x_vals = metrics_df[xc].dropna()
                                    if len(e_vals) &gt; 0 and len(x_vals) &gt; 0:
                                        data.append((e_vals.mean(), x_vals.mean()))
                                        labels_j.append(f&#34;J{jn}&#34;)
                            if data:
                                entry_means = [d[0] for d in data]
                                exit_means = [d[1] for d in data]
                                x = np.arange(len(labels_j))
                                width = 0.35
                                plt.figure(figsize=(max(6, len(labels_j) * 0.6), 4))
                                plt.bar(x - width/2, entry_means, width, label=&#39;Entry&#39;, color=&#39;#72B7B2&#39;)
                                plt.bar(x + width/2, exit_means, width, label=&#39;Exit&#39;, color=&#39;#E45756&#39;)
                                plt.xticks(x, labels_j)
                                plt.title(&#34;Entry vs Exit Speed by Junction&#34;)
                                plt.xlabel(&#34;Junction&#34;)
                                plt.ylabel(&#34;Speed&#34;)
                                plt.legend()
                                _save_fig(os.path.join(metrics_dir, &#34;entry_exit_speed_by_junction.png&#34;))

                        # 6) Junction Timing Comparison (average times)
                        if junction_time_cols:
                            jt_labels = []
                            jt_means = []
                            for tc in sorted(junction_time_cols, key=lambda c: int(c.split(&#39;_&#39;)[1])):
                                jn = tc.split(&#34;_&#34;)[1]
                                vals = metrics_df[tc].dropna()
                                if len(vals) &gt; 0:
                                    jt_labels.append(f&#34;J{jn}&#34;)
                                    jt_means.append(vals.mean())
                            if jt_labels:
                                x = np.arange(len(jt_labels))
                                plt.figure(figsize=(max(6, len(jt_labels) * 0.6), 4))
                                plt.bar(x, jt_means, color=&#34;#4C78A8&#34;)
                                plt.xticks(x, jt_labels)
                                plt.title(&#34;Average Junction Timing&#34;)
                                plt.xlabel(&#34;Junction&#34;)
                                plt.ylabel(&#34;Time (s)&#34;)
                                _save_fig(os.path.join(metrics_dir, &#34;junction_timing_comparison.png&#34;))

                        # 7) Individual Junction Timing Distributions (per junction) (+ KDE and best-fit distribution)
                        if junction_time_cols:
                            for tc in sorted(junction_time_cols, key=lambda c: int(c.split(&#39;_&#39;)[1])):
                                jn = tc.split(&#34;_&#34;)[1]
                                vals = metrics_df[tc].dropna().to_numpy()
                                if len(vals) &gt; 0:
                                    plt.figure(figsize=(8, 4))
                                    ax = plt.gca()
                                    ax.hist(vals, bins=30, color=&#34;#B279A2&#34;, alpha=0.55, density=True, edgecolor=&#34;none&#34;)
                                    _fit_and_plot_overlays(ax, vals, xlabel=&#34;Seconds&#34;, base_color=&#34;#A05FA3&#34;, alt_color=&#34;#B279A2&#34;)
                                    ax.set_title(f&#34;Junction {jn} Timing Distribution (s)&#34;)
                                    ax.set_xlabel(&#34;Seconds&#34;)
                                    ax.set_ylabel(&#34;Density&#34;)
                                    _save_fig(os.path.join(metrics_dir, f&#34;timing_distribution_J{jn}.png&#34;))

                        # Store paths in session for downstream tabs
                        try:
                            images = {}
                            for p in glob.glob(os.path.join(metrics_dir, &#34;*.png&#34;)):
                                images[os.path.basename(p)] = p
                            st.session_state.analysis_results.setdefault(&#34;metrics_images&#34;, images)
                        except Exception:
                            pass

                    except Exception as e:
                        st.warning(f&#34;⚠️ Could not generate metrics plots: {e}&#34;)

                    # Provide detailed success message
                    success_msg = f&#34;✅ Computed metrics for {len(metrics)} trajectories&#34;
                    if trajectories_with_time_data &gt; 0:
                        success_msg += f&#34; ({trajectories_with_time_data} with time data)&#34;
                    if trajectories_without_time_data &gt; 0:
                        success_msg += f&#34; ({trajectories_without_time_data} without valid time data)&#34;

                    st.success(success_msg)

                    # Show warning if many trajectories lack time data
                    if trajectories_without_time_data &gt; len(metrics) * 0.5:
                        st.warning(f&#34;⚠️ {trajectories_without_time_data} trajectories lack valid time data. This may indicate time data format issues. Check the debug information above.&#34;)

                        # Provide suggestions for fixing time data issues
                        with st.expander(&#34;💡 Suggestions for Fixing Time Data Issues&#34;, expanded=False):
                            st.write(&#34;**To fix time data issues, try these solutions:**&#34;)
                            st.write(&#34;&#34;)
                            st.write(&#34;**1. Check Column Mapping:**&#34;)
                            st.write(&#34;- Verify the time column is correctly mapped&#34;)
                            st.write(&#34;- Look for columns like &#39;Time&#39;, &#39;timestamp&#39;, &#39;t&#39;, etc.&#34;)
                            st.write(&#34;&#34;)
                            st.write(&#34;**2. Check Data Format:**&#34;)
                            st.write(&#34;- Time data should be numeric (e.g., 0, 1.5, 2.3)&#34;)
                            st.write(&#34;- Avoid text formats like &#39;Time: 1.23&#39; or &#39;1.23s&#39;&#34;)
                            st.write(&#34;- Remove quotes around time values&#34;)
                            st.write(&#34;&#34;)
                            st.write(&#34;**3. Data Cleaning:**&#34;)
                            st.write(&#34;- Remove empty rows or null values&#34;)
                            st.write(&#34;- Ensure consistent data types in time column&#34;)
                            st.write(&#34;- Check for mixed formats in the same column&#34;)
                            st.write(&#34;&#34;)
                            st.write(&#34;**4. File Format:**&#34;)
                            st.write(&#34;- Ensure CSV files are properly formatted&#34;)
                            st.write(&#34;- Check for encoding issues (UTF-8 recommended)&#34;)
                            st.write(&#34;- Verify file is not corrupted&#34;)
                            st.write(&#34;&#34;)
                            st.write(&#34;**5. Manual Inspection:**&#34;)
                            st.write(&#34;- Open the CSV file in a text editor&#34;)
                            st.write(&#34;- Look at the first few rows of the time column&#34;)
                            st.write(&#34;- Check for patterns in the data format&#34;)

                    # Generate CLI command for easy copying
                    # Build results dict for CLI command generation
                    metrics_results = {}
                    for i, junction in enumerate(junctions_to_use):
                        junction_key = f&#34;junction_{i}&#34;
                        metrics_results[junction_key] = {
                            &#34;junction&#34;: junction,
                            &#34;r_outer&#34;: st.session_state.junction_r_outer.get(i, 50.0) if i &lt; len(st.session_state.junctions) else 50.0,
                            &#34;decision_mode&#34;: getattr(st.session_state, &#39;metrics_decision_mode&#39;, &#39;pathlen&#39;),
                            &#34;distance&#34;: getattr(st.session_state, &#39;metrics_distance&#39;, 100.0),
                            &#34;r_outer_value&#34;: st.session_state.junction_r_outer.get(i, 50.0) if i &lt; len(st.session_state.junctions) else 50.0,
                            &#34;trend_window&#34;: getattr(st.session_state, &#39;metrics_trend_window&#39;, 5),
                            &#34;min_outward&#34;: getattr(st.session_state, &#39;metrics_min_outward&#39;, 0.0),
                        }
                    self.generate_cli_command(&#34;metrics&#34;, metrics_results, cluster_method, cluster_params, decision_mode, decision_params)

                elif analysis_type == &#34;gaze&#34;:
                    # Analyze gaze and physiological data
                    gaze_results = {}

                    # Create dedicated debug container for gaze analysis
                    gaze_debug_container = st.empty()

                    def update_gaze_debug_display():
                        &#34;&#34;&#34;Update the persistent gaze debug display&#34;&#34;&#34;

                    # Initialize gaze debug info
                    st.session_state[&#39;gaze_debug_info&#39;] = {}

                    # Check if we have proper gaze trajectory data (prefer trajectories that actually carry data)
                    active_trajs = st.session_state.trajectories
                    # Build a filtered list that actually has gaze OR physio data
                    try:
                        from verta.verta_data_loader import has_gaze_data as _has_gaze, has_physio_data as _has_physio
                        trajs_with_signals = [t for t in active_trajs if (_has_gaze(t) or _has_physio(t))]

                        # Debug: Show filtering results
                        st.info(f&#34;🔍 **Trajectory Filtering Debug:**&#34;)
                        st.write(f&#34;- Total trajectories: {len(active_trajs)}&#34;)
                        st.write(f&#34;- Trajectories with gaze/physio data: {len(trajs_with_signals)}&#34;)

                        if len(trajs_with_signals) &lt; len(active_trajs):
                            skipped_count = len(active_trajs) - len(trajs_with_signals)
                            st.info(f&#34;ℹ️ Skipped {skipped_count} trajectories without gaze/physiological data&#34;)

                    except Exception as e:
                        st.warning(f&#34;⚠️ Error filtering trajectories: {e}&#34;)
                        trajs_with_signals = active_trajs

                    # Use the filtered list if it has any valid trajectories
                    if trajs_with_signals and len(trajs_with_signals) &gt; 0:
                        active_trajs = trajs_with_signals
                    else:
                        st.warning(&#34;⚠️ No trajectories with gaze/physiological data found&#34;)
                        active_trajs = []
                    has_gaze_data = self._check_for_gaze_data(active_trajs)
                    # If global check still fails but we have some physio data, allow comprehensive path to proceed
                    try:
                        from verta.verta_data_loader import has_physio_data as _has_physio
                        has_any_physio = any(_has_physio(t) for t in active_trajs)
                    except Exception:
                        has_any_physio = False

                    # Get column mappings from session state (if they were specified)
                    column_mappings = self._get_gaze_column_mappings()

                    if not has_gaze_data and not has_any_physio and not column_mappings:
                        st.warning(&#34;⚠️ **No Gaze Data Available**&#34;)
                        st.info(&#34;&#34;&#34;
                        **Gaze analysis requires trajectories with:**
                        - Head tracking data (`head_forward_x`, `head_forward_z`)
                        - Eye tracking data (`gaze_x`, `gaze_y`)
                        - Physiological data (`pupil_l`, `pupil_r`, `heart_rate`)

                        **Current trajectories only have position data (x, z, t).**

                        Proceeding with movement-only fallback so visualizations still render.
                        &#34;&#34;&#34;)

                        # Compute movement-only fallback per junction
                        for i, junction in enumerate(st.session_state.junctions):
                            junction_key = f&#34;junction_{i}&#34;
                            r_outer = st.session_state.junction_r_outer.get(i, 50.0)
                            try:
                                movement_df = self._analyze_movement_patterns_optimized(
                                    active_trajs, junction, r_outer, decision_mode, path_length=100.0, epsilon=0.05
                                )
                                # Tag as movement analysis for downstream UI
                                if hasattr(movement_df, &#39;assign&#39;):
                                    movement_df = movement_df.assign(analysis_type=&#39;movement&#39;)
                                gaze_results[junction_key] = movement_df
                            except Exception as e:
                                st.warning(f&#34;⚠️ Movement fallback failed for {junction_key}: {e}&#34;)
                                gaze_results[junction_key] = None
                    else:
                        # Perform gaze analysis (use the active trajectories we already determined)

                        # Debug: Track session state before analysis
                        st.session_state.debug_session_state = {
                            &#39;trajectories_count&#39;: len(st.session_state.trajectories) if st.session_state.trajectories else 0,
                            &#39;gaze_trajectories_count&#39;: 0,
                            &#39;last_modified&#39;: &#39;before_gaze_analysis&#39;
                        }

                        # Create global heatmap ONCE (outside junction loop)
                        global_heatmap_data = None
                        cell_size = st.session_state.get(&#39;pupil_heatmap_cell_size&#39;, 50.0)
                        normalization = st.session_state.get(&#39;pupil_heatmap_normalization&#39;, &#39;relative&#39;)

                        # Define output directory for global plots
                        import os
                        global_out_dir = os.path.join(&#34;gui_outputs&#34;, &#34;gaze_plots&#34;)
                        os.makedirs(global_out_dir, exist_ok=True)

                        st.info(&#34;🗺️ Creating global pupil dilation heatmap...&#34;)
                        try:
                            global_heatmap_data = create_pupil_dilation_heatmap(
                                trajectories=active_trajs,
                                junctions=st.session_state.junctions,
                                cell_size=cell_size,
                                normalization=normalization
                            )
                            st.success(&#34;✅ Global heatmap created&#34;)

                            # Store global heatmap data for consistent scaling calculation
                            st.session_state[&#39;global_heatmap_data&#39;] = global_heatmap_data

                            # Generate global heatmap plot
                            try:
                                from verta.verta_gaze import plot_pupil_dilation_heatmap
                                import matplotlib.pyplot as plt
                                import os

                                # Create global heatmap plot
                                global_plot_path = os.path.join(global_out_dir, &#34;global_pupil_heatmap.png&#34;)

                                fig = plot_pupil_dilation_heatmap(
                                    heatmap_data=global_heatmap_data,
                                    junctions=st.session_state.junctions,
                                    trajectories=active_trajs,
                                    all_trajectories=active_trajs,
                                    title=&#34;Global Pupil Dilation Heatmap&#34;,
                                    show_sample_counts=False,
                                    show_minimap=False,
                                    vmin=None,  # Let the function determine scaling
                                    vmax=None
                                )

                                # Save the plot
                                fig.savefig(global_plot_path, dpi=150, bbox_inches=&#34;tight&#34;)
                                plt.close(fig)

                                st.info(f&#34;📁 Global heatmap plot saved to: {global_plot_path}&#34;)
                                st.write(f&#34;🔍 **Debug:** Global plot saved to: `{global_plot_path}`&#34;)
                                st.write(f&#34;🔍 **Debug:** File exists after save: {os.path.exists(global_plot_path)}&#34;)

                            except Exception as plot_e:
                                st.warning(f&#34;⚠️ Could not generate global heatmap plot: {plot_e}&#34;)

                        except Exception as e:
                            st.warning(f&#34;⚠️ Could not create global heatmap: {e}&#34;)

                        # Debug: Check junctions
                        st.info(f&#34;🔍 **Junction Debug:**&#34;)
                        st.write(f&#34;- Number of junctions: {len(st.session_state.junctions)}&#34;)
                        if st.session_state.junctions:
                            for i, junction in enumerate(st.session_state.junctions):
                                r_outer = st.session_state.junction_r_outer.get(i, 50.0)
                                st.write(f&#34;- Junction {i}: Circle(cx={junction.cx}, cz={junction.cz}, r={junction.r}), r_outer={r_outer}&#34;)
                        else:
                            st.error(&#34;❌ **No junctions defined!** Gaze analysis requires junctions to be defined.&#34;)
                            st.write(&#34;**Solution:** Go to the Junction Editor tab and define at least one junction.&#34;)
                            return

                        # CRITICAL FIX: Perform comprehensive gaze analysis for ALL junctions at once
                        # This ensures all junctions have access to the complete assignments DataFrame
                        st.info(&#34;🔍 **Performing comprehensive gaze analysis for all junctions...**&#34;)

                        # Create output directory for all junctions
                        import os
                        import pandas as pd
                        out_dir = os.path.join(&#34;gui_outputs&#34;, &#34;gaze_analysis&#34;)
                        os.makedirs(out_dir, exist_ok=True)

                        # Get r_outer values for all junctions
                        r_outer_list = [st.session_state.junction_r_outer.get(i, 50.0) for i in range(len(st.session_state.junctions))]

                        # Perform comprehensive gaze analysis for ALL junctions at once
                        try:
                            gaze_data_all = self._perform_comprehensive_gaze_analysis_all_junctions(
                                trajectories=active_trajs,
                                junctions=st.session_state.junctions,
                                r_outer_list=r_outer_list,
                                decision_mode=decision_mode,
                                path_length=100.0,
                                epsilon=0.05,
                                linger_delta=0.0,
                                out_dir=out_dir
                            )
                        except Exception as e:
                            st.error(f&#34;❌ **Comprehensive gaze analysis failed:** {e}&#34;)
                            st.write(f&#34;**Error type:** {type(e).__name__}&#34;)
                            st.write(f&#34;**Error message:** {str(e)}&#34;)

                            # Fall back to individual junction analysis
                            st.info(&#34;🔄 **Falling back to individual junction analysis...**&#34;)
                            for i, junction in enumerate(st.session_state.junctions):
                                junction_key = f&#34;junction_{i}&#34;
                                r_outer = r_outer_list[i]

                                try:
                                    gaze_data = self._perform_gaze_analysis_with_mappings(
                                        trajectories=active_trajs,
                                        junction=junction,
                                        r_outer=r_outer,
                                        decision_mode=decision_mode,
                                        path_length=100.0,
                                        epsilon=0.05,
                                        linger_delta=0.0,
                                        out_dir=out_dir,
                                        column_mappings=column_mappings,
                                        scale_factor=1.0
                                    )

                                    # Normalize columns so downstream plots find expected names
                                    if isinstance(gaze_data, dict):
                                        gaze_data = self._normalize_gaze_result_frames(gaze_data)

                                    # Store the comprehensive gaze analysis results
                                    gaze_results[junction_key] = gaze_data

                                    st.success(f&#34;✅ Completed fallback gaze analysis for {junction_key}&#34;)

                                    # Generate gaze plots immediately after analysis
                                    try:
                                        st.info(f&#34;📊 Generating gaze plots for {junction_key}...&#34;)
                                        self._generate_gaze_plots_during_analysis(gaze_data, junction_key, out_dir)
                                        st.success(f&#34;✅ Generated plots for {junction_key}&#34;)
                                    except Exception as e:
                                        st.warning(f&#34;⚠️ Plot generation failed for {junction_key}: {e}&#34;)

                                    # Update debug display after each junction
                                    update_gaze_debug_display()

                                except Exception as e:
                                    st.warning(f&#34;⚠️ Fallback gaze analysis failed for {junction_key}: {e}&#34;)

                                    # Store error information
                                    gaze_results[junction_key] = {
                                        &#39;error&#39;: str(e),
                                        &#39;error_type&#39;: type(e).__name__,
                                        &#39;junction&#39;: junction,
                                        &#39;r_outer&#39;: r_outer,
                                        &#39;physiological&#39;: None,
                                        &#39;pupil_dilation&#39;: None,
                                        &#39;head_yaw&#39;: None
                                    }

                                    # Update debug display even on error
                                    update_gaze_debug_display()
                                    continue

                            # Skip the rest of the multi-junction processing
                            gaze_data_all = None

                        # The comprehensive analysis returns data for all junctions
                        # We need to split it by junction for storage
                        if gaze_data_all is not None and isinstance(gaze_data_all, dict) and &#39;head_yaw&#39; in gaze_data_all:
                                # Split the results by junction
                                head_yaw_df = gaze_data_all[&#39;head_yaw&#39;]
                                physio_df = gaze_data_all.get(&#39;physiological&#39;)
                                pupil_df = gaze_data_all.get(&#39;pupil_dilation&#39;)
                                all_heatmaps = gaze_data_all.get(&#39;pupil_heatmap_junction&#39;, {})

                                # Process each junction&#39;s data
                                for i, junction in enumerate(st.session_state.junctions):
                                    junction_key = f&#34;junction_{i}&#34;

                                    # Filter data for this junction
                                    junction_head_yaw = head_yaw_df[head_yaw_df[&#39;junction&#39;] == i] if not head_yaw_df.empty else pd.DataFrame()
                                    junction_physio = physio_df[physio_df[&#39;junction&#39;] == i] if physio_df is not None and not physio_df.empty else None
                                    junction_pupil = pupil_df[pupil_df[&#39;junction&#39;] == i] if pupil_df is not None and not pupil_df.empty else None

                                    # Get heatmap data for this junction
                                    junction_heatmap = all_heatmaps.get(i) if all_heatmaps else None

                                    # Create junction-specific gaze data
                                    gaze_data = {
                                        &#39;head_yaw&#39;: junction_head_yaw,
                                        &#39;physiological&#39;: junction_physio,
                                        &#39;pupil_dilation&#39;: junction_pupil,
                                        &#39;pupil_heatmap_junction&#39;: {i: junction_heatmap} if junction_heatmap else {},
                                        &#39;junction&#39;: junction,
                                        &#39;r_outer&#39;: r_outer_list[i]
                                    }

                                    st.info(f&#34;🔍 **Processing junction {i}: {junction_key}**&#34;)
                                    st.write(f&#34;- Junction: Circle(cx={junction.cx}, cz={junction.cz}, r={junction.r})&#34;)
                                    st.write(f&#34;- R_outer: {r_outer_list[i]}&#34;)
                                    st.write(f&#34;- Head yaw records: {len(junction_head_yaw)}&#34;)
                                    st.write(f&#34;- Physiological records: {len(junction_physio) if junction_physio is not None else 0}&#34;)
                                    st.write(f&#34;- Pupil records: {len(junction_pupil) if junction_pupil is not None else 0}&#34;)

                                    # Normalize columns so downstream plots find expected names
                                    if isinstance(gaze_data, dict):
                                        gaze_data = self._normalize_gaze_result_frames(gaze_data)

                                    # Store the comprehensive gaze analysis results
                                    gaze_results[junction_key] = gaze_data

                                    # Debug: Verify data was saved correctly
                                    st.info(f&#34;🔍 **Data Storage Verification for {junction_key}:**&#34;)
                                    if isinstance(gaze_data, dict):
                                        for data_type, data in gaze_data.items():
                                            if data is not None:
                                                if hasattr(data, &#39;shape&#39;):
                                                    st.write(f&#34;- {data_type}: {data.shape} DataFrame&#34;)
                                                elif isinstance(data, list):
                                                    st.write(f&#34;- {data_type}: {len(data)} records&#34;)
                                                else:
                                                    st.write(f&#34;- {data_type}: {type(data).__name__}&#34;)
                                            else:
                                                st.write(f&#34;- {data_type}: None&#34;)
                                    else:
                                        st.write(f&#34;- Raw data type: {type(gaze_data).__name__}&#34;)

                                    st.success(f&#34;✅ Completed gaze analysis for {junction_key}&#34;)

                                    # Generate gaze plots immediately after analysis
                                    try:
                                        st.info(f&#34;📊 Generating gaze plots for {junction_key}...&#34;)
                                        self._generate_gaze_plots_during_analysis(gaze_data, junction_key, out_dir)
                                        st.success(f&#34;✅ Generated plots for {junction_key}&#34;)
                                    except Exception as e:
                                        st.warning(f&#34;⚠️ Plot generation failed for {junction_key}: {e}&#34;)

                                    # Update debug display after each junction
                                    update_gaze_debug_display()
                        else:
                            st.error(&#34;❌ **Comprehensive gaze analysis failed to return expected data structure**&#34;)
                            st.write(f&#34;Returned data type: {type(gaze_data_all)}&#34;)
                            if isinstance(gaze_data_all, dict):
                                st.write(f&#34;Keys: {list(gaze_data_all.keys())}&#34;)

                            # Fall back to individual junction analysis
                            st.info(&#34;🔄 **Falling back to individual junction analysis...**&#34;)
                            for i, junction in enumerate(st.session_state.junctions):
                                junction_key = f&#34;junction_{i}&#34;
                                r_outer = r_outer_list[i]

                                try:
                                    gaze_data = self._perform_gaze_analysis_with_mappings(
                                        trajectories=active_trajs,
                                        junction=junction,
                                        r_outer=r_outer,
                                        decision_mode=decision_mode,
                                        path_length=100.0,
                                        epsilon=0.05,
                                        linger_delta=0.0,
                                        out_dir=out_dir,
                                        column_mappings=column_mappings,
                                        scale_factor=1.0
                                    )

                                    # Normalize columns so downstream plots find expected names
                                    if isinstance(gaze_data, dict):
                                        gaze_data = self._normalize_gaze_result_frames(gaze_data)

                                    # Store the comprehensive gaze analysis results
                                    gaze_results[junction_key] = gaze_data

                                    st.success(f&#34;✅ Completed fallback gaze analysis for {junction_key}&#34;)

                                    # Generate gaze plots immediately after analysis
                                    try:
                                        st.info(f&#34;📊 Generating gaze plots for {junction_key}...&#34;)
                                        self._generate_gaze_plots_during_analysis(gaze_data, junction_key, out_dir)
                                        st.success(f&#34;✅ Generated plots for {junction_key}&#34;)
                                    except Exception as e:
                                        st.warning(f&#34;⚠️ Plot generation failed for {junction_key}: {e}&#34;)

                                    # Update debug display after each junction
                                    update_gaze_debug_display()

                                except Exception as e:
                                    st.warning(f&#34;⚠️ Fallback gaze analysis failed for {junction_key}: {e}&#34;)

                                    # Store error information
                                    gaze_results[junction_key] = {
                                        &#39;error&#39;: str(e),
                                        &#39;error_type&#39;: type(e).__name__,
                                        &#39;junction&#39;: junction,
                                        &#39;r_outer&#39;: r_outer,
                                        &#39;physiological&#39;: None,
                                        &#39;pupil_dilation&#39;: None,
                                        &#39;head_yaw&#39;: None
                                    }

                                    # Update debug display even on error
                                    update_gaze_debug_display()
                                    continue

                    # Store results - preserve existing analysis results
                    if st.session_state.analysis_results is None:
                        st.session_state.analysis_results = {}
                    st.session_state.analysis_results[&#34;gaze_results&#34;] = gaze_results

                    # Store global heatmap separately (only once)
                    if global_heatmap_data is not None:
                        st.session_state.analysis_results[&#34;pupil_heatmap_global&#34;] = global_heatmap_data

                    # Show summary
                    successful_junctions = len([k for k, v in gaze_results.items() if v is not None])
                    total_junctions = len(st.session_state.junctions)
                    if successful_junctions == total_junctions:
                        st.success(f&#34;✅ Gaze analysis completed successfully for all {total_junctions} junctions!&#34;)
                    else:
                        st.warning(f&#34;⚠️ Gaze analysis completed for {successful_junctions}/{total_junctions} junctions&#34;)

                    # Generate CLI command for easy copying
                    # Build results dict for CLI command generation
                    gaze_results_dict = {}
                    for i, junction in enumerate(st.session_state.junctions):
                        junction_key = f&#34;junction_{i}&#34;
                        gaze_results_dict[junction_key] = {
                            &#34;junction&#34;: junction,
                            &#34;r_outer&#34;: st.session_state.junction_r_outer.get(i, 50.0),
                            &#34;decision_mode&#34;: decision_mode,
                            &#34;path_length&#34;: decision_params.get(&#34;path_length&#34;, 100.0) if decision_params else 100.0,
                            &#34;epsilon&#34;: decision_params.get(&#34;epsilon&#34;, 0.05) if decision_params else 0.05,
                            &#34;linger_delta&#34;: decision_params.get(&#34;linger_delta&#34;, 5.0) if decision_params else 5.0,
                        }
                    self.generate_cli_command(&#34;gaze&#34;, gaze_results_dict, cluster_method, cluster_params, decision_mode, decision_params)

                elif analysis_type == &#34;predict&#34;:
                    # Run prediction analysis using spatial tracking only
                    # Create output directory for prediction results
                    import os
                    output_dir = &#34;gui_outputs&#34;
                    os.makedirs(output_dir, exist_ok=True)

                    # Skip discover_decision_chain - use spatial tracking only
                    # Create empty chain_df for compatibility
                    import pandas as pd
                    chain_df = pd.DataFrame(columns=[&#39;trajectory&#39;])
                    chain_df[&#39;trajectory&#39;] = [i for i in range(len(st.session_state.trajectories))]

                    print(f&#34;🔍 DEBUG: Using spatial tracking only - skipping discover_decision_chain&#34;)
                    print(f&#34;🔍 DEBUG: Created empty chain_df with {len(chain_df)} trajectories&#34;)

                    # Define r_outer_list for predict analysis
                    r_outer_list = [st.session_state.junction_r_outer.get(i, 50.0) for i in range(len(st.session_state.junctions))]

                    print(f&#34;🔍 DEBUG: Starting analyze_junction_choice_patterns with spatial tracking only...&#34;)

                    # Debug: Check what trajectories visit multiple junctions
                    print(f&#34;\n🔍 DEBUG: Analyzing trajectory junction visits...&#34;)
                    multi_junction_trajectories = 0
                    consecutive_junction_trajectories = 0

                    # COMMENTED OUT: This code referenced norm_df which we removed
                    # for idx, row in norm_df.iterrows():
                    #     traj_id = row[&#39;trajectory&#39;]
                    #     visited_junctions = []
                    #     for i in range(7):  # 7 junctions
                    #         col = f&#39;branch_j{i}&#39;
                    #         if pd.notna(row[col]) and row[col] &gt;= 0:  # Valid branch assignment
                    #             visited_junctions.append(i)
                    #
                    #     if len(visited_junctions) &gt; 1:
                    #         multi_junction_trajectories += 1
                    #         # Check if junctions are consecutive (for flow analysis)
                    #         if len(visited_junctions) &gt;= 2:
                    #             consecutive_junction_trajectories += 1
                    #             if consecutive_junction_trajectories &lt;= 5:  # Show first 5 examples
                    #                 print(f&#34;🔍 DEBUG: Trajectory {traj_id} visits junctions: {visited_junctions}&#34;)

                    print(f&#34;🔍 DEBUG: Trajectories visiting multiple junctions: {multi_junction_trajectories}&#34;)
                    print(f&#34;🔍 DEBUG: Trajectories with consecutive visits: {consecutive_junction_trajectories}&#34;)

                    # Debug: Check r_outer_list values
                    print(f&#34;\n🔍 DEBUG: r_outer_list values: {r_outer_list}&#34;)
                    print(f&#34;🔍 DEBUG: r_outer_list length: {len(r_outer_list)}&#34;)
                    print(f&#34;🔍 DEBUG: r_outer_list types: {[type(r) for r in r_outer_list]}&#34;)

                    # Debug: Check junction radii
                    print(f&#34;\n🔍 DEBUG: Junction radii:&#34;)
                    for i, junction in enumerate(st.session_state.junctions):
                        print(f&#34;  Junction {i}: radius={junction.r}, r_outer={r_outer_list[i] if i &lt; len(r_outer_list) else &#39;N/A&#39;}&#34;)

                    # Debug: Test trajectory sequence tracking with multiple trajectories
                    print(f&#34;\n🔍 DEBUG: Testing trajectory sequence tracking...&#34;)
                    from verta.verta_plotting import _track_trajectory_junction_sequence

                    # Test trajectories 1-3 instead of trajectory 0 (which has limited range)
                    test_trajectories = st.session_state.trajectories[1:4]  # Trajectories 1-3

                    for test_idx, test_traj in enumerate(test_trajectories):
                        traj_id = getattr(test_traj, &#39;tid&#39;, test_idx + 1)
                        print(f&#34;\n🔍 DEBUG: === Testing Trajectory {traj_id} ===&#34;)

                        # Debug: Check trajectory data
                        print(f&#34;🔍 DEBUG: Trajectory {traj_id} data:&#34;)
                        print(f&#34;  - Length: {len(test_traj.x)} points&#34;)
                        print(f&#34;  - X range: {min(test_traj.x):.2f} to {max(test_traj.x):.2f}&#34;)
                        print(f&#34;  - Z range: {min(test_traj.z):.2f} to {max(test_traj.z):.2f}&#34;)

                        # Debug: Check junction data
                        print(f&#34;🔍 DEBUG: Junction data:&#34;)
                        for i, junction in enumerate(st.session_state.junctions):
                            print(f&#34;  Junction {i}: center=({junction.cx:.2f}, {junction.cz:.2f}), radius={junction.r}, r_outer={r_outer_list[i]}&#34;)

                        # Test spatial tracking
                        test_sequence = _track_trajectory_junction_sequence(test_traj, st.session_state.junctions, r_outer_list)
                        print(f&#34;🔍 DEBUG: Trajectory {traj_id} sequence: {test_sequence}&#34;)

                        if len(test_sequence) &gt; 0:
                            print(f&#34;🔍 DEBUG: ✅ Trajectory {traj_id} has valid sequence!&#34;)
                            print(f&#34;🔍 DEBUG: Stopping debug testing - spatial tracking is working!&#34;)
                            break
                        else:
                            print(f&#34;🔍 DEBUG: ❌ Trajectory {traj_id} has no sequence&#34;)

                    # If no trajectories worked, test trajectory 0 as fallback
                    if all(len(_track_trajectory_junction_sequence(traj, st.session_state.junctions, r_outer_list)) == 0 for traj in test_trajectories):
                        print(f&#34;\n🔍 DEBUG: === Testing Trajectory 0 as fallback ===&#34;)
                        test_traj = st.session_state.trajectories[0]
                        test_sequence = _track_trajectory_junction_sequence(test_traj, st.session_state.junctions, r_outer_list)
                        print(f&#34;🔍 DEBUG: Trajectory 0 sequence: {test_sequence}&#34;)

                    # Debug: Show sample of norm_df data
                    # print(f&#34;\n🔍 DEBUG: Sample norm_df data (first 10 rows):&#34;)
                    # print(norm_df.head(10))

                    # Then analyze junction choice patterns
                    print(&#34;\n&#34; + &#34;=&#34;*60)
                    print(&#34;🔍 PREDICT ANALYSIS - FLOW GRAPH DEBUG OUTPUT&#34;)
                    print(&#34;=&#34;*60)

                    # Run prediction analysis using spatial tracking only
                    results = analyze_junction_choice_patterns(
                        trajectories=st.session_state.trajectories,
                        chain_df=chain_df,  # Empty chain_df for compatibility
                        junctions=st.session_state.junctions,
                        output_dir=output_dir,
                        r_outer_list=r_outer_list,
                        gui_mode=False  # Enable console debug output
                    )

                    print(&#34;=&#34;*60)
                    print(&#34;✅ Predict analysis completed&#34;)
                    print(&#34;=&#34;*60 + &#34;\n&#34;)

                    # Store results - preserve existing analysis results
                    if st.session_state.analysis_results is None:
                        st.session_state.analysis_results = {}
                    st.session_state.analysis_results[&#34;predictions&#34;] = results

                    # Generate CLI command for easy copying
                    self.generate_cli_command(&#34;predict&#34;, results, cluster_method, cluster_params, decision_mode, decision_params)

                elif analysis_type == &#34;intent&#34;:
                    # Run intent recognition analysis
                    import pandas as pd
                    import numpy as np
                    import os

                    st.info(&#34;🧠 Running Intent Recognition Analysis...&#34;)

                    # Get parameters
                    intent_params = st.session_state.get(&#39;intent_params&#39;, {
                        &#39;prediction_distances&#39;: [100.0, 75.0, 50.0, 25.0],
                        &#39;model_type&#39;: &#39;random_forest&#39;,
                        &#39;cv_folds&#39;: 5,
                        &#39;test_split&#39;: 0.2
                    })

                    # Check if we have time data
                    has_time = all(tr.t is not None and len(tr.t) &gt; 0 for tr in st.session_state.trajectories[:5])
                    if not has_time:
                        st.warning(&#34;⚠️ Time data not detected. Intent recognition requires temporal information for velocity/acceleration features.&#34;)
                        st.info(&#34;💡 Tip: Ensure your CSV files have a time column specified in column mapping.&#34;)

                    # Check for sklearn
                    try:
                        import sklearn
                    except ImportError:
                        st.error(&#34;❌ scikit-learn not installed!&#34;)
                        st.markdown(&#34;&#34;&#34;
                        Intent recognition requires scikit-learn. Install with:
                        ```bash
                        pip install scikit-learn
                        ```
                        &#34;&#34;&#34;)
                        return

                    # Create output directory
                    output_dir = &#34;gui_outputs/intent_recognition&#34;
                    os.makedirs(output_dir, exist_ok=True)

                    # For each junction, run intent recognition
                    intent_results = {}

                    progress_bar = st.progress(0)
                    status_text = st.empty()

                    for junction_idx, junction in enumerate(st.session_state.junctions):
                        status_text.text(f&#34;Analyzing junction {junction_idx + 1}/{len(st.session_state.junctions)}...&#34;)

                        try:
                            # Create junction output directory
                            junction_output = os.path.join(output_dir, f&#34;junction_{junction_idx}&#34;)
                            os.makedirs(junction_output, exist_ok=True)

                            # Try to load existing branch assignments from previous Discover analysis
                            assignments_df = None
                            centers = None

                            # Check if we have results from a previous analysis
                            if st.session_state.analysis_results and &#39;branches&#39; in st.session_state.analysis_results:
                                branch_results = st.session_state.analysis_results[&#39;branches&#39;]
                                junction_key = f&#34;junction_{junction_idx}&#34;

                                if junction_key in branch_results:
                                    assignments_df = branch_results[junction_key].get(&#39;assignments&#39;)
                                    centers = branch_results[junction_key].get(&#39;centers&#39;)

                                    if assignments_df is not None:
                                        st.info(f&#34;📋 Using existing branch assignments from previous Discover analysis for Junction {junction_idx}&#34;)

                            # If no existing assignments, run discovery
                            if assignments_df is None:
                                st.warning(f&#34;⚠️ No existing branch assignments found for Junction {junction_idx}&#34;)
                                st.info(f&#34;🔍 Running branch discovery with default parameters...&#34;)
                                st.info(&#34;💡 **Tip:** Run &#39;Discover Branches&#39; analysis first to control clustering parameters!&#34;)

                                r_outer = st.session_state.junction_r_outer.get(junction_idx, 50.0)

                                # Run branch discovery with default parameters
                                assignments_df, summary_df, centers = discover_branches(
                                    trajectories=st.session_state.trajectories,
                                    junction=junction,
                                    k=3,
                                    decision_mode=&#34;hybrid&#34;,
                                    r_outer=r_outer,
                                    path_length=100.0,
                                    epsilon=0.05,
                                    cluster_method=&#34;auto&#34;,
                                    out_dir=junction_output
                                )

                            # Filter valid branches (&gt;= 0)
                            valid_assignments = assignments_df[assignments_df[&#39;branch&#39;] &gt;= 0]

                            if len(valid_assignments) &lt; 10:
                                st.warning(f&#34;⚠️ Junction {junction_idx}: Insufficient valid trajectories ({len(valid_assignments)}). Skipping intent analysis.&#34;)
                                intent_results[f&#34;junction_{junction_idx}&#34;] = {
                                    &#39;error&#39;: &#39;insufficient_data&#39;,
                                    &#39;n_valid_trajectories&#39;: len(valid_assignments)
                                }
                                continue

                            # Count unique branches
                            n_branches = len(assignments_df[assignments_df[&#39;branch&#39;] &gt;= 0][&#39;branch&#39;].unique())
                            st.success(f&#34;✅ Using {n_branches} branches with {len(valid_assignments)} valid trajectories&#34;)

                            # Run intent recognition
                            st.info(f&#34;🤖 Training intent recognition models...&#34;)

                            results = analyze_intent_recognition(
                                trajectories=st.session_state.trajectories,
                                junction=junction,
                                actual_branches=assignments_df,
                                output_dir=junction_output,
                                prediction_distances=intent_params[&#39;prediction_distances&#39;],
                                previous_choices=None  # TODO: Could add multi-junction support
                            )

                            if &#39;error&#39; in results:
                                st.error(f&#34;❌ Junction {junction_idx}: {results[&#39;error&#39;]}&#34;)
                                intent_results[f&#34;junction_{junction_idx}&#34;] = results
                            else:
                                st.success(f&#34;✅ Junction {junction_idx}: Intent recognition complete!&#34;)

                                # Display quick summary
                                models_trained = results[&#39;training_results&#39;].get(&#39;models_trained&#39;, {})
                                if models_trained:
                                    avg_acc = np.mean([m[&#39;cv_mean_accuracy&#39;] for m in models_trained.values()])
                                    st.metric(f&#34;Junction {junction_idx} Avg Accuracy&#34;, f&#34;{avg_acc:.1%}&#34;)

                                intent_results[f&#34;junction_{junction_idx}&#34;] = results

                        except Exception as e:
                            st.error(f&#34;❌ Junction {junction_idx} failed: {str(e)}&#34;)
                            intent_results[f&#34;junction_{junction_idx}&#34;] = {
                                &#39;error&#39;: str(e),
                                &#39;error_type&#39;: type(e).__name__
                            }

                        progress_bar.progress((junction_idx + 1) / len(st.session_state.junctions))

                    status_text.text(&#34;✅ Intent recognition analysis complete!&#34;)
                    progress_bar.empty()

                    # Store results
                    if st.session_state.analysis_results is None:
                        st.session_state.analysis_results = {}
                    st.session_state.analysis_results[&#34;intent_recognition&#34;] = intent_results

                    # Display summary
                    st.markdown(&#34;### 📊 Intent Recognition Summary&#34;)

                    successful_junctions = [k for k, v in intent_results.items()
                                          if &#39;error&#39; not in v]

                    if successful_junctions:
                        st.success(f&#34;✅ Successfully analyzed {len(successful_junctions)}/{len(st.session_state.junctions)} junctions&#34;)

                        # Create summary table
                        summary_data = []
                        for junction_key in successful_junctions:
                            results = intent_results[junction_key]
                            models = results[&#39;training_results&#39;].get(&#39;models_trained&#39;, {})

                            for dist, model_info in models.items():
                                summary_data.append({
                                    &#39;Junction&#39;: junction_key.replace(&#39;junction_&#39;, &#39;J&#39;),
                                    &#39;Distance (units)&#39;: dist,
                                    &#39;Accuracy&#39;: f&#34;{model_info[&#39;cv_mean_accuracy&#39;]:.1%}&#34;,
                                    &#39;Std Dev&#39;: f&#34;±{model_info[&#39;cv_std_accuracy&#39;]:.1%}&#34;,
                                    &#39;Samples&#39;: model_info[&#39;n_samples&#39;]
                                })

                        if summary_data:
                            summary_df = pd.DataFrame(summary_data)
                            st.dataframe(summary_df, width=&#39;stretch&#39;)

                        # Show interpretation
                        st.markdown(&#34;#### 💡 Interpretation&#34;)
                        avg_accuracy = np.mean([float(d[&#39;Accuracy&#39;].strip(&#39;%&#39;)) / 100 for d in summary_data])

                        if avg_accuracy &gt; 0.85:
                            st.success(&#34;🟢 **Excellent Predictability**: User intent is highly predictable. Early intervention systems will be very effective!&#34;)
                        elif avg_accuracy &gt; 0.70:
                            st.info(&#34;🟡 **Good Predictability**: Clear patterns exist. Adaptive systems can benefit users.&#34;)
                        else:
                            st.warning(&#34;🔴 **Moderate Predictability**: Behavior is variable. Consider per-user models or additional features.&#34;)
                    else:
                        st.error(&#34;❌ Intent recognition failed for all junctions&#34;)

                    st.info(f&#34;📁 Detailed results saved to: {output_dir}&#34;)

                    # Generate CLI command for easy copying
                    # Build results dict for CLI command generation
                    intent_results_dict = {}
                    for i, junction in enumerate(st.session_state.junctions):
                        junction_key = f&#34;junction_{i}&#34;
                        intent_results_dict[junction_key] = {
                            &#34;junction&#34;: junction,
                            &#34;r_outer&#34;: st.session_state.junction_r_outer.get(i, 50.0),
                            &#34;decision_mode&#34;: decision_mode,
                            &#34;path_length&#34;: decision_params.get(&#34;path_length&#34;, 100.0) if decision_params else 100.0,
                            &#34;epsilon&#34;: decision_params.get(&#34;epsilon&#34;, 0.05) if decision_params else 0.05,
                            &#34;linger_delta&#34;: decision_params.get(&#34;linger_delta&#34;, 5.0) if decision_params else 5.0,
                            &#34;prediction_distances&#34;: intent_params.get(&#39;prediction_distances&#39;, [100.0, 75.0, 50.0, 25.0]),
                            &#34;model_type&#34;: intent_params.get(&#39;model_type&#39;, &#39;random_forest&#39;),
                            &#34;cv_folds&#34;: intent_params.get(&#39;cv_folds&#39;, 5),
                            &#34;test_split&#34;: intent_params.get(&#39;test_split&#39;, 0.2),
                        }
                    self.generate_cli_command(&#34;intent&#34;, intent_results_dict, cluster_method, cluster_params, decision_mode, decision_params)

                elif analysis_type == &#34;enhanced&#34;:
                    # Run enhanced analysis for evacuation planning and risk assessment
                    import os
                    output_dir = &#34;gui_outputs&#34;
                    os.makedirs(output_dir, exist_ok=True)

                    # First run discovery to get chain data
                    k_value = cluster_params.get(&#34;k&#34;, 3) if cluster_params else 3
                    min_samples = cluster_params.get(&#34;min_samples&#34;, 5) if cluster_params else 5
                    k_min = cluster_params.get(&#34;k_min&#34;, 2) if cluster_params else 2
                    k_max = cluster_params.get(&#34;k_max&#34;, 6) if cluster_params else 6
                    min_sep_deg = cluster_params.get(&#34;min_sep_deg&#34;, 12.0) if cluster_params else 12.0
                    angle_eps = cluster_params.get(&#34;angle_eps&#34;, 15.0) if cluster_params else 15.0

                    path_length = decision_params.get(&#34;path_length&#34;, 100.0) if decision_params else 100.0
                    epsilon = decision_params.get(&#34;epsilon&#34;, 0.05) if decision_params else 0.05
                    linger_delta = decision_params.get(&#34;linger_delta&#34;, 5.0) if decision_params else 5.0
                    r_outer_list = [st.session_state.junction_r_outer.get(i, 50.0) for i in range(len(st.session_state.junctions))]

                    # Run discovery first
                    chain_df, centers_list, decisions_chain_df = discover_decision_chain(
                        trajectories=st.session_state.trajectories,
                        junctions=st.session_state.junctions,
                        path_length=path_length,
                        epsilon=epsilon,
                        seed=seed,
                        decision_mode=discover_decision_mode,
                        r_outer_list=r_outer_list,
                        linger_delta=linger_delta,
                        out_dir=output_dir,
                        cluster_method=cluster_method,
                        k=k_value,
                        k_min=k_min,
                        k_max=k_max,
                        min_sep_deg=min_sep_deg,
                        angle_eps=angle_eps,
                        min_samples=min_samples,
                    )

                    # Run enhanced analysis
                    enhanced_results = self._run_enhanced_analysis(
                        trajectories=st.session_state.trajectories,
                        chain_df=chain_df,
                        junctions=st.session_state.junctions,
                        r_outer_list=r_outer_list,
                        centers_list=centers_list,
                        decisions_df=decisions_chain_df
                    )

                    # Store results
                    if st.session_state.analysis_results is None:
                        st.session_state.analysis_results = {}
                    st.session_state.analysis_results[&#34;enhanced&#34;] = enhanced_results

                #st.success(f&#34;✅ {analysis_type.capitalize()} analysis completed!&#34;)
                #st.rerun()

        except Exception as e:
            st.error(f&#34;❌ Analysis failed: {str(e)}&#34;)
            st.exception(e)

    def _run_enhanced_analysis(self, trajectories, chain_df, junctions, r_outer_list, centers_list, decisions_df):
        &#34;&#34;&#34;Run enhanced analysis for evacuation planning and risk assessment&#34;&#34;&#34;
        import numpy as np
        import pandas as pd
        import matplotlib.pyplot as plt
        import os

        results = {
            &#34;evacuation_analysis&#34;: {},
            &#34;recommendations&#34;: [],
            &#34;risk_assessment&#34;: {},
            &#34;efficiency_metrics&#34;: {}
        }

        # 1. Evacuation Analysis - Identify bottlenecks and optimal routes
        st.info(&#34;🚨 Running evacuation analysis...&#34;)
        evacuation_results = self._analyze_evacuation_patterns(
            trajectories, chain_df, junctions, r_outer_list, centers_list
        )
        results[&#34;evacuation_analysis&#34;] = evacuation_results

        # 2. Generate Recommendations
        st.info(&#34;💡 Generating recommendations...&#34;)
        recommendations = self._generate_recommendations(evacuation_results, chain_df, junctions)
        results[&#34;recommendations&#34;] = recommendations

        # 3. Risk Assessment
        st.info(&#34;⚠️ Assessing risks...&#34;)
        risk_results = self._assess_risks(trajectories, chain_df, junctions, r_outer_list)
        results[&#34;risk_assessment&#34;] = risk_results

        # 4. Efficiency Metrics
        st.info(&#34;📊 Computing efficiency metrics...&#34;)
        efficiency_results = self._compute_efficiency_metrics(trajectories, chain_df, junctions, r_outer_list)
        results[&#34;efficiency_metrics&#34;] = efficiency_results

        # Save enhanced analysis results to CSV files
        try:
            enhanced_data_dir = os.path.join(&#34;gui_outputs&#34;, &#34;enhanced_analysis&#34;)
            os.makedirs(enhanced_data_dir, exist_ok=True)

            # Save evacuation analysis results
            if evacuation_results[&#34;bottlenecks&#34;]:
                bottlenecks_df = pd.DataFrame(evacuation_results[&#34;bottlenecks&#34;])
                bottlenecks_file = os.path.join(enhanced_data_dir, &#34;evacuation_bottlenecks.csv&#34;)
                bottlenecks_df.to_csv(bottlenecks_file, index=False)
                st.info(f&#34;📁 Evacuation bottlenecks saved to: {bottlenecks_file}&#34;)

            if evacuation_results[&#34;optimal_routes&#34;]:
                optimal_routes_df = pd.DataFrame(evacuation_results[&#34;optimal_routes&#34;])
                optimal_routes_file = os.path.join(enhanced_data_dir, &#34;optimal_routes.csv&#34;)
                optimal_routes_df.to_csv(optimal_routes_file, index=False)
                st.info(f&#34;📁 Optimal routes saved to: {optimal_routes_file}&#34;)

            # Save flow analysis results
            if evacuation_results[&#34;flow_analysis&#34;]:
                flow_data = []
                for junction_key, flow_info in evacuation_results[&#34;flow_analysis&#34;].items():
                    flow_data.append({
                        &#34;junction&#34;: junction_key,
                        &#34;total_trajectories&#34;: flow_info[&#34;total_trajectories&#34;],
                        &#34;entropy&#34;: flow_info[&#34;entropy&#34;],
                        &#34;branch_distribution&#34;: str(flow_info[&#34;branch_distribution&#34;])
                    })
                flow_df = pd.DataFrame(flow_data)
                flow_file = os.path.join(enhanced_data_dir, &#34;flow_analysis.csv&#34;)
                flow_df.to_csv(flow_file, index=False)
                st.info(f&#34;📁 Flow analysis saved to: {flow_file}&#34;)

            # Save recommendations
            if recommendations:
                recommendations_df = pd.DataFrame(recommendations)
                recommendations_file = os.path.join(enhanced_data_dir, &#34;recommendations.csv&#34;)
                recommendations_df.to_csv(recommendations_file, index=False)
                st.info(f&#34;📁 Recommendations saved to: {recommendations_file}&#34;)

            # Save risk assessment results
            if risk_results[&#34;high_risk_junctions&#34;]:
                risk_df = pd.DataFrame(risk_results[&#34;high_risk_junctions&#34;])
                risk_file = os.path.join(enhanced_data_dir, &#34;risk_assessment.csv&#34;)
                risk_df.to_csv(risk_file, index=False)
                st.info(f&#34;📁 Risk assessment saved to: {risk_file}&#34;)

            # Save efficiency metrics
            if efficiency_results:
                efficiency_data = []
                for metric_name, metric_value in efficiency_results.items():
                    if isinstance(metric_value, dict):
                        for key, value in metric_value.items():
                            efficiency_data.append({
                                &#34;metric_category&#34;: metric_name,
                                &#34;metric_name&#34;: key,
                                &#34;value&#34;: value
                            })
                    else:
                        efficiency_data.append({
                            &#34;metric_category&#34;: &#34;overall&#34;,
                            &#34;metric_name&#34;: metric_name,
                            &#34;value&#34;: metric_value
                        })

                if efficiency_data:
                    efficiency_df = pd.DataFrame(efficiency_data)
                    efficiency_file = os.path.join(enhanced_data_dir, &#34;efficiency_metrics.csv&#34;)
                    efficiency_df.to_csv(efficiency_file, index=False)
                    st.info(f&#34;📁 Efficiency metrics saved to: {efficiency_file}&#34;)

            # Save overall enhanced analysis summary
            summary_data = {
                &#34;overall_risk_score&#34;: risk_results.get(&#34;overall_risk_score&#34;, 0),
                &#34;total_bottlenecks&#34;: len(evacuation_results[&#34;bottlenecks&#34;]),
                &#34;total_optimal_routes&#34;: len(evacuation_results[&#34;optimal_routes&#34;]),
                &#34;total_recommendations&#34;: len(recommendations),
                &#34;high_risk_junctions_count&#34;: len(risk_results.get(&#34;high_risk_junctions&#34;, [])),
                &#34;analysis_timestamp&#34;: pd.Timestamp.now().isoformat()
            }

            summary_df = pd.DataFrame([summary_data])
            summary_file = os.path.join(enhanced_data_dir, &#34;enhanced_analysis_summary.csv&#34;)
            summary_df.to_csv(summary_file, index=False)
            st.info(f&#34;📁 Enhanced analysis summary saved to: {summary_file}&#34;)

        except Exception as e:
            st.warning(f&#34;⚠️ Could not save enhanced analysis files: {e}&#34;)

        return results

    def _analyze_evacuation_patterns(self, trajectories, chain_df, junctions, r_outer_list, centers_list):
        &#34;&#34;&#34;Analyze evacuation patterns and identify bottlenecks&#34;&#34;&#34;
        import numpy as np
        import pandas as pd

        evacuation_results = {
            &#34;bottlenecks&#34;: [],
            &#34;optimal_routes&#34;: [],
            &#34;flow_analysis&#34;: {},
            &#34;capacity_analysis&#34;: {}
        }

        # Analyze flow patterns for each junction
        for i, junction in enumerate(junctions):
            branch_col = f&#34;branch_j{i}&#34;
            if branch_col in chain_df.columns:
                junction_assignments = chain_df[[&#39;trajectory&#39;, branch_col]].copy()
                junction_assignments = junction_assignments.rename(columns={branch_col: &#39;branch&#39;})
                junction_assignments = junction_assignments[junction_assignments[&#39;branch&#39;] &gt;= 0]

                # Calculate branch flow rates
                branch_counts = junction_assignments[&#39;branch&#39;].value_counts()
                total_trajectories = len(junction_assignments)

                # Identify bottlenecks (branches with high concentration)
                for branch, count in branch_counts.items():
                    concentration = count / total_trajectories
                    if concentration &gt; 0.6:  # More than 60% use same route
                        evacuation_results[&#34;bottlenecks&#34;].append({
                            &#34;junction&#34;: i,
                            &#34;branch&#34;: branch,
                            &#34;concentration&#34;: concentration,
                            &#34;trajectory_count&#34;: count,
                            &#34;risk_level&#34;: &#34;HIGH&#34; if concentration &gt; 0.8 else &#34;MEDIUM&#34;
                        })

                # Identify optimal routes (balanced distribution)
                if len(branch_counts) &gt; 1:
                    entropy = -sum((count/total_trajectories) * np.log2(count/total_trajectories)
                                for count in branch_counts.values)
                    max_entropy = np.log2(len(branch_counts))
                    balance_ratio = entropy / max_entropy

                    if balance_ratio &gt; 0.7:  # Well-balanced distribution
                        evacuation_results[&#34;optimal_routes&#34;].append({
                            &#34;junction&#34;: i,
                            &#34;balance_ratio&#34;: balance_ratio,
                            &#34;entropy&#34;: entropy,
                            &#34;branch_count&#34;: len(branch_counts)
                        })

                evacuation_results[&#34;flow_analysis&#34;][f&#34;junction_{i}&#34;] = {
                    &#34;total_trajectories&#34;: total_trajectories,
                    &#34;branch_distribution&#34;: branch_counts.to_dict(),
                    &#34;entropy&#34;: entropy if len(branch_counts) &gt; 1 else 0
                }

        return evacuation_results

    def _generate_recommendations(self, evacuation_results, chain_df, junctions):
        &#34;&#34;&#34;Generate actionable recommendations based on analysis&#34;&#34;&#34;
        recommendations = []

        # Recommendations based on bottlenecks
        for bottleneck in evacuation_results[&#34;bottlenecks&#34;]:
            if bottleneck[&#34;risk_level&#34;] == &#34;HIGH&#34;:
                recommendations.append({
                    &#34;type&#34;: &#34;Signage&#34;,
                    &#34;priority&#34;: &#34;HIGH&#34;,
                    &#34;junction&#34;: bottleneck[&#34;junction&#34;],
                    &#34;message&#34;: f&#34;Add directional signage at Junction {bottleneck[&#39;junction&#39;]} to distribute traffic away from Branch {int(bottleneck[&#39;branch&#39;])} (currently {bottleneck[&#39;concentration&#39;]:.1%} of traffic)&#34;
                })
                recommendations.append({
                    &#34;type&#34;: &#34;Route Modification&#34;,
                    &#34;priority&#34;: &#34;HIGH&#34;,
                    &#34;junction&#34;: bottleneck[&#34;junction&#34;],
                    &#34;message&#34;: f&#34;Consider widening or adding alternative routes at Junction {bottleneck[&#39;junction&#39;]} to reduce bottleneck risk&#34;
                })

        # Recommendations based on optimal routes
        for route in evacuation_results[&#34;optimal_routes&#34;]:
            recommendations.append({
                &#34;type&#34;: &#34;Maintenance&#34;,
                &#34;priority&#34;: &#34;LOW&#34;,
                &#34;junction&#34;: route[&#34;junction&#34;],
                &#34;message&#34;: f&#34;Junction {route[&#39;junction&#39;]} shows good traffic distribution (balance ratio: {route[&#39;balance_ratio&#39;]:.2f}) - maintain current design&#34;
            })

        # General recommendations
        if len(evacuation_results[&#34;bottlenecks&#34;]) &gt; len(junctions) * 0.5:
            recommendations.append({
                &#34;type&#34;: &#34;System-wide&#34;,
                &#34;priority&#34;: &#34;MEDIUM&#34;,
                &#34;junction&#34;: &#34;ALL&#34;,
                &#34;message&#34;: &#34;High number of bottlenecks detected - consider system-wide evacuation route optimization&#34;
            })

        return recommendations

    def _assess_risks(self, trajectories, chain_df, junctions, r_outer_list):
        &#34;&#34;&#34;Assess potential safety risks in flow patterns&#34;&#34;&#34;
        risk_results = {
            &#34;high_risk_junctions&#34;: [],
            &#34;overall_risk_score&#34;: 0
        }

        total_risk_score = 0

        for i, junction in enumerate(junctions):
            branch_col = f&#34;branch_j{i}&#34;
            if branch_col in chain_df.columns:
                junction_assignments = chain_df[[&#39;trajectory&#39;, branch_col]].copy()
                junction_assignments = junction_assignments.rename(columns={branch_col: &#39;branch&#39;})
                junction_assignments = junction_assignments[junction_assignments[&#39;branch&#39;] &gt;= 0]

                branch_counts = junction_assignments[&#39;branch&#39;].value_counts()
                total_trajectories = len(junction_assignments)

                # Calculate unified risk factors
                risk_factors = []
                risk_score = 0.0

                # 1. Concentration Risk (0.0-1.0)
                if len(branch_counts) &gt; 0:
                    max_concentration = branch_counts.max() / total_trajectories
                    if max_concentration &gt; 0.7:
                        concentration_risk = (max_concentration - 0.7) / 0.3  # Scale 0.7-1.0 to 0.0-1.0
                        risk_factors.append((&#34;high_concentration&#34;, concentration_risk))
                        risk_score += concentration_risk

                # 2. Diversity Risk (0.0-1.0)
                if len(branch_counts) &lt; 2:
                    diversity_risk = 1.0
                    risk_factors.append((&#34;low_diversity&#34;, diversity_risk))
                    risk_score += diversity_risk
                elif len(branch_counts) == 2:
                    diversity_risk = 0.3  # Moderate risk for only 2 routes
                    risk_factors.append((&#34;limited_diversity&#34;, diversity_risk))
                    risk_score += diversity_risk

                # 3. Crowding Risk (0.0-1.0)
                if total_trajectories &gt; 50:
                    if total_trajectories &gt; 100:
                        crowding_risk = 1.0  # High crowding
                        risk_factors.append((&#34;high_crowding&#34;, crowding_risk))
                    else:
                        crowding_risk = (total_trajectories - 50) / 50  # Scale 50-100 to 0.0-1.0
                        risk_factors.append((&#34;moderate_crowding&#34;, crowding_risk))
                    risk_score += crowding_risk

                # Normalize total risk score to 0-1 scale
                # Max possible: 1.0 (concentration) + 1.0 (diversity) + 1.0 (crowding) = 3.0
                risk_score = min(risk_score / 3.0, 1.0)
                total_risk_score += risk_score

                # Classify risk level
                if risk_score &gt;= 0.7:
                    risk_level = &#34;HIGH&#34;
                elif risk_score &gt;= 0.4:
                    risk_level = &#34;MEDIUM&#34;
                else:
                    risk_level = &#34;LOW&#34;

                # Include all junctions with risk score &gt;= 0.4 for comprehensive assessment
                if risk_score &gt;= 0.4:
                    risk_results[&#34;high_risk_junctions&#34;].append({
                        &#34;junction&#34;: i,
                        &#34;risk_score&#34;: risk_score,
                        &#34;risk_level&#34;: risk_level,
                        &#34;risk_factors&#34;: risk_factors,
                        &#34;trajectory_count&#34;: total_trajectories,
                        &#34;concentration&#34;: max_concentration if len(branch_counts) &gt; 0 else 0,
                        &#34;route_count&#34;: len(branch_counts)
                    })

        # Calculate overall risk score
        risk_results[&#34;overall_risk_score&#34;] = total_risk_score / len(junctions) if junctions else 0

        return risk_results

    def _compute_efficiency_metrics(self, trajectories, chain_df, junctions, r_outer_list):
        &#34;&#34;&#34;Compute efficiency metrics for navigation&#34;&#34;&#34;
        efficiency_results = {
            &#34;average_travel_times&#34;: {},
            &#34;route_efficiency&#34;: {},
            &#34;capacity_utilization&#34;: {},
            &#34;overall_efficiency&#34;: 0
        }

        total_efficiency = 0

        for i, junction in enumerate(junctions):
            branch_col = f&#34;branch_j{i}&#34;
            if branch_col in chain_df.columns:
                junction_assignments = chain_df[[&#39;trajectory&#39;, branch_col]].copy()
                junction_assignments = junction_assignments.rename(columns={branch_col: &#39;branch&#39;})
                junction_assignments = junction_assignments[junction_assignments[&#39;branch&#39;] &gt;= 0]

                # Calculate efficiency metrics for this junction
                branch_counts = junction_assignments[&#39;branch&#39;].value_counts()
                total_trajectories = len(junction_assignments)

                # Route efficiency (entropy-based)
                if len(branch_counts) &gt; 1:
                    entropy = -sum((count/total_trajectories) * np.log2(count/total_trajectories)
                                for count in branch_counts.values)
                    max_entropy = np.log2(len(branch_counts))
                    route_efficiency = entropy / max_entropy
                else:
                    route_efficiency = 0

                # Capacity utilization
                capacity_utilization = total_trajectories / 100.0  # Assuming capacity of 100
                capacity_utilization = min(capacity_utilization, 1.0)  # Cap at 100%

                efficiency_results[&#34;route_efficiency&#34;][f&#34;junction_{i}&#34;] = route_efficiency
                efficiency_results[&#34;capacity_utilization&#34;][f&#34;junction_{i}&#34;] = capacity_utilization

                total_efficiency += route_efficiency

        # Calculate overall efficiency
        efficiency_results[&#34;overall_efficiency&#34;] = total_efficiency / len(junctions) if junctions else 0

        return efficiency_results

    def render_visualization(self):
        &#34;&#34;&#34;Render the visualization interface&#34;&#34;&#34;
        st.markdown(&#39;&lt;h2 class=&#34;section-header&#34;&gt;📈 Visualization&lt;/h2&gt;&#39;, unsafe_allow_html=True)

        if not st.session_state.analysis_results:
            st.warning(&#34;⚠️ Please run an analysis first&#34;)
            return

        # Debug: Show what analysis results are available
        with st.expander(&#34;🔍 Debug: Available Analysis Results&#34;, expanded=False):
            if st.session_state.analysis_results is not None:
                st.write(&#34;Analysis results keys:&#34;, list(st.session_state.analysis_results.keys()))
            else:
                st.write(&#34;No analysis results available&#34;)

        # Show different visualizations based on analysis type
        # If multiple analysis types are available, let user choose
        # Prioritize &#34;branches&#34; as the default selection
        available_analyses = []

        # Add &#34;branches&#34; first if available (for default selection)
        if &#34;branches&#34; in st.session_state.analysis_results:
            available_analyses.append(&#34;branches&#34;)

        # Add other analysis types
        if st.session_state.analysis_results is not None:
            if &#34;metrics&#34; in st.session_state.analysis_results:
                available_analyses.append(&#34;metrics&#34;)
            if &#34;assignments&#34; in st.session_state.analysis_results:
                available_analyses.append(&#34;assignments&#34;)
        if &#34;predictions&#34; in st.session_state.analysis_results:
            available_analyses.append(&#34;predictions&#34;)
        if &#34;choice_patterns&#34; in st.session_state.analysis_results:
            available_analyses.append(&#34;choice_patterns&#34;)
        if &#34;intent_recognition&#34; in st.session_state.analysis_results:
            available_analyses.append(&#34;intent_recognition&#34;)
        if &#34;enhanced&#34; in st.session_state.analysis_results:
            available_analyses.append(&#34;enhanced&#34;)
        if &#34;gaze_results&#34; in st.session_state.analysis_results:
            available_analyses.append(&#34;gaze_results&#34;)

        if len(available_analyses) &gt; 1:
            # Multiple analysis types available - let user choose
            st.markdown(&#34;### Multiple Analysis Results Available&#34;)
            selected_analysis = st.selectbox(
                &#34;Choose analysis to visualize:&#34;,
                available_analyses,
                help=&#34;Select which analysis results to display&#34;
            )

            if selected_analysis == &#34;metrics&#34;:
                self.render_metrics_visualizations()
            elif selected_analysis == &#34;assignments&#34;:
                self.render_assign_visualizations()
            elif selected_analysis == &#34;branches&#34;:
                self.render_discover_visualizations()
            elif selected_analysis == &#34;predictions&#34;:
                self.render_predict_visualizations()
            elif selected_analysis == &#34;choice_patterns&#34;:
                self.render_flow_graphs()
                self.render_conditional_probabilities()
                self.render_pattern_analysis()
            elif selected_analysis == &#34;intent_recognition&#34;:
                self.render_intent_visualizations()
            elif selected_analysis == &#34;enhanced&#34;:
                self.render_enhanced_visualizations()
            elif selected_analysis == &#34;gaze_results&#34;:
                self.render_gaze_visualizations()
        else:
            # Single analysis type - show automatically
            if st.session_state.analysis_results is not None:
                if &#34;metrics&#34; in st.session_state.analysis_results:
                    self.render_metrics_visualizations()
                elif &#34;assignments&#34; in st.session_state.analysis_results:
                    self.render_assign_visualizations()
                elif &#34;branches&#34; in st.session_state.analysis_results:
                    self.render_discover_visualizations()
                    # Also show flow graphs if available
                    if &#34;flow_graph_map&#34; in st.session_state.analysis_results:
                        self.render_flow_graphs()
                elif &#34;predictions&#34; in st.session_state.analysis_results:
                    self.render_predict_visualizations()
                elif &#34;intent_recognition&#34; in st.session_state.analysis_results:
                    self.render_intent_visualizations()
                elif &#34;enhanced&#34; in st.session_state.analysis_results:
                    self.render_enhanced_visualizations()
                elif &#34;gaze_results&#34; in st.session_state.analysis_results:
                    self.render_gaze_visualizations()
                else:
                    st.info(&#34;No visualizations available for this analysis type&#34;)
                    st.write(&#34;Available analysis results:&#34;, list(st.session_state.analysis_results.keys()))
            else:
                st.info(&#34;No analysis results available. Please run an analysis first.&#34;)

    def render_predict_visualizations(self):
        &#34;&#34;&#34;Render predict analysis visualizations&#34;&#34;&#34;
        st.markdown(&#34;### Predict Analysis Results&#34;)

        # Check if predict analysis results exist
        if (st.session_state.analysis_results is None or
            &#34;predictions&#34; not in st.session_state.analysis_results):
            st.info(&#34;No predict analysis results available. Run predict analysis first.&#34;)
            return

        predictions_data = st.session_state.analysis_results[&#34;predictions&#34;]

        # Display flow graphs
        st.markdown(&#34;#### Flow Graphs&#34;)
        col1, col2 = st.columns(2)

        with col1:
            st.markdown(&#34;##### Overall Flow Graph&#34;)
            flow_map_path = os.path.join(&#34;gui_outputs&#34;, &#34;Flow_Graph_Map.png&#34;)
            if os.path.exists(flow_map_path):
                st.image(flow_map_path, width=&#39;stretch&#39;)
            else:
                st.info(&#34;Flow graph map not available&#34;)

        with col2:
            st.markdown(&#34;##### Per-Junction Flow Graph&#34;)
            per_junction_path = os.path.join(&#34;gui_outputs&#34;, &#34;Per_Junction_Flow_Graph.png&#34;)
            if os.path.exists(per_junction_path):
                st.image(per_junction_path, width=&#39;stretch&#39;)
            else:
                st.info(&#34;Per-junction flow graph not available&#34;)

        # Display conditional probability heatmap
        st.markdown(&#34;#### Conditional Probability Analysis&#34;)
        heatmap_path = os.path.join(&#34;gui_outputs&#34;, &#34;conditional_probability_heatmap.png&#34;)
        if os.path.exists(heatmap_path):
            st.image(heatmap_path, width=&#39;stretch&#39;)
        else:
            st.info(&#34;Conditional probability heatmap not available&#34;)

        # Display behavioral pattern analysis
        st.markdown(&#34;#### Behavioral Pattern Distribution&#34;)
        pattern_path = os.path.join(&#34;gui_outputs&#34;, &#34;behavioral_patterns.png&#34;)
        if os.path.exists(pattern_path):
            st.image(pattern_path, width=&#39;stretch&#39;)
        else:
            st.info(&#34;Behavioral pattern analysis not available&#34;)

        # Display summary statistics
        if &#34;summary&#34; in predictions_data:
            st.markdown(&#34;#### Analysis Summary&#34;)
            summary = predictions_data[&#34;summary&#34;]

            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric(&#34;Total Trajectories&#34;, summary.get(&#34;total_trajectories&#34;, 0))
            with col2:
                st.metric(&#34;Total Junctions&#34;, summary.get(&#34;total_junctions&#34;, 0))
            with col3:
                st.metric(&#34;Total Transitions&#34;, summary.get(&#34;total_transitions&#34;, 0))
            with col4:
                st.metric(&#34;Unique Patterns&#34;, summary.get(&#34;unique_patterns&#34;, 0))

        # Interactive Junction Prediction Tool
        st.markdown(&#34;#### Interactive Junction Prediction&#34;)
        self._render_interactive_prediction_tool(predictions_data)

    def _render_interactive_prediction_tool(self, predictions_data):
        &#34;&#34;&#34;Render interactive junction prediction tool&#34;&#34;&#34;
        if &#34;conditional_probabilities&#34; not in predictions_data:
            st.info(&#34;No conditional probability data available for predictions.&#34;)
            return

        conditional_probs = predictions_data[&#34;conditional_probabilities&#34;]

        # Get available junctions from the conditional probabilities
        available_junctions = []
        for origin_key in conditional_probs.keys():
            junction_num = int(origin_key.split(&#39;_&#39;)[1][1:])  # Extract from &#34;from_J0&#34;
            available_junctions.append(junction_num)

        available_junctions = sorted(set(available_junctions))

        if not available_junctions:
            st.info(&#34;No junction data available for predictions.&#34;)
            return

        st.markdown(&#34;Select a decision junction and analyze probabilities for connected junctions:&#34;)

        col1, col2 = st.columns(2)

        with col1:
            # Decision junction selection
            decision_junction = st.selectbox(
                &#34;Decision Junction&#34;,
                options=available_junctions,
                format_func=lambda x: f&#34;J{x}&#34;,
                key=&#34;prediction_decision_junction&#34;
            )

        with col2:
            # Direction selection
            direction = st.selectbox(
                &#34;Analysis Direction&#34;,
                options=[&#34;Predecessor Analysis&#34;, &#34;Successor Analysis&#34;],
                key=&#34;prediction_direction&#34;
            )

        # Get connected junctions based on direction
        if direction == &#34;Predecessor Analysis&#34;:
            # Find junctions that lead TO the decision junction
            connected_junctions = []
            for origin_key, destinations in conditional_probs.items():
                origin_num = int(origin_key.split(&#39;_&#39;)[1][1:])
                if f&#34;J{decision_junction}&#34; in destinations:
                    connected_junctions.append(origin_num)

            if not connected_junctions:
                st.info(f&#34;No predecessors found for J{decision_junction}&#34;)
                return

            # Predecessor selection
            predecessor = st.selectbox(
                &#34;Select Predecessor Junction&#34;,
                options=sorted(connected_junctions),
                format_func=lambda x: f&#34;J{x}&#34;,
                key=&#34;prediction_predecessor&#34;
            )

            # Calculate probabilities
            self._calculate_predecessor_probabilities(conditional_probs, decision_junction, predecessor)

        else:  # Successor Analysis
            # Find junctions that the decision junction leads TO
            origin_key = f&#34;from_J{decision_junction}&#34;
            if origin_key not in conditional_probs:
                st.info(f&#34;No successors found for J{decision_junction}&#34;)
                return

            destinations = conditional_probs[origin_key]
            connected_junctions = [int(dest[1:]) for dest in destinations.keys()]

            if not connected_junctions:
                st.info(f&#34;No successors found for J{decision_junction}&#34;)
                return

            # Successor selection
            successor = st.selectbox(
                &#34;Select Successor Junction&#34;,
                options=sorted(connected_junctions),
                format_func=lambda x: f&#34;J{x}&#34;,
                key=&#34;prediction_successor&#34;
            )

            # Calculate probabilities
            self._calculate_successor_probabilities(conditional_probs, decision_junction, successor)

    def _calculate_predecessor_probabilities(self, conditional_probs, decision_junction, predecessor):
        &#34;&#34;&#34;Calculate probabilities for predecessor analysis - what happens AFTER decision junction when coming FROM predecessor&#34;&#34;&#34;
        st.markdown(f&#34;### Analysis: J{predecessor} → J{decision_junction} → ?&#34;)
        st.markdown(f&#34;**Question**: What junctions do trajectories visit AFTER J{decision_junction} when they came FROM J{predecessor}? (excluding self-loops)**&#34;)

        # Get cached sequences from analysis results
        if &#34;cached_sequences&#34; not in st.session_state.analysis_results.get(&#34;predictions&#34;, {}):
            st.error(&#34;No trajectory sequence data available. Please rerun the predict analysis.&#34;)
            return

        cached_sequences = st.session_state.analysis_results[&#34;predictions&#34;][&#34;cached_sequences&#34;]

        # Find trajectories that follow the J{predecessor} → J{decision_junction} sequence
        relevant_trajectories = []
        successor_counts = {}

        for traj_idx, sequence in cached_sequences.items():
            # Check if this trajectory follows the predecessor → decision sequence
            for i in range(len(sequence) - 1):
                if sequence[i] == predecessor and sequence[i + 1] == decision_junction:
                    relevant_trajectories.append(traj_idx)

                    # Find what happens after the decision junction (only count different junctions)
                    if i + 2 &lt; len(sequence):  # There&#39;s a junction after the decision junction
                        successor = sequence[i + 2]
                        # Only count as successor if it&#39;s a different junction (no self-loops)
                        if successor != decision_junction:
                            successor_counts[successor] = successor_counts.get(successor, 0) + 1
                    break

        if not relevant_trajectories:
            st.info(f&#34;No trajectories found that follow the J{predecessor} → J{decision_junction} sequence&#34;)
            return

        total_trajectories = len(relevant_trajectories)
        st.markdown(f&#34;**Found {total_trajectories} trajectories that follow J{predecessor} → J{decision_junction}**&#34;)

        if not successor_counts:
            st.info(f&#34;None of these trajectories continue to another junction after J{decision_junction}&#34;)
            return

        # Calculate probabilities
        successor_probs = {}
        for successor, count in successor_counts.items():
            prob = (count / total_trajectories) * 100
            successor_probs[f&#34;J{successor}&#34;] = prob

        # Create visualization
        import matplotlib.pyplot as plt
        import pandas as pd

        successor_names = list(successor_probs.keys())
        probabilities = list(successor_probs.values())

        fig, ax = plt.subplots(figsize=(10, 6))
        bars = ax.bar(successor_names, probabilities, color=&#39;skyblue&#39;, alpha=0.7)

        ax.set_xlabel(&#39;Successor Junction&#39;)
        ax.set_ylabel(&#39;Probability (%)&#39;)
        ax.set_title(f&#39;Direct Successor Probabilities: J{predecessor} → J{decision_junction} → ?&#39;)
        ax.set_ylim(0, max(probabilities) * 1.1)

        # Add value labels on bars
        for bar, prob in zip(bars, probabilities):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                   f&#39;{prob:.1f}%&#39;, ha=&#39;center&#39;, va=&#39;bottom&#39;)

        plt.xticks(rotation=45)
        plt.tight_layout()
        st.pyplot(fig)

        # Show detailed table
        st.markdown(&#34;#### Detailed Analysis:&#34;)
        prob_data = []
        for successor, prob in successor_probs.items():
            count = successor_counts[int(successor[1:])]
            prob_data.append({
                &#39;Successor Junction&#39;: successor,
                &#39;Trajectory Count&#39;: count,
                &#39;Probability (%)&#39;: f&#34;{prob:.1f}&#34;,
                &#39;Sequence&#39;: f&#34;J{predecessor} → J{decision_junction} → {successor}&#34;
            })

        df = pd.DataFrame(prob_data)
        st.dataframe(df, width=&#39;stretch&#39;)

        # Show trajectory examples
        st.markdown(&#34;#### Example Trajectory Sequences:&#34;)
        example_count = 0
        for traj_idx in relevant_trajectories[:5]:  # Show first 5 examples
            sequence = cached_sequences[traj_idx]
            seq_str = &#34; → &#34;.join([f&#34;J{j}&#34; for j in sequence])
            st.write(f&#34;**Trajectory {traj_idx}**: {seq_str}&#34;)
            example_count += 1

        if len(relevant_trajectories) &gt; 5:
            st.write(f&#34;... and {len(relevant_trajectories) - 5} more trajectories&#34;)

    def _calculate_successor_probabilities(self, conditional_probs, decision_junction, successor):
        &#34;&#34;&#34;Calculate probabilities for successor analysis - what happened BEFORE decision junction when going TO successor&#34;&#34;&#34;
        st.markdown(f&#34;### Analysis: ? → J{decision_junction} → J{successor}&#34;)
        st.markdown(f&#34;**Question**: What junctions did trajectories visit BEFORE J{decision_junction} when they went TO J{successor}? (excluding self-loops)**&#34;)

        # Get cached sequences from analysis results
        if &#34;cached_sequences&#34; not in st.session_state.analysis_results.get(&#34;predictions&#34;, {}):
            st.error(&#34;No trajectory sequence data available. Please rerun the predict analysis.&#34;)
            return

        cached_sequences = st.session_state.analysis_results[&#34;predictions&#34;][&#34;cached_sequences&#34;]

        # Find trajectories that follow the J{decision_junction} → J{successor} sequence
        relevant_trajectories = []
        predecessor_counts = {}

        for traj_idx, sequence in cached_sequences.items():
            # Check if this trajectory follows the decision → successor sequence
            for i in range(len(sequence) - 1):
                if sequence[i] == decision_junction and sequence[i + 1] == successor:
                    relevant_trajectories.append(traj_idx)

                    # Find what happened before the decision junction (only count different junctions)
                    if i &gt; 0:  # There&#39;s a junction before the decision junction
                        predecessor = sequence[i - 1]
                        # Only count as predecessor if it&#39;s a different junction (no self-loops)
                        if predecessor != decision_junction:
                            predecessor_counts[predecessor] = predecessor_counts.get(predecessor, 0) + 1
                        else:
                            # Skip self-loops and look further back
                            j = i - 1
                            while j &gt;= 0 and sequence[j] == decision_junction:
                                j -= 1
                            if j &gt;= 0:  # Found a different junction
                                predecessor = sequence[j]
                                predecessor_counts[predecessor] = predecessor_counts.get(predecessor, 0) + 1
                    break

        if not relevant_trajectories:
            st.info(f&#34;No trajectories found that follow the J{decision_junction} → J{successor} sequence&#34;)
            return

        total_trajectories = len(relevant_trajectories)
        st.markdown(f&#34;**Found {total_trajectories} trajectories that follow J{decision_junction} → J{successor}**&#34;)

        if not predecessor_counts:
            st.info(f&#34;None of these trajectories came from another junction before J{decision_junction}&#34;)
            return

        # Calculate probabilities
        predecessor_probs = {}
        for predecessor, count in predecessor_counts.items():
            prob = (count / total_trajectories) * 100
            predecessor_probs[f&#34;J{predecessor}&#34;] = prob

        # Create visualization
        import matplotlib.pyplot as plt
        import pandas as pd

        predecessor_names = list(predecessor_probs.keys())
        probabilities = list(predecessor_probs.values())

        fig, ax = plt.subplots(figsize=(10, 6))
        bars = ax.bar(predecessor_names, probabilities, color=&#39;lightgreen&#39;, alpha=0.7)

        ax.set_xlabel(&#39;Predecessor Junction&#39;)
        ax.set_ylabel(&#39;Probability (%)&#39;)
        ax.set_title(f&#39;Direct Predecessor Probabilities: ? → J{decision_junction} → J{successor}&#39;)
        ax.set_ylim(0, max(probabilities) * 1.1)

        # Add value labels on bars
        for bar, prob in zip(bars, probabilities):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                   f&#39;{prob:.1f}%&#39;, ha=&#39;center&#39;, va=&#39;bottom&#39;)

        plt.xticks(rotation=45)
        plt.tight_layout()
        st.pyplot(fig)

        # Show detailed table
        st.markdown(&#34;#### Detailed Analysis:&#34;)
        prob_data = []
        for predecessor, prob in predecessor_probs.items():
            count = predecessor_counts[int(predecessor[1:])]
            prob_data.append({
                &#39;Predecessor Junction&#39;: predecessor,
                &#39;Trajectory Count&#39;: count,
                &#39;Probability (%)&#39;: f&#34;{prob:.1f}&#34;,
                &#39;Sequence&#39;: f&#34;{predecessor} → J{decision_junction} → J{successor}&#34;
            })

        df = pd.DataFrame(prob_data)
        st.dataframe(df, width=&#39;stretch&#39;)

        # Show trajectory examples
        st.markdown(&#34;#### Example Trajectory Sequences:&#34;)
        example_count = 0
        for traj_idx in relevant_trajectories[:5]:  # Show first 5 examples
            sequence = cached_sequences[traj_idx]
            seq_str = &#34; → &#34;.join([f&#34;J{j}&#34; for j in sequence])
            st.write(f&#34;**Trajectory {traj_idx}**: {seq_str}&#34;)
            example_count += 1

        if len(relevant_trajectories) &gt; 5:
            st.write(f&#34;... and {len(relevant_trajectories) - 5} more trajectories&#34;)

    def render_flow_graphs(self):
        &#34;&#34;&#34;Render flow graph visualizations&#34;&#34;&#34;
        st.markdown(&#34;### Flow Graphs&#34;)

        col1, col2 = st.columns(2)

        with col1:
            st.markdown(&#34;#### Overall Flow Graph&#34;)
            if &#34;flow_graph_map&#34; in st.session_state.analysis_results:
                st.image(st.session_state.analysis_results[&#34;flow_graph_map&#34;], width=&#39;stretch&#39;)

        with col2:
            st.markdown(&#34;#### Per-Junction Flow Graph&#34;)
            if &#34;per_junction_flow_graph&#34; in st.session_state.analysis_results:
                st.image(st.session_state.analysis_results[&#34;per_junction_flow_graph&#34;], width=&#39;stretch&#39;)

    def render_discover_visualizations(self):
        &#34;&#34;&#34;Render discover analysis visualizations&#34;&#34;&#34;
        st.markdown(&#34;### Discover Analysis Results&#34;)

        # Display decision intercepts for each junction
        for junction_key, branches_data in st.session_state.analysis_results[&#34;branches&#34;].items():
            if junction_key == &#34;chain_decisions&#34;:  # Skip the chain decisions data
                continue

            st.markdown(f&#34;#### {junction_key.replace(&#39;_&#39;, &#39; &#39;).title()}&#34;)

            # Show decision intercepts plot
            junction_num = junction_key.split(&#39;_&#39;)[1]
            junction_dir = os.path.join(&#34;gui_outputs&#34;, f&#34;junction_{junction_num}&#34;)

            # Display available plots
            intercepts_path = os.path.join(junction_dir, &#34;Decision_Intercepts.png&#34;)
            if os.path.exists(intercepts_path):
                st.image(intercepts_path, caption=f&#34;Decision Intercepts - {junction_key}&#34;, width=&#39;stretch&#39;)
            else:
                st.warning(f&#34;Decision intercepts plot not found for {junction_key}&#34;)

            # Check for other available plots that might be generated
            other_plots = [
                (&#34;Decision_Map.png&#34;, &#34;Decision Map&#34;),
                (&#34;Branch_Counts.png&#34;, &#34;Branch Counts&#34;),
                (&#34;Branch_Directions.png&#34;, &#34;Branch Directions&#34;)
            ]

            for plot_file, plot_name in other_plots:
                plot_path = os.path.join(junction_dir, plot_file)
                if os.path.exists(plot_path):
                    st.image(plot_path, caption=f&#34;{plot_name} - {junction_key}&#34;, width=&#39;stretch&#39;)

            # Show branch summary
            if &#34;summary&#34; in branches_data and branches_data[&#34;summary&#34;] is not None:
                st.markdown(&#34;**Branch Summary:**&#34;)
                st.dataframe(branches_data[&#34;summary&#34;], width=&#39;stretch&#39;)

            # Show assignments preview
            if &#34;assignments&#34; in branches_data and branches_data[&#34;assignments&#34;] is not None:
                st.markdown(&#34;**Branch Assignments (first 20):**&#34;)
                st.dataframe(branches_data[&#34;assignments&#34;].head(20), width=&#39;stretch&#39;)

    def render_assign_visualizations(self):
        &#34;&#34;&#34;Render assign analysis visualizations&#34;&#34;&#34;
        st.markdown(&#34;### Assign Analysis Results&#34;)

        # Display assignment results for each junction
        for junction_key, assignments_data in st.session_state.analysis_results[&#34;assignments&#34;].items():
            st.markdown(f&#34;#### {junction_key.replace(&#39;_&#39;, &#39; &#39;).title()}&#34;)

            # Extract the actual assignments DataFrame from the nested structure
            if isinstance(assignments_data, dict) and &#34;assignments&#34; in assignments_data:
                assignments_df = assignments_data[&#34;assignments&#34;]
            else:
                assignments_df = assignments_data

            # Show assignment data
            if assignments_df is not None and hasattr(assignments_df, &#39;head&#39;):
                st.markdown(&#34;**Branch Assignments:**&#34;)
                st.dataframe(assignments_df.head(20), width=&#39;stretch&#39;)

                if len(assignments_df) &gt; 20:
                    st.info(f&#34;Showing first 20 of {len(assignments_df)} assignments&#34;)

                # Show assignment statistics
                if &#39;branch&#39; in assignments_df.columns:
                    branch_counts = assignments_df[&#39;branch&#39;].value_counts()
                    st.markdown(&#34;**Branch Distribution:**&#34;)
                    st.bar_chart(branch_counts)

                    # Show detailed statistics
                    st.markdown(&#34;**Assignment Statistics:**&#34;)
                    total_trajectories = len(assignments_df)
                    for branch, count in branch_counts.items():
                        percentage = (count / total_trajectories) * 100
                        st.write(f&#34;- Branch {branch}: {count} trajectories ({percentage:.1f}%)&#34;)
            else:
                st.info(f&#34;No assignment data available for {junction_key}&#34;)

    def render_metrics_visualizations(self):
        &#34;&#34;&#34;Render metrics analysis visualizations&#34;&#34;&#34;
        st.markdown(&#34;### Metrics Analysis Results&#34;)

        metrics_data = st.session_state.analysis_results[&#34;metrics&#34;]
        metrics_images = st.session_state.analysis_results.get(&#34;metrics_images&#34;, {})

        if metrics_data:
            # Convert to DataFrame for better display
            import pandas as pd
            df = pd.DataFrame(metrics_data)

            # Display metrics table
            st.markdown(&#34;**Trajectory Metrics:**&#34;)
            st.dataframe(df, width=&#39;stretch&#39;)

            # Create distribution visualizations (prefer pre-generated images)
            col1, col2 = st.columns(2)

            with col1:
                st.markdown(&#34;**Total Time Distribution**&#34;)
                img = metrics_images.get(&#34;total_time_distribution.png&#34;)
                if img and os.path.exists(img):
                    st.image(img, width=&#39;stretch&#39;)
                else:
                    if &#39;total_time&#39; in df.columns:
                        valid_times = df[&#39;total_time&#39;].dropna()
                        if len(valid_times) &gt; 0:
                            sorted_times = valid_times.sort_values().reset_index(drop=True)
                            st.bar_chart(sorted_times)
                            st.caption(f&#34;Range: {sorted_times.min():.1f}s - {sorted_times.max():.1f}s&#34;)
                            st.caption(f&#34;Mean: {sorted_times.mean():.1f}s, Median: {sorted_times.median():.1f}s&#34;)
                        else:
                            st.info(&#34;No valid time data available&#34;)

            with col2:
                st.markdown(&#34;**Average Speed Distribution**&#34;)
                img = metrics_images.get(&#34;average_speed_distribution.png&#34;)
                if img and os.path.exists(img):
                    st.image(img, width=&#39;stretch&#39;)
                else:
                    if &#39;average_speed&#39; in df.columns:
                        valid_speeds = df[&#39;average_speed&#39;].dropna()
                        if len(valid_speeds) &gt; 0:
                            sorted_speeds = valid_speeds.sort_values().reset_index(drop=True)
                            st.bar_chart(sorted_speeds)
                            st.caption(f&#34;Range: {sorted_speeds.min():.2f} - {sorted_speeds.max():.2f}&#34;)
                            st.caption(f&#34;Mean: {sorted_speeds.mean():.2f}, Median: {sorted_speeds.median():.2f}&#34;)
                        else:
                            st.info(&#34;No valid speed data available&#34;)

            # Add distance visualization
            col3, col4 = st.columns(2)

            with col3:
                st.markdown(&#34;**Total Distance Distribution**&#34;)
                img = metrics_images.get(&#34;total_distance_distribution.png&#34;)
                if img and os.path.exists(img):
                    st.image(img, width=&#39;stretch&#39;)
                else:
                    if &#39;total_distance&#39; in df.columns:
                        valid_distances = df[&#39;total_distance&#39;].dropna()
                        if len(valid_distances) &gt; 0:
                            sorted_distances = valid_distances.sort_values().reset_index(drop=True)
                            st.bar_chart(sorted_distances)
                            st.caption(f&#34;Range: {sorted_distances.min():.1f} - {sorted_distances.max():.1f}&#34;)
                            st.caption(f&#34;Mean: {sorted_distances.mean():.1f}, Median: {sorted_distances.median():.1f}&#34;)
                        else:
                            st.info(&#34;No valid distance data available&#34;)

            with col4:
                st.markdown(&#34;**Summary Statistics**&#34;)
                if len(df) &gt; 0:
                    summary_stats = {
                        &#34;Total Trajectories&#34;: len(df),
                        &#34;Avg Total Time&#34;: f&#34;{df[&#39;total_time&#39;].mean():.2f}s&#34; if &#39;total_time&#39; in df.columns else &#34;N/A&#34;,
                        &#34;Avg Total Distance&#34;: f&#34;{df[&#39;total_distance&#39;].mean():.2f}&#34; if &#39;total_distance&#39; in df.columns else &#34;N/A&#34;,
                        &#34;Avg Speed&#34;: f&#34;{df[&#39;average_speed&#39;].mean():.2f}&#34; if &#39;average_speed&#39; in df.columns else &#34;N/A&#34;
                    }
                    for key, value in summary_stats.items():
                        st.metric(key, value)

            # Define junction columns early for use in speed analysis
            junction_cols = [col for col in df.columns if col.startswith(&#39;junction_&#39;) and col.endswith(&#39;_time&#39;)]

            # Speed analysis visualizations
            speed_cols = [col for col in df.columns if col.startswith(&#39;junction_&#39;) and col.endswith(&#39;_speed&#39;)]
            if speed_cols:
                st.markdown(&#34;### Junction Speed Analysis&#34;)

                # Create speed analysis summary
                speed_summary = []
                for col in speed_cols:
                    junction_num = col.split(&#39;_&#39;)[1]
                    speed_mode_col = f&#34;junction_{junction_num}_speed_mode&#34;
                    entry_speed_col = f&#34;junction_{junction_num}_entry_speed&#34;
                    exit_speed_col = f&#34;junction_{junction_num}_exit_speed&#34;
                    avg_transit_col = f&#34;junction_{junction_num}_avg_transit_speed&#34;

                    valid_speeds = df[col].dropna()
                    total_trajectories = len(df)
                    valid_count = len(valid_speeds)

                    if valid_count &gt; 0:
                        speed_summary.append({
                            &#34;Junction&#34;: f&#34;Junction {junction_num}&#34;,
                            &#34;Avg Speed Through&#34;: f&#34;{valid_speeds.mean():.2f}&#34;,
                            &#34;Std Speed Through&#34;: f&#34;{valid_speeds.std():.2f}&#34;,
                            &#34;Valid Count&#34;: valid_count,
                            &#34;NaN Count&#34;: total_trajectories - valid_count
                        })

                if speed_summary:
                    speed_df = pd.DataFrame(speed_summary)
                    st.markdown(&#34;**Junction Speed Statistics:**&#34;)
                    st.dataframe(speed_df, width=&#39;stretch&#39;)

                    # One concise explanation above both diagrams
                    st.markdown(&#34;### Speed Analysis&#34;)
                    st.info(&#34;&#34;&#34;
                    **Available Speed Metrics:** Entry (2–5 s before), Exit (2–5 s after), and Average Transit (inside junction).
                    Use the selector in the correlation plot to switch the speed metric.
                    &#34;&#34;&#34;)

                    # Show correlation (left) and entry/exit bars (right) side-by-side
                    col_speed1, col_speed2 = st.columns(2)

                    with col_speed1:
                        st.markdown(&#34;**Speed vs Time Correlation**&#34;)
                        img = metrics_images.get(&#34;speed_vs_time_correlation.png&#34;)
                        if img and os.path.exists(img):
                            st.image(img, width=&#39;stretch&#39;)
                        else:
                            st.info(&#34;Correlation plot not available yet. Re-run metrics analysis to generate.&#34;)

                    with col_speed2:
                        st.markdown(&#34;**Entry vs Exit Speed Analysis**&#34;)
                        st.caption(&#34;**Entry Speed**: Average speed in 2-5 second window before entering junction&#34;)
                        st.caption(&#34;**Exit Speed**: Average speed in 2-5 second window after leaving junction&#34;)
                        img = metrics_images.get(&#34;entry_exit_speed_by_junction.png&#34;)
                        if img and os.path.exists(img):
                            st.image(img, width=&#39;stretch&#39;)
                        else:
                            st.info(&#34;Entry/Exit bar chart not available yet. Re-run metrics analysis to generate.&#34;)

                # Detailed speed metrics table
                st.markdown(&#34;### Detailed Speed Metrics&#34;)
                speed_detail_cols = [col for col in df.columns if &#39;speed&#39; in col.lower()]
                if speed_detail_cols:
                    speed_detail_df = df[speed_detail_cols + [&#39;trajectory_id&#39;, &#39;trajectory_tid&#39;]]
                    st.dataframe(speed_detail_df, width=&#39;stretch&#39;)

            # Junction-specific metrics if available
            if junction_cols:
                st.markdown(&#34;### Junction Timing Analysis&#34;)

                # Check for NaN values and provide explanation
                total_junction_measurements = len(df) * len(junction_cols)
                valid_junction_measurements = sum(len(df[col].dropna()) for col in junction_cols)
                nan_count = total_junction_measurements - valid_junction_measurements

                if nan_count &gt; 0:
                    st.info(f&#34;ℹ️ **Note**: {nan_count} out of {total_junction_measurements} junction timing measurements returned NaN. This typically means trajectories didn&#39;t pass through those junctions or timing couldn&#39;t be computed.&#34;)

                # Create junction timing summary
                junction_summary = []
                for col in junction_cols:
                    junction_num = col.split(&#39;_&#39;)[1]
                    mode_col = f&#34;junction_{junction_num}_mode&#34;
                    if mode_col in df.columns:
                        valid_times = df[col].dropna()
                        total_trajectories = len(df)
                        valid_count = len(valid_times)
                        nan_count_junction = total_trajectories - valid_count

                        if valid_count &gt; 0:
                            junction_summary.append({
                                &#34;Junction&#34;: f&#34;Junction {junction_num}&#34;,
                                &#34;Avg Time&#34;: f&#34;{valid_times.mean():.2f}s&#34;,
                                &#34;Std Time&#34;: f&#34;{valid_times.std():.2f}s&#34;,
                                &#34;Valid Count&#34;: valid_count,
                                &#34;NaN Count&#34;: nan_count_junction
                            })
                        else:
                            junction_summary.append({
                                &#34;Junction&#34;: f&#34;Junction {junction_num}&#34;,
                                &#34;Avg Time&#34;: &#34;N/A&#34;,
                                &#34;Std Time&#34;: &#34;N/A&#34;,
                                &#34;Valid Count&#34;: 0,
                                &#34;NaN Count&#34;: total_trajectories
                            })

                if junction_summary:
                    junction_df = pd.DataFrame(junction_summary)
                    st.markdown(&#34;**Junction Statistics (Only trajectories that actually pass through each junction):**&#34;)
                    st.dataframe(junction_df, width=&#39;stretch&#39;)

                    # Junction timing visualization
                    st.markdown(&#34;**Junction Timing Comparison**&#34;)
                    img = metrics_images.get(&#34;junction_timing_comparison.png&#34;)
                    if img and os.path.exists(img):
                        st.image(img, width=&#39;stretch&#39;)
                    else:
                        st.info(&#34;Timing comparison chart not available yet. Re-run metrics analysis to generate.&#34;)

                    # Show individual junction timing distributions
                    st.markdown(&#34;**Individual Junction Timing Distributions**&#34;)
                    if metrics_images:
                        # display per-junction histograms if present
                        for name, path in sorted(metrics_images.items()):
                            if name.startswith(&#34;timing_distribution_J&#34;) and os.path.exists(path):
                                jlabel = name.replace(&#34;timing_distribution_&#34;, &#34;&#34;).replace(&#34;.png&#34;, &#34;&#34;)
                                st.markdown(f&#34;**{jlabel.replace(&#39;_&#39;, &#39; &#39;)}**&#34;)
                                st.image(path, width=&#39;stretch&#39;)
                    else:
                        for col in junction_cols:
                            junction_num = col.split(&#39;_&#39;)[1]
                            valid_times = df[col].dropna()
                            if len(valid_times) &gt; 0:
                                st.markdown(f&#34;**Junction {junction_num} Timing Distribution**&#34;)
                                sorted_times = valid_times.sort_values().reset_index(drop=True)
                                st.bar_chart(sorted_times)
                                st.caption(f&#34;Range: {sorted_times.min():.2f}s - {sorted_times.max():.2f}s, Mean: {sorted_times.mean():.2f}s&#34;)


    def _analyze_movement_patterns_at_junction(self, trajectories, junction, r_outer, decision_mode, path_length, epsilon):
        &#34;&#34;&#34;Analyze movement patterns at a junction for regular trajectories (without head tracking data).&#34;&#34;&#34;
        import numpy as np
        import pandas as pd

        results = []

        for traj_idx, trajectory in enumerate(trajectories):
            # Find decision point using the same logic as the gaze analysis
            if decision_mode == &#34;radial&#34; or (decision_mode == &#34;hybrid&#34; and r_outer &gt; junction.r):
                # Use radial decision point
                decision_idx = self._find_radial_decision_point(trajectory, junction, r_outer)
            else:
                # Use path length decision point
                decision_idx = self._find_path_length_decision_point(trajectory, junction, path_length, epsilon)

            if decision_idx is None:
                # Fallback to nearest point to junction center
                decision_idx = self._find_nearest_to_center(trajectory, junction)

            if decision_idx is not None and decision_idx &lt; len(trajectory.x):
                # Calculate movement direction at decision point with better edge case handling
                movement_yaw = np.nan
                if decision_idx &gt; 0 and decision_idx &lt; len(trajectory.x) - 1:
                    dx = trajectory.x[decision_idx + 1] - trajectory.x[decision_idx - 1]
                    dz = trajectory.z[decision_idx + 1] - trajectory.z[decision_idx - 1]
                    movement_magnitude = np.hypot(dx, dz)
                    if movement_magnitude &gt; 1e-3:  # Increased threshold for numerical stability
                        movement_yaw = np.degrees(np.arctan2(dx, dz))

                # Calculate approach direction (direction from previous point to decision point)
                approach_yaw = np.nan
                if decision_idx &gt; 0:
                    dx_approach = trajectory.x[decision_idx] - trajectory.x[decision_idx - 1]
                    dz_approach = trajectory.z[decision_idx] - trajectory.z[decision_idx - 1]
                    approach_magnitude = np.hypot(dx_approach, dz_approach)
                    if approach_magnitude &gt; 1e-3:
                        approach_yaw = np.degrees(np.arctan2(dx_approach, dz_approach))

                # Calculate exit direction (direction from decision point to next point)
                exit_yaw = np.nan
                if decision_idx &lt; len(trajectory.x) - 1:
                    dx_exit = trajectory.x[decision_idx + 1] - trajectory.x[decision_idx]
                    dz_exit = trajectory.z[decision_idx + 1] - trajectory.z[decision_idx]
                    exit_magnitude = np.hypot(dx_exit, dz_exit)
                    if exit_magnitude &gt; 1e-3:
                        exit_yaw = np.degrees(np.arctan2(dx_exit, dz_exit))

                # Calculate distance from junction center
                distance_from_center = np.sqrt(
                    (trajectory.x[decision_idx] - junction.cx)**2 +
                    (trajectory.z[decision_idx] - junction.cz)**2
                )

                # Calculate trajectory length for context
                trajectory_length = len(trajectory.x)

                results.append({
                    &#34;trajectory&#34;: traj_idx,
                    &#34;junction&#34;: 0,  # Single junction analysis
                    &#34;decision_idx&#34;: decision_idx,
                    &#34;trajectory_length&#34;: trajectory_length,
                    &#34;decision_ratio&#34;: decision_idx / trajectory_length if trajectory_length &gt; 0 else 0,
                    &#34;movement_yaw&#34;: movement_yaw,
                    &#34;approach_yaw&#34;: approach_yaw,
                    &#34;exit_yaw&#34;: exit_yaw,
                    &#34;distance_from_center&#34;: distance_from_center,
                    &#34;decision_x&#34;: trajectory.x[decision_idx],
                    &#34;decision_z&#34;: trajectory.z[decision_idx],
                    &#34;time_at_decision&#34;: self._safe_get_time_value(trajectory, decision_idx)
                })

        return pd.DataFrame(results)

    def _analyze_movement_patterns_optimized(self, trajectories, junction, r_outer, decision_mode, path_length, epsilon):
        &#34;&#34;&#34;Optimized movement pattern analysis for a single junction.&#34;&#34;&#34;
        import numpy as np
        import pandas as pd

        results = []

        # Pre-calculate junction center for efficiency
        jx, jz = junction.cx, junction.cz

        for traj_idx, trajectory in enumerate(trajectories):
            # Fast decision point detection
            decision_idx = self._find_decision_point_fast(trajectory, jx, jz, r_outer, decision_mode, path_length, epsilon)

            if decision_idx is not None and decision_idx &lt; len(trajectory.x):
                # Calculate movement direction with optimized approach
                movement_yaw = np.nan
                if decision_idx &gt; 0 and decision_idx &lt; len(trajectory.x) - 1:
                    dx = trajectory.x[decision_idx + 1] - trajectory.x[decision_idx - 1]
                    dz = trajectory.z[decision_idx + 1] - trajectory.z[decision_idx - 1]
                    movement_magnitude = np.hypot(dx, dz)
                    if movement_magnitude &gt; 1e-3:
                        movement_yaw = np.degrees(np.arctan2(dx, dz))

                # Calculate approach and exit directions
                approach_yaw = np.nan
                if decision_idx &gt; 0:
                    dx_approach = trajectory.x[decision_idx] - trajectory.x[decision_idx - 1]
                    dz_approach = trajectory.z[decision_idx] - trajectory.z[decision_idx - 1]
                    approach_magnitude = np.hypot(dx_approach, dz_approach)
                    if approach_magnitude &gt; 1e-3:
                        approach_yaw = np.degrees(np.arctan2(dx_approach, dz_approach))

                exit_yaw = np.nan
                if decision_idx &lt; len(trajectory.x) - 1:
                    dx_exit = trajectory.x[decision_idx + 1] - trajectory.x[decision_idx]
                    dz_exit = trajectory.z[decision_idx + 1] - trajectory.z[decision_idx]
                    exit_magnitude = np.hypot(dx_exit, dz_exit)
                    if exit_magnitude &gt; 1e-3:
                        exit_yaw = np.degrees(np.arctan2(dx_exit, dz_exit))

                # Calculate distance from junction center
                distance_from_center = np.sqrt(
                    (trajectory.x[decision_idx] - jx)**2 +
                    (trajectory.z[decision_idx] - jz)**2
                )

                # Calculate trajectory position metrics
                trajectory_length = len(trajectory.x)
                decision_ratio = decision_idx / trajectory_length if trajectory_length &gt; 0 else 0

                results.append({
                    &#34;trajectory&#34;: traj_idx,
                    &#34;junction&#34;: 0,  # Single junction analysis
                    &#34;decision_idx&#34;: decision_idx,
                    &#34;trajectory_length&#34;: trajectory_length,
                    &#34;decision_ratio&#34;: decision_ratio,
                    &#34;movement_yaw&#34;: movement_yaw,
                    &#34;approach_yaw&#34;: approach_yaw,
                    &#34;exit_yaw&#34;: exit_yaw,
                    &#34;distance_from_center&#34;: distance_from_center,
                    &#34;decision_x&#34;: trajectory.x[decision_idx],
                    &#34;decision_z&#34;: trajectory.z[decision_idx],
                    &#34;time_at_decision&#34;: self._safe_get_time_value(trajectory, decision_idx)
                })

        return pd.DataFrame(results)

    def _find_decision_point_fast(self, trajectory, jx, jz, r_outer, decision_mode, path_length, epsilon):
        &#34;&#34;&#34;Fast decision point detection optimized for performance.&#34;&#34;&#34;
        import numpy as np

        # Vectorized distance calculation
        distances = np.sqrt((trajectory.x - jx)**2 + (trajectory.z - jz)**2)

        if decision_mode == &#34;radial&#34; or (decision_mode == &#34;hybrid&#34; and r_outer &gt; 50.0):
            # Find first point within junction
            within_junction = distances &lt;= r_outer
            if np.any(within_junction):
                return np.argmax(within_junction)  # First True value
        else:
            # Find closest point and search around it
            closest_idx = np.argmin(distances)
            search_window = min(int(path_length), len(trajectory.x) // 10, 50)
            start_idx = max(0, closest_idx - search_window)
            end_idx = min(len(trajectory.x), closest_idx + search_window)

            # Look for point within junction radius
            for i in range(start_idx, end_idx):
                if distances[i] &lt;= 50.0 + epsilon:  # Use junction radius + epsilon
                    return i
            return closest_idx

        return None

    def _check_for_gaze_data(self, trajectories):
        &#34;&#34;&#34;Check if trajectories have gaze/physiological data using unified model.&#34;&#34;&#34;
        if not trajectories:
            return False

        # Use capability helpers from unified model
        from verta.verta_data_loader import has_gaze_data, has_physio_data, has_vr_headset_data

        # Check if ANY trajectory has gaze/physio capabilities
        has_gaze = any(has_gaze_data(traj) for traj in trajectories)
        has_physio = any(has_physio_data(traj) for traj in trajectories)
        has_vr = any(has_vr_headset_data(traj) for traj in trajectories)

        # Also check the first trajectory for debugging
        sample_traj = trajectories[0]
        sample_has_gaze = has_gaze_data(sample_traj)
        sample_has_physio = has_physio_data(sample_traj)
        sample_has_vr = has_vr_headset_data(sample_traj)

        st.write(f&#34;- Debug _check_for_gaze_data: has_gaze={has_gaze}, has_physio={has_physio}, has_vr_headset={has_vr}&#34;)
        st.write(f&#34;- Sample trajectory (first): has_gaze={sample_has_gaze}, has_physio={sample_has_physio}, has_vr_headset={sample_has_vr}&#34;)

        return has_gaze or has_physio or has_vr



    def _get_gaze_column_mappings(self):
        &#34;&#34;&#34;Get gaze column mappings from session state.&#34;&#34;&#34;
        return getattr(st.session_state, &#39;gaze_column_mappings&#39;, {})

    def _normalize_gaze_result_frames(self, results: dict) -&gt; dict:
        &#34;&#34;&#34;Normalize result column names so GUI plots find expected columns.&#34;&#34;&#34;
        # Physiological: ensure heart_rate_change, pupil_change
        if &#39;physiological&#39; in results and results[&#39;physiological&#39;] is not None:
            phys = results[&#39;physiological&#39;]
            # Handle both DataFrames and lists (converted from DataFrames)
            if hasattr(phys, &#39;rename&#39;):  # DataFrame
                phys = phys.rename(columns={
                    &#39;hr_change&#39;: &#39;heart_rate_change&#39;,
                    &#39;hr_delta&#39;: &#39;heart_rate_change&#39;,
                    &#39;pupil_delta&#39;: &#39;pupil_change&#39;,
                    &#39;pupil_dilation_change&#39;: &#39;pupil_change&#39;
                })
                results[&#39;physiological&#39;] = phys
            elif isinstance(phys, list):  # List of dictionaries (converted DataFrame)
                # Convert back to DataFrame, rename, then convert back to list
                import pandas as pd
                phys_df = pd.DataFrame(phys)
                phys_df = phys_df.rename(columns={
                    &#39;hr_change&#39;: &#39;heart_rate_change&#39;,
                    &#39;hr_delta&#39;: &#39;heart_rate_change&#39;,
                    &#39;pupil_delta&#39;: &#39;pupil_change&#39;,
                    &#39;pupil_dilation_change&#39;: &#39;pupil_change&#39;
                })
                results[&#39;physiological&#39;] = phys_df.to_dict(&#39;records&#39;)

        # Pupil dilation: ensure pupil_change
        if &#39;pupil_dilation&#39; in results and results[&#39;pupil_dilation&#39;] is not None:
            pup = results[&#39;pupil_dilation&#39;]
            if hasattr(pup, &#39;rename&#39;):  # DataFrame
                pup = pup.rename(columns={
                    &#39;pupil_delta&#39;: &#39;pupil_change&#39;,
                    &#39;pupil_dilation_change&#39;: &#39;pupil_change&#39;
                })
                results[&#39;pupil_dilation&#39;] = pup
            elif isinstance(pup, list):  # List of dictionaries (converted DataFrame)
                import pandas as pd
                pup_df = pd.DataFrame(pup)
                pup_df = pup_df.rename(columns={
                    &#39;pupil_delta&#39;: &#39;pupil_change&#39;,
                    &#39;pupil_dilation_change&#39;: &#39;pupil_change&#39;
                })
                results[&#39;pupil_dilation&#39;] = pup_df.to_dict(&#39;records&#39;)

        # Head yaw: ensure head_yaw, yaw_difference, intercept_x, intercept_z
        if &#39;head_yaw&#39; in results and results[&#39;head_yaw&#39;] is not None:
            yaw = results[&#39;head_yaw&#39;]
            if hasattr(yaw, &#39;rename&#39;):  # DataFrame
                yaw = yaw.rename(columns={
                    &#39;yaw&#39;: &#39;head_yaw&#39;,
                    &#39;delta_yaw&#39;: &#39;yaw_difference&#39;,
                    &#39;gaze_movement_diff&#39;: &#39;yaw_difference&#39;,
                    &#39;decision_x&#39;: &#39;intercept_x&#39;,
                    &#39;decision_z&#39;: &#39;intercept_z&#39;
                })
                results[&#39;head_yaw&#39;] = yaw
            elif isinstance(yaw, list):  # List of dictionaries (converted DataFrame)
                import pandas as pd
                yaw_df = pd.DataFrame(yaw)
                yaw_df = yaw_df.rename(columns={
                    &#39;yaw&#39;: &#39;head_yaw&#39;,
                    &#39;delta_yaw&#39;: &#39;yaw_difference&#39;,
                    &#39;gaze_movement_diff&#39;: &#39;yaw_difference&#39;,
                    &#39;decision_x&#39;: &#39;intercept_x&#39;,
                    &#39;decision_z&#39;: &#39;intercept_z&#39;
                })
                results[&#39;head_yaw&#39;] = yaw_df.to_dict(&#39;records&#39;)

        return results

    # (Removed) _plot_head_yaw_arrows_at_intercepts helper per request

    def _perform_gaze_analysis_with_mappings(self, trajectories, junction, r_outer, decision_mode, path_length, epsilon, linger_delta, out_dir, column_mappings, scale_factor=1.0):
        &#34;&#34;&#34;Perform gaze analysis using column mappings with scaling support.&#34;&#34;&#34;
        import pandas as pd
        import numpy as np
        from verta.verta_geometry import Circle

        # Filter trajectories to only include those with gaze or physiological data
        try:
            from verta.verta_data_loader import has_gaze_data as _has_gaze, has_physio_data as _has_physio
            filtered_trajectories = [t for t in trajectories if (_has_gaze(t) or _has_physio(t))]

            st.info(f&#34;🔍 **Trajectory Filtering in Analysis:**&#34;)
            st.write(f&#34;- Total trajectories: {len(trajectories)}&#34;)
            st.write(f&#34;- Trajectories with gaze/physio data: {len(filtered_trajectories)}&#34;)

            if len(filtered_trajectories) &lt; len(trajectories):
                skipped_count = len(trajectories) - len(filtered_trajectories)
                st.info(f&#34;ℹ️ Skipped {skipped_count} trajectories without gaze/physiological data&#34;)

            if not filtered_trajectories:
                st.warning(&#34;⚠️ No trajectories with gaze/physiological data found&#34;)
                return None

            trajectories = filtered_trajectories

        except Exception as e:
            st.warning(f&#34;⚠️ Error filtering trajectories: {e}&#34;)
            # Continue with original trajectories if filtering fails

        # Check if we have standard gaze data first
        has_gaze_data = self._check_for_gaze_data(trajectories)

        # Debug: Show which code path is being used
        st.info(f&#34;🔍 **Gaze Analysis Code Path Debug:**&#34;)
        st.write(f&#34;- Has gaze data: {has_gaze_data}&#34;)
        st.write(f&#34;- Column mappings provided: {bool(column_mappings)}&#34;)
        if column_mappings:
            st.write(f&#34;- Column mappings: {list(column_mappings.keys())}&#34;)

        # Debug: Check what data is actually available in trajectories
        if trajectories:
            sample_traj = trajectories[0]
            st.write(f&#34;**Sample trajectory data availability:**&#34;)
            st.write(f&#34;- head_forward_x: {hasattr(sample_traj, &#39;head_forward_x&#39;) and sample_traj.head_forward_x is not None}&#34;)
            st.write(f&#34;- head_forward_z: {hasattr(sample_traj, &#39;head_forward_z&#39;) and sample_traj.head_forward_z is not None}&#34;)
            st.write(f&#34;- gaze_x: {hasattr(sample_traj, &#39;gaze_x&#39;) and sample_traj.gaze_x is not None}&#34;)
            st.write(f&#34;- gaze_y: {hasattr(sample_traj, &#39;gaze_y&#39;) and sample_traj.gaze_y is not None}&#34;)
            st.write(f&#34;- pupil_l: {hasattr(sample_traj, &#39;pupil_l&#39;) and sample_traj.pupil_l is not None}&#34;)
            st.write(f&#34;- pupil_r: {hasattr(sample_traj, &#39;pupil_r&#39;) and sample_traj.pupil_r is not None}&#34;)
            st.write(f&#34;- heart_rate: {hasattr(sample_traj, &#39;heart_rate&#39;) and sample_traj.heart_rate is not None}&#34;)

            if hasattr(sample_traj, &#39;pupil_l&#39;) and sample_traj.pupil_l is not None:
                st.write(f&#34;- pupil_l length: {len(sample_traj.pupil_l)}&#34;)
                st.write(f&#34;- pupil_l sample: {sample_traj.pupil_l[:5] if len(sample_traj.pupil_l) &gt; 5 else sample_traj.pupil_l}&#34;)
            if hasattr(sample_traj, &#39;heart_rate&#39;) and sample_traj.heart_rate is not None:
                st.write(f&#34;- heart_rate length: {len(sample_traj.heart_rate)}&#34;)
                st.write(f&#34;- heart_rate sample: {sample_traj.heart_rate[:5] if len(sample_traj.heart_rate) &gt; 5 else sample_traj.heart_rate}&#34;)

        if has_gaze_data:
            st.write(&#34;**→ Using comprehensive gaze analysis (with enhanced debugging)**&#34;)
            # Apply scaling to trajectories AND junction if needed
            if scale_factor != 1.0:
                st.info(f&#34;🔧 Applying scale factor {scale_factor} to trajectory coordinates and junction...&#34;)
                scaled_trajectories = []
                for traj in trajectories:
                    # Create a copy with scaled coordinates
                    scaled_traj = Trajectory(
                        tid=traj.tid,
                        x=traj.x * scale_factor,
                        z=traj.z * scale_factor,
                        t=traj.t,
                        head_forward_x=traj.head_forward_x,
                        head_forward_z=traj.head_forward_z,
                        gaze_x=traj.gaze_x,
                        gaze_y=traj.gaze_y,
                        pupil_l=traj.pupil_l,
                        pupil_r=traj.pupil_r,
                        heart_rate=traj.heart_rate
                    )
                    scaled_trajectories.append(scaled_traj)
                trajectories = scaled_trajectories

                # Scale the junction coordinates to match scaled trajectories
                scaled_junction = Circle(
                    cx=junction.cx * scale_factor,
                    cz=junction.cz * scale_factor,
                    r=junction.r * scale_factor
                )
                junction = scaled_junction
            else:
                # Junction coordinates are correct and should NOT be scaled
                # The issue is that trajectories might be getting double-scaled somewhere
                st.info(f&#34;🔧 Using original junction coordinates (no scaling needed)&#34;)
                st.write(f&#34;- Junction coordinates: ({junction.cx}, {junction.cz}), r={junction.r}&#34;)

            # Call the comprehensive gaze analysis function
            return self._perform_comprehensive_gaze_analysis(
                trajectories=trajectories,
                junction=junction,
                r_outer=r_outer,
                decision_mode=decision_mode,
                path_length=path_length,
                epsilon=epsilon,
                linger_delta=linger_delta,
                out_dir=out_dir,
                run_custom_discover=st.session_state.get(&#39;run_custom_discover&#39;, False)
            )

        elif column_mappings:
            st.write(&#34;**→ Using custom gaze analysis (with column mappings)**&#34;)
            # Use custom gaze analysis with column mappings
            gaze_data = self._perform_custom_gaze_analysis(
                trajectories, junction, r_outer, decision_mode, path_length, epsilon, out_dir, column_mappings, scale_factor=1.0
            )
        else:
            # Check if we have any physiological data (even if incomplete)
            has_any_physio = any(
                (hasattr(traj, &#39;pupil_l&#39;) and traj.pupil_l is not None) or
                (hasattr(traj, &#39;pupil_r&#39;) and traj.pupil_r is not None) or
                (hasattr(traj, &#39;heart_rate&#39;) and traj.heart_rate is not None)
                for traj in trajectories
            )

            if has_any_physio:
                st.write(&#34;**→ Using physiological-only analysis (incomplete gaze data)**&#34;)
                # Try to perform analysis with whatever physiological data we have
                return self._perform_comprehensive_gaze_analysis(
                    trajectories=trajectories,
                    junction=junction,
                    r_outer=r_outer,
                    decision_mode=decision_mode,
                    path_length=path_length,
                    epsilon=epsilon,
                    linger_delta=linger_delta,
                    out_dir=out_dir,
                    run_custom_discover=st.session_state.get(&#39;run_custom_discover&#39;, False)
                )
            else:
                st.error(&#34;❌ No gaze data, physiological data, or column mappings available&#34;)
            return None

        # Debug: Check trajectory types
        st.info(f&#34;🔍 **Trajectory Type Debug:**&#34;)
        if trajectories:
            sample_traj = trajectories[0]
            st.write(f&#34;- Sample trajectory type: {type(sample_traj).__name__}&#34;)
            st.write(f&#34;- Sample trajectory ID: {sample_traj.tid}&#34;)
            st.write(f&#34;- Has gaze_x: {hasattr(sample_traj, &#39;gaze_x&#39;) and sample_traj.gaze_x is not None}&#34;)
            st.write(f&#34;- Has heart_rate: {hasattr(sample_traj, &#39;heart_rate&#39;) and sample_traj.heart_rate is not None}&#34;)
            st.write(f&#34;- Has pupil_l: {hasattr(sample_traj, &#39;pupil_l&#39;) and sample_traj.pupil_l is not None}&#34;)

        # CRITICAL FIX: Use the same trajectory objects that were used for discover analysis
        # to ensure trajectory IDs match between assignments and gaze analysis
        if &#34;branches&#34; in st.session_state.analysis_results and st.session_state.analysis_results[&#34;branches&#34;]:
            st.info(&#34;🔧 **Using trajectory objects from discover analysis to ensure ID consistency**&#34;)
            # Use the original trajectories that were used for discover analysis
            discover_trajectories = st.session_state.trajectories
            st.write(f&#34;- Discover trajectories: {len(discover_trajectories)}&#34;)
            st.write(f&#34;- Gaze trajectories: {len(trajectories)}&#34;)

            # DEBUG: Check coordinate ranges
            if discover_trajectories and trajectories:
                sample_discover = discover_trajectories[0]
                sample_gaze = trajectories[0]
                st.error(&#34;🔍 **COORDINATE SYSTEM DEBUG:**&#34;)
                st.write(f&#34;- Discover trajectory {sample_discover.tid}: X range {np.min(sample_discover.x):.1f} to {np.max(sample_discover.x):.1f}&#34;)

                # Check if gaze trajectory has valid coordinates
                if np.all(np.isnan(sample_gaze.x)):
                    st.error(f&#34;❌ **CRITICAL ISSUE:** Gaze trajectory {sample_gaze.tid} has ALL NaN coordinates!&#34;)
                    st.write(&#34;**This explains why arrows are not visible - trajectory coordinates are invalid!**&#34;)
                    st.write(&#34;**Root cause:** Gaze trajectories were loaded BEFORE NaN handling was added&#34;)

                    # Show sample data for debugging
                    st.write(f&#34;**Sample gaze trajectory data:**&#34;)
                    st.write(f&#34;- X values: {sample_gaze.x[:5]} (first 5)&#34;)
                    st.write(f&#34;- Z values: {sample_gaze.z[:5]} (first 5)&#34;)
                    st.write(f&#34;- X type: {type(sample_gaze.x)}&#34;)
                    st.write(f&#34;- Z type: {type(sample_gaze.z)}&#34;)

                    # Provide solution
                    st.error(&#34;🔧 **SOLUTION:** Reload gaze data to apply NaN handling&#34;)
                    st.write(&#34;**Steps to fix:**&#34;)
                    st.write(&#34;1. Go to &#39;Data Loader&#39; tab&#34;)
                    st.write(&#34;2. Re-upload or reload your gaze trajectory files&#34;)
                    st.write(&#34;3. The new NaN handling will clean the coordinate data&#34;)
                    st.write(&#34;4. Run gaze analysis again&#34;)
                else:
                    st.write(f&#34;- Gaze trajectory {sample_gaze.tid}: X range {np.min(sample_gaze.x):.1f} to {np.max(sample_gaze.x):.1f}&#34;)

                    # Check if coordinates are in the same scale
                    discover_scale = np.max(sample_discover.x) / np.max(sample_gaze.x) if np.max(sample_gaze.x) &gt; 0 else 1
                    st.write(f&#34;- Coordinate scale ratio (discover/gaze): {discover_scale:.2f}&#34;)
                    if abs(discover_scale - 1.0) &gt; 0.1:
                        st.error(f&#34;❌ **COORDINATE MISMATCH DETECTED!** Scale ratio: {discover_scale:.2f}&#34;)
                        st.write(&#34;**This explains why arrows are not visible - they&#39;re in different coordinate systems!**&#34;)

                st.write(f&#34;- Junction coordinates: ({junction.cx}, {junction.cz})&#34;)

            # Check if we can convert discover trajectories to Trajectory objects
            if True:
                st.write(&#34;🔄 Using unified Trajectory objects (IDs already consistent)...&#34;)
                # Create a mapping from trajectory ID to trajectories currently loaded
                gaze_traj_map = {gt.tid: gt for gt in trajectories}
                # Create a mapping for discover trajectories to allow coordinate repair when needed
                discover_traj_map = {dt.tid: dt for dt in discover_trajectories}

                # Convert discover trajectories to Trajectory objects where possible
                converted_trajectories = []
                for dt in discover_trajectories:
                    if dt.tid in gaze_traj_map:
                        gt = gaze_traj_map[dt.tid]
                        # Repair NaN coordinate issue by borrowing x/z/t from discover trajectory when needed
                        try:
                            needs_repair = (
                                gt.x is None or gt.z is None or
                                (hasattr(gt.x, &#39;shape&#39;) and hasattr(gt.z, &#39;shape&#39;) and
                                 (np.all(np.isnan(gt.x)) or np.all(np.isnan(gt.z))))
                            )
                        except Exception:
                            needs_repair = True
                        if needs_repair and dt.tid in discover_traj_map:
                            repaired = Trajectory(
                                tid=gt.tid,
                                x=np.asarray(discover_traj_map[dt.tid].x),
                                z=np.asarray(discover_traj_map[dt.tid].z),
                                t=getattr(discover_traj_map[dt.tid], &#39;t&#39;, None),
                                head_forward_x=getattr(gt, &#39;head_forward_x&#39;, None),
                                head_forward_z=getattr(gt, &#39;head_forward_z&#39;, None),
                                gaze_x=getattr(gt, &#39;gaze_x&#39;, None),
                                gaze_y=getattr(gt, &#39;gaze_y&#39;, None),
                                pupil_l=getattr(gt, &#39;pupil_l&#39;, None),
                                pupil_r=getattr(gt, &#39;pupil_r&#39;, None),
                                heart_rate=getattr(gt, &#39;heart_rate&#39;, None),
                            )
                            converted_trajectories.append(repaired)
                        else:
                            converted_trajectories.append(gt)
                    else:
                        st.warning(f&#34;⚠️ No Trajectory found for discover trajectory ID: {dt.tid}&#34;)

                st.write(f&#34;- Converted trajectories: {len(converted_trajectories)}&#34;)
                trajectories = converted_trajectories
            else:
                st.warning(&#34;⚠️ No gaze trajectories available - using discover trajectories (may not have gaze data)&#34;)
                trajectories = discover_trajectories

        # Return the movement data as fallback (this should not be reached if has_gaze_data is True)
        return self._analyze_movement_patterns_optimized(trajectories, junction, r_outer, decision_mode, path_length, epsilon)


    def _perform_custom_gaze_analysis(self, trajectories, junction, r_outer, decision_mode, path_length, epsilon, column_mappings, scale_factor=1.0):
        &#34;&#34;&#34;Perform gaze analysis using custom column mappings with scaling support.&#34;&#34;&#34;
        import pandas as pd
        import numpy as np
        from verta.verta_geometry import Circle

        results = []

        # Apply scaling to junction coordinates if needed
        if scale_factor != 1.0:
            jx = junction.cx * scale_factor
            jz = junction.cz * scale_factor
            junction_radius = junction.r * scale_factor
        else:
            # Junction coordinates are correct and should NOT be scaled
            jx = junction.cx
            jz = junction.cz
            junction_radius = junction.r

        for traj_idx, trajectory in enumerate(trajectories):
            # Apply scaling to coordinates if needed
            if scale_factor != 1.0:
                scaled_x = trajectory.x * scale_factor
                scaled_z = trajectory.z * scale_factor
            else:
                scaled_x = trajectory.x
                scaled_z = trajectory.z

            # Find decision point using scaled coordinates
            decision_idx = self._find_decision_point_fast(trajectory, jx, jz, r_outer, decision_mode, path_length, epsilon)

            if decision_idx is not None and decision_idx &lt; len(scaled_x):
                # Extract gaze data using column mappings
                gaze_data = self._extract_gaze_data_from_trajectory(trajectory, decision_idx, column_mappings)

                # Calculate trajectory position metrics
                trajectory_length = len(scaled_x)
                decision_ratio = decision_idx / trajectory_length if trajectory_length &gt; 0 else 0

                # Calculate distance from junction center using scaled coordinates
                distance_from_center = np.sqrt(
                    (scaled_x[decision_idx] - jx)**2 +
                    (scaled_z[decision_idx] - jz)**2
                )

                results.append({
                    &#34;trajectory&#34;: traj_idx,
                    &#34;junction&#34;: 0,  # Single junction analysis
                    &#34;decision_idx&#34;: decision_idx,
                    &#34;trajectory_length&#34;: trajectory_length,
                    &#34;decision_ratio&#34;: decision_ratio,
                    &#34;distance_from_center&#34;: distance_from_center,
                    &#34;decision_x&#34;: trajectory.x[decision_idx],
                    &#34;decision_z&#34;: trajectory.z[decision_idx],
                    &#34;time_at_decision&#34;: self._safe_get_time_value(trajectory, decision_idx),
                    # Gaze-specific data
                    &#34;head_yaw&#34;: gaze_data.get(&#39;head_yaw&#39;, np.nan),
                    &#34;gaze_x&#34;: gaze_data.get(&#39;gaze_x&#39;, np.nan),
                    &#34;gaze_y&#34;: gaze_data.get(&#39;gaze_y&#39;, np.nan),
                    &#34;pupil_l&#34;: gaze_data.get(&#39;pupil_l&#39;, np.nan),
                    &#34;pupil_r&#34;: gaze_data.get(&#39;pupil_r&#39;, np.nan),
                    &#34;heart_rate&#34;: gaze_data.get(&#39;heart_rate&#39;, np.nan),
                    &#34;analysis_type&#34;: &#34;gaze&#34;
                })

        return pd.DataFrame(results)

    def _extract_gaze_data_from_trajectory(self, trajectory, decision_idx, column_mappings):
        &#34;&#34;&#34;Extract gaze data from trajectory using column mappings.&#34;&#34;&#34;
        import numpy as np

        gaze_data = {}

        # Map the standard gaze fields to the actual column names
        field_mappings = {
            &#39;head_yaw&#39;: [&#39;head_forward_x&#39;, &#39;head_forward_z&#39;],
            &#39;gaze_x&#39;: [&#39;gaze_x&#39;],
            &#39;gaze_y&#39;: [&#39;gaze_y&#39;],
            &#39;pupil_l&#39;: [&#39;pupil_l&#39;],
            &#39;pupil_r&#39;: [&#39;pupil_r&#39;],
            &#39;heart_rate&#39;: [&#39;heart_rate&#39;]
        }

        for field, required_columns in field_mappings.items():
            try:
                if field == &#39;head_yaw&#39;:
                    # Calculate head yaw from forward direction
                    head_x_col = column_mappings.get(&#39;head_forward_x&#39;, &#39;&#39;)
                    head_z_col = column_mappings.get(&#39;head_forward_z&#39;, &#39;&#39;)

                    if head_x_col and head_z_col and hasattr(trajectory, head_x_col) and hasattr(trajectory, head_z_col):
                        head_x = getattr(trajectory, head_x_col)[decision_idx] if decision_idx &lt; len(getattr(trajectory, head_x_col)) else np.nan
                        head_z = getattr(trajectory, head_z_col)[decision_idx] if decision_idx &lt; len(getattr(trajectory, head_z_col)) else np.nan

                        if not (np.isnan(head_x) or np.isnan(head_z)):
                            gaze_data[field] = np.degrees(np.arctan2(head_x, head_z))
                        else:
                            gaze_data[field] = np.nan
                    else:
                        gaze_data[field] = np.nan

                else:
                    # For other fields, directly map the column
                    col_name = column_mappings.get(required_columns[0], &#39;&#39;)
                    if col_name and hasattr(trajectory, col_name):
                        data_array = getattr(trajectory, col_name)
                        if decision_idx &lt; len(data_array):
                            gaze_data[field] = data_array[decision_idx]
                        else:
                            gaze_data[field] = np.nan
                    else:
                        gaze_data[field] = np.nan

            except (IndexError, AttributeError, KeyError):
                gaze_data[field] = np.nan

        return gaze_data

    def _perform_comprehensive_gaze_analysis_all_junctions(self, trajectories, junctions, r_outer_list, decision_mode, path_length, epsilon, linger_delta, out_dir):
        &#34;&#34;&#34;Perform comprehensive gaze analysis for all junctions at once.&#34;&#34;&#34;
        import pandas as pd
        import numpy as np
        from verta.verta_gaze import compute_head_yaw_at_decisions, analyze_physiological_at_junctions, analyze_pupil_dilation_trajectory

        st.info(&#34;🔍 **Performing comprehensive gaze analysis for all junctions...**&#34;)

        # First check if we have existing branch assignments from previous discover analysis
        existing_assignments = None
        use_existing_assignments = False

        if &#34;branches&#34; in st.session_state.analysis_results:
            st.info(&#34;🔍 Found existing branch assignments from previous discover analysis!&#34;)

            # Try to get chain_decisions (contains all junctions&#39; assignments)
            if &#34;chain_decisions&#34; in st.session_state.analysis_results[&#34;branches&#34;]:
                existing_assignments = st.session_state.analysis_results[&#34;branches&#34;][&#34;chain_decisions&#34;]
                st.write(f&#34;🔍 **Chain decisions shape:** {existing_assignments.shape}&#34;)
                st.write(f&#34;🔍 **Chain decisions columns:** {list(existing_assignments.columns)}&#34;)

                # Debug: Check what type of data we&#39;re getting
                if &#39;junction_index&#39; in existing_assignments.columns:
                    st.error(&#34;❌ **ERROR: Found junction_index column in chain_decisions - this is decision points data, not branch assignments!**&#34;)
                    st.write(&#34;🔍 **This means the wrong data was stored as chain_decisions**&#34;)
                else:
                    st.success(&#34;✅ **No junction_index column found - this looks like proper branch assignments**&#34;)

                # Check if we have assignments for all junctions
                branch_cols = [col for col in existing_assignments.columns if col.startswith(&#39;branch_j&#39;)]
                st.write(f&#34;🔍 **Found branch columns:** {branch_cols}&#34;)

                # If no branch_j columns found, check if we have junction_index column
                if len(branch_cols) == 0 and &#39;junction_index&#39; in existing_assignments.columns:
                    st.info(&#34;🔍 **Found junction_index column - this appears to be decision points data, not branch assignments**&#34;)
                    st.write(&#34;**Solution:** Run &#39;🔍 Discover Branches&#39; analysis first to create proper branch assignments&#34;)
                    existing_assignments = None
                elif len(branch_cols) &gt;= len(junctions):
                    use_existing_assignments = True
                    st.success(f&#34;✅ **Found assignments for all {len(junctions)} junctions!**&#34;)
                else:
                    st.warning(f&#34;⚠️ **Only found assignments for {len(branch_cols)} junctions, but have {len(junctions)} junctions**&#34;)
                    existing_assignments = None

        if existing_assignments is not None and use_existing_assignments:
            chain_df = existing_assignments

            # For multi-junction analysis, we&#39;ll pass decision points separately to each gaze function
            # This avoids the complex merging issues with junction-specific data
            decisions_chain_df = st.session_state.analysis_results.get(&#34;branches&#34;, {}).get(&#34;decision_points&#34;)
            if decisions_chain_df is not None and len(decisions_chain_df) &gt; 0:
                st.info(&#34;🔗 **Found decision points - will use precomputed intercept coordinates**&#34;)
                st.write(f&#34;🔍 **Decision points available:** {len(decisions_chain_df)} records&#34;)
            else:
                st.warning(&#34;⚠️ **No decision points found in session state - will calculate from scratch**&#34;)

            st.success(f&#34;✅ Using existing assignments - found {len(chain_df)} assignments&#34;)
            st.info(f&#34;🔍 **GAZE ANALYSIS PATH:** Using existing assignments from previous discover analysis&#34;)
        else:
            st.error(&#34;❌ **No existing assignments found!**&#34;)
            st.write(&#34;**Solution:** Run &#39;🔍 Discover Branches&#39; analysis first to create proper assignments&#34;)
            return {
                &#39;head_yaw&#39;: pd.DataFrame(),
                &#39;physiological&#39;: pd.DataFrame(),
                &#39;pupil_dilation&#39;: pd.DataFrame(),
                &#39;error&#39;: &#39;No assignments found&#39;
            }

        # Preprocess trajectories to convert time values to numeric format
        processed_trajectories = []
        for traj in trajectories:
            if hasattr(traj, &#39;t&#39;) and traj.t is not None:
                # Convert time values to numeric if they&#39;re strings
                if isinstance(traj.t[0], str):
                    try:
                        import pandas as pd
                        # Convert string time format to numeric seconds
                        numeric_times = []
                        for t_val in traj.t:
                            if isinstance(t_val, str):
                                # Parse time string like &#34;00:00:17.425&#34;
                                time_parts = t_val.split(&#39;:&#39;)
                                if len(time_parts) == 3:
                                    hours = float(time_parts[0])
                                    minutes = float(time_parts[1])
                                    seconds = float(time_parts[2])
                                    total_seconds = hours * 3600 + minutes * 60 + seconds
                                    numeric_times.append(total_seconds)
                                else:
                                    numeric_times.append(float(t_val))
                            else:
                                numeric_times.append(float(t_val))
                        traj.t = np.array(numeric_times)
                    except Exception as e:
                        st.warning(f&#34;⚠️ Could not convert time values for trajectory {traj.tid}: {e}&#34;)
                        # Keep original time values
                processed_trajectories.append(traj)
            else:
                processed_trajectories.append(traj)

        # Merge decision points with assignments for each junction to avoid multi-junction merging issues
        chain_df_with_decisions = chain_df.copy()
        if decisions_chain_df is not None and len(decisions_chain_df) &gt; 0:
            st.info(&#34;🔗 **Merging decision points with assignments per junction...**&#34;)
            try:
                from verta.verta_consistency import normalize_assignments

                # For each junction, merge its decision points
                # Ensure junction_index is numeric for comparison (do this once outside the loop)
                decisions_df_numeric = decisions_chain_df.copy()
                st.write(f&#34;🔍 **Debug: Original junction_index values:** {decisions_df_numeric[&#39;junction_index&#39;].unique()[:10]}&#34;)
                st.write(f&#34;🔍 **Debug: Original junction_index types:** {[type(x) for x in decisions_df_numeric[&#39;junction_index&#39;].unique()[:5]]}&#34;)

                decisions_df_numeric[&#34;junction_index&#34;] = pd.to_numeric(decisions_df_numeric[&#34;junction_index&#34;], errors=&#39;coerce&#39;)
                st.write(f&#34;🔍 **Debug: After conversion junction_index values:** {decisions_df_numeric[&#39;junction_index&#39;].unique()[:10]}&#34;)
                st.write(f&#34;🔍 **Debug: NaN count after conversion:** {decisions_df_numeric[&#39;junction_index&#39;].isna().sum()}&#34;)

                for junction_idx in range(len(junctions)):
                    # Filter decision points for this junction
                    junction_decisions = decisions_df_numeric[decisions_df_numeric[&#34;junction_index&#34;] == junction_idx].copy()

                    if len(junction_decisions) &gt; 0:
                        # Remove junction_index column to avoid filtering conflicts in normalize_assignments
                        junction_decisions_clean = junction_decisions.drop(columns=[&#39;junction_index&#39;]).copy()

                        # Debug: Check trajectory ID types
                        st.write(f&#34;🔍 **Debug Junction {junction_idx}:**&#34;)
                        st.write(f&#34;- Chain_df trajectory types: {[type(x) for x in chain_df[&#39;trajectory&#39;].unique()[:5]]}&#34;)
                        st.write(f&#34;- Junction_decisions trajectory types: {[type(x) for x in junction_decisions_clean[&#39;trajectory&#39;].unique()[:5]]}&#34;)
                        st.write(f&#34;- Chain_df trajectory values: {chain_df[&#39;trajectory&#39;].unique()[:5]}&#34;)
                        st.write(f&#34;- Junction_decisions trajectory values: {junction_decisions_clean[&#39;trajectory&#39;].unique()[:5]}&#34;)

                        # Normalize assignments for this junction
                        chain_df_normalized, norm_report = normalize_assignments(
                            assignments_df=chain_df,
                            trajectories=processed_trajectories,
                            junctions=[junctions[junction_idx]],  # Single junction
                            current_junction_idx=None,  # Don&#39;t filter by junction since we already did
                            decisions_df=junction_decisions_clean,
                            prefer_decisions=True,
                            include_outliers=False,
                            strict=False,
                        )

                        # Merge the decision columns from this junction&#39;s normalized data
                        decision_cols = [&#39;decision_idx&#39;, &#39;intercept_x&#39;, &#39;intercept_z&#39;]
                        for col in decision_cols:
                            if col in chain_df_normalized.columns:
                                # Create junction-specific column names
                                junction_col = f&#34;{col}_j{junction_idx}&#34;
                                chain_df_with_decisions[junction_col] = chain_df_normalized[col]

                # Check if any decision points were merged
                decision_cols_merged = [col for col in chain_df_with_decisions.columns if col.startswith(&#39;decision_idx_&#39;)]
                if decision_cols_merged:
                    st.success(f&#34;✅ **Successfully merged decision points for {len(decision_cols_merged)} junctions**&#34;)

                    # Debug: Show coverage of decision points
                    for col in decision_cols_merged:
                        junction_num = col.split(&#39;_&#39;)[-1]
                        non_null_count = chain_df_with_decisions[col].notna().sum()
                        total_count = len(chain_df_with_decisions)
                        st.write(f&#34;🔍 **Junction {junction_num}:** {non_null_count}/{total_count} trajectories have precomputed decision points&#34;)

                    chain_df = chain_df_with_decisions
                else:
                    st.warning(&#34;⚠️ **No decision points merged - will calculate from scratch**&#34;)

            except Exception as e:
                st.warning(f&#34;⚠️ **Could not merge decision points:** {e}&#34;)
                st.write(&#34;**Will calculate decision points from scratch**&#34;)

        # Use the actual gaze analysis functions with proper assignments
        # Get decision mode and parameters from discover analysis if available
        discover_decision_mode = decision_mode  # Default fallback
        discover_path_length = path_length
        discover_epsilon = epsilon
        discover_linger_delta = linger_delta
        discover_r_outer = r_outer_list[0] if r_outer_list else None  # Default fallback

        if &#34;branches&#34; in st.session_state.analysis_results:
            # Try to get parameters from the first junction&#39;s stored data
            for junction_key, branch_data in st.session_state.analysis_results[&#34;branches&#34;].items():
                if isinstance(branch_data, dict) and &#34;decision_mode&#34; in branch_data:
                    discover_decision_mode = branch_data[&#34;decision_mode&#34;]
                    discover_path_length = branch_data.get(&#34;path_length&#34;, path_length)
                    discover_epsilon = branch_data.get(&#34;epsilon&#34;, epsilon)
                    discover_linger_delta = branch_data.get(&#34;linger_delta&#34;, linger_delta)
                    discover_r_outer = branch_data.get(&#34;r_outer&#34;, discover_r_outer)
                    st.info(f&#34;🔧 **Using discover analysis parameters:** decision_mode={discover_decision_mode}, path_length={discover_path_length}, epsilon={discover_epsilon}, linger_delta={discover_linger_delta}, r_outer={discover_r_outer}&#34;)
                    break

        try:
            st.info(&#34;🔬 Analyzing head yaw data for all junctions...&#34;)
            with st.spinner(&#34;Processing head yaw data...&#34;):
                head_yaw_df = compute_head_yaw_at_decisions(
                    trajectories=processed_trajectories,
                    junctions=junctions,
                    assignments_df=chain_df,
                    decision_mode=discover_decision_mode,  # Use discover decision mode
                    r_outer_list=r_outer_list,
                    path_length=discover_path_length,  # Use discover path length
                    epsilon=discover_epsilon,  # Use discover epsilon
                    linger_delta=discover_linger_delta,  # Use discover linger delta
                    base_index=0  # Start from 0 for all junctions
                )

            st.info(&#34;🔬 Analyzing physiological data for all junctions...&#34;)
            with st.spinner(&#34;Processing physiological data...&#34;):
                physio_df = analyze_physiological_at_junctions(
                    trajectories=processed_trajectories,
                    junctions=junctions,
                    assignments_df=chain_df,
                    decision_mode=discover_decision_mode,  # Use discover decision mode
                    r_outer_list=r_outer_list,
                    path_length=discover_path_length,  # Use discover path length
                    epsilon=discover_epsilon,  # Use discover epsilon
                    linger_delta=discover_linger_delta,  # Use discover linger delta
                    physio_window=3.0,
                    base_index=0,
                )

            st.info(&#34;🔬 Analyzing pupil dilation trajectories for all junctions...&#34;)
            with st.spinner(&#34;Processing pupil dilation data...&#34;):
                pupil_df = analyze_pupil_dilation_trajectory(
                    trajectories=processed_trajectories,
                    junctions=junctions,
                    assignments_df=chain_df,
                    decision_mode=discover_decision_mode,  # Use discover decision mode
                    r_outer_list=r_outer_list,
                    path_length=discover_path_length,  # Use discover path length
                    epsilon=discover_epsilon,  # Use discover epsilon
                    linger_delta=discover_linger_delta,  # Use discover linger delta
                    physio_window=3.0,
                    base_index=0,
                )

            # Generate pupil dilation heatmaps for all junctions
            st.info(&#34;🗺️ Generating pupil dilation heatmaps for all junctions...&#34;)
            with st.spinner(&#34;Creating spatial heatmaps...&#34;):
                from verta.verta_gaze import create_per_junction_pupil_heatmap

                # Get heatmap parameters from session state
                cell_size = st.session_state.get(&#39;pupil_heatmap_cell_size&#39;, 3.0)
                normalization = st.session_state.get(&#39;pupil_heatmap_normalization&#39;, &#39;relative&#39;)

                # Generate heatmaps for all junctions
                all_heatmaps = create_per_junction_pupil_heatmap(
                    trajectories=processed_trajectories,
                    junctions=junctions,
                    r_outer_list=r_outer_list,
                    cell_size=cell_size,
                    normalization=normalization,
                    base_index=0  # Start from 0 for all junctions
                )

                st.write(f&#34;🔍 **Generated heatmaps for {len(all_heatmaps)} junctions**&#34;)

            # Debug: Show results summary
            st.info(f&#34;🔍 **Gaze Analysis Results Summary:**&#34;)
            st.write(f&#34;- Head yaw records: {len(head_yaw_df)}&#34;)
            st.write(f&#34;- Physiological records: {len(physio_df)}&#34;)
            st.write(f&#34;- Pupil dilation records: {len(pupil_df)}&#34;)
            st.write(f&#34;- Heatmaps generated: {len(all_heatmaps)}&#34;)

            if len(head_yaw_df) &gt; 0:
                st.write(f&#34;- Junctions with head yaw data: {sorted(head_yaw_df[&#39;junction&#39;].unique())}&#34;)
            if len(physio_df) &gt; 0:
                st.write(f&#34;- Junctions with physiological data: {sorted(physio_df[&#39;junction&#39;].unique())}&#34;)
            if len(pupil_df) &gt; 0:
                st.write(f&#34;- Junctions with pupil data: {sorted(pupil_df[&#39;junction&#39;].unique())}&#34;)

            return {
                &#39;head_yaw&#39;: head_yaw_df,
                &#39;physiological&#39;: physio_df,
                &#39;pupil_dilation&#39;: pupil_df,
                &#39;pupil_heatmap_junction&#39;: all_heatmaps,  # Add heatmaps to results
                &#39;junction&#39;: junctions[0],  # Reference junction
                &#39;r_outer&#39;: r_outer_list[0]  # Reference r_outer
            }

        except Exception as e:
            st.error(f&#34;❌ **Gaze analysis failed:** {e}&#34;)
            st.write(f&#34;**Error type:** {type(e).__name__}&#34;)
            st.write(f&#34;**Error message:** {str(e)}&#34;)

            # Show suggestions based on error type
            if &#34;No assignments found&#34; in str(e):
                st.info(&#34;💡 **Suggestion:** Run &#39;🔍 Discover Branches&#39; analysis first to create proper assignments&#34;)
            elif &#34;trajectory&#34; in str(e).lower():
                st.info(&#34;💡 **Suggestion:** Check if trajectories actually pass through the junctions&#34;)
            elif &#34;column&#34; in str(e).lower():
                st.info(&#34;💡 **Suggestion:** Check your gaze column mappings in the Data tab&#34;)

            return {
                &#39;head_yaw&#39;: pd.DataFrame(),
                &#39;physiological&#39;: pd.DataFrame(),
                &#39;pupil_dilation&#39;: pd.DataFrame(),
                &#39;error&#39;: str(e),
                &#39;error_type&#39;: type(e).__name__
            }

    def _perform_comprehensive_gaze_analysis(self, trajectories, junction, r_outer, decision_mode, path_length, epsilon, linger_delta, out_dir, run_custom_discover=False):
        &#34;&#34;&#34;Perform comprehensive gaze analysis using the actual gaze functions.&#34;&#34;&#34;
        import pandas as pd
        import numpy as np
        from verta.verta_decisions import discover_decision_chain

        # Debug: Check r_outer parameter at function start
        st.write(f&#34;🔍 **DEBUG: r_outer parameter received:** {r_outer} (type: {type(r_outer)})&#34;)

        # Get decision mode and parameters from discover analysis if available
        discover_decision_mode = decision_mode  # Default fallback
        discover_path_length = path_length
        discover_epsilon = epsilon
        discover_linger_delta = linger_delta

        if &#34;branches&#34; in st.session_state.analysis_results:
            # Try to get parameters from the first junction&#39;s stored data
            for junction_key, branch_data in st.session_state.analysis_results[&#34;branches&#34;].items():
                if isinstance(branch_data, dict) and &#34;decision_mode&#34; in branch_data:
                    discover_decision_mode = branch_data[&#34;decision_mode&#34;]
                    discover_path_length = branch_data.get(&#34;path_length&#34;, path_length)
                    discover_epsilon = branch_data.get(&#34;epsilon&#34;, epsilon)
                    discover_linger_delta = branch_data.get(&#34;linger_delta&#34;, linger_delta)
                    st.info(f&#34;🔧 **Using discover analysis parameters:** decision_mode={discover_decision_mode}, path_length={discover_path_length}, epsilon={discover_epsilon}&#34;)
                    break

        # Always define r_outer_list upfront to avoid unbound local errors later
        r_outer_list = [r_outer] if r_outer is not None else [None]

        # First check if we have existing branch assignments from previous discover analysis
        existing_assignments = None
        use_existing_assignments = False

        if &#34;branches&#34; in st.session_state.analysis_results:
            st.info(&#34;🔍 Found existing branch assignments from previous discover analysis!&#34;)

            # Debug: Show available keys
            available_keys = list(st.session_state.analysis_results[&#34;branches&#34;].keys())
            st.write(f&#34;🔍 **Available branch keys:** {available_keys}&#34;)
            st.write(f&#34;🔍 **Looking for junction:** ({junction.cx}, {junction.cz}, r={junction.r})&#34;)

            # First try to find junction-specific assignments (from recent discover analysis)
            junction_found = False
            for junction_key, junction_data in st.session_state.analysis_results[&#34;branches&#34;].items():
                if junction_key.startswith(&#34;junction_&#34;) and isinstance(junction_data, dict):
                    junction_obj = junction_data.get(&#34;junction&#34;)
                    if junction_obj and junction_obj.cx == junction.cx and junction_obj.cz == junction.cz and junction_obj.r == junction.r:
                        st.success(f&#34;✅ **Found junction-specific assignments for {junction_key}!**&#34;)
                        assignments_df = junction_data.get(&#34;assignments&#34;)
                        if assignments_df is not None and not assignments_df.empty:
                            st.write(f&#34;🔍 **Assignments shape:** {assignments_df.shape}&#34;)
                            st.write(f&#34;🔍 **Assignments columns:** {list(assignments_df.columns)}&#34;)
                            st.write(f&#34;🔍 **Sample assignments:**&#34;)
                            st.write(assignments_df.head())

                            existing_assignments = assignments_df
                            use_existing_assignments = True
                            junction_found = True
                            break
                        else:
                            st.warning(f&#34;⚠️ **No assignments found in {junction_key}**&#34;)

            # If no junction-specific assignments found, try chain_decisions as fallback
            if not junction_found and &#34;chain_decisions&#34; in st.session_state.analysis_results[&#34;branches&#34;]:
                st.info(&#34;🔍 **No junction-specific assignments found, trying chain_decisions as fallback**&#34;)
                existing_assignments = st.session_state.analysis_results[&#34;branches&#34;][&#34;chain_decisions&#34;]
                st.write(f&#34;🔍 **Chain decisions shape:** {existing_assignments.shape}&#34;)
                st.write(f&#34;🔍 **Chain decisions columns:** {list(existing_assignments.columns)}&#34;)

                # Check if this junction has assignments in the chain decisions
                try:
                    junction_index = next(i for i, j in enumerate(st.session_state.junctions)
                                          if j.cx == junction.cx and j.cz == junction.cz and j.r == junction.r)

                    # Look for junction-specific branch column
                    branch_col = f&#34;branch_j{junction_index}&#34;
                    if branch_col in existing_assignments.columns:
                        st.success(f&#34;✅ **Found assignments for Junction {junction_index} in chain_decisions!**&#34;)
                        st.write(f&#34;🔍 **Branch column:** {branch_col}&#34;)

                        # Filter to only trajectories with assignments for this junction
                        assigned_mask = existing_assignments[branch_col].notna() &amp; (existing_assignments[branch_col] &gt;= 0)
                        junction_assignments = existing_assignments[assigned_mask].copy()

                        st.write(f&#34;🔍 **Junction {junction_index} assignments:** {len(junction_assignments)} trajectories&#34;)
                        if len(junction_assignments) &gt; 0:
                            st.write(f&#34;🔍 **Sample assignments:**&#34;)
                            st.write(junction_assignments[[&#39;trajectory&#39;, branch_col]].head())

                            # Set the found assignments
                            existing_assignments = junction_assignments
                            use_existing_assignments = True
                        else:
                            st.warning(f&#34;⚠️ **No assignments found for Junction {junction_index} in chain_decisions**&#34;)
                            existing_assignments = None
                    else:
                        st.warning(f&#34;⚠️ **Branch column {branch_col} not found in chain_decisions**&#34;)
                        st.write(f&#34;Available branch columns: {[col for col in existing_assignments.columns if col.startswith(&#39;branch_j&#39;)]}&#34;)
                        existing_assignments = None
                        found_key = None

                except StopIteration:
                    st.warning(&#34;⚠️ **Could not find junction index in session state junctions**&#34;)
                    existing_assignments = None

            # If we found assignments, show summary
            if existing_assignments is not None and use_existing_assignments:
                try:
                    num_rows = len(existing_assignments)
                    st.success(f&#34;✅ **Using existing assignments: {num_rows} trajectories**&#34;)
                except Exception:
                    st.success(&#34;✅ **Using existing assignments**&#34;)

                # Debug: Show sample of assignments data
                st.write(f&#34;🔍 **Sample assignments data:**&#34;)
                if hasattr(existing_assignments, &#39;head&#39;):
                    st.write(existing_assignments.head())
                st.write(f&#34;🔍 **Assignments columns:** {list(existing_assignments.columns)}&#34;)

                # Check if assignments have the expected structure
                if &#39;branch&#39; in existing_assignments.columns:
                    assigned_count = existing_assignments[&#39;branch&#39;].notna().sum()
                    st.write(f&#34;🔍 **Total assigned trajectories:** {assigned_count}&#34;)
                elif any(col.startswith(&#39;branch_j&#39;) for col in existing_assignments.columns):
                    branch_cols = [col for col in existing_assignments.columns if col.startswith(&#39;branch_j&#39;)]
                    st.write(f&#34;🔍 **Found branch columns:** {branch_cols}&#34;)
                else:
                    st.warning(&#34;⚠️ **Unexpected assignments structure**&#34;)

            # If no existing assignments found, we&#39;ll need to run discover analysis
            if existing_assignments is None:
                st.info(&#34;🔍 **No existing assignments found - will run discover analysis**&#34;)
                use_existing_assignments = False
        else:
            st.info(&#34;🔍 **No existing branch assignments found - will run discover analysis**&#34;)
            use_existing_assignments = False

        # Use existing assignments if available and selected, otherwise run discover analysis
        chain_df = None

        # Use the current checkbox selections (fall back to session_state if not set)
        # CRITICAL: Respect user&#39;s choice first, then check for existing assignments
        user_wants_existing = st.session_state.get(&#39;use_existing_assignments&#39;, False)
        user_wants_custom = st.session_state.get(&#39;run_custom_discover&#39;, False)

        # If user explicitly chose custom parameters, ignore existing assignments
        if user_wants_custom:
            use_existing_assignments = False
            run_custom_discover = True
        else:
            use_existing_assignments = use_existing_assignments and user_wants_existing
            run_custom_discover = user_wants_custom

        # Debug: Show what parameters are being used
        st.info(f&#34;🔍 **Parameter Source Debug:**&#34;)
        st.write(f&#34;- Using existing assignments: {use_existing_assignments}&#34;)
        st.write(f&#34;- Running custom discover: {run_custom_discover}&#34;)
        if run_custom_discover and &#39;custom_discover_params&#39; in st.session_state:
            st.write(f&#34;- Custom parameters available: {list(st.session_state.custom_discover_params.keys())}&#34;)
            st.write(f&#34;- Custom decision mode: {st.session_state.custom_discover_params.get(&#39;decision_mode&#39;, &#39;not set&#39;)}&#34;)

            # Show actual parameter values being used
            st.write(&#34;🔧 **Custom Parameters Being Used:**&#34;)
            custom_params = st.session_state.custom_discover_params
            for param_name, param_value in custom_params.items():
                st.write(f&#34;- {param_name}: {param_value}&#34;)

            # Check if parameters might be too restrictive
            st.write(&#34;🔍 **Parameter Restrictiveness Check:**&#34;)
            if custom_params.get(&#39;eps&#39;, 0.5) &lt; 1.0:
                st.warning(f&#34;⚠️ DBSCAN eps={custom_params.get(&#39;eps&#39;, 0.5)} might be too restrictive (try 1.0-2.0)&#34;)
            if custom_params.get(&#39;min_samples&#39;, 5) &gt; 3:
                st.warning(f&#34;⚠️ DBSCAN min_samples={custom_params.get(&#39;min_samples&#39;, 5)} might be too high (try 2-3)&#34;)
            if custom_params.get(&#39;path_length&#39;, 100.0) &gt; 50.0:
                st.warning(f&#34;⚠️ Path length={custom_params.get(&#39;path_length&#39;, 100.0)} might be too high (try 20-50)&#34;)
        else:
            st.write(&#34;- Using default parameters&#34;)

        if existing_assignments is not None and use_existing_assignments:
            # CRITICAL FIX: Ensure we&#39;re using the correct junction-specific assignments
            # If we&#39;re using chain_decisions, we need to extract the specific junction&#39;s assignments
            if &#34;chain_decisions&#34; in str(type(existing_assignments)) or &#34;branch_j&#34; in existing_assignments.columns:
                # This is the chain_decisions DataFrame - extract junction-specific assignments
                junction_index = next(i for i, j in enumerate(st.session_state.junctions)
                                    if j.cx == junction.cx and j.cz == junction.cz and j.r == junction.r)

                st.info(f&#34;🔧 **Extracting Junction {junction_index} assignments from chain_decisions...**&#34;)

                # Create a junction-specific assignments DataFrame
                junction_assignments = existing_assignments[[&#34;trajectory&#34;]].copy()

                # Add the junction-specific branch column
                branch_col = f&#34;branch_j{junction_index}&#34;
                if branch_col in existing_assignments.columns:
                    junction_assignments[&#34;branch&#34;] = existing_assignments[branch_col]

                    # Add decision point columns if available
                    if &#34;decision_idx&#34; in existing_assignments.columns:
                        junction_assignments[&#34;decision_idx&#34;] = existing_assignments[&#34;decision_idx&#34;]
                    if &#34;intercept_x&#34; in existing_assignments.columns:
                        junction_assignments[&#34;intercept_x&#34;] = existing_assignments[&#34;intercept_x&#34;]
                    if &#34;intercept_z&#34; in existing_assignments.columns:
                        junction_assignments[&#34;intercept_z&#34;] = existing_assignments[&#34;intercept_z&#34;]

                    # Filter to only assigned trajectories
                    assigned_mask = junction_assignments[&#34;branch&#34;].notna() &amp; (junction_assignments[&#34;branch&#34;] &gt;= 0)
                    junction_assignments = junction_assignments[assigned_mask]

                    st.success(f&#34;✅ **Extracted Junction {junction_index} assignments:** {len(junction_assignments)} trajectories&#34;)
                    st.write(f&#34;🔍 **Branch column used:** {branch_col}&#34;)
                    st.write(f&#34;🔍 **Sample assignments:**&#34;)
                    st.write(junction_assignments.head())

                    chain_df = junction_assignments
                else:
                    st.error(f&#34;❌ **Branch column {branch_col} not found in chain_decisions!**&#34;)
                    st.write(f&#34;Available branch columns: {[col for col in existing_assignments.columns if col.startswith(&#39;branch_j&#39;)]}&#34;)
                    chain_df = pd.DataFrame()  # Empty DataFrame
            else:
                # This is already a junction-specific assignments DataFrame
                chain_df = existing_assignments
                st.success(f&#34;✅ Using existing assignments - found {len(chain_df)} assignments&#34;)

            st.info(f&#34;🔍 **GAZE ANALYSIS PATH:** Using existing assignments from previous discover analysis&#34;)
        else:
            # Show discover analysis parameters
            st.info(&#34;🔍 Running discover analysis to get branch assignments...&#34;)
            st.info(f&#34;🔍 **GAZE ANALYSIS PATH:** Running discover analysis (not using existing assignments)&#34;)

            # Add parameter adjustment suggestions
            st.warning(&#34;⚠️ **Low assignment rate detected!** Try these adjustments:&#34;)
            st.write(&#34;**Suggested parameter changes:**&#34;)
            st.write(&#34;- **Path length**: Try reducing from 100.0 to 20.0-50.0&#34;)
            st.write(&#34;- **Decision mode**: Try &#39;hybrid&#39; instead of &#39;pathlen&#39;&#34;)
            st.write(&#34;- **Junction position/radius**: Verify they match your trajectory data&#34;)
            st.write(&#34;&#34;)
            st.write(&#34;**Or run &#39;🔍 Discover Branches&#39; analysis first to create proper assignments!**&#34;)

            try:
                # Use custom parameters if provided
                if run_custom_discover and &#39;custom_discover_params&#39; in st.session_state:
                    custom_params = st.session_state.custom_discover_params

                    # Update parameters with custom values
                    cluster_method = custom_params.get(&#39;cluster_method&#39;, &#39;kmeans&#39;)
                    seed = custom_params.get(&#39;seed&#39;, 0)
                    decision_mode = custom_params.get(&#39;decision_mode&#39;, &#39;hybrid&#39;)

                    # Decision mode specific parameters
                    if decision_mode == &#34;radial&#34;:
                        # Don&#39;t override r_outer - use the value passed to the function
                        # r_outer = custom_params.get(&#39;r_outer&#39;, 50.0)  # REMOVED: This was overriding the correct value
                        epsilon = custom_params.get(&#39;epsilon&#39;, 0.05)
                        path_length = None
                        linger_delta = None
                    elif decision_mode == &#34;pathlen&#34;:
                        path_length = custom_params.get(&#39;path_length&#39;, 100.0)
                        linger_delta = custom_params.get(&#39;linger_delta&#39;, 0.0)
                        # Don&#39;t override r_outer - use the value passed to the function
                        # r_outer = None  # REMOVED: This was causing the TypeError
                        epsilon = custom_params.get(&#39;epsilon&#39;, 0.05)  # Provide default epsilon for pathlen mode
                    elif decision_mode == &#34;hybrid&#34;:
                        # Don&#39;t override r_outer - use the value passed to the function
                        # r_outer = custom_params.get(&#39;r_outer&#39;, 50.0)  # REMOVED: This was overriding the correct value
                        path_length = custom_params.get(&#39;path_length&#39;, 100.0)
                        epsilon = custom_params.get(&#39;epsilon&#39;, 0.05)  # Hybrid mode needs epsilon for DBSCAN
                        linger_delta = custom_params.get(&#39;linger_delta&#39;, 0.0)

                    # Cluster method specific parameters
                    if cluster_method == &#34;dbscan&#34;:
                        # Use epsilon parameter for DBSCAN (not eps)
                        epsilon = custom_params.get(&#39;eps&#39;, 0.5)  # Map eps to epsilon
                        min_samples = custom_params.get(&#39;min_samples&#39;, 5)
                        angle_eps = custom_params.get(&#39;angle_eps&#39;, 15.0)
                        # DBSCAN doesn&#39;t use k parameters, but discover_decision_chain expects them
                        k = 3  # Default value for discover_decision_chain
                        k_min = 2  # Default value for discover_decision_chain
                        k_max = 6  # Default value for discover_decision_chain
                        min_sep_deg = 12.0  # Default value for discover_decision_chain

                        # Ensure no None values are passed to discover_decision_chain
                        if epsilon is None:
                            epsilon = 0.5
                        if min_samples is None:
                            min_samples = 5
                        if angle_eps is None:
                            angle_eps = 15.0

                        # Debug: Show DBSCAN parameters
                        st.write(f&#34;**DBSCAN Parameters:** epsilon={epsilon}, min_samples={min_samples}, angle_eps={angle_eps}&#34;)
                    elif cluster_method == &#34;kmeans&#34;:
                        k = custom_params.get(&#39;k&#39;, 3)
                        k_min = custom_params.get(&#39;k_min&#39;, 2)
                        k_max = custom_params.get(&#39;k_max&#39;, 6)
                        eps = None
                        min_samples = None
                        angle_eps = None
                        min_sep_deg = None

                        # Ensure no None values are passed to discover_decision_chain
                        if k is None:
                            k = 3
                        if k_min is None:
                            k_min = 2
                        if k_max is None:
                            k_max = 6
                    elif cluster_method == &#34;auto&#34;:
                        k_min = custom_params.get(&#39;k_min&#39;, 2)
                        k_max = custom_params.get(&#39;k_max&#39;, 6)
                        min_sep_deg = custom_params.get(&#39;min_sep_deg&#39;, 12.0)
                        angle_eps = custom_params.get(&#39;angle_eps&#39;, 15.0)
                        eps = None
                        min_samples = None
                        k = None

                        # Ensure no None values are passed to discover_decision_chain
                        if k_min is None:
                            k_min = 2
                        if k_max is None:
                            k_max = 6
                        if min_sep_deg is None:
                            min_sep_deg = 12.0
                        if angle_eps is None:
                            angle_eps = 15.0

                    st.info(f&#34;🔧 Using custom parameters: cluster_method={cluster_method}, decision_mode={decision_mode}, seed={seed}&#34;)
                else:
                    # Use default parameters
                    cluster_method = &#34;kmeans&#34;
                    seed = 0
                    decision_mode = &#34;hybrid&#34;
                    # Use the r_outer value defined in the junctions tab
                    # Don&#39;t override it with hardcoded logic
                    path_length = 100.0
                    epsilon = 0.05  # Default epsilon for hybrid mode
                    linger_delta = 0.0
                    # Even if not used by the selected cluster method, avoid passing None
                    eps = 0.5
                    min_samples = 5
                    angle_eps = 15.0
                    k = 3
                    k_min = 2
                    k_max = 6
                    min_sep_deg = 12.0  # Default value for discover_decision_chain

                # r_outer_list already initialized above; update if user changed r_outer
                r_outer_list = [r_outer] if r_outer is not None else [None]

                # Debug: Show parameters being used
                st.info(f&#34;🔍 **Discover Analysis Parameters:**&#34;)
                st.write(f&#34;- Junction: Circle(cx={junction.cx}, cz={junction.cz}, r={junction.r})&#34;)
                st.write(f&#34;- Decision mode: {decision_mode}&#34;)
                st.write(f&#34;🔍 **DEBUG: r_outer before debug message:** {r_outer} (type: {type(r_outer)})&#34;)
                st.write(f&#34;- R outer: {r_outer} (from junctions tab, r_outer_list: {r_outer_list})&#34;)
                st.write(f&#34;- Path length: {path_length}&#34;)
                st.write(f&#34;- Cluster method: {cluster_method}, k: {k}&#34;)
                st.write(f&#34;- Trajectories: {len(trajectories)}&#34;)

                # Additional debugging for radial mode issues
                if decision_mode == &#34;radial&#34;:
                    st.error(f&#34;🚨 **RADIAL MODE DETECTED - LIKELY CAUSE OF NO ASSIGNMENTS!**&#34;)
                    st.write(f&#34;- Junction radius: {junction.r}&#34;)
                    st.write(f&#34;- R outer: {r_outer}&#34;)
                    st.write(f&#34;- Ratio (r_outer/junction.r): {r_outer/junction.r:.2f}&#34;)
                    if r_outer &lt;= junction.r:
                        st.error(f&#34;❌ **CRITICAL:** r_outer ({r_outer}) &lt;= junction radius ({junction.r})!&#34;)
                        st.write(&#34;In radial mode, r_outer must be significantly larger than junction radius.&#34;)
                    elif r_outer/junction.r &lt; 2.0:
                        st.warning(f&#34;⚠️ **WARNING:** r_outer/junction.r ratio ({r_outer/junction.r:.2f}) is too low!&#34;)
                        st.write(&#34;For radial mode, r_outer should be at least 2x the junction radius for reliable detection.&#34;)

                    st.error(f&#34;🔧 **RECOMMENDED FIXES:**&#34;)
                    st.write(&#34;1. **Change decision mode to &#39;hybrid&#39;** in the gaze analysis custom parameters&#34;)
                    st.write(&#34;2. **Or increase r_outer values** in the Junctions tab to at least 2x the junction radius&#34;)
                    st.write(&#34;3. **Or use default parameters** instead of custom parameters&#34;)

                    # Show current parameter status
                    st.write(f&#34;**Current decision mode:** {decision_mode}&#34;)
                    st.write(&#34;**To fix:** Change the decision mode to &#39;hybrid&#39; in the analysis parameters&#34;)

                # Debug: Check if trajectories pass through junction
                trajectories_through_junction = 0
                for traj in trajectories[:5]:  # Check first 5 trajectories
                    # Handle NaN values in trajectory coordinates
                    valid_mask = ~(np.isnan(traj.x) | np.isnan(traj.z))
                    if np.any(valid_mask):
                        valid_x = traj.x[valid_mask]
                        valid_z = traj.z[valid_mask]
                        distances = np.sqrt((valid_x - junction.cx)**2 + (valid_z - junction.cz)**2)
                        min_distance = np.min(distances)
                        if min_distance &lt;= junction.r:
                            trajectories_through_junction += 1

                st.write(f&#34;- Sample trajectories through junction: {trajectories_through_junction}/5&#34;)
                if trajectories_through_junction == 0:
                    st.warning(&#34;⚠️ **Warning: No sample trajectories pass through the junction!**&#34;)
                    st.write(&#34;This might explain why no assignments are found.&#34;)
                    st.write(&#34;Check if junction coordinates match your trajectory data.&#34;)

                    # Additional debugging: Show coordinate ranges
                    st.error(&#34;🔍 **Coordinate Range Analysis:**&#34;)
                    all_x = np.concatenate([traj.x for traj in trajectories[:10]])  # Check first 10 trajectories
                    all_z = np.concatenate([traj.z for traj in trajectories[:10]])

                    # Handle NaN values in trajectory coordinates
                    valid_x = all_x[~np.isnan(all_x)]
                    valid_z = all_z[~np.isnan(all_z)]

                    if len(valid_x) &gt; 0 and len(valid_z) &gt; 0:
                        st.write(f&#34;- Trajectory X range: {np.min(valid_x):.1f} to {np.max(valid_x):.1f}&#34;)
                        st.write(f&#34;- Trajectory Z range: {np.min(valid_z):.1f} to {np.max(valid_z):.1f}&#34;)
                        st.write(f&#34;- Junction position: ({junction.cx}, {junction.cz})&#34;)
                        st.write(f&#34;- Junction radius: {junction.r}&#34;)

                        # Check if junction is within trajectory bounds
                        x_in_bounds = np.min(valid_x) &lt;= junction.cx &lt;= np.max(valid_x)
                        z_in_bounds = np.min(valid_z) &lt;= junction.cz &lt;= np.max(valid_z)
                        st.write(f&#34;- Junction X in trajectory bounds: {x_in_bounds}&#34;)
                        st.write(f&#34;- Junction Z in trajectory bounds: {z_in_bounds}&#34;)

                        if not (x_in_bounds and z_in_bounds):
                            st.error(&#34;❌ **CRITICAL:** Junction is outside trajectory coordinate bounds!&#34;)
                            st.write(&#34;**Solution:** Adjust junction coordinates in the Junctions tab to match your trajectory data.&#34;)
                    else:
                        st.error(&#34;❌ **CRITICAL:** All trajectory coordinates are NaN!&#34;)
                        st.write(&#34;**This indicates a data loading or scaling issue.**&#34;)
                        st.write(&#34;**Solutions:**&#34;)
                        st.write(&#34;1. Check if trajectory data was loaded correctly&#34;)
                        st.write(&#34;2. Check if scaling factor is appropriate&#34;)
                        st.write(&#34;3. Check if coordinate columns are mapped correctly&#34;)
                        st.write(&#34;4. Try reloading the data with different parameters&#34;)

                        # Show sample trajectory data for debugging
                        if len(trajectories) &gt; 0:
                            sample_traj = trajectories[0]
                            st.write(f&#34;**Sample trajectory data:**&#34;)
                            st.write(f&#34;- X values: {sample_traj.x[:5]} (first 5)&#34;)
                            st.write(f&#34;- Z values: {sample_traj.z[:5]} (first 5)&#34;)
                            st.write(f&#34;- X type: {type(sample_traj.x)}&#34;)
                            st.write(f&#34;- Z type: {type(sample_traj.z)}&#34;)

                # Debug: Show all parameters being passed to discover_decision_chain
                st.write(f&#34;🔍 **DEBUG: Parameters being passed to discover_decision_chain:**&#34;)
                st.write(f&#34;- path_length: {path_length} (type: {type(path_length)})&#34;)
                st.write(f&#34;- epsilon: {epsilon} (type: {type(epsilon)})&#34;)
                st.write(f&#34;- seed: {seed} (type: {type(seed)})&#34;)
                st.write(f&#34;- decision_mode: {decision_mode} (type: {type(decision_mode)})&#34;)
                st.write(f&#34;- r_outer_list: {r_outer_list} (type: {type(r_outer_list)})&#34;)
                st.write(f&#34;- linger_delta: {linger_delta} (type: {type(linger_delta)})&#34;)
                st.write(f&#34;- cluster_method: {cluster_method} (type: {type(cluster_method)})&#34;)
                st.write(f&#34;- k: {k} (type: {type(k)})&#34;)
                st.write(f&#34;- k_min: {k_min} (type: {type(k_min)})&#34;)
                st.write(f&#34;- k_max: {k_max} (type: {type(k_max)})&#34;)
                st.write(f&#34;- min_sep_deg: {min_sep_deg} (type: {type(min_sep_deg)})&#34;)
                st.write(f&#34;- angle_eps: {angle_eps} (type: {type(angle_eps)})&#34;)
                st.write(f&#34;- min_samples: {min_samples} (type: {type(min_samples)})&#34;)

                try:
                    chain_df, centers_list, decisions_chain_df = discover_decision_chain(
                        trajectories=trajectories,
                        junctions=[junction],
                        path_length=path_length,
                        epsilon=epsilon,
                        seed=seed,
                        decision_mode=decision_mode,
                        r_outer_list=r_outer_list,
                        linger_delta=linger_delta if linger_delta is not None else 0.0,
                        out_dir=out_dir,
                        cluster_method=cluster_method,
                        k=k,
                        k_min=k_min,
                        k_max=k_max,
                        min_sep_deg=min_sep_deg,
                        angle_eps=angle_eps,
                        min_samples=min_samples,
                    )
                    # Save decisions into session state for reuse by gaze
                    try:
                        if decisions_chain_df is not None and len(decisions_chain_df) &gt; 0:
                            st.session_state.analysis_results.setdefault(&#34;branches&#34;, {})
                            st.session_state.analysis_results[&#34;branches&#34;][&#34;chain_decisions&#34;] = decisions_chain_df
                    except Exception:
                        pass
                except Exception as e:
                    st.error(f&#34;❌ **Discover analysis failed - this will prevent gaze analysis!**&#34;)
                    st.write(f&#34;**Error type:** {type(e).__name__}&#34;)
                    st.write(f&#34;**Error message:** {str(e)}&#34;)

                    # Show the most common issues and solutions
                    st.error(&#34;🔧 **Common Solutions:**&#34;)
                    if &#34;int()&#34; in str(e) and &#34;NoneType&#34; in str(e):
                        st.write(&#34;**Issue:** NoneType to int() conversion error&#34;)
                        st.write(&#34;**Solution:** Use &#39;Use existing branch assignments&#39; or run &#39;🔍 Discover Branches&#39; first&#34;)
                    elif &#34;DBSCAN&#34; in str(e) or &#34;eps&#34; in str(e):
                        st.write(&#34;**Issue:** DBSCAN clustering failed&#34;)
                        st.write(&#34;**Solution:** Try &#39;Use existing branch assignments&#39; or adjust DBSCAN parameters&#34;)
                    elif &#34;trajectory&#34; in str(e).lower():
                        st.write(&#34;**Issue:** Trajectory data problem&#34;)
                        st.write(&#34;**Solution:** Check if trajectories pass through the junction&#34;)
                    else:
                        st.write(&#34;**Solution:** Use &#39;Use existing branch assignments&#39; or run &#39;🔍 Discover Branches&#39; first&#34;)

                    st.write(&#34;&#34;)
                    st.write(&#34;**💡 Recommended approach:**&#34;)
                    st.write(&#34;1. **Use existing assignments** (if available)&#34;)
                    st.write(&#34;2. **Or run &#39;🔍 Discover Branches&#39; analysis first**&#34;)
                    st.write(&#34;3. **Then return here for gaze analysis**&#34;)

                    # Fall back to empty assignments
                    import pandas as pd
                    chain_df = pd.DataFrame()
                    centers_list = []
                st.success(f&#34;✅ Discover analysis completed - found {len(chain_df)} assignments&#34;)
                st.write(f&#34;**Assignment Summary:**&#34;)
                st.write(f&#34;- Total trajectories processed: {len(trajectories)}&#34;)
                st.write(f&#34;- Trajectories with assignments: {len(chain_df)}&#34;)
                st.write(f&#34;- Assignment rate: {len(chain_df)/len(trajectories)*100:.1f}%&#34;)

                # Show sample of assignments
                if len(chain_df) &gt; 0:
                    st.write(f&#34;**Sample Assignments:**&#34;)
                    st.dataframe(chain_df.head(10), width=&#39;stretch&#39;)
                else:
                    st.warning(&#34;⚠️ No trajectories were assigned to this junction!&#34;)
                    st.write(&#34;**Possible causes:**&#34;)
                    st.write(&#34;- Junction position/radius may not match trajectory data&#34;)
                    st.write(&#34;- Decision parameters may be too restrictive&#34;)
                    st.write(&#34;- Trajectories may not actually pass through this junction&#34;)

                    # Add specific parameter suggestions
                    st.info(&#34;🔧 **Parameter Adjustment Suggestions:**&#34;)
                    st.write(&#34;**Current parameters:**&#34;)
                    st.write(f&#34;- Path length: {path_length} (try reducing to 20-50)&#34;)
                    st.write(f&#34;- Decision mode: {decision_mode}&#34;)
                    st.write(f&#34;- DBSCAN eps: {epsilon} (try increasing to 1.0-2.0)&#34;)
                    st.write(f&#34;- DBSCAN min_samples: {min_samples} (try reducing to 2-3)&#34;)
                    st.write(f&#34;- R_outer: {r_outer} (try increasing)&#34;)

                    st.write(&#34;**Suggested changes:**&#34;)
                    st.write(&#34;1. **Reduce path_length** from 100.0 to 20.0-50.0&#34;)
                    st.write(&#34;2. **Increase DBSCAN eps** from 0.5 to 1.0-2.0&#34;)
                    st.write(&#34;3. **Reduce min_samples** from 5 to 2-3&#34;)
                    st.write(&#34;4. **Check junction position** - ensure it matches your trajectory data&#34;)

                    # Debug: Show trajectory data around junction
                    st.write(&#34;🔍 **Trajectory Data Around Junction:**&#34;)
                    junction_x, junction_z = junction.cx, junction.cz
                    junction_r = junction.r

                    # Find trajectories that pass near the junction
                    nearby_trajectories = []
                    for i, traj in enumerate(trajectories[:10]):  # Check first 10 trajectories
                        distances = np.sqrt((traj.x - junction_x)**2 + (traj.z - junction_z)**2)
                        min_distance = np.min(distances)
                        if min_distance &lt; junction_r * 3:  # Within 3x junction radius
                            nearby_trajectories.append((i, min_distance))

                    if nearby_trajectories:
                        st.write(f&#34;- Found {len(nearby_trajectories)} trajectories within 3x junction radius&#34;)
                        for traj_idx, min_dist in nearby_trajectories[:5]:
                            st.write(f&#34;  - Trajectory {traj_idx}: min distance = {min_dist:.1f}&#34;)
                    else:
                        st.write(&#34;- No trajectories found within 3x junction radius&#34;)
                        st.write(&#34;- This suggests the junction position may not match your data&#34;)
            except Exception as e:
                st.warning(f&#34;⚠️ Discover analysis failed: {e}&#34;)
                st.info(&#34;🔄 Falling back to empty assignments...&#34;)
                chain_df = pd.DataFrame()

        # Preprocess trajectories to convert time values to numeric format
        processed_trajectories = []
        for traj in trajectories:
            if hasattr(traj, &#39;t&#39;) and traj.t is not None:
                # Convert time values to numeric if they&#39;re strings
                if isinstance(traj.t[0], str):
                    try:
                        import pandas as pd
                        # Convert string time format to numeric seconds
                        numeric_times = []
                        for t_val in traj.t:
                            if isinstance(t_val, str):
                                # Parse time string like &#34;00:00:17.425&#34;
                                time_parts = t_val.split(&#39;:&#39;)
                                if len(time_parts) == 3:
                                    hours = float(time_parts[0])
                                    minutes = float(time_parts[1])
                                    seconds = float(time_parts[2])
                                    total_seconds = hours * 3600 + minutes * 60 + seconds
                                    numeric_times.append(total_seconds)
                                else:
                                    numeric_times.append(float(t_val))
                            else:
                                numeric_times.append(float(t_val))
                        traj.t = np.array(numeric_times)
                    except Exception as e:
                        st.warning(f&#34;⚠️ Could not convert time values for trajectory {traj.tid}: {e}&#34;)
                        # Keep original time values
                processed_trajectories.append(traj)
            else:
                processed_trajectories.append(traj)

        # Store debug information in centralized gaze debug info
        junction_key = f&#34;junction_{junction.cx}_{junction.cz}_{junction.r}&#34;
        if &#39;gaze_debug_info&#39; not in st.session_state:
            st.session_state[&#39;gaze_debug_info&#39;] = {}

        st.session_state[&#39;gaze_debug_info&#39;][junction_key] = {
            &#39;trajectories_count&#39;: len(processed_trajectories),
            &#39;assignments_count&#39;: len(chain_df),
            &#39;junction&#39;: f&#34;Circle(cx={junction.cx}, cz={junction.cz}, r={junction.r})&#34;,
            &#39;decision_mode&#39;: decision_mode,
            &#39;r_outer&#39;: r_outer,
            &#39;status&#39;: &#39;processing&#39;
        }

        # Check if we have any assignments
        if len(chain_df) == 0:
            st.session_state[&#39;gaze_debug_info&#39;][junction_key][&#39;status&#39;] = &#39;no_assignments&#39;

            st.warning(&#34;⚠️ No trajectory assignments found - cannot perform gaze analysis&#34;)
            st.info(&#34;💡 **Suggestions:**&#34;)
            st.write(&#34;1. **Adjust parameters**: Try different decision mode or parameters&#34;)
            st.write(&#34;2. **Check junction position**: Verify junction coordinates match your data&#34;)
            st.write(&#34;3. **Run discover analysis first**: Use &#39;🔍 Discover Branches&#39; to create assignments&#34;)
            st.write(&#34;4. **Check trajectory data**: Ensure trajectories actually pass through junctions&#34;)

            # Return empty results
            return {
                &#39;physiological&#39;: None,
                &#39;pupil_dilation&#39;: None,
                &#39;head_yaw&#39;: None,
                &#39;junction&#39;: junction,
                &#39;r_outer&#39;: r_outer,
                &#39;error&#39;: &#39;No assignments found&#39;
            }

        # Use the actual gaze analysis functions with proper assignments
        try:
            # Update debug status
            st.session_state[&#39;gaze_debug_info&#39;][junction_key][&#39;status&#39;] = &#39;analyzing&#39;

            # Debug: Check what trajectories are being passed to gaze analysis
            st.info(f&#34;🔍 **Gaze Analysis Input Debug:**&#34;)
            st.write(f&#34;- Trajectories passed: {len(processed_trajectories)}&#34;)
            if processed_trajectories:
                sample_traj = processed_trajectories[0]
                st.write(f&#34;- Sample trajectory type: {type(sample_traj).__name__}&#34;)
                st.write(f&#34;- Sample trajectory ID: {sample_traj.tid}&#34;)
                st.write(f&#34;- Has gaze_x: {hasattr(sample_traj, &#39;gaze_x&#39;) and sample_traj.gaze_x is not None}&#34;)
                st.write(f&#34;- Has heart_rate: {hasattr(sample_traj, &#39;heart_rate&#39;) and sample_traj.heart_rate is not None}&#34;)
                st.write(f&#34;- Has pupil_l: {hasattr(sample_traj, &#39;pupil_l&#39;) and sample_traj.pupil_l is not None}&#34;)

            # Debug: Check assignments DataFrame structure
            st.info(f&#34;🔍 **Assignments DataFrame Debug:**&#34;)
            st.write(f&#34;- Assignments shape: {chain_df.shape}&#34;)
            st.write(f&#34;- Assignments columns: {list(chain_df.columns)}&#34;)
            if len(chain_df) &gt; 0:
                st.write(f&#34;- Sample assignment row:&#34;)
                st.write(chain_df.head(1))
                # Safe sorting of trajectory IDs (handle mixed types)
                try:
                    unique_ids = chain_df[&#39;trajectory&#39;].unique()
                    # Convert to strings for safe sorting
                    unique_ids_str = [str(id) for id in unique_ids]
                    sorted_ids = sorted(unique_ids_str)[:10]
                    st.write(f&#34;- Unique trajectory IDs in assignments: {sorted_ids}...&#34;)
                except Exception as e:
                    st.write(f&#34;- Unique trajectory IDs in assignments: {list(unique_ids)[:10]}... (sorting failed: {e})&#34;)
                st.write(f&#34;- Sample trajectory IDs from Trajectory objects: {[tr.tid for tr in processed_trajectories[:5]]}&#34;)

                # Check for ID mismatch (handle mixed types)
                assignment_ids = set(str(id) for id in chain_df[&#39;trajectory&#39;].unique())
                trajectory_ids = set(str(tr.tid) for tr in processed_trajectories)
                common_ids = assignment_ids.intersection(trajectory_ids)
                st.write(f&#34;- Common IDs between assignments and trajectories: {len(common_ids)}&#34;)
                if len(common_ids) &lt; 10:
                    st.write(f&#34;- Common IDs: {sorted(common_ids)}&#34;)
                else:
                    st.write(f&#34;- Common IDs (first 10): {sorted(list(common_ids))[:10]}&#34;)

            # Debug: Check session state at the beginning of gaze analysis
            st.write(f&#34;🔍 **Gaze Analysis Session State Check:**&#34;)
            st.write(f&#34;- analysis_results exists: {st.session_state.analysis_results is not None}&#34;)
            if st.session_state.analysis_results is not None:
                st.write(f&#34;- branches exists: {&#39;branches&#39; in st.session_state.analysis_results}&#34;)
                if &#39;branches&#39; in st.session_state.analysis_results:
                    branches = st.session_state.analysis_results[&#39;branches&#39;]
                    st.write(f&#34;- chain_decisions exists: {&#39;chain_decisions&#39; in branches}&#34;)
                    if &#39;chain_decisions&#39; in branches:
                        decisions_chain_df = branches[&#39;chain_decisions&#39;]
                        st.write(f&#34;- chain_decisions length: {len(decisions_chain_df)}&#34;)
                        st.write(f&#34;- Junction indices in chain_decisions: {sorted(decisions_chain_df[&#39;junction_index&#39;].unique())}&#34;)
                    else:
                        st.write(&#34;- chain_decisions not found in branches!&#34;)
                else:
                    st.write(&#34;- branches not found in analysis_results!&#34;)
            else:
                st.write(&#34;- analysis_results is None!&#34;)

            # Analyze physiological data
            st.info(&#34;🔬 Analyzing physiological data...&#34;)
            with st.spinner(&#34;Processing physiological data...&#34;):
                # Debug: Test trajectory matching before calling the function
                st.write(&#34;🔍 **Pre-analysis Debug:**&#34;)
                test_traj = processed_trajectories[0]
                test_assignments = chain_df[chain_df[&#34;trajectory&#34;] == test_traj.tid]
                st.write(f&#34;- Test trajectory ID: {test_traj.tid}&#34;)
                st.write(f&#34;- Test trajectory assignments: {len(test_assignments)} rows&#34;)
                if len(test_assignments) &gt; 0:
                    st.write(f&#34;- Test assignment: {test_assignments.iloc[0].to_dict()}&#34;)
                else:
                    st.write(&#34;- ❌ No assignments found for test trajectory!&#34;)

                # Fix: Convert single &#39;branch&#39; column to junction-specific format expected by gaze functions
                current_junction_idx = next(i for i, j in enumerate(st.session_state.junctions)
                                           if j.cx == junction.cx and j.cz == junction.cz and j.r == junction.r)
                current_branch_col = f&#39;branch_j{current_junction_idx}&#39;

                if &#39;branch&#39; in chain_df.columns:
                    if current_branch_col not in chain_df.columns:
                        st.info(f&#34;🔧 Converting single &#39;branch&#39; column to &#39;{current_branch_col}&#39; format for gaze analysis...&#34;)
                        chain_df_fixed = chain_df.copy()
                        chain_df_fixed[current_branch_col] = chain_df_fixed[&#39;branch&#39;]
                        st.write(f&#34;- Converted {len(chain_df_fixed)} assignments to gaze analysis format for Junction {current_junction_idx}&#34;)
                    else:
                        chain_df_fixed = chain_df
                else:
                    chain_df_fixed = chain_df

                # CRITICAL FIX: Filter out unassigned trajectories (branch = -1 or NaN)
                original_count = len(chain_df_fixed)
                if current_branch_col in chain_df_fixed.columns:
                    # Filter out unassigned trajectories
                    assigned_mask = (chain_df_fixed[current_branch_col] != -1) &amp; (chain_df_fixed[current_branch_col].notna())
                    chain_df_fixed = chain_df_fixed[assigned_mask]
                    filtered_count = len(chain_df_fixed)
                    unassigned_count = original_count - filtered_count

                    st.write(f&#34;🔍 **Assignment Filtering:**&#34;)
                    st.write(f&#34;- Original assignments: {original_count}&#34;)
                    st.write(f&#34;- Unassigned trajectories (branch=-1 or NaN): {unassigned_count}&#34;)
                    st.write(f&#34;- Assigned trajectories: {filtered_count}&#34;)

                    if unassigned_count &gt; 0:
                        st.info(f&#34;✅ Filtered out {unassigned_count} unassigned trajectories - only processing {filtered_count} assigned trajectories&#34;)
                    else:
                        st.info(f&#34;✅ All {original_count} trajectories are assigned to branches&#34;)

                    # Debug: Show sample of filtered assignments
                    if filtered_count &gt; 0:
                        st.write(f&#34;🔍 **Sample filtered assignments for Junction {current_junction_idx}:**&#34;)
                        sample_assignments = chain_df_fixed[[&#39;trajectory&#39;, current_branch_col]].head()
                        st.write(sample_assignments)

                        # Show branch distribution
                        branch_counts = chain_df_fixed[current_branch_col].value_counts().sort_index()
                        st.write(f&#34;🔍 **Branch distribution:**&#34;)
                        for branch, count in branch_counts.items():
                            st.write(f&#34;  - Branch {branch}: {count} trajectories&#34;)

                        # Debug: Show trajectory ID matching
                        st.write(f&#34;🔍 **Trajectory ID Matching Debug:**&#34;)
                        st.write(f&#34;- Assignments trajectory IDs (first 10): {list(chain_df_fixed[&#39;trajectory&#39;].astype(str))[:10]}&#34;)
                        st.write(f&#34;- Processed trajectories IDs (first 10): {[str(getattr(t, &#39;tid&#39;, getattr(t, &#39;id&#39;, &#39;NA&#39;))) for t in processed_trajectories[:10]]}&#34;)

                        # Check if trajectory ID 0 exists in assignments
                        traj_0_in_assignments = &#39;0&#39; in chain_df_fixed[&#39;trajectory&#39;].astype(str).values
                        st.write(f&#34;- Trajectory ID 0 in assignments: {traj_0_in_assignments}&#34;)

                        # ALWAYS apply trajectory ID mapping to ensure sequential IDs for gaze analysis
                        st.info(&#34;🔧 **APPLYING TRAJECTORY ID MAPPING**: Ensuring sequential trajectory IDs for gaze analysis...&#34;)

                        if True:  # Always apply mapping
                            st.write(f&#34;🔍 **Trajectory ID Analysis for Junction {current_junction_idx}:**&#34;)
                            st.write(&#34;The gaze analysis functions expect sequential trajectory IDs [0, 1, 2, ...] for proper decision point matching.&#34;)

                            # Show the actual range of trajectory IDs in assignments
                            traj_ids = chain_df_fixed[&#39;trajectory&#39;].astype(str).values
                            st.write(f&#34;- Assignment trajectory IDs (first 5): {traj_ids[:5]}&#34;)
                            st.write(f&#34;- Processed trajectory IDs (first 5): {[str(getattr(t, &#39;tid&#39;, getattr(t, &#39;id&#39;, i))) for i, t in enumerate(processed_trajectories[:5])]}&#34;)

                            # Check if trajectory IDs match between assignments and processed trajectories
                            assignment_ids = set(traj_ids)
                            processed_ids = set(str(getattr(t, &#39;tid&#39;, getattr(t, &#39;id&#39;, i))) for i, t in enumerate(processed_trajectories))
                            common_ids = assignment_ids.intersection(processed_ids)
                            st.write(f&#34;- Common trajectory IDs: {len(common_ids)} out of {len(assignment_ids)} assignments&#34;)

                            st.error(&#34;❌ **SOLUTION NEEDED**: The trajectory ID mapping between discover and gaze analysis is broken!&#34;)
                            st.write(&#34;The discover function assigns trajectories with string IDs (filenames) but gaze analysis expects sequential integer IDs [0, 1, 2, ...]&#34;)

                            # IMPLEMENT FIX: Create trajectory ID mapping
                            st.info(&#34;🔧 **IMPLEMENTING FIX**: Creating trajectory ID mapping...&#34;)

                            # Create a mapping from assignment trajectory IDs to processed trajectory indices
                            # Map assignment IDs to sequential indices based on the order they appear in processed_trajectories
                            assignment_ids = chain_df_fixed[&#39;trajectory&#39;].unique()
                            traj_id_to_index = {tid: i for i, tid in enumerate(sorted(assignment_ids))}

                            st.write(f&#34;🔧 **Trajectory ID Mapping Debug:**&#34;)
                            st.write(f&#34;- Assignment trajectory IDs (first 5): {list(chain_df_fixed[&#39;trajectory&#39;].unique()[:5])}&#34;)
                            st.write(f&#34;- Processed trajectory IDs (first 5): {list(traj_id_to_index.keys())[:5]}&#34;)
                            st.write(f&#34;- Mapping dictionary (first 5): {dict(list(traj_id_to_index.items())[:5])}&#34;)

                            # Map assignment trajectory IDs to processed trajectory indices
                            chain_df_fixed[&#39;trajectory_index&#39;] = chain_df_fixed[&#39;trajectory&#39;].map(traj_id_to_index)

                            # Remove rows where trajectory ID couldn&#39;t be mapped
                            original_mapped_count = len(chain_df_fixed)
                            chain_df_fixed = chain_df_fixed.dropna(subset=[&#39;trajectory_index&#39;])
                            mapped_count = len(chain_df_fixed)
                            unmapped_count = original_mapped_count - mapped_count

                            st.write(f&#34;🔧 **Trajectory ID Mapping Results:**&#34;)
                            st.write(f&#34;- Original assignments: {original_mapped_count}&#34;)
                            st.write(f&#34;- Successfully mapped: {mapped_count}&#34;)
                            st.write(f&#34;- Unmapped (removed): {unmapped_count}&#34;)

                            if mapped_count &gt; 0:
                                st.success(f&#34;✅ **FIX APPLIED**: Mapped {mapped_count} trajectory assignments to processed trajectory indices!&#34;)
                                st.write(f&#34;- Sample mapping: {chain_df_fixed[[&#39;trajectory&#39;, &#39;trajectory_index&#39;, current_branch_col]].head()}&#34;)

                                # Update trajectory IDs to be sequential starting from 0
                                chain_df_fixed[&#39;trajectory&#39;] = chain_df_fixed[&#39;trajectory_index&#39;].astype(int)
                                chain_df_fixed = chain_df_fixed.drop(&#39;trajectory_index&#39;, axis=1)

                                st.write(f&#34;🔧 **Updated assignments with sequential IDs:**&#34;)
                                st.write(f&#34;- Sample: {chain_df_fixed[[&#39;trajectory&#39;, current_branch_col]].head()}&#34;)
                            else:
                                st.error(&#34;❌ **FIX FAILED**: No trajectory assignments could be mapped!&#34;)
                                st.write(&#34;This suggests a fundamental mismatch between discover and gaze analysis trajectory handling.&#34;)
                else:
                    st.error(f&#34;❌ Expected column &#39;{current_branch_col}&#39; not found in assignments!&#34;)
                    st.write(f&#34;Available columns: {list(chain_df_fixed.columns)}&#34;)

                # Ensure expected branch column exists for physiological analysis
                chain_df_call = chain_df_fixed.copy()

                # Ensure the junction-specific branch column exists
                if current_branch_col not in chain_df_call.columns:
                    st.error(f&#34;❌ **Missing branch column**: {current_branch_col} not found for Junction {current_junction_idx}&#34;)
                else:
                    st.write(f&#34;🔧 **Using junction-specific column**: {current_branch_col} for Junction {current_junction_idx}&#34;)

                    # CRITICAL FIX: Use verta_consistency.normalize_assignments for proper trajectory ID mapping
                    # This will automatically handle branch column naming and trajectory ID mapping
                    st.info(f&#34;🔧 **Using verta_consistency.normalize_assignments for proper trajectory ID mapping...**&#34;)

                    try:
                        from verta.verta_consistency import normalize_assignments

                        # Use the proper normalization function with the fixed assignments
                        # It will automatically create branch_j{i} for single junction analysis
                        normalized_df, report = normalize_assignments(
                            assignments_df=chain_df_fixed,  # Use the fixed assignments with proper trajectory IDs
                            trajectories=processed_trajectories,
                            junctions=[junction],  # Single junction for this analysis
                            current_junction_idx=current_junction_idx,
                            prefer_decisions=False,  # We already have the assignments
                            include_outliers=False,  # Filter out negative branches
                            strict=False  # Don&#39;t fail on low coverage
                        )

                        st.write(f&#34;🔧 **Normalization report:**&#34;)
                        st.write(f&#34;- Input rows: {report[&#39;input_rows&#39;]}&#34;)
                        st.write(f&#34;- Kept after ID mapping: {report[&#39;kept_after_tid_map&#39;]}&#34;)
                        st.write(f&#34;- Dropped unmapped IDs: {report[&#39;dropped_unmapped_ids&#39;]}&#34;)
                        st.write(f&#34;- Has decisions: {report[&#39;has_decisions&#39;]}&#34;)

                        if report[&#39;kept_after_tid_map&#39;] &gt; 0:
                            st.success(f&#34;✅ **Assignment normalization successful!** {report[&#39;kept_after_tid_map&#39;]} assignments ready for gaze analysis&#34;)

                            # Show available branch columns after normalization
                            branch_cols = [col for col in normalized_df.columns if col.startswith(&#39;branch&#39;)]
                            st.write(f&#34;🔧 **Available branch columns after normalization:** {branch_cols}&#34;)

                            # Show sample data with available branch columns
                            display_cols = [&#39;trajectory&#39;] + branch_cols
                            st.write(f&#34;🔧 **Sample normalized assignments:**&#34;)
                            st.write(normalized_df[display_cols].head())

                            # Use the normalized DataFrame for gaze analysis
                            chain_df_call = normalized_df
                        else:
                            st.error(f&#34;❌ **Assignment normalization failed!** No assignments could be mapped&#34;)
                            st.write(&#34;This suggests a fundamental mismatch between trajectory IDs and assignment IDs&#34;)

                    except Exception as e:
                        st.error(f&#34;❌ **Error during assignment normalization:** {e}&#34;)
                        st.write(&#34;Falling back to manual trajectory ID mapping...&#34;)

                        # Fallback to manual mapping if normalization fails
                        traj_id_mapping = {}
                        for i, traj in enumerate(processed_trajectories):
                            original_id = getattr(traj, &#39;_original_tid&#39;, traj.tid)
                            traj_id_mapping[original_id] = i

                        if &#39;trajectory&#39; in chain_df_call.columns:
                            chain_df_call[&#39;trajectory&#39;] = chain_df_call[&#39;trajectory&#39;].map(traj_id_mapping)
                            chain_df_call = chain_df_call.dropna(subset=[&#39;trajectory&#39;])

                # CRITICAL FIX: Ensure we&#39;re using the correct junction-specific assignments
                # If we have a single-junction assignment (single &#39;branch&#39; column),
                # we need to make sure the decision points are calculated for THIS junction,
                # not some other junction&#39;s decision points.
                st.write(f&#34;🔍 **Junction-Specific Assignment Debug:**&#34;)
                st.write(f&#34;- Current junction index: {current_junction_idx}&#34;)
                st.write(f&#34;- Current branch column: {current_branch_col}&#34;)
                st.write(f&#34;- Available columns: {list(chain_df_call.columns)}&#34;)

                # Show branch column values dynamically
                branch_cols = [col for col in chain_df_call.columns if col.startswith(&#39;branch&#39;)]
                for branch_col in branch_cols:
                    st.write(f&#34;- {branch_col} values: {chain_df_call[branch_col].value_counts().to_dict()}&#34;)

                if not branch_cols:
                    st.write(&#34;- ❌ No branch columns found!&#34;)

                # Create copies of trajectories and remap their IDs to match the assignments DataFrame
                # This ensures the analysis functions can find the correct assignments
                import copy as _copy
                trajectories_for_analysis = [_copy.copy(tr) for tr in processed_trajectories]

                # CRITICAL FIX: Update trajectory IDs to match the assignments DataFrame
                # The assignments DataFrame has original trajectory IDs, so we need to match them
                assignment_ids = chain_df_fixed[&#39;trajectory&#39;].unique()

                # Update trajectory IDs to match assignments DataFrame IDs
                for i, _tr in enumerate(trajectories_for_analysis):
                    if i &lt; len(assignment_ids):
                        _tr.tid = assignment_ids[i]
                    else:
                        _tr.tid = i

                # Normalize assignments using shared consistency layer (replaces ad-hoc fixes)
                try:
                    from .verta_consistency import normalize_assignments
                except Exception:
                    from verta.verta_consistency import normalize_assignments

                decisions_chain_df = st.session_state.analysis_results.get(&#34;branches&#34;, {}).get(&#34;decision_points&#34;)
                if decisions_chain_df is None:
                    # Fallback: try to load from default GUI outputs dir
                    try:
                        import os as _os
                        import pandas as _pd
                        _p = _os.path.join(&#34;gui_outputs&#34;, &#34;branch_decisions_chain.csv&#34;)
                        if _os.path.exists(_p):
                            decisions_chain_df = _pd.read_csv(_p)
                            st.write(&#34;🔗 Loaded decisions from gui_outputs/branch_decisions_chain.csv (fallback)&#34;)
                    except Exception:
                        pass
                norm_df, norm_report = normalize_assignments(
                    chain_df_fixed,
                    trajectories=trajectories_for_analysis,
                    junctions=[junction],
                    current_junction_idx=current_junction_idx,
                    decisions_df=decisions_chain_df,
                    prefer_decisions=True,
                    include_outliers=False,
                )
                chain_df_call = norm_df  # override with normalized assignments
                st.write(&#34;🧭 Assignments normalization report:&#34;)
                st.write(norm_report)

                physio_data = analyze_physiological_at_junctions(
                    trajectories=trajectories_for_analysis,
                    junctions=[junction],
                    assignments_df=chain_df_call,  # Use the assignments with merged decision points
                    decision_mode=discover_decision_mode,  # Use discover decision mode
                    r_outer_list=r_outer_list,
                    path_length=discover_path_length,  # Use discover path length
                    epsilon=discover_epsilon,  # Use discover epsilon
                    linger_delta=discover_linger_delta,  # Use discover linger delta
                    physio_window=3.0
                )

                # DEBUG: Check what was passed to physiological analysis
                st.write(f&#34;🔍 **DEBUG: Physiological Analysis Input Check:**&#34;)
                st.write(f&#34;- Trajectories passed: {len(trajectories_for_analysis)}&#34;)
                st.write(f&#34;- First trajectory ID: {trajectories_for_analysis[0].tid} (type: {type(trajectories_for_analysis[0].tid)})&#34;)
                st.write(f&#34;- Assignments DataFrame shape: {chain_df_fixed.shape}&#34;)
                st.write(f&#34;- Assignments trajectory column dtype: {chain_df_fixed[&#39;trajectory&#39;].dtype}&#34;)
                st.write(f&#34;- Assignments trajectory sample: {chain_df_fixed[&#39;trajectory&#39;].head().tolist()}&#34;)
                st.write(f&#34;- Junction: {junction}&#34;)
                st.write(f&#34;- Junction index: {current_junction_idx}&#34;)

            st.success(&#34;✅ Physiological analysis completed&#34;)
            st.write(f&#34;🔍 **Physiological Results:** {len(physio_data) if physio_data is not None else 0} rows&#34;)

            # Debug: Show physiological results details
            if physio_data is not None and len(physio_data) &gt; 0:
                st.write(f&#34;🔍 **Physiological Results Debug:**&#34;)
                st.write(f&#34;- Rows: {len(physio_data)}&#34;)
                st.write(f&#34;- Columns: {list(physio_data.columns)}&#34;)
                if &#39;heart_rate_change&#39; in physio_data.columns:
                    hr_change_count = physio_data[&#39;heart_rate_change&#39;].notna().sum()
                    st.write(f&#34;- Heart rate change values: {hr_change_count}&#34;)
                else:
                    st.write(f&#34;- ❌ &#39;heart_rate_change&#39; column missing!&#34;)
            else:
                st.write(f&#34;🔍 **Physiological Results Debug:** No data returned&#34;)

            # Analyze pupil dilation trajectories
            st.info(&#34;👁️ Analyzing pupil dilation trajectories...&#34;)
            with st.spinner(&#34;Processing pupil dilation data...&#34;):
                # Ensure expected branch column exists for pupil analysis
                chain_df_call = chain_df_fixed.copy()

                # Ensure the junction-specific branch column exists
                if current_branch_col not in chain_df_call.columns:
                    st.error(f&#34;❌ **Missing branch column**: {current_branch_col} not found for Junction {current_junction_idx}&#34;)
                else:
                    st.write(f&#34;🔧 **Using junction-specific column**: {current_branch_col} for Junction {current_junction_idx}&#34;)

                # CRITICAL FIX: Ensure we&#39;re using the correct junction-specific assignments
                # If we have a single-junction assignment (single &#39;branch&#39; column),
                # we need to make sure the decision points are calculated for THIS junction,
                # not some other junction&#39;s decision points.
                st.write(f&#34;🔍 **Pupil Analysis - Junction-Specific Assignment Debug:**&#34;)
                st.write(f&#34;- Current junction index: {current_junction_idx}&#34;)
                st.write(f&#34;- Current branch column: {current_branch_col}&#34;)
                st.write(f&#34;- Available columns: {list(chain_df_call.columns)}&#34;)
                if current_branch_col in chain_df_call.columns:
                    st.write(f&#34;- {current_branch_col} values: {chain_df_call[current_branch_col].value_counts().to_dict()}&#34;)
                else:
                    st.write(f&#34;- ❌ {current_branch_col} column not found!&#34;)

                # Create copies of trajectories and remap their IDs to match the assignments DataFrame
                # This ensures the analysis functions can find the correct assignments
                import copy as _copy
                trajectories_for_analysis = [_copy.copy(tr) for tr in processed_trajectories]

                # CRITICAL FIX: Update trajectory IDs to match the assignments DataFrame
                # The assignments DataFrame has original trajectory IDs, so we need to match them
                assignment_ids = chain_df_fixed[&#39;trajectory&#39;].unique()

                # Update trajectory IDs to match assignments DataFrame IDs
                for i, _tr in enumerate(trajectories_for_analysis):
                    if i &lt; len(assignment_ids):
                        _tr.tid = assignment_ids[i]
                    else:
                        _tr.tid = i

                # Normalize assignments using shared consistency layer (replaces ad-hoc fixes)
                try:
                    from .verta_consistency import normalize_assignments
                except Exception:
                    from verta.verta_consistency import normalize_assignments

                decisions_chain_df = st.session_state.analysis_results.get(&#34;branches&#34;, {}).get(&#34;decision_points&#34;)
                if decisions_chain_df is None:
                    # Fallback: try to load from default GUI outputs dir
                    try:
                        import os as _os
                        import pandas as _pd
                        _p = _os.path.join(&#34;gui_outputs&#34;, &#34;branch_decisions_chain.csv&#34;)
                        if _os.path.exists(_p):
                            decisions_chain_df = _pd.read_csv(_p)
                            st.write(&#34;🔗 Loaded decisions from gui_outputs/branch_decisions_chain.csv (fallback)&#34;)
                    except Exception:
                        pass
                norm_df, norm_report = normalize_assignments(
                    chain_df_fixed,
                    trajectories=trajectories_for_analysis,
                    junctions=[junction],
                    current_junction_idx=current_junction_idx,
                    decisions_df=decisions_chain_df,
                    prefer_decisions=True,
                    include_outliers=False,
                )
                chain_df_call = norm_df  # override with normalized assignments
                st.write(&#34;🧭 Assignments normalization report:&#34;)
                st.write(norm_report)
                # Debug: Check if normalize_assignments already merged decision points
                precomputed_count = chain_df_call[&#39;decision_idx&#39;].notna().sum() if &#39;decision_idx&#39; in chain_df_call.columns else 0
                st.write(f&#34;🔍 Decision points after normalize_assignments (pupil): {precomputed_count} trajectories have precomputed decision points&#34;)
                if precomputed_count &gt; 0:
                    st.write(f&#34;🔍 Sample decision points: {chain_df_call[[&#39;trajectory&#39;, &#39;decision_idx&#39;, &#39;intercept_x&#39;, &#39;intercept_z&#39;]].head(3).to_dict(&#39;records&#39;)}&#34;)
                else:
                    st.write(&#34;⚠️ No decision points found - this may cause analysis to fail&#34;)

                pupil_data = analyze_pupil_dilation_trajectory(
                    trajectories=trajectories_for_analysis,
                    junctions=[junction],
                    assignments_df=chain_df_call,  # Use the assignments with merged decision points
                    decision_mode=discover_decision_mode,  # Use discover decision mode
                    r_outer_list=r_outer_list,
                    path_length=discover_path_length,  # Use discover path length
                    epsilon=discover_epsilon,  # Use discover epsilon
                    linger_delta=discover_linger_delta,  # Use discover linger delta
                    physio_window=3.0
                )

            st.success(&#34;✅ Pupil dilation analysis completed&#34;)
            st.write(f&#34;🔍 **Pupil Results:** {len(pupil_data) if pupil_data is not None else 0} rows&#34;)

            # Debug: Show pupil results details
            if pupil_data is not None and len(pupil_data) &gt; 0:
                st.write(f&#34;🔍 **Pupil Results Debug:**&#34;)
                st.write(f&#34;- Rows: {len(pupil_data)}&#34;)
                st.write(f&#34;- Columns: {list(pupil_data.columns)}&#34;)
                if &#39;pupil_change&#39; in pupil_data.columns:
                    pupil_change_count = pupil_data[&#39;pupil_change&#39;].notna().sum()
                    st.write(f&#34;- Pupil change values: {pupil_change_count}&#34;)
                else:
                    st.write(f&#34;- ❌ &#39;pupil_change&#39; column missing!&#34;)
            else:
                st.write(f&#34;🔍 **Pupil Results Debug:** No data returned&#34;)

            # Analyze head yaw at decisions
            st.info(&#34;🧭 Analyzing head yaw at decisions...&#34;)
            with st.spinner(&#34;Processing head yaw data...&#34;):
                # Ensure expected branch column exists for head yaw analysis
                chain_df_call = chain_df_fixed.copy()

                # Ensure the junction-specific branch column exists
                if current_branch_col not in chain_df_call.columns:
                    st.error(f&#34;❌ **Missing branch column**: {current_branch_col} not found for Junction {current_junction_idx}&#34;)
                else:
                    st.write(f&#34;🔧 **Using junction-specific column**: {current_branch_col} for Junction {current_junction_idx}&#34;)

                # CRITICAL FIX: Ensure we&#39;re using the correct junction-specific assignments
                # If we have a single-junction assignment (single &#39;branch&#39; column),
                # we need to make sure the decision points are calculated for THIS junction,
                # not some other junction&#39;s decision points.
                st.write(f&#34;🔍 **Head Yaw Analysis - Junction-Specific Assignment Debug:**&#34;)
                st.write(f&#34;- Current junction index: {current_junction_idx}&#34;)
                st.write(f&#34;- Current branch column: {current_branch_col}&#34;)
                st.write(f&#34;- Available columns: {list(chain_df_call.columns)}&#34;)
                if current_branch_col in chain_df_call.columns:
                    st.write(f&#34;- {current_branch_col} values: {chain_df_call[current_branch_col].value_counts().to_dict()}&#34;)
                else:
                    st.write(f&#34;- ❌ {current_branch_col} column not found!&#34;)

                # Create copies of trajectories and remap their IDs to match the assignments DataFrame
                # This ensures the analysis functions can find the correct assignments
                import copy as _copy
                trajectories_for_analysis = [_copy.copy(tr) for tr in processed_trajectories]

                # CRITICAL FIX: Update trajectory IDs to match the assignments DataFrame
                # The assignments DataFrame has original trajectory IDs, so we need to match them
                assignment_ids = chain_df_fixed[&#39;trajectory&#39;].unique()

                # Update trajectory IDs to match assignments DataFrame IDs
                for i, _tr in enumerate(trajectories_for_analysis):
                    if i &lt; len(assignment_ids):
                        _tr.tid = assignment_ids[i]
                    else:
                        _tr.tid = i

                # Normalize assignments using shared consistency layer (replaces ad-hoc fixes)
                try:
                    from .verta_consistency import normalize_assignments
                except Exception:
                    from verta.verta_consistency import normalize_assignments

                decisions_chain_df = st.session_state.analysis_results.get(&#34;branches&#34;, {}).get(&#34;decision_points&#34;)
                if decisions_chain_df is None:
                    # Fallback: try to load from default GUI outputs dir
                    try:
                        import os as _os
                        import pandas as _pd
                        _p = _os.path.join(&#34;gui_outputs&#34;, &#34;branch_decisions_chain.csv&#34;)
                        if _os.path.exists(_p):
                            decisions_chain_df = _pd.read_csv(_p)
                            st.write(&#34;🔗 Loaded decisions from gui_outputs/branch_decisions_chain.csv (fallback)&#34;)
                    except Exception:
                        pass
                norm_df, norm_report = normalize_assignments(
                    chain_df_fixed,
                    trajectories=trajectories_for_analysis,
                    junctions=[junction],
                    current_junction_idx=current_junction_idx,
                    decisions_df=decisions_chain_df,
                    prefer_decisions=True,
                    include_outliers=False,
                )
                chain_df_call = norm_df  # override with normalized assignments
                st.write(&#34;🧭 Assignments normalization report:&#34;)
                st.write(norm_report)
                # Debug: Check if normalize_assignments already merged decision points
                precomputed_count = chain_df_call[&#39;decision_idx&#39;].notna().sum() if &#39;decision_idx&#39; in chain_df_call.columns else 0
                st.write(f&#34;🔍 Decision points after normalize_assignments: {precomputed_count} trajectories have precomputed decision points&#34;)
                if precomputed_count &gt; 0:
                    st.write(f&#34;🔍 Sample decision points: {chain_df_call[[&#39;trajectory&#39;, &#39;decision_idx&#39;, &#39;intercept_x&#39;, &#39;intercept_z&#39;]].head(3).to_dict(&#39;records&#39;)}&#34;)
                else:
                    st.write(&#34;⚠️ No decision points found - this may cause analysis to fail&#34;)

                # Get the decision mode used by the discover analysis
                discover_decision_mode = &#34;pathlen&#34;  # Default to pathlen since that&#39;s what you&#39;re using
                if existing_assignments is not None:
                    # Try to get the decision mode from the junction data
                    discover_junction_key = f&#34;junction_{current_junction_idx}&#34;
                    if discover_junction_key in st.session_state.analysis_results.get(&#34;branches&#34;, {}):
                        junction_data = st.session_state.analysis_results[&#34;branches&#34;][discover_junction_key]
                        discover_decision_mode = junction_data.get(&#34;decision_mode&#34;, &#34;pathlen&#34;)

                st.write(f&#34;🔧 **Using discover decision mode**: {discover_decision_mode} (same as discover analysis)&#34;)

                # Debug: Check what DataFrame is being passed to gaze analysis
                st.write(f&#34;🔍 **DataFrame being passed to gaze analysis:**&#34;)
                st.write(f&#34;- Shape: {chain_df_call.shape}&#34;)
                st.write(f&#34;- Columns: {list(chain_df_call.columns)}&#34;)
                st.write(f&#34;- Has decision_idx: {&#39;decision_idx&#39; in chain_df_call.columns}&#34;)
                st.write(f&#34;- Has intercept_x: {&#39;intercept_x&#39; in chain_df_call.columns}&#34;)
                st.write(f&#34;- Has intercept_z: {&#39;intercept_z&#39; in chain_df_call.columns}&#34;)
                if &#39;decision_idx&#39; in chain_df_call.columns:
                    precomputed_count = chain_df_call[&#39;decision_idx&#39;].notna().sum()
                    st.write(f&#34;- Precomputed decision points: {precomputed_count} out of {len(chain_df_call)}&#34;)
                    st.write(f&#34;- Sample precomputed data: {chain_df_call[[&#39;trajectory&#39;, &#39;decision_idx&#39;, &#39;intercept_x&#39;, &#39;intercept_z&#39;]].head(3).to_dict(&#39;records&#39;)}&#34;)

                head_yaw_data = compute_head_yaw_at_decisions(
                    trajectories=trajectories_for_analysis,
                    junctions=[junction],
                    assignments_df=chain_df_call,  # Use the assignments with merged decision points
                    decision_mode=discover_decision_mode,  # Use the same decision mode as discover analysis
                    r_outer_list=r_outer_list,
                    path_length=path_length,
                    epsilon=epsilon,
                    linger_delta=linger_delta,  # Use the same linger_delta as discover analysis
                    base_index=current_junction_idx if current_junction_idx is not None else 0,
                )

            st.success(&#34;✅ Head yaw analysis completed&#34;)
            st.write(f&#34;🔍 **Head Yaw Results:** {len(head_yaw_data) if head_yaw_data is not None else 0} rows&#34;)

            # Debug: Show head yaw results details
            if head_yaw_data is not None and len(head_yaw_data) &gt; 0:
                st.write(f&#34;🔍 **Head Yaw Results Debug:**&#34;)
                st.write(f&#34;- Rows: {len(head_yaw_data)}&#34;)
                st.write(f&#34;- Columns: {list(head_yaw_data.columns)}&#34;)
                if &#39;head_yaw&#39; in head_yaw_data.columns:
                    head_yaw_count = head_yaw_data[&#39;head_yaw&#39;].notna().sum()
                    st.write(f&#34;- Head yaw values: {head_yaw_count}&#34;)
                else:
                    st.write(f&#34;- ❌ &#39;head_yaw&#39; column missing!&#34;)
                if &#39;yaw_difference&#39; in head_yaw_data.columns:
                    yaw_diff_count = head_yaw_data[&#39;yaw_difference&#39;].notna().sum()
                    st.write(f&#34;- Yaw difference values: {yaw_diff_count}&#34;)
                else:
                    st.write(f&#34;- ❌ &#39;yaw_difference&#39; column missing!&#34;)
            else:
                st.write(f&#34;🔍 **Head Yaw Results Debug:** No data returned&#34;)

            # Create per-junction pupil dilation heatmap
            st.info(&#34;🗺️ Creating junction-specific pupil dilation heatmap...&#34;)
            with st.spinner(&#34;Generating junction heatmap...&#34;):
                # Get heatmap settings from session state
                cell_size = st.session_state.get(&#39;pupil_heatmap_cell_size&#39;, 50.0)
                normalization = st.session_state.get(&#39;pupil_heatmap_normalization&#39;, &#39;relative&#39;)

                # Create per-junction heatmap (focused on current junction)
                # Get junction index from junction_key in session state
                junction_idx = None
                for idx, j in enumerate(st.session_state.junctions):
                    if j.cx == junction.cx and j.cz == junction.cz and j.r == junction.r:
                        junction_idx = idx
                        break

                junction_heatmaps = create_per_junction_pupil_heatmap(
                    trajectories=processed_trajectories,
                    junctions=[junction],
                    r_outer_list=[r_outer],
                    cell_size=cell_size,
                    normalization=normalization,
                    base_index=junction_idx if junction_idx is not None else 0
                )

                # Fix the junction index in the heatmap data
                # The function returns a dict with key=0 (local index), but we need the global index
                # Re-key the dictionary to use the global junction index
                if junction_idx is not None and 0 in junction_heatmaps:
                    junction_heatmaps = {junction_idx: junction_heatmaps[0]}


            # Update debug status to completed
            st.session_state[&#39;gaze_debug_info&#39;][junction_key][&#39;status&#39;] = &#39;completed&#39;
            st.session_state[&#39;gaze_debug_info&#39;][junction_key][&#39;physio_rows&#39;] = len(physio_data) if physio_data is not None else 0
            st.session_state[&#39;gaze_debug_info&#39;][junction_key][&#39;pupil_rows&#39;] = len(pupil_data) if pupil_data is not None else 0
            st.session_state[&#39;gaze_debug_info&#39;][junction_key][&#39;head_yaw_rows&#39;] = len(head_yaw_data) if head_yaw_data is not None else 0

            # Save CSV files to gui_outputs
            try:
                import os
                gaze_data_dir = os.path.join(&#34;gui_outputs&#34;, &#34;gaze_data&#34;)
                os.makedirs(gaze_data_dir, exist_ok=True)

                # Get junction index for file naming
                junction_idx = None
                for idx, j in enumerate(st.session_state.junctions):
                    if j.cx == junction.cx and j.cz == junction.cz and j.r == junction.r:
                        junction_idx = idx
                        break

                if junction_idx is not None:
                    junction_prefix = f&#34;junction_{junction_idx}&#34;

                    # Save physiological analysis data
                    if physio_data is not None and len(physio_data) &gt; 0:
                        # Ensure all numpy arrays are converted to lists for CSV compatibility
                        physio_data_clean = physio_data.copy()
                        for col in physio_data_clean.columns:
                            if physio_data_clean[col].dtype == &#39;object&#39;:
                                # Check if column contains numpy arrays
                                physio_data_clean[col] = physio_data_clean[col].apply(
                                    lambda x: x.tolist() if hasattr(x, &#39;tolist&#39;) else x
                                )

                        physio_file = os.path.join(gaze_data_dir, f&#34;{junction_prefix}_physiological_analysis.csv&#34;)
                        physio_data_clean.to_csv(physio_file, index=False)
                        st.info(f&#34;📁 Physiological data saved to: {physio_file}&#34;)

                    # Save pupil dilation data
                    if pupil_data is not None and len(pupil_data) &gt; 0:
                        # Ensure all numpy arrays are converted to lists for CSV compatibility
                        pupil_data_clean = pupil_data.copy()
                        for col in pupil_data_clean.columns:
                            if pupil_data_clean[col].dtype == &#39;object&#39;:
                                # Check if column contains numpy arrays
                                pupil_data_clean[col] = pupil_data_clean[col].apply(
                                    lambda x: x.tolist() if hasattr(x, &#39;tolist&#39;) else x
                                )

                        pupil_file = os.path.join(gaze_data_dir, f&#34;{junction_prefix}_pupil_trajectory_analysis.csv&#34;)
                        pupil_data_clean.to_csv(pupil_file, index=False)
                        st.info(f&#34;📁 Pupil trajectory data saved to: {pupil_file}&#34;)

                    # Save head yaw data
                    if head_yaw_data is not None and len(head_yaw_data) &gt; 0:
                        # Ensure all numpy arrays are converted to lists for CSV compatibility
                        head_yaw_data_clean = head_yaw_data.copy()
                        for col in head_yaw_data_clean.columns:
                            if head_yaw_data_clean[col].dtype == &#39;object&#39;:
                                # Check if column contains numpy arrays
                                head_yaw_data_clean[col] = head_yaw_data_clean[col].apply(
                                    lambda x: x.tolist() if hasattr(x, &#39;tolist&#39;) else x
                                )

                        gaze_file = os.path.join(gaze_data_dir, f&#34;{junction_prefix}_gaze_analysis.csv&#34;)
                        head_yaw_data_clean.to_csv(gaze_file, index=False)
                        st.info(f&#34;📁 Gaze analysis data saved to: {gaze_file}&#34;)

                    # Save pupil heatmap data as JSON
                    if junction_heatmaps:
                        heatmap_file = os.path.join(gaze_data_dir, f&#34;{junction_prefix}_pupil_heatmap.json&#34;)
                        import json
                        # Convert numpy arrays to lists for JSON serialization
                        def convert_numpy_to_list(obj):
                            if hasattr(obj, &#39;tolist&#39;):
                                return obj.tolist()
                            elif isinstance(obj, dict):
                                return {k: convert_numpy_to_list(v) for k, v in obj.items()}
                            elif isinstance(obj, list):
                                return [convert_numpy_to_list(item) for item in obj]
                            elif hasattr(obj, &#39;to_dict&#39;):  # Handle pandas DataFrames
                                return obj.to_dict(&#39;records&#39;)
                            elif hasattr(obj, &#39;__dict__&#39;):  # Handle dataclass objects (like Circle)
                                return {k: convert_numpy_to_list(v) for k, v in obj.__dict__.items()}
                            else:
                                return obj

                        # Use the converted version for JSON serialization
                        heatmap_data = convert_numpy_to_list(junction_heatmaps)
                        with open(heatmap_file, &#39;w&#39;) as f:
                            json.dump(heatmap_data, f, indent=2)
                        st.info(f&#34;📁 Pupil heatmap data saved to: {heatmap_file}&#34;)

            except Exception as e:
                st.warning(f&#34;⚠️ Could not save gaze data files: {e}&#34;)

            # Create comprehensive results
            results = {
                &#39;physiological&#39;: physio_data,
                &#39;pupil_dilation&#39;: pupil_data,
                &#39;head_yaw&#39;: head_yaw_data,
                &#39;pupil_heatmap_junction&#39;: junction_heatmaps,
                &#39;junction&#39;: junction,
                &#39;r_outer&#39;: r_outer
            }

            # Convert any numpy arrays to lists for JSON serialization
            def convert_numpy_to_list(obj):
                if hasattr(obj, &#39;tolist&#39;):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {k: convert_numpy_to_list(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy_to_list(item) for item in obj]
                elif hasattr(obj, &#39;to_dict&#39;):  # Handle pandas DataFrames
                    return obj.to_dict(&#39;records&#39;)
                elif hasattr(obj, &#39;__dict__&#39;):  # Handle dataclass objects (like Circle)
                    return {k: convert_numpy_to_list(v) for k, v in obj.__dict__.items()}
                else:
                    return obj

            # Apply conversion to results for JSON serialization
            results_for_json = convert_numpy_to_list(results)

            # Return original results (with DataFrames and Circle objects) for plotting functions
            return results

        except Exception as e:
            # Update debug status to error
            st.session_state[&#39;gaze_debug_info&#39;][junction_key][&#39;status&#39;] = &#39;error&#39;
            st.session_state[&#39;gaze_debug_info&#39;][junction_key][&#39;error&#39;] = str(e)

            # More detailed error information
            import traceback
            st.error(f&#34;❌ Comprehensive gaze analysis failed: {e}&#34;)
            st.error(f&#34;**Error details:** {str(e)}&#34;)
            st.error(f&#34;**Error type:** {type(e).__name__}&#34;)

            # Show traceback for debugging
            st.code(traceback.format_exc())

            if processed_trajectories:
                sample_traj = processed_trajectories[0]
                st.write(f&#34;- Sample trajectory type: {type(sample_traj)}&#34;)
                st.write(f&#34;- Sample trajectory attributes: {[attr for attr in dir(sample_traj) if not attr.startswith(&#39;_&#39;)]}&#34;)

            st.info(&#34;🔄 Falling back to movement pattern analysis...&#34;)
            return self._analyze_movement_patterns_optimized(
                trajectories=trajectories,
                junction=junction,
                r_outer=r_outer,
                decision_mode=decision_mode,
                path_length=path_length,
                epsilon=epsilon
            )

    def _get_filtered_trajectories_for_junction(self, trajectories, junction, r_outer):
        &#34;&#34;&#34;Get trajectories that pass through a specific junction area, with clipped coordinates.&#34;&#34;&#34;
        import numpy as np

        filtered_trajs = []

        for traj in trajectories:
            # Calculate distances from junction center
            rx = traj.x - junction.cx
            rz = traj.z - junction.cz
            r = np.hypot(rx, rz)

            # Keep trajectories that pass through the junction area
            if np.any(r &lt;= r_outer):
                # Create a clipped version that only shows the junction-relevant portion
                # This helps with zoom by reducing the data extent
                junction_mask = r &lt;= (r_outer * 1.5)  # Slightly larger than r_outer for context

                if np.any(junction_mask):
                    # Create a simple object to hold the clipped data
                    class ClippedTraj:
                        def __init__(self, tid, x, z):
                            self.tid = tid
                            self.x = x
                            self.z = z

                    clipped_traj = ClippedTraj(
                        tid=traj.tid,
                        x=traj.x[junction_mask],
                        z=traj.z[junction_mask]
                    )
                    filtered_trajs.append(clipped_traj)

        return filtered_trajs

    def _safe_get_time_value(self, trajectory, decision_idx):
        &#34;&#34;&#34;Safely get time value from trajectory, handling different data types.&#34;&#34;&#34;
        import numpy as np

        if not hasattr(trajectory, &#39;t&#39;) or trajectory.t is None or decision_idx &gt;= len(trajectory.t):
            return np.nan

        try:
            time_value = trajectory.t[decision_idx]

            # Handle string time values
            if isinstance(time_value, str):
                try:
                    import pandas as pd
                    return pd.to_timedelta(time_value).total_seconds()
                except:
                    return np.nan

            # Handle numeric values
            if isinstance(time_value, (int, float)) and not np.isnan(time_value):
                return float(time_value)

            return np.nan
        except:
            return np.nan

    def _add_pupil_dilation_analysis(self, gaze_data, trajectories, junction, r_outer):
        &#34;&#34;&#34;Add simplified pupil dilation analysis for regular trajectories.&#34;&#34;&#34;
        import numpy as np

        # Check if any trajectory has time data
        has_time_data = any(hasattr(traj, &#39;t&#39;) and traj.t is not None for traj in trajectories)

        if not has_time_data:
            # Add placeholder columns
            gaze_data[&#39;pupil_baseline&#39;] = np.nan
            gaze_data[&#39;pupil_decision&#39;] = np.nan
            gaze_data[&#39;pupil_dilation&#39;] = np.nan
            gaze_data[&#39;pupil_analysis_available&#39;] = False
            return gaze_data

        # Add pupil analysis columns
        gaze_data[&#39;pupil_baseline&#39;] = np.nan
        gaze_data[&#39;pupil_decision&#39;] = np.nan
        gaze_data[&#39;pupil_dilation&#39;] = np.nan
        gaze_data[&#39;pupil_analysis_available&#39;] = True

        # For regular trajectories, we can&#39;t do real pupil analysis, but we can analyze timing patterns
        for idx, row in gaze_data.iterrows():
            traj_idx = int(row[&#39;trajectory&#39;])
            decision_idx = int(row[&#39;decision_idx&#39;])

            if traj_idx &lt; len(trajectories):
                trajectory = trajectories[traj_idx]

                if hasattr(trajectory, &#39;t&#39;) and trajectory.t is not None and decision_idx &lt; len(trajectory.t):
                    try:
                        decision_time = self._safe_get_time_value(trajectory, decision_idx)

                        if np.isnan(decision_time):
                            continue

                        # Analyze timing patterns around decision point
                        time_window = 3.0  # 3 second window
                        time_mask = (trajectory.t &gt;= decision_time - time_window) &amp; (trajectory.t &lt;= decision_time + time_window)

                        if np.any(time_mask):
                            # Calculate timing-based metrics as proxy for physiological analysis
                            pre_decision_times = trajectory.t[(trajectory.t &gt;= decision_time - time_window) &amp; (trajectory.t &lt; decision_time)]
                            post_decision_times = trajectory.t[(trajectory.t &gt; decision_time) &amp; (trajectory.t &lt;= decision_time + time_window)]

                            # Use timing patterns as proxy for pupil analysis
                            gaze_data.loc[idx, &#39;pupil_baseline&#39;] = len(pre_decision_times) if len(pre_decision_times) &gt; 0 else 0
                            gaze_data.loc[idx, &#39;pupil_decision&#39;] = len(post_decision_times) if len(post_decision_times) &gt; 0 else 0
                            gaze_data.loc[idx, &#39;pupil_dilation&#39;] = len(post_decision_times) - len(pre_decision_times)
                    except Exception as e:
                        # Skip this trajectory if there&#39;s any error with time data
                        continue

        return gaze_data

    def _analyze_movement_patterns_all_junctions(self, trajectories, junctions, r_outer_list, decision_mode, path_length, epsilon):
        &#34;&#34;&#34;Analyze movement patterns across all junctions in temporal order.&#34;&#34;&#34;
        import numpy as np
        import pandas as pd

        results = []

        for traj_idx, trajectory in enumerate(trajectories):
            # Find all junction visits in temporal order for this trajectory
            junction_sequence = self._find_junction_sequence(trajectory, junctions, r_outer_list)

            # Analyze each junction visit in the sequence
            for visit_idx, junction_idx in enumerate(junction_sequence):
                junction = junctions[junction_idx]
                r_outer = r_outer_list[junction_idx]

                # Find decision point for this specific junction visit
                decision_idx = self._find_decision_point_for_junction_visit(
                    trajectory, junction, r_outer, decision_mode, path_length, epsilon, junction_sequence, visit_idx
                )

                if decision_idx is not None and decision_idx &lt; len(trajectory.x):
                    # Calculate movement metrics for this junction visit
                    movement_yaw = np.nan
                    if decision_idx &gt; 0 and decision_idx &lt; len(trajectory.x) - 1:
                        dx = trajectory.x[decision_idx + 1] - trajectory.x[decision_idx - 1]
                        dz = trajectory.z[decision_idx + 1] - trajectory.z[decision_idx - 1]
                        movement_magnitude = np.hypot(dx, dz)
                        if movement_magnitude &gt; 1e-3:
                            movement_yaw = np.degrees(np.arctan2(dx, dz))

                    # Calculate approach and exit directions
                    approach_yaw = np.nan
                    if decision_idx &gt; 0:
                        dx_approach = trajectory.x[decision_idx] - trajectory.x[decision_idx - 1]
                        dz_approach = trajectory.z[decision_idx] - trajectory.z[decision_idx - 1]
                        approach_magnitude = np.hypot(dx_approach, dz_approach)
                        if approach_magnitude &gt; 1e-3:
                            approach_yaw = np.degrees(np.arctan2(dx_approach, dz_approach))

                    exit_yaw = np.nan
                    if decision_idx &lt; len(trajectory.x) - 1:
                        dx_exit = trajectory.x[decision_idx + 1] - trajectory.x[decision_idx]
                        dz_exit = trajectory.z[decision_idx + 1] - trajectory.z[decision_idx]
                        exit_magnitude = np.hypot(dx_exit, dz_exit)
                        if exit_magnitude &gt; 1e-3:
                            exit_yaw = np.degrees(np.arctan2(dx_exit, dz_exit))

                    # Calculate distance from junction center
                    distance_from_center = np.sqrt(
                        (trajectory.x[decision_idx] - junction.cx)**2 +
                        (trajectory.z[decision_idx] - junction.cz)**2
                    )

                    # Calculate trajectory position metrics
                    trajectory_length = len(trajectory.x)
                    decision_ratio = decision_idx / trajectory_length if trajectory_length &gt; 0 else 0

                    results.append({
                        &#34;trajectory&#34;: traj_idx,
                        &#34;junction&#34;: junction_idx,
                        &#34;visit_order&#34;: visit_idx,  # Order of this junction in the trajectory
                        &#34;decision_idx&#34;: decision_idx,
                        &#34;trajectory_length&#34;: trajectory_length,
                        &#34;decision_ratio&#34;: decision_ratio,
                        &#34;movement_yaw&#34;: movement_yaw,
                        &#34;approach_yaw&#34;: approach_yaw,
                        &#34;exit_yaw&#34;: exit_yaw,
                        &#34;distance_from_center&#34;: distance_from_center,
                        &#34;decision_x&#34;: trajectory.x[decision_idx],
                        &#34;decision_z&#34;: trajectory.z[decision_idx],
                        &#34;time_at_decision&#34;: self._safe_get_time_value(trajectory, decision_idx)
                    })

        return pd.DataFrame(results)

    def _find_junction_sequence(self, trajectory, junctions, r_outer_list):
        &#34;&#34;&#34;Find the temporal sequence of junction visits for a trajectory.&#34;&#34;&#34;
        import numpy as np

        sequence = []
        current_junction = None

        for point_idx in range(len(trajectory.x)):
            x, z = trajectory.x[point_idx], trajectory.z[point_idx]

            # Check if we&#39;re at a new junction
            for junction_idx, (junction, r_outer) in enumerate(zip(junctions, r_outer_list)):
                distance = np.sqrt((x - junction.cx)**2 + (z - junction.cz)**2)

                if distance &lt;= r_outer:
                    if current_junction != junction_idx:
                        sequence.append(junction_idx)
                        current_junction = junction_idx
                    break
            else:
                # Not at any junction
                current_junction = None

        return sequence

    def _find_decision_point_for_junction_visit(self, trajectory, junction, r_outer, decision_mode, path_length, epsilon, junction_sequence, visit_idx):
        &#34;&#34;&#34;Find decision point for a specific junction visit in the sequence.&#34;&#34;&#34;
        import numpy as np

        # Find the range of points where this junction was visited
        junction_points = []
        for point_idx in range(len(trajectory.x)):
            x, z = trajectory.x[point_idx], trajectory.z[point_idx]
            distance = np.sqrt((x - junction.cx)**2 + (z - junction.cz)**2)
            if distance &lt;= r_outer:
                junction_points.append(point_idx)

        if not junction_points:
            return None

        # Use the first point of junction visit as decision point
        # This represents when the user first entered the junction
        return junction_points[0]

    def _find_radial_decision_point(self, trajectory, junction, r_outer):
        &#34;&#34;&#34;Find decision point using radial method.&#34;&#34;&#34;
        import numpy as np

        # Find the first point that enters the junction
        for i in range(len(trajectory.x)):
            distance = np.sqrt((trajectory.x[i] - junction.cx)**2 + (trajectory.z[i] - junction.cz)**2)
            if distance &lt;= r_outer:
                return i
        return None

    def _find_path_length_decision_point(self, trajectory, junction, path_length, epsilon):
        &#34;&#34;&#34;Find decision point using path length method.&#34;&#34;&#34;
        import numpy as np

        # Find closest point to junction center
        distances = np.sqrt((trajectory.x - junction.cx)**2 + (trajectory.z - junction.cz)**2)
        closest_idx = np.argmin(distances)

        # Use a more reasonable search window
        search_window = min(int(path_length), len(trajectory.x) // 10, 50)  # Smaller, more reasonable window
        start_idx = max(0, closest_idx - search_window)
        end_idx = min(len(trajectory.x), closest_idx + search_window)

        # Look for decision point within the search window
        for i in range(start_idx, end_idx):
            distance = np.sqrt((trajectory.x[i] - junction.cx)**2 + (trajectory.z[i] - junction.cz)**2)
            if distance &lt;= junction.r + epsilon:
                return i

        # If no point found within junction radius, return closest point
        return closest_idx

    def _find_nearest_to_center(self, trajectory, junction):
        &#34;&#34;&#34;Find nearest point to junction center.&#34;&#34;&#34;
        import numpy as np

        distances = np.sqrt((trajectory.x - junction.cx)**2 + (trajectory.z - junction.cz)**2)
        return np.argmin(distances)

    def render_gaze_visualizations(self):
        &#34;&#34;&#34;Render gaze analysis visualizations&#34;&#34;&#34;
        st.markdown(&#34;### Gaze and Physiological Analysis Results&#34;)

        # Check if analysis results and gaze_results exist
        if (st.session_state.analysis_results is None or
            &#34;gaze_results&#34; not in st.session_state.analysis_results):
            st.info(&#34;No gaze analysis results available. Run gaze analysis first.&#34;)
            return

        # Display gaze results for each junction
        for junction_key, gaze_data in st.session_state.analysis_results[&#34;gaze_results&#34;].items():
            st.markdown(f&#34;#### {junction_key.replace(&#39;_&#39;, &#39; &#39;).title()}&#34;)

            if gaze_data is None:
                st.info(&#34;No gaze analysis data available for this junction&#34;)
                continue

            # Check if we have comprehensive gaze data or fallback data
            if isinstance(gaze_data, dict):
                if &#39;error&#39; in gaze_data:
                    # Show error information
                    st.error(f&#34;❌ **Gaze Analysis Failed for {junction_key}**&#34;)
                    st.write(f&#34;**Error:** {gaze_data[&#39;error&#39;]}&#34;)
                    st.write(f&#34;**Error Type:** {gaze_data[&#39;error_type&#39;]}&#34;)

                    # Show suggestions based on error type
                    if &#34;No assignments found&#34; in gaze_data[&#39;error&#39;]:
                        st.info(&#34;💡 **Solution:** Run &#39;🔍 Discover Branches&#39; analysis first to create proper assignments&#34;)
                    elif &#34;trajectory&#34; in gaze_data[&#39;error&#39;].lower():
                        st.info(&#34;💡 **Solution:** Check if trajectories actually pass through this junction&#34;)
                    elif &#34;column&#34; in gaze_data[&#39;error&#39;].lower():
                        st.info(&#34;💡 **Solution:** Check your gaze column mappings in the Data tab&#34;)

                    # Show empty plots with error messages
                    self._render_error_gaze_results(gaze_data, junction_key)
                elif &#39;physiological&#39; in gaze_data:
                    # Comprehensive gaze analysis results
                    self._render_comprehensive_gaze_results(gaze_data, junction_key)
                else:
                    # Fallback movement pattern results
                    st.warning(&#34;⚠️ Using fallback visualization&#34;)
                    self._render_fallback_gaze_results(gaze_data, junction_key)
            else:
                # Fallback movement pattern results
                st.warning(&#34;⚠️ Using fallback visualization&#34;)
                self._render_fallback_gaze_results(gaze_data, junction_key)

    def _render_fallback_gaze_results(self, gaze_data, junction_key):
        &#34;&#34;&#34;Render fallback gaze results when no proper gaze analysis was performed.&#34;&#34;&#34;
        st.markdown(&#34;**Gaze Analysis Results:**&#34;)

        if gaze_data is not None:
            # Check if this is actual gaze data DataFrame or movement data
            if hasattr(gaze_data, &#39;columns&#39;) and len(gaze_data) &gt; 0:
                if &#39;analysis_type&#39; in gaze_data.columns and gaze_data[&#39;analysis_type&#39;].iloc[0] == &#39;gaze&#39;:
                    # This is actual gaze data
                    st.success(&#34;✅ Gaze analysis completed successfully!&#34;)

                    # Display gaze-specific metrics
                    col1, col2, col3, col4 = st.columns(4)

                    with col1:
                        st.metric(&#34;Total Trajectories&#34;, len(gaze_data))

                    with col2:
                        valid_head_yaw = gaze_data[&#39;head_yaw&#39;].dropna()
                        if len(valid_head_yaw) &gt; 0:
                            st.metric(&#34;Valid Head Directions&#34;, len(valid_head_yaw))
                        else:
                            st.metric(&#34;Valid Head Directions&#34;, 0)

                    with col3:
                        valid_pupil = gaze_data[[&#39;pupil_l&#39;, &#39;pupil_r&#39;]].dropna()
                        if len(valid_pupil) &gt; 0:
                            st.metric(&#34;Valid Pupil Data&#34;, len(valid_pupil))
                        else:
                            st.metric(&#34;Valid Pupil Data&#34;, 0)

                    with col4:
                        valid_hr = gaze_data[&#39;heart_rate&#39;].dropna()
                        if len(valid_hr) &gt; 0:
                            st.metric(&#34;Valid Heart Rate&#34;, len(valid_hr))
                        else:
                            st.metric(&#34;Valid Heart Rate&#34;, 0)

                    # Show gaze data table
                    st.dataframe(gaze_data.head(20), width=&#39;stretch&#39;)

                    if len(gaze_data) &gt; 20:
                        st.info(f&#34;Showing first 20 of {len(gaze_data)} gaze records&#34;)

                    # Debug: Show what type of trajectory objects we have
                    st.markdown(&#34;**🔍 Debug Information:**&#34;)
                    sample_traj = st.session_state.trajectories[0] if st.session_state.trajectories else None
                    if sample_traj:
                        st.write(f&#34;- Trajectory type: {type(sample_traj).__name__}&#34;)
                        st.write(f&#34;- Available attributes: {[attr for attr in dir(sample_traj) if not attr.startswith(&#39;_&#39;)]}&#34;)
                        if hasattr(sample_traj, &#39;headset_gaze_x&#39;):
                            st.write(f&#34;- Has gaze data: ✅&#34;)
                        else:
                            st.write(f&#34;- Has gaze data: ❌&#34;)

                    # Create gaze-specific visualizations
                    self._create_gaze_visualizations(gaze_data, junction_key)

                else:
                    # This is movement data - shouldn&#39;t happen in gaze analysis
                    st.warning(&#34;⚠️ **No Gaze Data Available**&#34;)
                    st.info(&#34;&#34;&#34;
                    **Gaze analysis requires:**
                    - Eye tracking data (`Headset.Gaze.X`, `Headset.Gaze.Y`)
                    - Head orientation data (`Headset.Head.Forward.X`, `Headset.Head.Forward.Z`)
                    - Physiological data (`Headset.PupilDilation.L`, `Headset.PupilDilation.R`, `Headset.HeartRate`)

                    **Current data only contains position information (x, z, t).**

                    **To perform gaze analysis:**
                    1. Load data with eye tracking sensors
                    2. Ensure column mappings are correct
                    3. Use VR headset with gaze tracking capabilities
                    &#34;&#34;&#34;)
        else:
            st.warning(&#34;⚠️ **No Gaze Analysis Performed**&#34;)
            st.info(&#34;&#34;&#34;
            **Gaze analysis was not performed because:**
            - No eye tracking data detected
            - Column mappings not properly configured
            - Data doesn&#39;t contain required gaze/physiological fields

            **Please check:**
            1. Your data contains gaze tracking columns
            2. Column mappings are correctly specified
            3. Data format matches VR headset export format
            &#34;&#34;&#34;)

    def _render_error_gaze_results(self, gaze_data, junction_key):
        &#34;&#34;&#34;Render error gaze results with informative empty plots.&#34;&#34;&#34;
        import matplotlib.pyplot as plt
        import numpy as np

        st.markdown(&#34;**Gaze Analysis Results:**&#34;)

        # Create empty plots with error messages
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        fig.suptitle(f&#39;Comprehensive Gaze Analysis - {junction_key}&#39;, fontsize=16, fontweight=&#39;bold&#39;)

        # 1. Heart Rate Change Distribution
        ax1 = axes[0, 0]
        ax1.text(0.5, 0.5, f&#39;Analysis Failed\n{gaze_data[&#34;error&#34;]}&#39;,
                ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax1.transAxes, fontsize=10)
        ax1.set_title(&#39;Heart Rate Change Distribution&#39;)
        ax1.set_xlim(0, 1)
        ax1.set_ylim(0, 1)

        # 2. Pupil Dilation Change Distribution
        ax2 = axes[0, 1]
        ax2.text(0.5, 0.5, f&#39;Analysis Failed\n{gaze_data[&#34;error&#34;]}&#39;,
                ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax2.transAxes, fontsize=10)
        ax2.set_title(&#39;Pupil Dilation Change Distribution&#39;)
        ax2.set_xlim(0, 1)
        ax2.set_ylim(0, 1)

        # 3. Head Yaw Distribution
        ax3 = axes[1, 0]
        ax3.text(0.5, 0.5, f&#39;Analysis Failed\n{gaze_data[&#34;error&#34;]}&#39;,
                ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax3.transAxes, fontsize=10)
        ax3.set_title(&#39;Head Yaw Distribution&#39;)
        ax3.set_xlim(0, 1)
        ax3.set_ylim(0, 1)

        # 4. Gaze-Movement Difference Distribution
        ax4 = axes[1, 1]
        ax4.text(0.5, 0.5, f&#39;Analysis Failed\n{gaze_data[&#34;error&#34;]}&#39;,
                ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax4.transAxes, fontsize=10)
        ax4.set_title(&#39;Gaze-Movement Difference Distribution&#39;)
        ax4.set_xlim(0, 1)
        ax4.set_ylim(0, 1)

        plt.tight_layout()
        st.pyplot(fig)
        plt.close()

    def _render_comprehensive_gaze_results(self, gaze_data, junction_key):
        &#34;&#34;&#34;Render comprehensive gaze analysis results (dictionary format).&#34;&#34;&#34;
        import pandas as pd

        # Display physiological data
        if &#39;physiological&#39; in gaze_data and gaze_data[&#39;physiological&#39;] is not None:
            physio_df = gaze_data[&#39;physiological&#39;]
            if len(physio_df) &gt; 0:
                st.markdown(&#34;**Physiological Analysis:**&#34;)

                col1, col2, col3 = st.columns(3)
                with col1:
                    if &#39;heart_rate_change&#39; in physio_df.columns:
                        valid_hr = physio_df[&#39;heart_rate_change&#39;].dropna()
                        if len(valid_hr) &gt; 0:
                            st.metric(&#34;Avg Heart Rate Change&#34;, f&#34;{valid_hr.mean():.1f} bpm&#34;)
                        else:
                            st.metric(&#34;Avg Heart Rate Change&#34;, &#34;N/A&#34;)
                    else:
                        st.metric(&#34;Avg Heart Rate Change&#34;, &#34;Column not found&#34;)

                with col2:
                    if &#39;pupil_change&#39; in physio_df.columns:
                        valid_pupil = physio_df[&#39;pupil_change&#39;].dropna()
                        if len(valid_pupil) &gt; 0:
                            st.metric(&#34;Avg Pupil Change&#34;, f&#34;{valid_pupil.mean():.2f}&#34;)
                        else:
                            st.metric(&#34;Avg Pupil Change&#34;, &#34;N/A&#34;)
                    else:
                        st.metric(&#34;Avg Pupil Change&#34;, &#34;Column not found&#34;)

                with col3:
                    st.metric(&#34;Total Measurements&#34;, len(physio_df))

                st.dataframe(physio_df.head(10), width=&#39;stretch&#39;)

        # Display pupil dilation data
        if &#39;pupil_dilation&#39; in gaze_data and gaze_data[&#39;pupil_dilation&#39;] is not None:
            pupil_df = gaze_data[&#39;pupil_dilation&#39;]
            if len(pupil_df) &gt; 0:
                st.markdown(&#34;**Pupil Dilation Analysis:**&#34;)

                col1, col2, col3 = st.columns(3)
                with col1:
                    valid_baseline = pupil_df[&#39;pupil_baseline&#39;].dropna()
                    if len(valid_baseline) &gt; 0:
                        st.metric(&#34;Avg Pupil Baseline&#34;, f&#34;{valid_baseline.mean():.2f}&#34;)

                with col2:
                    valid_decision = pupil_df[&#39;pupil_decision&#39;].dropna()
                    if len(valid_decision) &gt; 0:
                        st.metric(&#34;Avg Pupil at Decision&#34;, f&#34;{valid_decision.mean():.2f}&#34;)

                with col3:
                    valid_change = pupil_df[&#39;pupil_change&#39;].dropna()
                    if len(valid_change) &gt; 0:
                        st.metric(&#34;Avg Pupil Change&#34;, f&#34;{valid_change.mean():.2f}&#34;)

                st.dataframe(pupil_df.head(10), width=&#39;stretch&#39;)

        # Display head yaw data
        if &#39;head_yaw&#39; in gaze_data and gaze_data[&#39;head_yaw&#39;] is not None:
            head_df = gaze_data[&#39;head_yaw&#39;]
            if len(head_df) &gt; 0:
                st.markdown(&#34;**Head Yaw Analysis:**&#34;)

                col1, col2, col3 = st.columns(3)
                with col1:
                    valid_yaw = head_df[&#39;head_yaw&#39;].dropna()
                    if len(valid_yaw) &gt; 0:
                        st.metric(&#34;Avg Head Yaw&#34;, f&#34;{valid_yaw.mean():.1f}°&#34;)

                with col2:
                    valid_diff = head_df[&#39;yaw_difference&#39;].dropna()
                    if len(valid_diff) &gt; 0:
                        st.metric(&#34;Avg Gaze-Movement Diff&#34;, f&#34;{valid_diff.mean():.1f}°&#34;)

                with col3:
                    st.metric(&#34;Total Measurements&#34;, len(head_df))

                st.dataframe(head_df.head(10), width=&#39;stretch&#39;)

        # Create visualizations for comprehensive gaze data
        self._create_comprehensive_gaze_visualizations(gaze_data, junction_key)

        # Add advanced gaze plotting features from CLI version
        self._create_advanced_gaze_plots(gaze_data, junction_key)

        # Display pupil dilation heatmaps
        has_global_heatmap = st.session_state.analysis_results and &#39;pupil_heatmap_global&#39; in st.session_state.analysis_results

        # Check if any junction has heatmap data
        has_junction_heatmaps = False
        if st.session_state.analysis_results and &#39;gaze_results&#39; in st.session_state.analysis_results:
            gaze_results = st.session_state.analysis_results[&#39;gaze_results&#39;]
            has_junction_heatmaps = any(
                isinstance(gaze_data_item, dict) and &#39;pupil_heatmap_junction&#39; in gaze_data_item
                for gaze_data_item in gaze_results.values()
            )

        if has_global_heatmap or has_junction_heatmaps:
            st.markdown(&#34;---&#34;)
            st.markdown(&#34;### 🗺️ Pupil Dilation Spatial Heatmaps&#34;)
            st.info(&#34;Spatial distribution of pupil dilation changes across the map&#34;)

            # Calculate consistent scaling across all heatmaps
            from verta.verta_gaze import get_consistent_pupil_scaling
            heatmap_data_list = []
            if has_global_heatmap:
                heatmap_data_list.append(st.session_state.analysis_results[&#39;pupil_heatmap_global&#39;])
            if has_junction_heatmaps:
                gaze_results = st.session_state.analysis_results[&#39;gaze_results&#39;]
                for gaze_data_item in gaze_results.values():
                    if isinstance(gaze_data_item, dict) and &#39;pupil_heatmap_junction&#39; in gaze_data_item:
                        heatmap_data_list.extend(gaze_data_item[&#39;pupil_heatmap_junction&#39;].values())

            normalization = st.session_state.get(&#39;pupil_heatmap_normalization&#39;, &#39;relative&#39;)
            vmin, vmax = get_consistent_pupil_scaling(heatmap_data_list, normalization)

            st.write(f&#34;🎨 **Consistent Color Scaling:** {vmin:.1f}% to {vmax:.1f}% (realistic pupil dilation range)&#34;)

            # Create tabs for global and per-junction views
            tab_junction, tab_global = st.tabs([&#34;🎯 Junction Heatmap&#34;, &#34;🌍 Global Heatmap&#34;])

            with tab_junction:
                if has_junction_heatmaps:
                    st.markdown(&#34;#### Junction-Specific Pupil Patterns&#34;)
                    st.caption(&#34;Focused view of pupil changes at each junction (includes approach paths)&#34;)

                    # Display heatmap for current junction only
                    current_junction_heatmaps = {}
                    if isinstance(gaze_data, dict) and &#39;pupil_heatmap_junction&#39; in gaze_data:
                        current_junction_heatmaps = gaze_data[&#39;pupil_heatmap_junction&#39;]

                    if len(current_junction_heatmaps) == 0:
                        st.info(&#34;ℹ️ No junction heatmaps available&#34;)
                    else:
                        for junction_idx, heatmap_data in current_junction_heatmaps.items():
                            with st.expander(f&#34;**Junction {junction_idx}**&#34;, expanded=True):
                                # Check for errors
                                if heatmap_data.get(&#39;error&#39;):
                                    st.warning(f&#34;⚠️ {heatmap_data[&#39;error&#39;]}&#34;)
                                else:
                                    # Get junction info
                                    junction = heatmap_data.get(&#39;junction&#39;)
                                    r_outer = heatmap_data.get(&#39;r_outer&#39;, &#39;N/A&#39;)

                                    col_info1, col_info2 = st.columns(2)
                                    with col_info1:
                                        st.caption(f&#34;📍 Center: ({junction.cx:.1f}, {junction.cz:.1f})&#34;)
                                    with col_info2:
                                        st.caption(f&#34;📏 Analysis radius (r_outer): {r_outer}&#34;)

                                    # Check if pre-generated plot exists first
                                    junction_key = f&#34;junction_{junction_idx}&#34;
                                    pre_generated_plot_path = os.path.join(&#34;gui_outputs&#34;, f&#34;junction_{junction_idx}&#34;, &#34;gaze_plots&#34;, f&#34;{junction_key}_pupil_heatmap.png&#34;)

                                    # Debug: Show the path being checked
                                    st.write(f&#34;🔍 **Debug:** Looking for junction heatmap at: `{pre_generated_plot_path}`&#34;)
                                    st.write(f&#34;🔍 **Debug:** File exists: {os.path.exists(pre_generated_plot_path)}&#34;)

                                    if os.path.exists(pre_generated_plot_path):
                                        # Load and display pre-generated plot
                                        st.image(pre_generated_plot_path, caption=f&#34;Junction {junction_idx} Pupil Dilation (Pre-generated)&#34;)
                                        st.caption(&#34;📁 Plot generated during analysis&#34;)
                                    else:
                                        # Generate plot on-demand (fallback)
                                        st.caption(&#34;🔄 Generating plot on-demand...&#34;)
                                        st.write(f&#34;⚠️ **Debug:** Pre-generated plot not found at `{pre_generated_plot_path}`&#34;)

                                        # Get filtered trajectories for this junction (only those passing through)
                                        filtered_trajs_for_plot = self._get_filtered_trajectories_for_junction(
                                            st.session_state.trajectories, junction, r_outer
                                        )

                                        # Plot junction heatmap (with minimap, with filtered trajectories only)
                                        fig_junction = plot_pupil_dilation_heatmap(
                                            heatmap_data=heatmap_data,
                                            junctions=[junction] if junction else None,
                                            trajectories=filtered_trajs_for_plot,
                                            all_trajectories=st.session_state.trajectories,  # Pass all trajectories for minimap
                                            title=f&#34;Junction {junction_idx} Pupil Dilation&#34;,
                                            show_sample_counts=False,
                                            show_minimap=True,
                                            vmin=vmin,
                                            vmax=vmax
                                        )
                                        st.pyplot(fig_junction)
                                        import matplotlib.pyplot as plt
                                        plt.close(fig_junction)

                                    # Show junction statistics
                                    heatmap = heatmap_data[&#39;heatmap&#39;]
                                    # Ensure heatmap is a numpy array
                                    if not isinstance(heatmap, np.ndarray):
                                        heatmap = np.array(heatmap)
                                    valid_bins = heatmap[~np.isnan(heatmap)]
                                    norm_method = heatmap_data[&#39;normalization_used&#39;]

                                    if len(valid_bins) &gt; 0:
                                        col1, col2, col3 = st.columns(3)

                                        with col1:
                                            if norm_method == &#34;relative&#34;:
                                                st.metric(&#34;Mean Change&#34;, f&#34;{np.mean(valid_bins):.2f}%&#34;)
                                            else:
                                                st.metric(&#34;Mean Z-score&#34;, f&#34;{np.mean(valid_bins):.2f}&#34;)

                                        with col2:
                                            if norm_method == &#34;relative&#34;:
                                                st.metric(&#34;Max Absolute Change&#34;, f&#34;±{np.max(np.abs(valid_bins)):.2f}%&#34;)
                                            else:
                                                st.metric(&#34;Max Absolute Z-score&#34;, f&#34;{np.max(np.abs(valid_bins)):.2f}&#34;)

                                        with col3:
                                            st.metric(&#34;Trajectories&#34;, f&#34;{heatmap_data[&#39;valid_trajectories&#39;]}&#34;)
                else:
                    st.info(&#34;ℹ️ No junction heatmaps available&#34;)

            with tab_global:
                if has_global_heatmap:
                    st.markdown(&#34;#### Global Pupil Dilation Patterns&#34;)
                    st.caption(&#34;Shows pupil changes across the entire environment&#34;)

                    global_heatmap_data = st.session_state.analysis_results[&#39;pupil_heatmap_global&#39;]

                    # Check for errors
                    if global_heatmap_data.get(&#39;error&#39;):
                        st.warning(f&#34;⚠️ {global_heatmap_data[&#39;error&#39;]}&#34;)
                    else:
                        # Check if pre-generated global plot exists first
                        pre_generated_global_path = os.path.join(&#34;gui_outputs&#34;, &#34;gaze_plots&#34;, &#34;global_pupil_heatmap.png&#34;)

                        # Debug: Show the path being checked
                        st.write(f&#34;🔍 **Debug:** Looking for global heatmap at: `{pre_generated_global_path}`&#34;)
                        st.write(f&#34;🔍 **Debug:** File exists: {os.path.exists(pre_generated_global_path)}&#34;)

                        if os.path.exists(pre_generated_global_path):
                            # Load and display pre-generated plot
                            st.image(pre_generated_global_path, caption=&#34;Global Pupil Dilation Heatmap (Pre-generated)&#34;)
                            st.caption(&#34;📁 Plot generated during analysis&#34;)
                        else:
                            # Generate plot on-demand (fallback)
                            st.caption(&#34;🔄 Generating global heatmap on-demand...&#34;)
                            st.write(f&#34;⚠️ **Debug:** Pre-generated plot not found at `{pre_generated_global_path}`&#34;)

                            # Plot global heatmap (only once, without minimap)
                            fig_global = plot_pupil_dilation_heatmap(
                                heatmap_data=global_heatmap_data,
                                junctions=st.session_state.junctions,
                                trajectories=st.session_state.trajectories,
                                title=&#34;Global Pupil Dilation Heatmap&#34;,
                                show_sample_counts=False,
                                show_minimap=False,
                                vmin=vmin,
                                vmax=vmax
                            )
                            st.pyplot(fig_global)
                            import matplotlib.pyplot as plt
                            plt.close(fig_global)

                        # Show statistics
                        heatmap = global_heatmap_data[&#39;heatmap&#39;]
                        # Ensure heatmap is a numpy array
                        if not isinstance(heatmap, np.ndarray):
                            heatmap = np.array(heatmap)
                        valid_bins = heatmap[~np.isnan(heatmap)]
                        norm_method = global_heatmap_data[&#39;normalization_used&#39;]

                        if len(valid_bins) &gt; 0:
                            col1, col2, col3, col4 = st.columns(4)

                            with col1:
                                if norm_method == &#34;relative&#34;:
                                    st.metric(&#34;Mean Change&#34;, f&#34;{np.mean(valid_bins):.2f}%&#34;)
                                else:
                                    st.metric(&#34;Mean Z-score&#34;, f&#34;{np.mean(valid_bins):.2f}&#34;)

                            with col2:
                                if norm_method == &#34;relative&#34;:
                                    st.metric(&#34;Max Dilation&#34;, f&#34;+{np.max(valid_bins):.2f}%&#34;)
                                else:
                                    st.metric(&#34;Max Z-score&#34;, f&#34;{np.max(valid_bins):.2f}&#34;)

                            with col3:
                                if norm_method == &#34;relative&#34;:
                                    st.metric(&#34;Max Constriction&#34;, f&#34;{np.min(valid_bins):.2f}%&#34;)
                                else:
                                    st.metric(&#34;Min Z-score&#34;, f&#34;{np.min(valid_bins):.2f}&#34;)

                            with col4:
                                st.metric(&#34;Valid Bins&#34;, f&#34;{len(valid_bins)}/{heatmap.size}&#34;)
                else:
                    st.info(&#34;ℹ️ Global heatmap not available&#34;)

    def _create_comprehensive_gaze_visualizations(self, gaze_data, junction_key):
        &#34;&#34;&#34;Create visualizations for comprehensive gaze analysis results.&#34;&#34;&#34;
        import matplotlib.pyplot as plt
        import numpy as np

        # Ensure expected column names exist for plotting
        if isinstance(gaze_data, dict):
            gaze_data = self._normalize_gaze_result_frames(gaze_data)

        # Create figure with subplots
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle(f&#39;Comprehensive Gaze Analysis - {junction_key.replace(&#34;_&#34;, &#34; &#34;).title()}&#39;, fontsize=14)

        # 1. Heart Rate Distribution
        ax1 = axes[0, 0]
        if &#39;physiological&#39; in gaze_data and gaze_data[&#39;physiological&#39;] is not None:
            physio_data = gaze_data[&#39;physiological&#39;]

            # Handle both DataFrame and list formats
            if isinstance(physio_data, list):
                import pandas as pd
                physio_df = pd.DataFrame(physio_data)
            else:
                physio_df = physio_data

            if &#39;heart_rate_change&#39; in physio_df.columns:
                hr_data = physio_df[&#39;heart_rate_change&#39;].dropna()
                if len(hr_data) &gt; 0:
                    ax1.hist(hr_data, bins=15, alpha=0.7, color=&#39;red&#39;, edgecolor=&#39;black&#39;)
                    ax1.set_title(&#39;Heart Rate Change Distribution\n(Baseline: Normal navigation 2-5s before junction entry)&#39;)
                    ax1.set_xlabel(&#39;Heart Rate Change (bpm)&#39;)
                    ax1.set_ylabel(&#39;Frequency&#39;)
                    ax1.grid(True, alpha=0.3)
                else:
                    ax1.text(0.5, 0.5, &#39;No heart rate data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax1.transAxes)
                    ax1.set_title(&#39;Heart Rate Change Distribution\n(Baseline: Normal navigation 2-5s before junction entry)&#39;)
            else:
                ax1.text(0.5, 0.5, &#39;heart_rate_change column not found&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax1.transAxes)
                ax1.set_title(&#39;Heart Rate Change Distribution&#39;)
        else:
            ax1.text(0.5, 0.5, &#39;No physiological data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax1.transAxes)
            ax1.set_title(&#39;Heart Rate Change Distribution&#39;)

        # 2. Pupil Dilation Change
        ax2 = axes[0, 1]
        if &#39;pupil_dilation&#39; in gaze_data and gaze_data[&#39;pupil_dilation&#39;] is not None:
            pupil_data = gaze_data[&#39;pupil_dilation&#39;]

            # Handle both DataFrame and list formats
            if isinstance(pupil_data, list):
                import pandas as pd
                pupil_df = pd.DataFrame(pupil_data)
            else:
                pupil_df = pupil_data

            if &#39;pupil_change&#39; in pupil_df.columns:
                pupil_values = pupil_df[&#39;pupil_change&#39;].dropna()
                if len(pupil_values) &gt; 0:
                    ax2.hist(pupil_values, bins=15, alpha=0.7, color=&#39;green&#39;, edgecolor=&#39;black&#39;)
                    ax2.set_title(&#39;Pupil Dilation Change Distribution\n(Baseline: Normal navigation 2-5s before junction entry)&#39;)
                    ax2.set_xlabel(&#39;Pupil Change (mm)&#39;)
                    ax2.set_ylabel(&#39;Frequency&#39;)
                    ax2.grid(True, alpha=0.3)
                else:
                    ax2.text(0.5, 0.5, &#39;No pupil data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax2.transAxes)
                    ax2.set_title(&#39;Pupil Dilation Change Distribution\n(Baseline: Normal navigation 2-5s before junction entry)&#39;)
            else:
                ax2.text(0.5, 0.5, &#39;pupil_change column not found&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax2.transAxes)
                ax2.set_title(&#39;Pupil Dilation Change Distribution&#39;)
        else:
            ax2.text(0.5, 0.5, &#39;No pupil dilation data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax2.transAxes)
            ax2.set_title(&#39;Pupil Dilation Change Distribution&#39;)

        # 3. Head Yaw Distribution
        ax3 = axes[1, 0]
        if &#39;head_yaw&#39; in gaze_data and gaze_data[&#39;head_yaw&#39;] is not None:
            yaw_data = gaze_data[&#39;head_yaw&#39;]

            # Handle both DataFrame and list formats
            if isinstance(yaw_data, list):
                import pandas as pd
                yaw_df = pd.DataFrame(yaw_data)
            else:
                yaw_df = yaw_data

            if &#39;head_yaw&#39; in yaw_df.columns:
                yaw_values = yaw_df[&#39;head_yaw&#39;].dropna()
                if len(yaw_values) &gt; 0:
                    ax3.hist(yaw_values, bins=20, alpha=0.7, color=&#39;blue&#39;, edgecolor=&#39;black&#39;)
                    ax3.set_title(&#39;Head Yaw Distribution&#39;)
                    ax3.set_xlabel(&#39;Head Yaw (degrees)&#39;)
                    ax3.set_ylabel(&#39;Frequency&#39;)
                    ax3.grid(True, alpha=0.3)
                else:
                    ax3.text(0.5, 0.5, &#39;No head yaw data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax3.transAxes)
                    ax3.set_title(&#39;Head Yaw Distribution&#39;)
            else:
                ax3.text(0.5, 0.5, &#39;head_yaw column not found&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax3.transAxes)
                ax3.set_title(&#39;Head Yaw Distribution&#39;)
        else:
            ax3.text(0.5, 0.5, &#39;No head yaw data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax3.transAxes)
            ax3.set_title(&#39;Head Yaw Distribution&#39;)

        # 4. Gaze-Movement Difference
        ax4 = axes[1, 1]
        if &#39;head_yaw&#39; in gaze_data and gaze_data[&#39;head_yaw&#39;] is not None:
            yaw_data = gaze_data[&#39;head_yaw&#39;]

            # Handle both DataFrame and list formats
            if isinstance(yaw_data, list):
                import pandas as pd
                yaw_df = pd.DataFrame(yaw_data)
            else:
                yaw_df = yaw_data

            if &#39;yaw_difference&#39; in yaw_df.columns:
                diff_values = yaw_df[&#39;yaw_difference&#39;].dropna()
                if len(diff_values) &gt; 0:
                    ax4.hist(diff_values, bins=20, alpha=0.7, color=&#39;purple&#39;, edgecolor=&#39;black&#39;)
                    ax4.set_title(&#39;Gaze-Movement Difference Distribution&#39;)
                    ax4.set_xlabel(&#39;Difference (degrees)&#39;)
                    ax4.set_ylabel(&#39;Frequency&#39;)
                    ax4.grid(True, alpha=0.3)
                else:
                    ax4.text(0.5, 0.5, &#39;No gaze-movement data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax4.transAxes)
                    ax4.set_title(&#39;Gaze-Movement Difference Distribution&#39;)
            else:
                ax4.text(0.5, 0.5, &#39;yaw_difference column not found&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax4.transAxes)
                ax4.set_title(&#39;Gaze-Movement Difference Distribution&#39;)
        else:
            ax4.text(0.5, 0.5, &#39;No gaze-movement data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax4.transAxes)
            ax4.set_title(&#39;Gaze-Movement Difference Distribution&#39;)

        plt.tight_layout()
        st.pyplot(fig)
        plt.close()

    def _generate_gaze_plots_during_analysis(self, gaze_data, junction_key, out_dir):
        &#34;&#34;&#34;Generate gaze plots during analysis (not just in visualization tab).&#34;&#34;&#34;
        import os
        import matplotlib.pyplot as plt

        # Create gaze plots directory
        gaze_plots_dir = os.path.join(&#34;gui_outputs&#34;, f&#34;junction_{junction_key.split(&#39;_&#39;)[1]}&#34;, &#34;gaze_plots&#34;)
        os.makedirs(gaze_plots_dir, exist_ok=True)

        # Normalize result frames so downstream plotting functions find expected columns
        if isinstance(gaze_data, dict):
            gaze_data = self._normalize_gaze_result_frames(gaze_data)

        # Get trajectories from session state for plotting
        trajectories = st.session_state.get(&#39;trajectories&#39;, [])
        if not trajectories:
            return

        # Get junction information
        junction = gaze_data.get(&#39;junction&#39;)
        r_outer = gaze_data.get(&#39;r_outer&#39;)

        if junction is None:
            return

        # Generate gaze directions plot
        try:
            from verta.verta_gaze import plot_gaze_directions_at_junctions

            plot_path = os.path.join(gaze_plots_dir, f&#34;{junction_key}_gaze_directions.png&#34;)

            # Use fresh head_yaw data from gaze analysis (not old cached data)
            gaze_df = gaze_data.get(&#39;head_yaw&#39;)
            if gaze_df is not None and not gaze_df.empty:
                # Debug: Show branch information in the fresh data
                unique_branches = sorted(gaze_df[&#39;branch&#39;].unique()) if &#39;branch&#39; in gaze_df.columns else []
                print(f&#34;🔍 **Fresh head_yaw data branches**: {unique_branches}&#34;)
                print(f&#34;🔍 **Fresh head_yaw data shape**: {gaze_df.shape}&#34;)
                print(f&#34;🔍 **Fresh head_yaw data columns**: {list(gaze_df.columns)}&#34;)

                # Additional debugging for J3+ junctions
                junction_num = junction_key.split(&#39;_&#39;)[1] if &#39;_&#39; in junction_key else &#39;0&#39;
                if int(junction_num) &gt;= 3:
                    print(f&#34;🔍 **JUNCTION {junction_num} DEBUG**:&#34;)
                    print(f&#34;- Junction: {junction}&#34;)
                    print(f&#34;- R_outer: {r_outer}&#34;)
                    print(f&#34;- Trajectories passed to plotting: {len(trajectories)}&#34;)
                    print(f&#34;- Head_yaw data rows: {len(gaze_df)}&#34;)
                    print(f&#34;- Branch distribution: {gaze_df[&#39;branch&#39;].value_counts().to_dict() if &#39;branch&#39; in gaze_df.columns else &#39;No branch column&#39;}&#34;)

                # Show sample of the fresh data
                if len(gaze_df) &gt; 0:
                    print(f&#34;🔍 **Sample fresh head_yaw data**:&#34;)
                    print(gaze_df.head())

                plot_gaze_directions_at_junctions(
                    trajectories=trajectories,
                    junctions=[junction],
                    gaze_df=gaze_df,
                    out_path=plot_path,
                    r_outer_list=[r_outer] if r_outer else [None],
                    junction_labels=[f&#34;Junction {junction_key.split(&#39;_&#39;)[1]}&#34;],
                )
        except Exception as e:
            print(f&#34;Could not generate gaze directions plot: {e}&#34;)

        # Generate physiological analysis plot
        try:
            from verta.verta_gaze import plot_physiological_by_branch

            plot_path = os.path.join(gaze_plots_dir, f&#34;{junction_key}_physiological_analysis.png&#34;)
            physio_df = gaze_data.get(&#39;physiological&#39;)

            if physio_df is not None and not physio_df.empty:
                # Debug: Show branch information in the fresh physiological data
                unique_branches = sorted(physio_df[&#39;branch&#39;].unique()) if &#39;branch&#39; in physio_df.columns else []
                print(f&#34;**Fresh physiological data branches**: {unique_branches}&#34;)
                print(f&#34;**Fresh physiological data shape**: {physio_df.shape}&#34;)

                # Additional debugging for J3+ junctions
                junction_num = junction_key.split(&#39;_&#39;)[1] if &#39;_&#39; in junction_key else &#39;0&#39;
                if int(junction_num) &gt;= 3:
                    print(f&#34;🔍 **JUNCTION {junction_num} PHYSIO DEBUG**:&#34;)
                    print(f&#34;- Physiological data rows: {len(physio_df)}&#34;)
                    print(f&#34;- Branch distribution: {physio_df[&#39;branch&#39;].value_counts().to_dict() if &#39;branch&#39; in physio_df.columns else &#39;No branch column&#39;}&#34;)
                    print(f&#34;- Sample physio data:&#34;)
                    print(physio_df.head())

                plot_physiological_by_branch(
                    physio_df=physio_df,
                    out_path=plot_path,
                )
        except Exception as e:
            print(f&#34;Could not generate physiological analysis plot: {e}&#34;)

        # Generate pupil trajectory analysis plot
        try:
            from verta.verta_gaze import plot_pupil_trajectory_analysis

            plot_path = os.path.join(gaze_plots_dir, f&#34;{junction_key}_pupil_trajectory.png&#34;)
            pupil_df = gaze_data.get(&#39;pupil&#39;)

            if pupil_df is not None and not pupil_df.empty:
                plot_pupil_trajectory_analysis(
                    pupil_traj_df=pupil_df,
                    out_path=plot_path,
                )
        except Exception as e:
            print(f&#34;Could not generate pupil trajectory plot: {e}&#34;)

        # Generate junction-specific heatmap plot
        try:
            from verta.verta_gaze import plot_pupil_dilation_heatmap
            import numpy as np
            import matplotlib.pyplot as plt

            # Get heatmap data from gaze_data
            junction_heatmaps = gaze_data.get(&#39;pupil_heatmap_junction&#39;)
            st.write(f&#34;🔍 **Debug:** Junction heatmaps available: {junction_heatmaps is not None}&#34;)
            if junction_heatmaps:
                st.write(f&#34;🔍 **Debug:** Junction heatmaps length: {len(junction_heatmaps)}&#34;)
                st.write(f&#34;🔍 **Debug:** Junction heatmaps keys: {list(junction_heatmaps.keys())}&#34;)
            else:
                st.write(f&#34;🔍 **Debug:** No junction heatmaps found in gaze_data&#34;)

            if junction_heatmaps and len(junction_heatmaps) &gt; 0:
                # Get the heatmap data for this junction
                junction_idx = None
                st.write(f&#34;🔍 **Debug:** Looking for junction: ({junction.cx}, {junction.cz}, r={junction.r})&#34;)
                for idx, junc in enumerate(st.session_state.junctions):
                    st.write(f&#34;🔍 **Debug:** Checking junction {idx}: ({junc.cx}, {junc.cz}, r={junc.r})&#34;)
                    if (junc.cx == junction.cx and junc.cz == junction.cz and junc.r == junction.r):
                        junction_idx = idx
                        st.write(f&#34;🔍 **Debug:** Found matching junction at index {junction_idx}&#34;)
                        break

                st.write(f&#34;🔍 **Debug:** Junction index found: {junction_idx}&#34;)

                if junction_idx is not None and junction_idx in junction_heatmaps:
                    heatmap_data = junction_heatmaps[junction_idx]
                    st.write(f&#34;🔍 **Debug:** Found heatmap data for junction {junction_idx}&#34;)

                    # Create heatmap plot
                    plot_path = os.path.join(gaze_plots_dir, f&#34;{junction_key}_pupil_heatmap.png&#34;)
                    st.write(f&#34;🔍 **Debug:** Plot path: {plot_path}&#34;)

                    # Filter trajectories for this junction (same logic as visualizations tab)
                    filtered_trajs_for_plot = []
                    for traj in trajectories:
                        # Check if trajectory passes through this junction
                        rx = traj.x - junction.cx
                        rz = traj.z - junction.cz
                        r = np.hypot(rx, rz)
                        if np.any(r &lt;= junction.r):
                            filtered_trajs_for_plot.append(traj)

                    st.write(f&#34;🔍 **Debug:** Filtered trajectories for plot: {len(filtered_trajs_for_plot)}&#34;)

                    # Create the heatmap plot
                    st.write(f&#34;🔍 **Debug:** Creating heatmap plot...&#34;)
                    try:
                        fig = plot_pupil_dilation_heatmap(
                            heatmap_data=heatmap_data,
                            junctions=[junction],
                            trajectories=filtered_trajs_for_plot,
                            all_trajectories=trajectories,  # Pass all trajectories for minimap
                            title=f&#34;Junction {junction_idx} Pupil Dilation&#34;,
                            show_sample_counts=False,
                            show_minimap=True,
                            vmin=None,  # Let the function determine scaling
                            vmax=None
                        )

                        st.write(f&#34;🔍 **Debug:** Heatmap plot created successfully&#34;)

                        # Save the plot
                        st.write(f&#34;🔍 **Debug:** Saving plot to {plot_path}...&#34;)
                        fig.savefig(plot_path, dpi=150, bbox_inches=&#34;tight&#34;)
                        plt.close(fig)

                        print(f&#34;Junction heatmap plot saved to: {plot_path}&#34;)
                        st.write(f&#34;🔍 **Debug:** Junction plot saved to: `{plot_path}`&#34;)
                        st.write(f&#34;🔍 **Debug:** File exists after save: {os.path.exists(plot_path)}&#34;)
                    except Exception as plot_error:
                        st.write(f&#34;❌ **Debug:** Error creating heatmap plot: {plot_error}&#34;)
                        st.write(f&#34;❌ **Debug:** Error type: {type(plot_error)}&#34;)
                        import traceback
                        st.write(f&#34;❌ **Debug:** Traceback: {traceback.format_exc()}&#34;)
                        print(f&#34;Error creating junction heatmap plot: {plot_error}&#34;)
                        print(f&#34;Traceback: {traceback.format_exc()}&#34;)
                else:
                    st.write(f&#34;🔍 **Debug:** No heatmap data found for junction {junction_idx}&#34;)
                    print(f&#34;No heatmap data found for junction {junction_idx}&#34;)
            else:
                print(f&#34;No junction heatmaps available for {junction_key}&#34;)

        except Exception as e:
            print(f&#34;Could not generate junction heatmap plot: {e}&#34;)

    def _create_advanced_gaze_plots(self, gaze_data, junction_key):
        &#34;&#34;&#34;Create advanced gaze plots using CLI plotting functions.&#34;&#34;&#34;
        import matplotlib.pyplot as plt
        import tempfile
        import os

        # Normalize result frames so downstream plotting functions find expected columns
        if isinstance(gaze_data, dict):
            gaze_data = self._normalize_gaze_result_frames(gaze_data)

        st.markdown(&#34;### Advanced Gaze Analysis Plots&#34;)

        # Get trajectories from session state for plotting
        trajectories = st.session_state.get(&#39;trajectories&#39;, [])
        if not trajectories:
            st.warning(&#34;⚠️ No trajectories available for advanced plotting&#34;)
            return

        # Get junction information
        junction = gaze_data.get(&#39;junction&#39;)
        r_outer = gaze_data.get(&#39;r_outer&#39;)

        if junction is None:
            st.warning(&#34;⚠️ No junction information available for plotting&#34;)
            return

        # Check if plots were already generated during analysis
        junction_num = junction_key.split(&#39;_&#39;)[1] if &#39;_&#39; in junction_key else &#39;0&#39;
        analysis_plots_dir = os.path.join(&#34;gui_outputs&#34;, f&#34;junction_{junction_num}&#34;, &#34;gaze_plots&#34;)

        # Define plot paths
        gaze_directions_path = os.path.join(analysis_plots_dir, f&#34;{junction_key}_gaze_directions.png&#34;)
        physio_path = os.path.join(analysis_plots_dir, f&#34;{junction_key}_physiological_analysis.png&#34;)
        pupil_path = os.path.join(analysis_plots_dir, f&#34;{junction_key}_pupil_trajectory.png&#34;)

        # If plots exist from analysis, display them instead of regenerating
        if os.path.exists(analysis_plots_dir):
            st.info(f&#34;📊 **Displaying plots generated during analysis**&#34;)


            if os.path.exists(gaze_directions_path):
                st.markdown(&#34;#### Gaze Directions at Junction&#34;)
                st.image(gaze_directions_path, caption=&#34;👁️ Gaze directions at decision points&#34;)

            if os.path.exists(physio_path):
                st.markdown(&#34;#### Physiological Analysis&#34;)
                st.image(physio_path, caption=&#34;📊 Decision Point Analysis: Physiological changes during junction approach&#34;)

            if os.path.exists(pupil_path):
                st.markdown(&#34;#### Pupil Trajectory Analysis&#34;)
                st.image(pupil_path, caption=&#34;🗺️ Junction Area Analysis: Pupil changes across entire junction region&#34;)

            return

        # Fallback: Generate plots if they don&#39;t exist (for backward compatibility)
        st.info(f&#34;📊 **Generating plots on-demand**&#34;)

        # Create gaze plots directory in gui_outputs
        gaze_plots_dir = os.path.join(&#34;gui_outputs&#34;, &#34;gaze_plots&#34;)
        os.makedirs(gaze_plots_dir, exist_ok=True)

        # Only generate consistency report (plots are now generated during analysis)
        try:
            # Gaze Consistency Report
            if &#39;head_yaw&#39; in gaze_data and gaze_data[&#39;head_yaw&#39;] is not None:
                head_yaw_df = gaze_data[&#39;head_yaw&#39;]
                if len(head_yaw_df) &gt; 0:
                    st.markdown(&#34;#### Gaze-Movement Consistency Report&#34;)

                    # Import the reporting function
                    from verta.verta_gaze import gaze_movement_consistency_report

                    consistency_report = gaze_movement_consistency_report(head_yaw_df)

                    # Display the report
                    if &#39;error&#39; not in consistency_report:
                        col1, col2, col3 = st.columns(3)

                        with col1:
                            st.metric(
                                &#34;Mean Absolute Yaw Difference&#34;,
                                f&#34;{consistency_report.get(&#39;mean_absolute_yaw_difference&#39;, 0):.1f}°&#34;
                            )

                        with col2:
                            st.metric(
                                &#34;Aligned Percentage&#34;,
                                f&#34;{consistency_report.get(&#39;aligned_percentage&#39;, 0):.1f}%&#34;
                            )

                        with col3:
                            st.metric(
                                &#34;Total Decisions&#34;,
                                consistency_report.get(&#39;total_decisions&#39;, 0)
                            )

                        # Show branch-specific alignment
                        branch_metrics = {k: v for k, v in consistency_report.items()
                                       if k.startswith(&#39;branch_&#39;) and k.endswith(&#39;_alignment&#39;)}

                        if branch_metrics:
                            st.markdown(&#34;**Branch-Specific Alignment:**&#34;)
                            for branch_key, alignment in branch_metrics.items():
                                branch_num = branch_key.replace(&#39;branch_&#39;, &#39;&#39;).replace(&#39;_alignment&#39;, &#39;&#39;)
                                st.write(f&#34;- Branch {branch_num}: {alignment:.1f}° average difference&#34;)

                        # Save consistency report to file
                        import json
                        consistency_file = os.path.join(gaze_plots_dir, f&#34;{junction_key}_gaze_consistency_report.json&#34;)
                        with open(consistency_file, &#39;w&#39;) as f:
                            json.dump(consistency_report, f, indent=2)
                        st.info(f&#34;📁 Consistency report saved to: {consistency_file}&#34;)

                    else:
                        st.warning(f&#34;⚠️ {consistency_report[&#39;error&#39;]}&#34;)

        except Exception as e:
            st.error(f&#34;❌ Error creating advanced gaze plots: {e}&#34;)
            import traceback
            st.error(f&#34;**Error details:** {traceback.format_exc()}&#34;)

    def _create_gaze_visualizations(self, gaze_data, junction_key):
        &#34;&#34;&#34;Create gaze-specific visualizations.&#34;&#34;&#34;
        import matplotlib.pyplot as plt
        import numpy as np

        if len(gaze_data) == 0:
            st.info(&#34;No gaze data available for visualization&#34;)
            return

        # Create figure with subplots
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle(f&#39;Gaze Analysis Visualizations - {junction_key.replace(&#34;_&#34;, &#34; &#34;).title()}&#39;, fontsize=14)

        # 1. Head Direction Distribution
        ax1 = axes[0, 0]
        head_yaw_data = gaze_data[&#39;head_yaw&#39;].dropna()
        if len(head_yaw_data) &gt; 0:
            ax1.hist(head_yaw_data, bins=20, alpha=0.7, color=&#39;skyblue&#39;, edgecolor=&#39;black&#39;)
            ax1.set_title(&#39;Head Direction Distribution&#39;)
            ax1.set_xlabel(&#39;Head Yaw (degrees)&#39;)
            ax1.set_ylabel(&#39;Frequency&#39;)
            ax1.grid(True, alpha=0.3)
        else:
            ax1.text(0.5, 0.5, &#39;No head direction data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax1.transAxes)
            ax1.set_title(&#39;Head Direction Distribution&#39;)

        # 2. Pupil Dilation Comparison
        ax2 = axes[0, 1]
        pupil_l_data = gaze_data[&#39;pupil_l&#39;].dropna()
        pupil_r_data = gaze_data[&#39;pupil_r&#39;].dropna()
        if len(pupil_l_data) &gt; 0 and len(pupil_r_data) &gt; 0:
            ax2.scatter(pupil_l_data, pupil_r_data, alpha=0.6, color=&#39;green&#39;)
            ax2.set_title(&#39;Left vs Right Pupil Dilation&#39;)
            ax2.set_xlabel(&#39;Left Pupil Size&#39;)
            ax2.set_ylabel(&#39;Right Pupil Size&#39;)
            ax2.grid(True, alpha=0.3)
        else:
            ax2.text(0.5, 0.5, &#39;No pupil data available&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax2.transAxes)
            ax2.set_title(&#39;Left vs Right Pupil Dilation&#39;)

        # 3. Heart Rate Distribution
        ax3 = axes[1, 0]
        hr_data = gaze_data[&#39;heart_rate&#39;].dropna()
        if len(hr_data) &gt; 0:
            ax3.hist(hr_data, bins=15, alpha=0.7, color=&#39;red&#39;, edgecolor=&#39;black&#39;)
            ax3.set_title(&#39;Heart Rate Distribution&#39;)
            ax3.set_xlabel(&#39;Heart Rate (bpm)&#39;)
            ax3.set_ylabel(&#39;Frequency&#39;)
            ax3.grid(True, alpha=0.3)
        else:
            ax3.text(0.5, 0.5, &#39;No heart rate data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax3.transAxes)
            ax3.set_title(&#39;Heart Rate Distribution&#39;)

        # 4. Gaze Direction Scatter
        ax4 = axes[1, 1]
        gaze_x_data = gaze_data[&#39;gaze_x&#39;].dropna()
        gaze_y_data = gaze_data[&#39;gaze_y&#39;].dropna()
        if len(gaze_x_data) &gt; 0 and len(gaze_y_data) &gt; 0:
            ax4.scatter(gaze_x_data, gaze_y_data, alpha=0.6, color=&#39;purple&#39;)
            ax4.set_title(&#39;Gaze Direction Scatter&#39;)
            ax4.set_xlabel(&#39;Gaze X&#39;)
            ax4.set_ylabel(&#39;Gaze Y&#39;)
            ax4.grid(True, alpha=0.3)
        else:
            ax4.text(0.5, 0.5, &#39;No gaze direction data&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax4.transAxes)
            ax4.set_title(&#39;Gaze Direction Scatter&#39;)

        plt.tight_layout()
        st.pyplot(fig)
        plt.close()

    def _create_movement_visualizations(self, gaze_data, junction_key):
        &#34;&#34;&#34;Create simple movement pattern visualizations.&#34;&#34;&#34;
        import matplotlib.pyplot as plt
        import numpy as np

        if len(gaze_data) == 0:
            return

        # Create figure with subplots
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))

        # 1. Movement direction histogram
        valid_movements = gaze_data[&#39;movement_yaw&#39;].dropna()
        if len(valid_movements) &gt; 0:
            ax1.hist(valid_movements, bins=20, alpha=0.7, edgecolor=&#39;black&#39;)
            ax1.set_title(&#39;Movement Direction Distribution&#39;)
            ax1.set_xlabel(&#39;Movement Yaw (degrees)&#39;)
            ax1.set_ylabel(&#39;Frequency&#39;)
            ax1.grid(True, alpha=0.3)
        else:
            ax1.text(0.5, 0.5, &#39;No movement data available&#39;,
                    ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax1.transAxes)
            ax1.set_title(&#39;Movement Direction Distribution&#39;)

        # 2. Junction visit order distribution (new diagnostic plot)
        if &#39;visit_order&#39; in gaze_data.columns:
            visit_orders = gaze_data[&#39;visit_order&#39;].dropna()
            if len(visit_orders) &gt; 0:
                ax2.hist(visit_orders, bins=range(int(visit_orders.max()) + 2), alpha=0.7, edgecolor=&#39;black&#39;, color=&#39;orange&#39;)
                ax2.set_title(&#39;Junction Visit Order Distribution&#39;)
                ax2.set_xlabel(&#39;Visit Order (0=first junction, 1=second junction, etc.)&#39;)
                ax2.set_ylabel(&#39;Frequency&#39;)
                ax2.grid(True, alpha=0.3)
            else:
                ax2.text(0.5, 0.5, &#39;No visit order data&#39;,
                        ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax2.transAxes)
                ax2.set_title(&#39;Junction Visit Order Distribution&#39;)
        else:
            ax2.text(0.5, 0.5, &#39;No visit order data available&#39;,
                    ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax2.transAxes)
            ax2.set_title(&#39;Junction Visit Order Distribution&#39;)

        # 3. Distance from center distribution
        distances = gaze_data[&#39;distance_from_center&#39;].dropna()
        if len(distances) &gt; 0:
            ax3.hist(distances, bins=20, alpha=0.7, edgecolor=&#39;black&#39;, color=&#39;green&#39;)
            ax3.set_title(&#39;Distance from Junction Center&#39;)
            ax3.set_xlabel(&#39;Distance (units)&#39;)
            ax3.set_ylabel(&#39;Frequency&#39;)
            ax3.grid(True, alpha=0.3)
        else:
            ax3.text(0.5, 0.5, &#39;No distance data available&#39;,
                    ha=&#39;center&#39;, va=&#39;center&#39;, transform=ax3.transAxes)
            ax3.set_title(&#39;Distance from Junction Center&#39;)

        # 4. Decision points scatter plot
        ax4.scatter(gaze_data[&#39;decision_x&#39;], gaze_data[&#39;decision_z&#39;], alpha=0.6, s=20)
        ax4.set_title(&#39;Decision Points Location&#39;)
        ax4.set_xlabel(&#39;X Position&#39;)
        ax4.set_ylabel(&#39;Z Position&#39;)
        ax4.grid(True, alpha=0.3)

        plt.tight_layout()

        # Save the plot
        import os
        junction_num = junction_key.split(&#39;_&#39;)[1]
        junction_dir = os.path.join(&#34;gui_outputs&#34;, f&#34;junction_{junction_num}&#34;)
        os.makedirs(junction_dir, exist_ok=True)

        plot_path = os.path.join(junction_dir, &#34;Movement_Patterns.png&#34;)
        plt.savefig(plot_path, dpi=300, bbox_inches=&#39;tight&#39;)
        plt.close()

        # Display the plot
        st.image(plot_path, caption=f&#34;Movement Pattern Analysis - {junction_key}&#34;, width=&#39;stretch&#39;)

    def generate_cli_command(self, analysis_type: str, results: dict, cluster_method: str = &#34;dbscan&#34;, cluster_params: dict = None, decision_mode: str = &#34;hybrid&#34;, decision_params: dict = None):
        &#34;&#34;&#34;Generate CLI command for easy copying&#34;&#34;&#34;
        st.markdown(&#34;### 📋 Command Line Output&#34;)
        st.markdown(&#34;Copy and paste this command to run the same analysis in the terminal:&#34;)

        if analysis_type == &#34;discover&#34;:
            # Generate discover commands for each junction
            for junction_key, branch_data in results.items():
                # Skip non-junction keys like &#34;chain_decisions&#34;
                if junction_key == &#34;chain_decisions&#34; or not junction_key.startswith(&#34;junction_&#34;):
                    continue

                junction_num = junction_key.split(&#39;_&#39;)[1]
                junction = st.session_state.junctions[int(junction_num)]
                r_outer = st.session_state.junction_r_outer.get(int(junction_num), 50.0)

                st.markdown(f&#34;#### {junction_key.replace(&#39;_&#39;, &#39; &#39;).title()}&#34;)

                # Generate the CLI command with current cluster method and parameters
                cli_command = f&#34;&#34;&#34;route-analyzer discover \\
  --input ./data \\
  --columns x=Headset.Head.Position.X,z=Headset.Head.Position.Z,t=Time \\
  --scale 0.2 \\
  --junction {junction.cx:.1f} {junction.cz:.1f} \\
  --radius {junction.r:.1f} \\
  --r_outer {r_outer:.1f} \\
  --distance 100.0 \\
  --epsilon 0.05 \\
  --k 3 \\
  --decision_mode {decision_mode} \\
  --cluster_method {cluster_method}&#34;&#34;&#34;

                # Add cluster method specific parameters
                if cluster_method == &#34;dbscan&#34; and cluster_params:
                    cli_command += f&#34; \\\n  --min_samples {cluster_params.get(&#39;min_samples&#39;, 5)} \\\n  --angle_eps {cluster_params.get(&#39;angle_eps&#39;, 15.0)}&#34;
                elif cluster_method == &#34;kmeans&#34; and cluster_params:
                    cli_command += f&#34; \\\n  --k_min {cluster_params.get(&#39;k_min&#39;, 2)} \\\n  --k_max {cluster_params.get(&#39;k_max&#39;, 6)}&#34;
                elif cluster_method == &#34;auto&#34; and cluster_params:
                    cli_command += f&#34; \\\n  --k_min {cluster_params.get(&#39;k_min&#39;, 2)} \\\n  --k_max {cluster_params.get(&#39;k_max&#39;, 6)} \\\n  --min_sep_deg {cluster_params.get(&#39;min_sep_deg&#39;, 12.0)} \\\n  --angle_eps {cluster_params.get(&#39;angle_eps&#39;, 15.0)}&#34;

                # Add decision mode specific parameters
                if decision_mode == &#34;radial&#34; and decision_params:
                    cli_command += f&#34; \\\n  --r_outer {decision_params.get(&#39;r_outer&#39;, 50.0)} \\\n  --epsilon {decision_params.get(&#39;epsilon&#39;, 0.05)}&#34;
                elif decision_mode == &#34;pathlen&#34; and decision_params:
                    cli_command += f&#34; \\\n  --distance {decision_params.get(&#39;path_length&#39;, 100.0)} \\\n  --linger_delta {decision_params.get(&#39;linger_delta&#39;, 0.0)}&#34;
                elif decision_mode == &#34;hybrid&#34; and decision_params:
                    cli_command += f&#34; \\\n  --r_outer {decision_params.get(&#39;r_outer&#39;, 50.0)} \\\n  --distance {decision_params.get(&#39;path_length&#39;, 100.0)}&#34;

                cli_command += f&#34; \\\n  --out ./outputs/{junction_key}&#34;

                st.code(cli_command, language=&#34;bash&#34;)

                # Show branch statistics
                if &#34;summary&#34; in branch_data and branch_data[&#34;summary&#34;] is not None:
                    st.markdown(&#34;**Branch Statistics:**&#34;)
                    summary_df = branch_data[&#34;summary&#34;]
                    for _, row in summary_df.iterrows():
                        st.write(f&#34;- Branch {int(row[&#39;branch&#39;])}: {int(row[&#39;count&#39;])} trajectories ({row[&#39;percent&#39;]:.1f}%)&#34;)

        elif analysis_type == &#34;assign&#34;:
            # Generate assign commands for each junction
            for junction_key, assignment_data in results.items():
                junction_num = junction_key.split(&#39;_&#39;)[1]

                # Get junction info from assignment data or session state
                if &#34;junction&#34; in assignment_data:
                    junction = assignment_data[&#34;junction&#34;]
                    # Try to get r_outer from assignment data, then session state, then default
                    r_outer = assignment_data.get(&#34;r_outer&#34;,
                                                st.session_state.junction_r_outer.get(int(junction_num), 50.0))
                else:
                    junction = st.session_state.junctions[int(junction_num)]
                    r_outer = st.session_state.junction_r_outer.get(int(junction_num), 50.0)

                st.markdown(f&#34;#### {junction_key.replace(&#39;_&#39;, &#39; &#39;).title()}&#34;)

                # Get parameters from assignment data
                path_length = assignment_data.get(&#34;path_length&#34;, 100.0)
                epsilon = assignment_data.get(&#34;epsilon&#34;, 0.05)
                assign_scale = assignment_data.get(&#34;assign_scale&#34;, 0.2)  # Get assign-specific scale factor

                # Generate the CLI command (requires centers file from discover)
                cli_command = f&#34;&#34;&#34;route-analyzer assign \\
  --input ./data \\
  --columns x=Headset.Head.Position.X,z=Headset.Head.Position.Z,t=Time \\
  --scale {assign_scale:.1f} \\
  --junction {junction.cx:.1f} {junction.cz:.1f} \\
  --radius {junction.r:.1f} \\
  --r_outer {r_outer:.1f} \\
  --distance {path_length:.1f} \\
  --epsilon {epsilon:.3f} \\
  --decision_mode pathlen \\
  --centers ./outputs/{junction_key}/branch_centers.npy \\
  --out ./outputs/{junction_key}_assign&#34;&#34;&#34;

                st.code(cli_command, language=&#34;bash&#34;)

                # Show assignment statistics
                if &#34;assignments&#34; in assignment_data:
                    assignments_df = assignment_data[&#34;assignments&#34;]
                    st.markdown(&#34;**Assignment Statistics:**&#34;)
                    branch_counts = assignments_df[&#39;branch&#39;].value_counts().sort_index()
                    total = len(assignments_df)
                    for branch, count in branch_counts.items():
                        percentage = (count / total * 100) if total &gt; 0 else 0
                        st.write(f&#34;- Branch {int(branch)}: {int(count)} trajectories ({percentage:.1f}%)&#34;)

        elif analysis_type == &#34;predict&#34;:
            # Generate predict command for all junctions
            junctions_str = &#34; &#34;.join([f&#34;{j.cx:.1f} {j.cz:.1f} {j.r:.1f}&#34; for j in st.session_state.junctions])
            r_outer_str = &#34; &#34;.join([str(st.session_state.junction_r_outer.get(i, 50.0)) for i in range(len(st.session_state.junctions))])

            cli_command = f&#34;&#34;&#34;route-analyzer predict \\
  --input ./data \\
  --columns x=Headset.Head.Position.X,z=Headset.Head.Position.Z,t=Time \\
  --scale 0.2 \\
  --junctions {junctions_str} \\
  --r_outer_list {r_outer_str} \\
  --distance 100.0 \\
  --decision_mode {decision_mode} \\
  --cluster_method {cluster_method}&#34;&#34;&#34;

            # Add cluster method specific parameters
            if cluster_method == &#34;dbscan&#34; and cluster_params:
                cli_command += f&#34; \\\n  --min_samples {cluster_params.get(&#39;min_samples&#39;, 5)} \\\n  --angle_eps {cluster_params.get(&#39;angle_eps&#39;, 15.0)}&#34;
            elif cluster_method == &#34;kmeans&#34; and cluster_params:
                cli_command += f&#34; \\\n  --k_min {cluster_params.get(&#39;k_min&#39;, 2)} \\\n  --k_max {cluster_params.get(&#39;k_max&#39;, 6)}&#34;
            elif cluster_method == &#34;auto&#34; and cluster_params:
                cli_command += f&#34; \\\n  --k_min {cluster_params.get(&#39;k_min&#39;, 2)} \\\n  --k_max {cluster_params.get(&#39;k_max&#39;, 6)} \\\n  --min_sep_deg {cluster_params.get(&#39;min_sep_deg&#39;, 12.0)} \\\n  --angle_eps {cluster_params.get(&#39;angle_eps&#39;, 15.0)}&#34;

            # Add decision mode specific parameters
            if decision_mode == &#34;radial&#34; and decision_params:
                cli_command += f&#34; \\\n  --r_outer_list {decision_params.get(&#39;r_outer&#39;, 50.0)} \\\n  --epsilon {decision_params.get(&#39;epsilon&#39;, 0.05)}&#34;
            elif decision_mode == &#34;pathlen&#34; and decision_params:
                cli_command += f&#34; \\\n  --distance {decision_params.get(&#39;path_length&#39;, 100.0)} \\\n  --linger_delta {decision_params.get(&#39;linger_delta&#39;, 0.0)}&#34;
            elif decision_mode == &#34;hybrid&#34; and decision_params:
                cli_command += f&#34; \\\n  --r_outer_list {decision_params.get(&#39;r_outer&#39;, 50.0)} \\\n  --distance {decision_params.get(&#39;path_length&#39;, 100.0)}&#34;

            cli_command += f&#34; \\\n  --out ./outputs/prediction&#34;

            st.code(cli_command, language=&#34;bash&#34;)

        elif analysis_type == &#34;metrics&#34;:
            # Generate metrics commands for each junction
            for junction_key, metrics_data in results.items():
                if not junction_key.startswith(&#34;junction_&#34;):
                    continue

                junction_num = junction_key.split(&#39;_&#39;)[1]
                junction = metrics_data.get(&#34;junction&#34;)
                if junction is None:
                    continue

                r_outer = metrics_data.get(&#34;r_outer_value&#34;, metrics_data.get(&#34;r_outer&#34;, 50.0))
                decision_mode = metrics_data.get(&#34;decision_mode&#34;, &#34;pathlen&#34;)
                distance = metrics_data.get(&#34;distance&#34;, 100.0)
                trend_window = metrics_data.get(&#34;trend_window&#34;, 5)
                min_outward = metrics_data.get(&#34;min_outward&#34;, 0.0)

                st.markdown(f&#34;#### {junction_key.replace(&#39;_&#39;, &#39; &#39;).title()}&#34;)

                # Generate the CLI command
                cli_command = f&#34;&#34;&#34;route-analyzer metrics \\
  --input ./data \\
  --columns x=Headset.Head.Position.X,z=Headset.Head.Position.Z,t=Time \\
  --scale 0.2 \\
  --junction {junction.cx:.1f} {junction.cz:.1f} \\
  --radius {junction.r:.1f} \\
  --decision_mode {decision_mode} \\
  --distance {distance:.1f}&#34;&#34;&#34;

                if decision_mode in [&#34;radial&#34;, &#34;hybrid&#34;]:
                    cli_command += f&#34; \\\n  --r_outer {r_outer:.1f}&#34;

                if decision_mode == &#34;radial&#34;:
                    cli_command += f&#34; \\\n  --trend_window {trend_window} \\\n  --min_outward {min_outward:.1f}&#34;

                cli_command += f&#34; \\\n  --out ./outputs/{junction_key}_metrics&#34;

                st.code(cli_command, language=&#34;bash&#34;)

        elif analysis_type == &#34;gaze&#34;:
            # Generate gaze command for all junctions
            if len(st.session_state.junctions) == 1:
                # Single junction
                junction = st.session_state.junctions[0]
                r_outer = st.session_state.junction_r_outer.get(0, 50.0)
                gaze_data = list(results.values())[0] if results else {}
                decision_mode = gaze_data.get(&#34;decision_mode&#34;, &#34;hybrid&#34;)
                path_length = gaze_data.get(&#34;path_length&#34;, 100.0)
                epsilon = gaze_data.get(&#34;epsilon&#34;, 0.05)
                linger_delta = gaze_data.get(&#34;linger_delta&#34;, 5.0)

                cli_command = f&#34;&#34;&#34;route-analyzer gaze \\
  --input ./data \\
  --columns x=Headset.Head.Position.X,z=Headset.Head.Position.Z,t=Time \\
  --scale 0.2 \\
  --junction {junction.cx:.1f} {junction.cz:.1f} \\
  --radius {junction.r:.1f} \\
  --r_outer {r_outer:.1f} \\
  --distance {path_length:.1f} \\
  --epsilon {epsilon:.3f} \\
  --decision_mode {decision_mode} \\
  --linger_delta {linger_delta:.1f} \\
  --cluster_method {cluster_method}&#34;&#34;&#34;
            else:
                # Multiple junctions
                junctions_str = &#34; &#34;.join([f&#34;{j.cx:.1f} {j.cz:.1f} {j.r:.1f}&#34; for j in st.session_state.junctions])
                r_outer_list = [st.session_state.junction_r_outer.get(i, 50.0) for i in range(len(st.session_state.junctions))]
                r_outer_str = &#34; &#34;.join([str(r) for r in r_outer_list])
                gaze_data = list(results.values())[0] if results else {}
                decision_mode = gaze_data.get(&#34;decision_mode&#34;, &#34;hybrid&#34;)
                path_length = gaze_data.get(&#34;path_length&#34;, 100.0)
                epsilon = gaze_data.get(&#34;epsilon&#34;, 0.05)
                linger_delta = gaze_data.get(&#34;linger_delta&#34;, 5.0)

                cli_command = f&#34;&#34;&#34;route-analyzer gaze \\
  --input ./data \\
  --columns x=Headset.Head.Position.X,z=Headset.Head.Position.Z,t=Time \\
  --scale 0.2 \\
  --junctions {junctions_str} \\
  --r_outer_list {r_outer_str} \\
  --distance {path_length:.1f} \\
  --epsilon {epsilon:.3f} \\
  --decision_mode {decision_mode} \\
  --linger_delta {linger_delta:.1f} \\
  --cluster_method {cluster_method}&#34;&#34;&#34;

            # Add cluster method specific parameters
            if cluster_method == &#34;dbscan&#34; and cluster_params:
                cli_command += f&#34; \\\n  --min_samples {cluster_params.get(&#39;min_samples&#39;, 5)} \\\n  --angle_eps {cluster_params.get(&#39;angle_eps&#39;, 15.0)}&#34;
            elif cluster_method == &#34;kmeans&#34; and cluster_params:
                cli_command += f&#34; \\\n  --k {cluster_params.get(&#39;k&#39;, 3)}&#34;
            elif cluster_method == &#34;auto&#34; and cluster_params:
                cli_command += f&#34; \\\n  --k_min {cluster_params.get(&#39;k_min&#39;, 2)} \\\n  --k_max {cluster_params.get(&#39;k_max&#39;, 6)} \\\n  --min_sep_deg {cluster_params.get(&#39;min_sep_deg&#39;, 12.0)} \\\n  --angle_eps {cluster_params.get(&#39;angle_eps&#39;, 15.0)}&#34;

            cli_command += f&#34; \\\n  --out ./outputs/gaze_analysis&#34;

            st.code(cli_command, language=&#34;bash&#34;)

        elif analysis_type == &#34;intent&#34;:
            # Generate intent command for all junctions
            if len(st.session_state.junctions) == 1:
                # Single junction
                junction = st.session_state.junctions[0]
                intent_data = list(results.values())[0] if results else {}
                decision_mode = intent_data.get(&#34;decision_mode&#34;, &#34;hybrid&#34;)
                path_length = intent_data.get(&#34;path_length&#34;, 100.0)
                epsilon = intent_data.get(&#34;epsilon&#34;, 0.05)
                linger_delta = intent_data.get(&#34;linger_delta&#34;, 5.0)
                prediction_distances = intent_data.get(&#34;prediction_distances&#34;, [100.0, 75.0, 50.0, 25.0])
                model_type = intent_data.get(&#34;model_type&#34;, &#34;random_forest&#34;)
                cv_folds = intent_data.get(&#34;cv_folds&#34;, 5)
                test_split = intent_data.get(&#34;test_split&#34;, 0.2)

                cli_command = f&#34;&#34;&#34;route-analyzer intent \\
  --input ./data \\
  --columns x=Headset.Head.Position.X,z=Headset.Head.Position.Z,t=Time \\
  --scale 0.2 \\
  --junction {junction.cx:.1f} {junction.cz:.1f} {junction.r:.1f} \\
  --distance {path_length:.1f} \\
  --epsilon {epsilon:.3f} \\
  --decision_mode {decision_mode} \\
  --linger_delta {linger_delta:.1f} \\
  --cluster_method {cluster_method}&#34;&#34;&#34;
            else:
                # Multiple junctions
                junctions_str = &#34; &#34;.join([f&#34;{j.cx:.1f} {j.cz:.1f} {j.r:.1f}&#34; for j in st.session_state.junctions])
                intent_data = list(results.values())[0] if results else {}
                decision_mode = intent_data.get(&#34;decision_mode&#34;, &#34;hybrid&#34;)
                path_length = intent_data.get(&#34;path_length&#34;, 100.0)
                epsilon = intent_data.get(&#34;epsilon&#34;, 0.05)
                linger_delta = intent_data.get(&#34;linger_delta&#34;, 5.0)
                prediction_distances = intent_data.get(&#34;prediction_distances&#34;, [100.0, 75.0, 50.0, 25.0])
                model_type = intent_data.get(&#34;model_type&#34;, &#34;random_forest&#34;)
                cv_folds = intent_data.get(&#34;cv_folds&#34;, 5)
                test_split = intent_data.get(&#34;test_split&#34;, 0.2)

                cli_command = f&#34;&#34;&#34;route-analyzer intent \\
  --input ./data \\
  --columns x=Headset.Head.Position.X,z=Headset.Head.Position.Z,t=Time \\
  --scale 0.2 \\
  --junctions {junctions_str} \\
  --distance {path_length:.1f} \\
  --epsilon {epsilon:.3f} \\
  --decision_mode {decision_mode} \\
  --linger_delta {linger_delta:.1f} \\
  --cluster_method {cluster_method}&#34;&#34;&#34;

            # Add cluster method specific parameters
            if cluster_method == &#34;dbscan&#34; and cluster_params:
                cli_command += f&#34; \\\n  --min_samples {cluster_params.get(&#39;min_samples&#39;, 5)} \\\n  --angle_eps {cluster_params.get(&#39;angle_eps&#39;, 15.0)}&#34;
            elif cluster_method == &#34;kmeans&#34; and cluster_params:
                cli_command += f&#34; \\\n  --k {cluster_params.get(&#39;k&#39;, 3)}&#34;
            elif cluster_method == &#34;auto&#34; and cluster_params:
                cli_command += f&#34; \\\n  --k_min {cluster_params.get(&#39;k_min&#39;, 2)} \\\n  --k_max {cluster_params.get(&#39;k_max&#39;, 6)} \\\n  --min_sep_deg {cluster_params.get(&#39;min_sep_deg&#39;, 12.0)} \\\n  --angle_eps {cluster_params.get(&#39;angle_eps&#39;, 15.0)}&#34;

            # Add intent-specific parameters
            prediction_distances_str = &#34; &#34;.join([str(d) for d in prediction_distances])
            cli_command += f&#34; \\\n  --prediction_distances {prediction_distances_str} \\\n  --model_type {model_type} \\\n  --cv_folds {cv_folds} \\\n  --test_split {test_split}&#34;

            cli_command += f&#34; \\\n  --out ./outputs/intent_recognition&#34;

            st.code(cli_command, language=&#34;bash&#34;)

    def render_conditional_probabilities(self):
        &#34;&#34;&#34;Render conditional probabilities&#34;&#34;&#34;
        st.markdown(&#34;### Conditional Probabilities&#34;)

        if &#34;conditional_probabilities&#34; in st.session_state.analysis_results:
            cond_probs = st.session_state.analysis_results[&#34;conditional_probabilities&#34;]

            # Create a DataFrame for better display
            df_data = []
            for junction_key, probs in cond_probs.items():
                junction_num = junction_key.split(&#39;_&#39;)[1] if &#39;_&#39; in junction_key else junction_key[1:]
                for origin, dest_probs in probs.items():
                    for dest, prob in dest_probs.items():
                        df_data.append({
                            &#39;Junction&#39;: f&#39;J{junction_num}&#39;,
                            &#39;From&#39;: origin,
                            &#39;To&#39;: dest,
                            &#39;Probability&#39;: f&#34;{prob:.1%}&#34;
                        })

            if df_data:
                df = pd.DataFrame(df_data)
                st.dataframe(df, width=&#39;stretch&#39;)
            else:
                st.info(&#34;No conditional probabilities available&#34;)

    def render_pattern_analysis(self):
        &#34;&#34;&#34;Render pattern analysis results&#34;&#34;&#34;
        st.markdown(&#34;### Pattern Analysis&#34;)

        if &#34;choice_patterns&#34; in st.session_state.analysis_results:
            patterns = st.session_state.analysis_results[&#34;choice_patterns&#34;]

            # Display pattern statistics
            st.markdown(&#34;#### Pattern Statistics&#34;)
            for junction_key, pattern_data in patterns.items():
                junction_num = junction_key.split(&#39;_&#39;)[1] if &#39;_&#39; in junction_key else junction_key[1:]
                st.markdown(f&#34;**Junction {junction_num}:**&#34;)

                if &#34;total_trajectories&#34; in pattern_data:
                    st.write(f&#34;- Total trajectories: {pattern_data[&#39;total_trajectories&#39;]}&#34;)

                if &#34;choice_counts&#34; in pattern_data:
                    st.write(f&#34;- Choice counts: {pattern_data[&#39;choice_counts&#39;]}&#34;)

    def render_intent_visualizations(self):
        &#34;&#34;&#34;Render intent recognition analysis visualizations&#34;&#34;&#34;
        st.markdown(&#34;### 🧠 Intent Recognition Results&#34;)

        if (st.session_state.analysis_results is None or
            &#34;intent_recognition&#34; not in st.session_state.analysis_results):
            st.info(&#34;No intent recognition results available.&#34;)
            return

        intent_data = st.session_state.analysis_results[&#34;intent_recognition&#34;]

        # Get successful junctions
        successful_junctions = {k: v for k, v in intent_data.items() if &#39;error&#39; not in v}

        if not successful_junctions:
            st.warning(&#34;⚠️ No successful intent recognition results to visualize&#34;)
            return

        # Junction selector
        junction_keys = list(successful_junctions.keys())
        if len(junction_keys) &gt; 1:
            selected_junction = st.selectbox(
                &#34;Select Junction:&#34;,
                junction_keys,
                format_func=lambda x: f&#34;Junction {x.replace(&#39;junction_&#39;, &#39;&#39;)}&#34;
            )
        else:
            selected_junction = junction_keys[0]

        junction_results = successful_junctions[selected_junction]
        junction_num = selected_junction.replace(&#39;junction_&#39;, &#39;&#39;)

        # Summary metrics
        st.markdown(f&#34;#### Junction {junction_num} Summary&#34;)

        models_trained = junction_results[&#39;training_results&#39;].get(&#39;models_trained&#39;, {})

        if models_trained:
            # Create metrics row
            cols = st.columns(len(models_trained))
            for idx, (dist, model_info) in enumerate(sorted(models_trained.items())):
                with cols[idx]:
                    st.metric(
                        f&#34;{dist} units&#34;,
                        f&#34;{model_info[&#39;cv_mean_accuracy&#39;]:.1%}&#34;,
                        f&#34;n={model_info[&#39;n_samples&#39;]}&#34;
                    )

            # Overall accuracy
            avg_acc = np.mean([m[&#39;cv_mean_accuracy&#39;] for m in models_trained.values()])
            st.markdown(f&#34;**Average Accuracy:** {avg_acc:.1%}&#34;)

            # Interpretation
            if avg_acc &gt; 0.85:
                st.success(&#34;🟢 Excellent Predictability&#34;)
            elif avg_acc &gt; 0.70:
                st.info(&#34;🟡 Good Predictability&#34;)
            else:
                st.warning(&#34;🔴 Moderate Predictability&#34;)

        # Feature Importance Plot
        st.markdown(&#34;#### Feature Importance&#34;)
        feature_importance_path = os.path.join(&#34;gui_outputs&#34;, &#34;intent_recognition&#34;,
                                               f&#34;junction_{junction_num}&#34;,
                                               &#34;intent_feature_importance.png&#34;)
        if os.path.exists(feature_importance_path):
            st.image(feature_importance_path, width=&#39;stretch&#39;)
        else:
            st.info(&#34;Feature importance plot not available&#34;)

        # Accuracy Analysis Plot
        st.markdown(&#34;#### Prediction Accuracy vs Distance&#34;)
        accuracy_path = os.path.join(&#34;gui_outputs&#34;, &#34;intent_recognition&#34;,
                                     f&#34;junction_{junction_num}&#34;,
                                     &#34;intent_accuracy_analysis.png&#34;)
        if os.path.exists(accuracy_path):
            st.image(accuracy_path, width=&#39;stretch&#39;)
            st.caption(&#34;This shows how prediction accuracy improves as users get closer to the junction&#34;)
        else:
            st.info(&#34;Accuracy analysis plot not available&#34;)

        # Test Predictions
        if &#39;test_predictions&#39; in junction_results:
            st.markdown(&#34;#### Sample Predictions&#34;)

            test_preds = junction_results[&#39;test_predictions&#39;]

            # Show a few example predictions
            example_count = min(5, len(test_preds))

            for traj_id in list(test_preds.keys())[:example_count]:
                pred_info = test_preds[traj_id]
                actual = pred_info[&#39;actual_branch&#39;]

                with st.expander(f&#34;Trajectory: {traj_id} (Actual: Branch {actual})&#34;):
                    predictions = pred_info[&#39;predictions_by_distance&#39;]

                    # Create visualization
                    distances = []
                    predicted_branches = []
                    confidences = []
                    correct_flags = []

                    for dist in sorted(predictions.keys(), reverse=True):
                        p = predictions[dist]
                        distances.append(f&#34;{dist}u&#34;)
                        predicted_branches.append(f&#34;Branch {p[&#39;predicted_branch&#39;]}&#34;)
                        confidences.append(p[&#39;confidence&#39;])
                        correct_flags.append(&#34;✓&#34; if p[&#39;correct&#39;] else &#34;✗&#34;)

                    # Create DataFrame
                    pred_df = pd.DataFrame({
                        &#39;Distance Before&#39;: distances,
                        &#39;Predicted&#39;: predicted_branches,
                        &#39;Confidence&#39;: [f&#34;{c:.1%}&#34; for c in confidences],
                        &#39;Correct&#39;: correct_flags
                    })

                    st.dataframe(pred_df, width=&#39;stretch&#39;)

                    # Confidence chart
                    import plotly.graph_objects as go

                    fig = go.Figure()
                    fig.add_trace(go.Scatter(
                        x=[float(d.replace(&#39;u&#39;, &#39;&#39;)) for d in distances],
                        y=confidences,
                        mode=&#39;lines+markers&#39;,
                        name=&#39;Confidence&#39;,
                        line=dict(color=&#39;blue&#39;, width=3),
                        marker=dict(size=10)
                    ))
                    fig.update_layout(
                        title=&#34;Prediction Confidence Over Distance&#34;,
                        xaxis_title=&#34;Distance to Junction (units)&#34;,
                        yaxis_title=&#34;Confidence&#34;,
                        yaxis_range=[0, 1],
                        height=300
                    )
                    st.plotly_chart(fig, width=&#39;stretch&#39;, key=f&#34;intent_confidence_{junction_num}_{traj_id}&#34;)

        # Feature importance table
        if &#39;feature_importance&#39; in junction_results[&#39;training_results&#39;]:
            st.markdown(&#34;#### Feature Importance (Detailed)&#34;)

            with st.expander(&#34;View Feature Importance by Distance&#34;):
                feature_imp = junction_results[&#39;training_results&#39;][&#39;feature_importance&#39;]

                for dist in sorted(feature_imp.keys()):
                    st.markdown(f&#34;**{dist} units before junction:**&#34;)

                    importance_dict = feature_imp[dist]
                    sorted_features = sorted(importance_dict.items(),
                                           key=lambda x: x[1], reverse=True)

                    feat_df = pd.DataFrame(sorted_features[:10],
                                          columns=[&#39;Feature&#39;, &#39;Importance&#39;])
                    feat_df[&#39;Importance&#39;] = feat_df[&#39;Importance&#39;].apply(lambda x: f&#34;{x:.3f}&#34;)

                    st.dataframe(feat_df, width=&#39;stretch&#39;)
                    st.markdown(&#34;---&#34;)

        # Download results
        st.markdown(&#34;#### Download Results&#34;)

        results_path = os.path.join(&#34;gui_outputs&#34;, &#34;intent_recognition&#34;,
                                    f&#34;junction_{junction_num}&#34;,
                                    &#34;intent_training_results.json&#34;)

        if os.path.exists(results_path):
            with open(results_path, &#39;r&#39;) as f:
                results_json = f.read()

            st.download_button(
                label=&#34;📥 Download Training Results (JSON)&#34;,
                data=results_json,
                file_name=f&#34;intent_recognition_junction_{junction_num}.json&#34;,
                mime=&#34;application/json&#34;
            )

        # Explanation
        with st.expander(&#34;ℹ️ Understanding Intent Recognition&#34;):
            st.markdown(&#34;&#34;&#34;
            **Intent Recognition** predicts which route users will choose **before** they reach decision points.

            **Key Insights:**
            - **Higher accuracy at closer distances**: Predictions improve as users approach junctions
            - **Feature importance**: Shows which trajectory features best predict choices
            - **Early prediction**: Enables proactive systems that respond before users act

            **Applications:**
            - 🗺️ Proactive wayfinding and navigation hints
            - 🎨 Adaptive UI that highlights likely options
            - 🚦 Congestion prediction and traffic management
            - ⚠️ Anomaly detection (unexpected behavior)
            - ⚡ Performance optimization (asset preloading)

            **Accuracy Interpretation:**
            - **&gt;85%**: Excellent - Highly predictable behavior
            - **70-85%**: Good - Clear patterns exist
            - **&lt;70%**: Moderate - Variable or exploratory behavior
            &#34;&#34;&#34;)

    def render_enhanced_visualizations(self):
        &#34;&#34;&#34;Render enhanced analysis visualizations&#34;&#34;&#34;
        st.markdown(&#34;### 🚨 Enhanced Analysis Results&#34;)

        # Check if enhanced analysis results exist
        if (st.session_state.analysis_results is None or
            &#34;enhanced&#34; not in st.session_state.analysis_results):
            st.info(&#34;No enhanced analysis results available. Run enhanced analysis first.&#34;)
            return

        enhanced_data = st.session_state.analysis_results[&#34;enhanced&#34;]

        # Create tabs for different analysis components
        tab1, tab2, tab3, tab4 = st.tabs([&#34;🚨 Evacuation Analysis&#34;, &#34;💡 Recommendations&#34;, &#34;⚠️ Risk Assessment&#34;, &#34;📊 Efficiency Metrics&#34;])

        with tab1:
            self._render_evacuation_analysis(enhanced_data[&#34;evacuation_analysis&#34;])

        with tab2:
            self._render_recommendations(enhanced_data[&#34;recommendations&#34;])

        with tab3:
            self._render_risk_assessment(enhanced_data[&#34;risk_assessment&#34;])

        with tab4:
            self._render_efficiency_metrics(enhanced_data[&#34;efficiency_metrics&#34;])

    def _render_evacuation_analysis(self, evacuation_data):
        &#34;&#34;&#34;Render evacuation analysis visualizations&#34;&#34;&#34;
        st.markdown(&#34;#### Evacuation Flow Analysis&#34;)

        # Add explanation
        st.info(&#34;&#34;&#34;
        **🚨 Evacuation Analysis Explanation:**
        - **Bottlenecks**: Junctions where &gt;60% of traffic uses the same route (HIGH risk: &gt;80%)
        - **Optimal Routes**: Junctions with balanced traffic distribution (balance ratio &gt;0.7)
        - **Balance Ratio**: Measures how evenly traffic is distributed across branches (0.0=all traffic in one route, 1.0=perfectly balanced)
        - **Entropy**: Information theory measure of traffic distribution diversity
        &#34;&#34;&#34;)

        # Bottlenecks
        if evacuation_data[&#34;bottlenecks&#34;]:
            st.markdown(&#34;##### 🚧 Identified Bottlenecks&#34;)
            for bottleneck in evacuation_data[&#34;bottlenecks&#34;]:
                risk_color = &#34;🔴&#34; if bottleneck[&#34;risk_level&#34;] == &#34;HIGH&#34; else &#34;🟡&#34;
                st.markdown(f&#34;&#34;&#34;
                {risk_color} **Junction {bottleneck[&#39;junction&#39;]}, Branch {int(bottleneck[&#39;branch&#39;])}**
                - Concentration: {bottleneck[&#39;concentration&#39;]:.1%}
                - Trajectories: {bottleneck[&#39;trajectory_count&#39;]}
                - Risk Level: {bottleneck[&#39;risk_level&#39;]}
                &#34;&#34;&#34;)
        else:
            st.success(&#34;✅ No significant bottlenecks detected&#34;)

        # Optimal routes
        if evacuation_data[&#34;optimal_routes&#34;]:
            st.markdown(&#34;##### ✅ Optimal Routes&#34;)
            st.info(&#34;**Optimal Routes**: Junctions with well-balanced traffic distribution (balance ratio &gt;0.7) - these are good for evacuation as traffic spreads evenly across multiple routes.&#34;)
            for route in evacuation_data[&#34;optimal_routes&#34;]:
                st.markdown(f&#34;&#34;&#34;
                **Junction {route[&#39;junction&#39;]}**
                - Balance Ratio: {route[&#39;balance_ratio&#39;]:.2f} (higher = more balanced)
                - Entropy: {route[&#39;entropy&#39;]:.2f} (higher = more diverse routes)
                - Branch Count: {route[&#39;branch_count&#39;]} (number of available routes)
                &#34;&#34;&#34;)
        else:
            st.info(&#34;ℹ️ No optimal routes identified (all junctions have concentrated traffic)&#34;)

        # Flow analysis chart
        if evacuation_data[&#34;flow_analysis&#34;]:
            st.markdown(&#34;##### 📊 Flow Distribution&#34;)
            import pandas as pd
            import plotly.express as px

            flow_data = []
            for junction_key, data in evacuation_data[&#34;flow_analysis&#34;].items():
                junction_num = junction_key.split(&#39;_&#39;)[1]
                for branch, count in data[&#34;branch_distribution&#34;].items():
                    flow_data.append({
                        &#34;Junction&#34;: f&#34;J{junction_num}&#34;,
                        &#34;Branch&#34;: f&#34;Branch {int(branch)}&#34;,
                        &#34;Trajectory Count&#34;: count,
                        &#34;Percentage&#34;: count / data[&#34;total_trajectories&#34;] * 100
                    })

            if flow_data:
                df = pd.DataFrame(flow_data)
                fig = px.bar(df, x=&#34;Junction&#34;, y=&#34;Trajectory Count&#34;, color=&#34;Branch&#34;,
                           title=&#34;Trajectory Distribution by Junction and Branch&#34;,
                           hover_data=[&#34;Percentage&#34;])
                st.plotly_chart(fig, width=&#39;stretch&#39;)

    def _render_recommendations(self, recommendations):
        &#34;&#34;&#34;Render recommendations&#34;&#34;&#34;
        st.markdown(&#34;#### 💡 Actionable Recommendations&#34;)

        # Add explanation
        st.info(&#34;&#34;&#34;
        **💡 Recommendations Explanation:**
        - **HIGH Priority**: Critical issues requiring immediate attention (bottlenecks &gt;80% concentration)
        - **MEDIUM Priority**: System-wide issues or moderate bottlenecks (60-80% concentration)
        - **LOW Priority**: Maintenance recommendations for well-performing junctions
        - **Signage**: Directional signs to distribute traffic away from bottlenecks
        - **Route Modification**: Physical changes like widening or adding alternative routes
        &#34;&#34;&#34;)

        if not recommendations:
            st.info(&#34;No specific recommendations generated&#34;)
            return

        # Group by priority
        high_priority = [r for r in recommendations if r[&#34;priority&#34;] == &#34;HIGH&#34;]
        medium_priority = [r for r in recommendations if r[&#34;priority&#34;] == &#34;MEDIUM&#34;]
        low_priority = [r for r in recommendations if r[&#34;priority&#34;] == &#34;LOW&#34;]

        if high_priority:
            st.markdown(&#34;##### 🔴 High Priority&#34;)
            for rec in high_priority:
                st.markdown(f&#34;&#34;&#34;
                **{rec[&#39;type&#39;]}** - Junction {rec[&#39;junction&#39;]}
                {rec[&#39;message&#39;]}
                &#34;&#34;&#34;)

        if medium_priority:
            st.markdown(&#34;##### 🟡 Medium Priority&#34;)
            for rec in medium_priority:
                st.markdown(f&#34;&#34;&#34;
                **{rec[&#39;type&#39;]}** - Junction {rec[&#39;junction&#39;]}
                {rec[&#39;message&#39;]}
                &#34;&#34;&#34;)

        if low_priority:
            st.markdown(&#34;##### 🟢 Low Priority&#34;)
            for rec in low_priority:
                st.markdown(f&#34;&#34;&#34;
                **{rec[&#39;type&#39;]}** - Junction {rec[&#39;junction&#39;]}
                {rec[&#39;message&#39;]}
                &#34;&#34;&#34;)

    def _render_risk_assessment(self, risk_data):
        &#34;&#34;&#34;Render risk assessment visualizations&#34;&#34;&#34;
        st.markdown(&#34;#### ⚠️ Risk Assessment&#34;)

        # Add explanation
        st.info(&#34;&#34;&#34;
        **⚠️ Unified Risk Assessment Explanation:**
        - **Overall Risk Score**: 0.0-1.0 scale (0.0=Low Risk, 1.0=High Risk) - normalized across all junctions
        - **Risk Factors**: Each junction assessed on 3 dimensions:
          • **Concentration Risk**: Traffic concentration in single route (&gt;70% = risk)
          • **Diversity Risk**: Number of available routes (&lt;2 routes = high risk, 2 routes = moderate risk)
          • **Crowding Risk**: Traffic volume (&gt;50 trajectories = moderate, &gt;100 = high)
        - **Risk Levels**: HIGH (≥0.7), MEDIUM (≥0.4), LOW (&lt;0.4)
        - **Unified Score**: All risk factors combined and normalized to 0-1 scale
        &#34;&#34;&#34;)

        # Overall risk score
        overall_score = risk_data[&#34;overall_risk_score&#34;]
        risk_level = &#34;HIGH&#34; if overall_score &gt; 0.7 else &#34;MEDIUM&#34; if overall_score &gt; 0.3 else &#34;LOW&#34;
        risk_color = &#34;🔴&#34; if risk_level == &#34;HIGH&#34; else &#34;🟡&#34; if risk_level == &#34;MEDIUM&#34; else &#34;🟢&#34;

        st.markdown(f&#34;&#34;&#34;
        ##### Overall Risk Score: {risk_color} {risk_level}
        **Score: {overall_score:.2f}** (0.0 = Low Risk, 1.0 = High Risk)
        &#34;&#34;&#34;)

        # High risk junctions (now includes all risk levels)
        if risk_data[&#34;high_risk_junctions&#34;]:
            st.markdown(&#34;##### 🚨 Risk Assessment by Junction&#34;)

            # Group by risk level
            high_risk = [j for j in risk_data[&#34;high_risk_junctions&#34;] if j[&#34;risk_level&#34;] == &#34;HIGH&#34;]
            medium_risk = [j for j in risk_data[&#34;high_risk_junctions&#34;] if j[&#34;risk_level&#34;] == &#34;MEDIUM&#34;]

            if high_risk:
                st.markdown(&#34;###### 🔴 HIGH Risk Junctions&#34;)
                for junction in high_risk:
                    st.markdown(f&#34;&#34;&#34;
                    **Junction {junction[&#39;junction&#39;]}**
                    - Risk Score: {junction[&#39;risk_score&#39;]:.2f}
                    - Trajectory Count: {junction[&#39;trajectory_count&#39;]}
                    - Concentration: {junction[&#39;concentration&#39;]:.1%}
                    - Route Count: {junction[&#39;route_count&#39;]}
                    - Risk Factors: {&#39;, &#39;.join([f[0] for f in junction[&#39;risk_factors&#39;]])}
                    &#34;&#34;&#34;)

            if medium_risk:
                st.markdown(&#34;###### 🟡 MEDIUM Risk Junctions&#34;)
                for junction in medium_risk:
                    st.markdown(f&#34;&#34;&#34;
                    **Junction {junction[&#39;junction&#39;]}**
                    - Risk Score: {junction[&#39;risk_score&#39;]:.2f}
                    - Trajectory Count: {junction[&#39;trajectory_count&#39;]}
                    - Concentration: {junction[&#39;concentration&#39;]:.1%}
                    - Route Count: {junction[&#39;route_count&#39;]}
                    - Risk Factors: {&#39;, &#39;.join([f[0] for f in junction[&#39;risk_factors&#39;]])}
                    &#34;&#34;&#34;)
        else:
            st.success(&#34;✅ No significant risks identified&#34;)

        # Risk visualization
        if risk_data[&#34;high_risk_junctions&#34;]:
            import plotly.express as px
            import pandas as pd

            risk_chart_data = []
            for junction in risk_data[&#34;high_risk_junctions&#34;]:
                risk_chart_data.append({
                    &#34;Junction&#34;: f&#34;J{junction[&#39;junction&#39;]}&#34;,
                    &#34;Risk Score&#34;: junction[&#39;risk_score&#39;],
                    &#34;Risk Level&#34;: junction[&#39;risk_level&#39;],
                    &#34;Trajectory Count&#34;: junction[&#39;trajectory_count&#39;],
                    &#34;Concentration&#34;: junction[&#39;concentration&#39;],
                    &#34;Route Count&#34;: junction[&#39;route_count&#39;]
                })

            if risk_chart_data:
                df = pd.DataFrame(risk_chart_data)
                fig = px.bar(df, x=&#34;Junction&#34;, y=&#34;Risk Score&#34;, color=&#34;Risk Level&#34;,
                           title=&#34;Unified Risk Assessment by Junction&#34;,
                           color_discrete_map={&#34;HIGH&#34;: &#34;red&#34;, &#34;MEDIUM&#34;: &#34;orange&#34;, &#34;LOW&#34;: &#34;green&#34;},
                           hover_data=[&#34;Trajectory Count&#34;, &#34;Concentration&#34;, &#34;Route Count&#34;])
                st.plotly_chart(fig, width=&#39;stretch&#39;)

    def _render_efficiency_metrics(self, efficiency_data):
        &#34;&#34;&#34;Render efficiency metrics visualizations&#34;&#34;&#34;
        st.markdown(&#34;#### 📊 Efficiency Metrics&#34;)

        # Add explanation
        st.info(&#34;&#34;&#34;
        **📊 Efficiency Metrics Explanation:**
        - **Route Efficiency**: Entropy-based measure of traffic distribution quality (0.0=all traffic in one route, 1.0=perfectly distributed)
        - **Capacity Utilization**: How well junctions handle their traffic load (trajectories/100, capped at 100%)
        - **Overall Efficiency**: Average route efficiency across all junctions
        - **Higher values = better evacuation performance**
        &#34;&#34;&#34;)

        # Overall efficiency
        overall_efficiency = efficiency_data[&#34;overall_efficiency&#34;]
        efficiency_level = &#34;HIGH&#34; if overall_efficiency &gt; 0.7 else &#34;MEDIUM&#34; if overall_efficiency &gt; 0.4 else &#34;LOW&#34;
        efficiency_color = &#34;🟢&#34; if efficiency_level == &#34;HIGH&#34; else &#34;🟡&#34; if efficiency_level == &#34;MEDIUM&#34; else &#34;🔴&#34;

        st.markdown(f&#34;&#34;&#34;
        ##### Overall Efficiency: {efficiency_color} {efficiency_level}
        **Score: {overall_efficiency:.2f}** (0.0 = Low Efficiency, 1.0 = High Efficiency)
        &#34;&#34;&#34;)

        # Route efficiency by junction
        if efficiency_data[&#34;route_efficiency&#34;]:
            st.markdown(&#34;##### 🛣️ Route Efficiency by Junction&#34;)
            import plotly.express as px
            import pandas as pd

            efficiency_chart_data = []
            for junction_key, efficiency in efficiency_data[&#34;route_efficiency&#34;].items():
                junction_num = junction_key.split(&#39;_&#39;)[1]
                efficiency_chart_data.append({
                    &#34;Junction&#34;: f&#34;J{junction_num}&#34;,
                    &#34;Route Efficiency&#34;: efficiency,
                    &#34;Capacity Utilization&#34;: efficiency_data[&#34;capacity_utilization&#34;].get(junction_key, 0)
                })

            if efficiency_chart_data:
                df = pd.DataFrame(efficiency_chart_data)

                # Route efficiency chart
                fig1 = px.bar(df, x=&#34;Junction&#34;, y=&#34;Route Efficiency&#34;,
                            title=&#34;Route Efficiency by Junction&#34;,
                            color=&#34;Route Efficiency&#34;,
                            color_continuous_scale=&#34;RdYlGn&#34;)
                st.plotly_chart(fig1, width=&#39;stretch&#39;)

                # Capacity utilization chart
                fig2 = px.bar(df, x=&#34;Junction&#34;, y=&#34;Capacity Utilization&#34;,
                            title=&#34;Capacity Utilization by Junction&#34;,
                            color=&#34;Capacity Utilization&#34;,
                            color_continuous_scale=&#34;RdYlGn&#34;)
                st.plotly_chart(fig2, width=&#39;stretch&#39;)

        # Efficiency summary
        st.markdown(&#34;##### 📈 Efficiency Summary&#34;)
        col1, col2 = st.columns(2)

        with col1:
            st.metric(&#34;Overall Route Efficiency&#34;, f&#34;{overall_efficiency:.2f}&#34;)

        with col2:
            avg_capacity = sum(efficiency_data[&#34;capacity_utilization&#34;].values()) / len(efficiency_data[&#34;capacity_utilization&#34;]) if efficiency_data[&#34;capacity_utilization&#34;] else 0
            st.metric(&#34;Average Capacity Utilization&#34;, f&#34;{avg_capacity:.2f}&#34;)

    def render_export(self):
        &#34;&#34;&#34;Render the export interface&#34;&#34;&#34;
        st.markdown(&#39;&lt;h2 class=&#34;section-header&#34;&gt;💾 Export Results&lt;/h2&gt;&#39;, unsafe_allow_html=True)

        if not st.session_state.analysis_results:
            st.warning(&#34;⚠️ Please run an analysis first&#34;)
            return

        st.markdown(&#34;### Export Options&#34;)

        # Export format selection
        export_format = st.selectbox(
            &#34;Export Format:&#34;,
            [&#34;JSON&#34;, &#34;CSV&#34;, &#34;ZIP Archive&#34;],
            help=&#34;Select the format for exporting results&#34;
        )

        if st.button(&#34;📥 Export Results&#34;):
            self.export_results(export_format)

    def export_results(self, format: str):
        &#34;&#34;&#34;Export analysis results&#34;&#34;&#34;
        try:
            if format == &#34;JSON&#34;:
                # Export as JSON
                json_str = json.dumps(st.session_state.analysis_results, indent=2, default=str)
                st.download_button(
                    label=&#34;Download JSON&#34;,
                    data=json_str,
                    file_name=&#34;analysis_results.json&#34;,
                    mime=&#34;application/json&#34;
                )

            elif format == &#34;CSV&#34;:
                # Export as CSV (if applicable)
                if &#34;metrics&#34; in st.session_state.analysis_results:
                    # Export metrics as CSV
                    import pandas as pd
                    df = pd.DataFrame(st.session_state.analysis_results[&#34;metrics&#34;])
                    csv_data = df.to_csv(index=False)
                    st.download_button(
                        label=&#34;Download Metrics CSV&#34;,
                        data=csv_data,
                        file_name=&#34;metrics_results.csv&#34;,
                        mime=&#34;text/csv&#34;
                    )
                else:
                    st.info(&#34;CSV export available for metrics data&#34;)

            elif format == &#34;ZIP Archive&#34;:
                # Create comprehensive ZIP archive with all files from gui_outputs
                with tempfile.TemporaryDirectory() as temp_dir:
                    # Save analysis results JSON
                    results_file = os.path.join(temp_dir, &#34;analysis_results.json&#34;)
                    with open(results_file, &#39;w&#39;) as f:
                        json.dump(st.session_state.analysis_results, f, indent=2, default=str)

                    # Create ZIP
                    zip_path = os.path.join(temp_dir, &#34;analysis_results.zip&#34;)
                    with zipfile.ZipFile(zip_path, &#39;w&#39;) as zipf:
                        # Add analysis results JSON
                        zipf.write(results_file, &#34;analysis_results.json&#34;)

                        # Add all files from gui_outputs directory
                        gui_outputs_dir = &#34;gui_outputs&#34;
                        if os.path.exists(gui_outputs_dir):
                            for root, dirs, files in os.walk(gui_outputs_dir):
                                for file in files:
                                    file_path = os.path.join(root, file)
                                    # Create relative path within ZIP
                                    rel_path = os.path.relpath(file_path, gui_outputs_dir)
                                    zipf.write(file_path, rel_path)

                    # Download ZIP
                    with open(zip_path, &#39;rb&#39;) as f:
                        zip_data = f.read()

                    st.download_button(
                        label=&#34;Download Complete Analysis Package&#34;,
                        data=zip_data,
                        file_name=&#34;complete_analysis_results.zip&#34;,
                        mime=&#34;application/zip&#34;
                    )

                    # Show what&#39;s included in the ZIP
                    st.info(&#34;📦 **Complete Analysis Package includes:**&#34;)
                    st.write(&#34;• Analysis results (JSON)&#34;)
                    st.write(&#34;• All visualizations (PNG files)&#34;)
                    st.write(&#34;• All data tables (CSV files)&#34;)
                    st.write(&#34;• Gaze analysis results&#34;)
                    st.write(&#34;• Physiological analysis data&#34;)
                    st.write(&#34;• Pupil trajectory data&#34;)
                    st.write(&#34;• Consistency reports&#34;)
                    st.write(&#34;• Branch assignments&#34;)
                    st.write(&#34;• Decision points&#34;)
                    st.write(&#34;• Metrics results&#34;)
                    st.write(&#34;• Enhanced analysis results&#34;)
                    st.write(&#34;• Risk assessment data&#34;)
                    st.write(&#34;• Efficiency metrics&#34;)
                    st.write(&#34;• Evacuation analysis&#34;)
                    st.write(&#34;• Recommendations&#34;)
                    st.write(&#34;• Intent recognition models&#34;)
                    st.write(&#34;• Feature importance analysis&#34;)
                    st.write(&#34;• ML prediction results&#34;)

            st.success(&#34;✅ Export ready!&#34;)

        except Exception as e:
            st.error(f&#34;❌ Export failed: {str(e)}&#34;)

    def run_quick_analysis(self):
        &#34;&#34;&#34;Run a quick analysis with default parameters&#34;&#34;&#34;
        if not st.session_state.trajectories or not st.session_state.junctions:
            st.warning(&#34;⚠️ Please load data and define junctions first&#34;)
            return

        st.session_state.current_step = &#34;analysis&#34;
        st.rerun()

    def clear_all_data(self):
        &#34;&#34;&#34;Clear all data and reset the application&#34;&#34;&#34;
        st.session_state.trajectories = []
        st.session_state.junctions = []
        st.session_state.junction_r_outer = {}
        st.session_state.analysis_results = None
        st.session_state.current_step = &#34;data_upload&#34;
        st.success(&#34;✅ All data cleared!&#34;)
        st.rerun()

    def run(self):
        &#34;&#34;&#34;Main GUI run method&#34;&#34;&#34;
        # Add custom CSS for image aspect ratio preservation
        st.markdown(&#34;&#34;&#34;
        &lt;style&gt;
        .stImage &gt; img {
            object-fit: contain !important;
            max-width: 100% !important;
            height: auto !important;
        }
        .stImage &gt; div {
            display: flex !important;
            justify-content: center !important;
        }
        &lt;/style&gt;
        &#34;&#34;&#34;, unsafe_allow_html=True)

        self.render_header()
        self.render_navigation()

        # Render current step
        if st.session_state.current_step == &#34;data_upload&#34;:
            self.render_data_upload()
        elif st.session_state.current_step == &#34;junction_editor&#34;:
            self.render_junction_editor()
        elif st.session_state.current_step == &#34;analysis&#34;:
            self.render_analysis()
        elif st.session_state.current_step == &#34;visualization&#34;:
            self.render_visualization()
        elif st.session_state.current_step == &#34;export&#34;:
            self.render_export()</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="verta.verta_gui.RouteAnalyzerGUI.clear_all_data"><code class="name flex">
<span>def <span class="ident">clear_all_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Clear all data and reset the application</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clear_all_data(self):
    &#34;&#34;&#34;Clear all data and reset the application&#34;&#34;&#34;
    st.session_state.trajectories = []
    st.session_state.junctions = []
    st.session_state.junction_r_outer = {}
    st.session_state.analysis_results = None
    st.session_state.current_step = &#34;data_upload&#34;
    st.success(&#34;✅ All data cleared!&#34;)
    st.rerun()</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.export_results"><code class="name flex">
<span>def <span class="ident">export_results</span></span>(<span>self, format: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Export analysis results</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export_results(self, format: str):
    &#34;&#34;&#34;Export analysis results&#34;&#34;&#34;
    try:
        if format == &#34;JSON&#34;:
            # Export as JSON
            json_str = json.dumps(st.session_state.analysis_results, indent=2, default=str)
            st.download_button(
                label=&#34;Download JSON&#34;,
                data=json_str,
                file_name=&#34;analysis_results.json&#34;,
                mime=&#34;application/json&#34;
            )

        elif format == &#34;CSV&#34;:
            # Export as CSV (if applicable)
            if &#34;metrics&#34; in st.session_state.analysis_results:
                # Export metrics as CSV
                import pandas as pd
                df = pd.DataFrame(st.session_state.analysis_results[&#34;metrics&#34;])
                csv_data = df.to_csv(index=False)
                st.download_button(
                    label=&#34;Download Metrics CSV&#34;,
                    data=csv_data,
                    file_name=&#34;metrics_results.csv&#34;,
                    mime=&#34;text/csv&#34;
                )
            else:
                st.info(&#34;CSV export available for metrics data&#34;)

        elif format == &#34;ZIP Archive&#34;:
            # Create comprehensive ZIP archive with all files from gui_outputs
            with tempfile.TemporaryDirectory() as temp_dir:
                # Save analysis results JSON
                results_file = os.path.join(temp_dir, &#34;analysis_results.json&#34;)
                with open(results_file, &#39;w&#39;) as f:
                    json.dump(st.session_state.analysis_results, f, indent=2, default=str)

                # Create ZIP
                zip_path = os.path.join(temp_dir, &#34;analysis_results.zip&#34;)
                with zipfile.ZipFile(zip_path, &#39;w&#39;) as zipf:
                    # Add analysis results JSON
                    zipf.write(results_file, &#34;analysis_results.json&#34;)

                    # Add all files from gui_outputs directory
                    gui_outputs_dir = &#34;gui_outputs&#34;
                    if os.path.exists(gui_outputs_dir):
                        for root, dirs, files in os.walk(gui_outputs_dir):
                            for file in files:
                                file_path = os.path.join(root, file)
                                # Create relative path within ZIP
                                rel_path = os.path.relpath(file_path, gui_outputs_dir)
                                zipf.write(file_path, rel_path)

                # Download ZIP
                with open(zip_path, &#39;rb&#39;) as f:
                    zip_data = f.read()

                st.download_button(
                    label=&#34;Download Complete Analysis Package&#34;,
                    data=zip_data,
                    file_name=&#34;complete_analysis_results.zip&#34;,
                    mime=&#34;application/zip&#34;
                )

                # Show what&#39;s included in the ZIP
                st.info(&#34;📦 **Complete Analysis Package includes:**&#34;)
                st.write(&#34;• Analysis results (JSON)&#34;)
                st.write(&#34;• All visualizations (PNG files)&#34;)
                st.write(&#34;• All data tables (CSV files)&#34;)
                st.write(&#34;• Gaze analysis results&#34;)
                st.write(&#34;• Physiological analysis data&#34;)
                st.write(&#34;• Pupil trajectory data&#34;)
                st.write(&#34;• Consistency reports&#34;)
                st.write(&#34;• Branch assignments&#34;)
                st.write(&#34;• Decision points&#34;)
                st.write(&#34;• Metrics results&#34;)
                st.write(&#34;• Enhanced analysis results&#34;)
                st.write(&#34;• Risk assessment data&#34;)
                st.write(&#34;• Efficiency metrics&#34;)
                st.write(&#34;• Evacuation analysis&#34;)
                st.write(&#34;• Recommendations&#34;)
                st.write(&#34;• Intent recognition models&#34;)
                st.write(&#34;• Feature importance analysis&#34;)
                st.write(&#34;• ML prediction results&#34;)

        st.success(&#34;✅ Export ready!&#34;)

    except Exception as e:
        st.error(f&#34;❌ Export failed: {str(e)}&#34;)</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.generate_cli_command"><code class="name flex">
<span>def <span class="ident">generate_cli_command</span></span>(<span>self, analysis_type: str, results: dict, cluster_method: str = 'dbscan', cluster_params: dict = None, decision_mode: str = 'hybrid', decision_params: dict = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate CLI command for easy copying</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">  def generate_cli_command(self, analysis_type: str, results: dict, cluster_method: str = &#34;dbscan&#34;, cluster_params: dict = None, decision_mode: str = &#34;hybrid&#34;, decision_params: dict = None):
      &#34;&#34;&#34;Generate CLI command for easy copying&#34;&#34;&#34;
      st.markdown(&#34;### 📋 Command Line Output&#34;)
      st.markdown(&#34;Copy and paste this command to run the same analysis in the terminal:&#34;)

      if analysis_type == &#34;discover&#34;:
          # Generate discover commands for each junction
          for junction_key, branch_data in results.items():
              # Skip non-junction keys like &#34;chain_decisions&#34;
              if junction_key == &#34;chain_decisions&#34; or not junction_key.startswith(&#34;junction_&#34;):
                  continue

              junction_num = junction_key.split(&#39;_&#39;)[1]
              junction = st.session_state.junctions[int(junction_num)]
              r_outer = st.session_state.junction_r_outer.get(int(junction_num), 50.0)

              st.markdown(f&#34;#### {junction_key.replace(&#39;_&#39;, &#39; &#39;).title()}&#34;)

              # Generate the CLI command with current cluster method and parameters
              cli_command = f&#34;&#34;&#34;route-analyzer discover \\
--input ./data \\
--columns x=Headset.Head.Position.X,z=Headset.Head.Position.Z,t=Time \\
--scale 0.2 \\
--junction {junction.cx:.1f} {junction.cz:.1f} \\
--radius {junction.r:.1f} \\
--r_outer {r_outer:.1f} \\
--distance 100.0 \\
--epsilon 0.05 \\
--k 3 \\
--decision_mode {decision_mode} \\
--cluster_method {cluster_method}&#34;&#34;&#34;

              # Add cluster method specific parameters
              if cluster_method == &#34;dbscan&#34; and cluster_params:
                  cli_command += f&#34; \\\n  --min_samples {cluster_params.get(&#39;min_samples&#39;, 5)} \\\n  --angle_eps {cluster_params.get(&#39;angle_eps&#39;, 15.0)}&#34;
              elif cluster_method == &#34;kmeans&#34; and cluster_params:
                  cli_command += f&#34; \\\n  --k_min {cluster_params.get(&#39;k_min&#39;, 2)} \\\n  --k_max {cluster_params.get(&#39;k_max&#39;, 6)}&#34;
              elif cluster_method == &#34;auto&#34; and cluster_params:
                  cli_command += f&#34; \\\n  --k_min {cluster_params.get(&#39;k_min&#39;, 2)} \\\n  --k_max {cluster_params.get(&#39;k_max&#39;, 6)} \\\n  --min_sep_deg {cluster_params.get(&#39;min_sep_deg&#39;, 12.0)} \\\n  --angle_eps {cluster_params.get(&#39;angle_eps&#39;, 15.0)}&#34;

              # Add decision mode specific parameters
              if decision_mode == &#34;radial&#34; and decision_params:
                  cli_command += f&#34; \\\n  --r_outer {decision_params.get(&#39;r_outer&#39;, 50.0)} \\\n  --epsilon {decision_params.get(&#39;epsilon&#39;, 0.05)}&#34;
              elif decision_mode == &#34;pathlen&#34; and decision_params:
                  cli_command += f&#34; \\\n  --distance {decision_params.get(&#39;path_length&#39;, 100.0)} \\\n  --linger_delta {decision_params.get(&#39;linger_delta&#39;, 0.0)}&#34;
              elif decision_mode == &#34;hybrid&#34; and decision_params:
                  cli_command += f&#34; \\\n  --r_outer {decision_params.get(&#39;r_outer&#39;, 50.0)} \\\n  --distance {decision_params.get(&#39;path_length&#39;, 100.0)}&#34;

              cli_command += f&#34; \\\n  --out ./outputs/{junction_key}&#34;

              st.code(cli_command, language=&#34;bash&#34;)

              # Show branch statistics
              if &#34;summary&#34; in branch_data and branch_data[&#34;summary&#34;] is not None:
                  st.markdown(&#34;**Branch Statistics:**&#34;)
                  summary_df = branch_data[&#34;summary&#34;]
                  for _, row in summary_df.iterrows():
                      st.write(f&#34;- Branch {int(row[&#39;branch&#39;])}: {int(row[&#39;count&#39;])} trajectories ({row[&#39;percent&#39;]:.1f}%)&#34;)

      elif analysis_type == &#34;assign&#34;:
          # Generate assign commands for each junction
          for junction_key, assignment_data in results.items():
              junction_num = junction_key.split(&#39;_&#39;)[1]

              # Get junction info from assignment data or session state
              if &#34;junction&#34; in assignment_data:
                  junction = assignment_data[&#34;junction&#34;]
                  # Try to get r_outer from assignment data, then session state, then default
                  r_outer = assignment_data.get(&#34;r_outer&#34;,
                                              st.session_state.junction_r_outer.get(int(junction_num), 50.0))
              else:
                  junction = st.session_state.junctions[int(junction_num)]
                  r_outer = st.session_state.junction_r_outer.get(int(junction_num), 50.0)

              st.markdown(f&#34;#### {junction_key.replace(&#39;_&#39;, &#39; &#39;).title()}&#34;)

              # Get parameters from assignment data
              path_length = assignment_data.get(&#34;path_length&#34;, 100.0)
              epsilon = assignment_data.get(&#34;epsilon&#34;, 0.05)
              assign_scale = assignment_data.get(&#34;assign_scale&#34;, 0.2)  # Get assign-specific scale factor

              # Generate the CLI command (requires centers file from discover)
              cli_command = f&#34;&#34;&#34;route-analyzer assign \\
--input ./data \\
--columns x=Headset.Head.Position.X,z=Headset.Head.Position.Z,t=Time \\
--scale {assign_scale:.1f} \\
--junction {junction.cx:.1f} {junction.cz:.1f} \\
--radius {junction.r:.1f} \\
--r_outer {r_outer:.1f} \\
--distance {path_length:.1f} \\
--epsilon {epsilon:.3f} \\
--decision_mode pathlen \\
--centers ./outputs/{junction_key}/branch_centers.npy \\
--out ./outputs/{junction_key}_assign&#34;&#34;&#34;

              st.code(cli_command, language=&#34;bash&#34;)

              # Show assignment statistics
              if &#34;assignments&#34; in assignment_data:
                  assignments_df = assignment_data[&#34;assignments&#34;]
                  st.markdown(&#34;**Assignment Statistics:**&#34;)
                  branch_counts = assignments_df[&#39;branch&#39;].value_counts().sort_index()
                  total = len(assignments_df)
                  for branch, count in branch_counts.items():
                      percentage = (count / total * 100) if total &gt; 0 else 0
                      st.write(f&#34;- Branch {int(branch)}: {int(count)} trajectories ({percentage:.1f}%)&#34;)

      elif analysis_type == &#34;predict&#34;:
          # Generate predict command for all junctions
          junctions_str = &#34; &#34;.join([f&#34;{j.cx:.1f} {j.cz:.1f} {j.r:.1f}&#34; for j in st.session_state.junctions])
          r_outer_str = &#34; &#34;.join([str(st.session_state.junction_r_outer.get(i, 50.0)) for i in range(len(st.session_state.junctions))])

          cli_command = f&#34;&#34;&#34;route-analyzer predict \\
--input ./data \\
--columns x=Headset.Head.Position.X,z=Headset.Head.Position.Z,t=Time \\
--scale 0.2 \\
--junctions {junctions_str} \\
--r_outer_list {r_outer_str} \\
--distance 100.0 \\
--decision_mode {decision_mode} \\
--cluster_method {cluster_method}&#34;&#34;&#34;

          # Add cluster method specific parameters
          if cluster_method == &#34;dbscan&#34; and cluster_params:
              cli_command += f&#34; \\\n  --min_samples {cluster_params.get(&#39;min_samples&#39;, 5)} \\\n  --angle_eps {cluster_params.get(&#39;angle_eps&#39;, 15.0)}&#34;
          elif cluster_method == &#34;kmeans&#34; and cluster_params:
              cli_command += f&#34; \\\n  --k_min {cluster_params.get(&#39;k_min&#39;, 2)} \\\n  --k_max {cluster_params.get(&#39;k_max&#39;, 6)}&#34;
          elif cluster_method == &#34;auto&#34; and cluster_params:
              cli_command += f&#34; \\\n  --k_min {cluster_params.get(&#39;k_min&#39;, 2)} \\\n  --k_max {cluster_params.get(&#39;k_max&#39;, 6)} \\\n  --min_sep_deg {cluster_params.get(&#39;min_sep_deg&#39;, 12.0)} \\\n  --angle_eps {cluster_params.get(&#39;angle_eps&#39;, 15.0)}&#34;

          # Add decision mode specific parameters
          if decision_mode == &#34;radial&#34; and decision_params:
              cli_command += f&#34; \\\n  --r_outer_list {decision_params.get(&#39;r_outer&#39;, 50.0)} \\\n  --epsilon {decision_params.get(&#39;epsilon&#39;, 0.05)}&#34;
          elif decision_mode == &#34;pathlen&#34; and decision_params:
              cli_command += f&#34; \\\n  --distance {decision_params.get(&#39;path_length&#39;, 100.0)} \\\n  --linger_delta {decision_params.get(&#39;linger_delta&#39;, 0.0)}&#34;
          elif decision_mode == &#34;hybrid&#34; and decision_params:
              cli_command += f&#34; \\\n  --r_outer_list {decision_params.get(&#39;r_outer&#39;, 50.0)} \\\n  --distance {decision_params.get(&#39;path_length&#39;, 100.0)}&#34;

          cli_command += f&#34; \\\n  --out ./outputs/prediction&#34;

          st.code(cli_command, language=&#34;bash&#34;)

      elif analysis_type == &#34;metrics&#34;:
          # Generate metrics commands for each junction
          for junction_key, metrics_data in results.items():
              if not junction_key.startswith(&#34;junction_&#34;):
                  continue

              junction_num = junction_key.split(&#39;_&#39;)[1]
              junction = metrics_data.get(&#34;junction&#34;)
              if junction is None:
                  continue

              r_outer = metrics_data.get(&#34;r_outer_value&#34;, metrics_data.get(&#34;r_outer&#34;, 50.0))
              decision_mode = metrics_data.get(&#34;decision_mode&#34;, &#34;pathlen&#34;)
              distance = metrics_data.get(&#34;distance&#34;, 100.0)
              trend_window = metrics_data.get(&#34;trend_window&#34;, 5)
              min_outward = metrics_data.get(&#34;min_outward&#34;, 0.0)

              st.markdown(f&#34;#### {junction_key.replace(&#39;_&#39;, &#39; &#39;).title()}&#34;)

              # Generate the CLI command
              cli_command = f&#34;&#34;&#34;route-analyzer metrics \\
--input ./data \\
--columns x=Headset.Head.Position.X,z=Headset.Head.Position.Z,t=Time \\
--scale 0.2 \\
--junction {junction.cx:.1f} {junction.cz:.1f} \\
--radius {junction.r:.1f} \\
--decision_mode {decision_mode} \\
--distance {distance:.1f}&#34;&#34;&#34;

              if decision_mode in [&#34;radial&#34;, &#34;hybrid&#34;]:
                  cli_command += f&#34; \\\n  --r_outer {r_outer:.1f}&#34;

              if decision_mode == &#34;radial&#34;:
                  cli_command += f&#34; \\\n  --trend_window {trend_window} \\\n  --min_outward {min_outward:.1f}&#34;

              cli_command += f&#34; \\\n  --out ./outputs/{junction_key}_metrics&#34;

              st.code(cli_command, language=&#34;bash&#34;)

      elif analysis_type == &#34;gaze&#34;:
          # Generate gaze command for all junctions
          if len(st.session_state.junctions) == 1:
              # Single junction
              junction = st.session_state.junctions[0]
              r_outer = st.session_state.junction_r_outer.get(0, 50.0)
              gaze_data = list(results.values())[0] if results else {}
              decision_mode = gaze_data.get(&#34;decision_mode&#34;, &#34;hybrid&#34;)
              path_length = gaze_data.get(&#34;path_length&#34;, 100.0)
              epsilon = gaze_data.get(&#34;epsilon&#34;, 0.05)
              linger_delta = gaze_data.get(&#34;linger_delta&#34;, 5.0)

              cli_command = f&#34;&#34;&#34;route-analyzer gaze \\
--input ./data \\
--columns x=Headset.Head.Position.X,z=Headset.Head.Position.Z,t=Time \\
--scale 0.2 \\
--junction {junction.cx:.1f} {junction.cz:.1f} \\
--radius {junction.r:.1f} \\
--r_outer {r_outer:.1f} \\
--distance {path_length:.1f} \\
--epsilon {epsilon:.3f} \\
--decision_mode {decision_mode} \\
--linger_delta {linger_delta:.1f} \\
--cluster_method {cluster_method}&#34;&#34;&#34;
          else:
              # Multiple junctions
              junctions_str = &#34; &#34;.join([f&#34;{j.cx:.1f} {j.cz:.1f} {j.r:.1f}&#34; for j in st.session_state.junctions])
              r_outer_list = [st.session_state.junction_r_outer.get(i, 50.0) for i in range(len(st.session_state.junctions))]
              r_outer_str = &#34; &#34;.join([str(r) for r in r_outer_list])
              gaze_data = list(results.values())[0] if results else {}
              decision_mode = gaze_data.get(&#34;decision_mode&#34;, &#34;hybrid&#34;)
              path_length = gaze_data.get(&#34;path_length&#34;, 100.0)
              epsilon = gaze_data.get(&#34;epsilon&#34;, 0.05)
              linger_delta = gaze_data.get(&#34;linger_delta&#34;, 5.0)

              cli_command = f&#34;&#34;&#34;route-analyzer gaze \\
--input ./data \\
--columns x=Headset.Head.Position.X,z=Headset.Head.Position.Z,t=Time \\
--scale 0.2 \\
--junctions {junctions_str} \\
--r_outer_list {r_outer_str} \\
--distance {path_length:.1f} \\
--epsilon {epsilon:.3f} \\
--decision_mode {decision_mode} \\
--linger_delta {linger_delta:.1f} \\
--cluster_method {cluster_method}&#34;&#34;&#34;

          # Add cluster method specific parameters
          if cluster_method == &#34;dbscan&#34; and cluster_params:
              cli_command += f&#34; \\\n  --min_samples {cluster_params.get(&#39;min_samples&#39;, 5)} \\\n  --angle_eps {cluster_params.get(&#39;angle_eps&#39;, 15.0)}&#34;
          elif cluster_method == &#34;kmeans&#34; and cluster_params:
              cli_command += f&#34; \\\n  --k {cluster_params.get(&#39;k&#39;, 3)}&#34;
          elif cluster_method == &#34;auto&#34; and cluster_params:
              cli_command += f&#34; \\\n  --k_min {cluster_params.get(&#39;k_min&#39;, 2)} \\\n  --k_max {cluster_params.get(&#39;k_max&#39;, 6)} \\\n  --min_sep_deg {cluster_params.get(&#39;min_sep_deg&#39;, 12.0)} \\\n  --angle_eps {cluster_params.get(&#39;angle_eps&#39;, 15.0)}&#34;

          cli_command += f&#34; \\\n  --out ./outputs/gaze_analysis&#34;

          st.code(cli_command, language=&#34;bash&#34;)

      elif analysis_type == &#34;intent&#34;:
          # Generate intent command for all junctions
          if len(st.session_state.junctions) == 1:
              # Single junction
              junction = st.session_state.junctions[0]
              intent_data = list(results.values())[0] if results else {}
              decision_mode = intent_data.get(&#34;decision_mode&#34;, &#34;hybrid&#34;)
              path_length = intent_data.get(&#34;path_length&#34;, 100.0)
              epsilon = intent_data.get(&#34;epsilon&#34;, 0.05)
              linger_delta = intent_data.get(&#34;linger_delta&#34;, 5.0)
              prediction_distances = intent_data.get(&#34;prediction_distances&#34;, [100.0, 75.0, 50.0, 25.0])
              model_type = intent_data.get(&#34;model_type&#34;, &#34;random_forest&#34;)
              cv_folds = intent_data.get(&#34;cv_folds&#34;, 5)
              test_split = intent_data.get(&#34;test_split&#34;, 0.2)

              cli_command = f&#34;&#34;&#34;route-analyzer intent \\
--input ./data \\
--columns x=Headset.Head.Position.X,z=Headset.Head.Position.Z,t=Time \\
--scale 0.2 \\
--junction {junction.cx:.1f} {junction.cz:.1f} {junction.r:.1f} \\
--distance {path_length:.1f} \\
--epsilon {epsilon:.3f} \\
--decision_mode {decision_mode} \\
--linger_delta {linger_delta:.1f} \\
--cluster_method {cluster_method}&#34;&#34;&#34;
          else:
              # Multiple junctions
              junctions_str = &#34; &#34;.join([f&#34;{j.cx:.1f} {j.cz:.1f} {j.r:.1f}&#34; for j in st.session_state.junctions])
              intent_data = list(results.values())[0] if results else {}
              decision_mode = intent_data.get(&#34;decision_mode&#34;, &#34;hybrid&#34;)
              path_length = intent_data.get(&#34;path_length&#34;, 100.0)
              epsilon = intent_data.get(&#34;epsilon&#34;, 0.05)
              linger_delta = intent_data.get(&#34;linger_delta&#34;, 5.0)
              prediction_distances = intent_data.get(&#34;prediction_distances&#34;, [100.0, 75.0, 50.0, 25.0])
              model_type = intent_data.get(&#34;model_type&#34;, &#34;random_forest&#34;)
              cv_folds = intent_data.get(&#34;cv_folds&#34;, 5)
              test_split = intent_data.get(&#34;test_split&#34;, 0.2)

              cli_command = f&#34;&#34;&#34;route-analyzer intent \\
--input ./data \\
--columns x=Headset.Head.Position.X,z=Headset.Head.Position.Z,t=Time \\
--scale 0.2 \\
--junctions {junctions_str} \\
--distance {path_length:.1f} \\
--epsilon {epsilon:.3f} \\
--decision_mode {decision_mode} \\
--linger_delta {linger_delta:.1f} \\
--cluster_method {cluster_method}&#34;&#34;&#34;

          # Add cluster method specific parameters
          if cluster_method == &#34;dbscan&#34; and cluster_params:
              cli_command += f&#34; \\\n  --min_samples {cluster_params.get(&#39;min_samples&#39;, 5)} \\\n  --angle_eps {cluster_params.get(&#39;angle_eps&#39;, 15.0)}&#34;
          elif cluster_method == &#34;kmeans&#34; and cluster_params:
              cli_command += f&#34; \\\n  --k {cluster_params.get(&#39;k&#39;, 3)}&#34;
          elif cluster_method == &#34;auto&#34; and cluster_params:
              cli_command += f&#34; \\\n  --k_min {cluster_params.get(&#39;k_min&#39;, 2)} \\\n  --k_max {cluster_params.get(&#39;k_max&#39;, 6)} \\\n  --min_sep_deg {cluster_params.get(&#39;min_sep_deg&#39;, 12.0)} \\\n  --angle_eps {cluster_params.get(&#39;angle_eps&#39;, 15.0)}&#34;

          # Add intent-specific parameters
          prediction_distances_str = &#34; &#34;.join([str(d) for d in prediction_distances])
          cli_command += f&#34; \\\n  --prediction_distances {prediction_distances_str} \\\n  --model_type {model_type} \\\n  --cv_folds {cv_folds} \\\n  --test_split {test_split}&#34;

          cli_command += f&#34; \\\n  --out ./outputs/intent_recognition&#34;

          st.code(cli_command, language=&#34;bash&#34;)</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.initialize_session_state"><code class="name flex">
<span>def <span class="ident">initialize_session_state</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize Streamlit session state variables</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize_session_state(self):
    &#34;&#34;&#34;Initialize Streamlit session state variables&#34;&#34;&#34;
    if &#39;junctions&#39; not in st.session_state:
        st.session_state.junctions = []
    if &#39;junction_r_outer&#39; not in st.session_state:
        st.session_state.junction_r_outer = {}  # Store r_outer for each junction
    if &#39;trajectories&#39; not in st.session_state:
        st.session_state.trajectories = []
    # Unified model: trajectories may include optional gaze/physio fields
    if &#39;gaze_column_mappings&#39; not in st.session_state:
        st.session_state.gaze_column_mappings = {}
    if &#39;analysis_results&#39; not in st.session_state:
        st.session_state.analysis_results = None
    if &#39;current_step&#39; not in st.session_state:
        st.session_state.current_step = &#34;data_upload&#34;
    if &#39;scale_factor&#39; not in st.session_state:
        st.session_state.scale_factor = 0.2  # Default scale factor
    if &#39;data_loaded&#39; not in st.session_state:
        st.session_state.data_loaded = False
    # Flash message shown after reruns (tuple: (level, text))
    if &#39;flash_message&#39; not in st.session_state:
        st.session_state.flash_message = None

    # Track junction state for UI refresh
    if &#39;junction_state_hash&#39; not in st.session_state:
        st.session_state.junction_state_hash = 0

    # Debug: Track session state changes
    if &#39;debug_session_state&#39; not in st.session_state:
        st.session_state.debug_session_state = {
            &#39;trajectories_count&#39;: len(st.session_state.trajectories) if st.session_state.trajectories else 0,
            &#39;gaze_trajectories_count&#39;: 0,
            &#39;last_modified&#39;: &#39;initialize&#39;
        }</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.load_assign_centers"><code class="name flex">
<span>def <span class="ident">load_assign_centers</span></span>(<span>self, assign_params: dict = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Load junction centers for assign function based on centers option</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_assign_centers(self, assign_params: dict = None):
    &#34;&#34;&#34;Load junction centers for assign function based on centers option&#34;&#34;&#34;
    try:
        centers_option = assign_params.get(&#34;centers_option&#34;, &#34;Use session centers&#34;) if assign_params else &#34;Use session centers&#34;

        if centers_option == &#34;Use session centers&#34;:
            # Get centers from previous discover analysis
            if &#34;branches&#34; not in st.session_state.analysis_results:
                st.error(&#34;❌ No centers found from previous discover analysis&#34;)
                return None

            centers_dict = {}
            for junction_key, branch_data in st.session_state.analysis_results[&#34;branches&#34;].items():
                if &#34;centers&#34; in branch_data:
                    centers_dict[junction_key] = branch_data[&#34;centers&#34;]

            return centers_dict

        elif centers_option == &#34;Upload files&#34;:
            centers_files = assign_params.get(&#34;centers_files&#34;) if assign_params else None
            if not centers_files:
                st.error(&#34;❌ No centers files uploaded&#34;)
                return None

            centers_dict = {}
            for i, centers_file in enumerate(centers_files):
                # Save uploaded file to temporary location
                with tempfile.NamedTemporaryFile(delete=False, suffix=&#39;.npy&#39;) as tmp_file:
                    tmp_file.write(centers_file.getvalue())
                    tmp_path = tmp_file.name

                # Load centers
                centers = np.load(tmp_path)
                centers_dict[f&#34;junction_{i}&#34;] = centers

                # Clean up temporary file
                os.unlink(tmp_path)

            return centers_dict

        else:  # Select folder
            centers_folder = assign_params.get(&#34;centers_folder&#34;) if assign_params else None
            if not centers_folder or not centers_folder.strip():
                st.error(&#34;❌ No centers folder path specified&#34;)
                return None

            # Load centers from folder (search subfolders)
            centers_dict = {}
            for root, dirs, files in os.walk(centers_folder):
                for file in files:
                    if file.startswith(&#34;branch_centers_j&#34;) and file.endswith(&#34;.npy&#34;):
                        # Extract junction number from filename
                        junction_num = file.split(&#34;_&#34;)[-1].split(&#34;.&#34;)[0]
                        centers_path = os.path.join(root, file)
                        centers = np.load(centers_path)
                        centers_dict[f&#34;junction_{junction_num}&#34;] = centers

            if not centers_dict:
                st.error(f&#34;❌ No center files found in folder: {centers_folder}&#34;)
                st.info(&#34;💡 Looking for files named: branch_centers_j*.npy&#34;)
                return None

            return centers_dict

    except Exception as e:
        st.error(f&#34;❌ Failed to load assign centers: {str(e)}&#34;)
        return None</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.load_assign_trajectories"><code class="name flex">
<span>def <span class="ident">load_assign_trajectories</span></span>(<span>self, assign_params: dict = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Load trajectories for assign function based on trajectory option</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_assign_trajectories(self, assign_params: dict = None):
    &#34;&#34;&#34;Load trajectories for assign function based on trajectory option&#34;&#34;&#34;
    try:
        # Get scale factor from assign parameters
        assign_scale = assign_params.get(&#34;assign_scale&#34;, 0.2) if assign_params else 0.2
        trajectory_option = assign_params.get(&#34;trajectory_option&#34;, &#34;Upload files&#34;) if assign_params else &#34;Upload files&#34;

        if trajectory_option == &#34;Upload files&#34;:
            trajectory_files = assign_params.get(&#34;trajectory_files&#34;) if assign_params else None
            if not trajectory_files:
                st.error(&#34;❌ No trajectory files uploaded&#34;)
                return None

            # Process uploaded files
            trajectories = []
            for uploaded_file in trajectory_files:
                # Save uploaded file to temporary location
                with tempfile.NamedTemporaryFile(delete=False, suffix=&#39;.csv&#39;) as tmp_file:
                    tmp_file.write(uploaded_file.getvalue())
                    tmp_path = tmp_file.name

                # Load trajectory data
                df = pd.read_csv(tmp_path)

                # Use default column names if not specified
                x_col = &#34;Headset.Head.Position.X&#34; if &#34;Headset.Head.Position.X&#34; in df.columns else df.columns[0]
                z_col = &#34;Headset.Head.Position.Z&#34; if &#34;Headset.Head.Position.Z&#34; in df.columns else df.columns[1]
                t_col = &#34;Time&#34; if &#34;Time&#34; in df.columns else df.columns[2] if len(df.columns) &gt; 2 else None

                # Filter out NaN values in coordinate columns
                coord_mask = df[[x_col, z_col]].notnull().all(axis=1)
                df_clean = df[coord_mask].copy()

                if len(df_clean) == 0:
                    st.warning(f&#34;⚠️ Skipping {uploaded_file.name}: All coordinates are NaN&#34;)
                    continue

                if len(df_clean) &lt; len(df):
                    st.info(f&#34;ℹ️ Cleaned {uploaded_file.name}: Removed {len(df) - len(df_clean)} rows with NaN coordinates&#34;)

                # Create trajectory with scale factor applied
                trajectory = Trajectory(
                    tid=uploaded_file.name,
                    x=df_clean[x_col].values * assign_scale,  # Apply scale factor
                    z=df_clean[z_col].values * assign_scale,  # Apply scale factor
                    t=df_clean[t_col].values if t_col else np.arange(len(df_clean))
                )
                trajectories.append(trajectory)

                # Clean up temporary file
                os.unlink(tmp_path)

            return trajectories

        else:  # Select folder
            trajectory_folder = assign_params.get(&#34;trajectory_folder&#34;) if assign_params else None
            if not trajectory_folder or not trajectory_folder.strip():
                st.error(&#34;❌ No trajectory folder path specified&#34;)
                return None

            # Load from folder
            column_mapping = {
                &#34;x&#34;: &#34;Headset.Head.Position.X&#34;,
                &#34;z&#34;: &#34;Headset.Head.Position.Z&#34;,
                &#34;t&#34;: &#34;Time&#34;
            }

            trajectories = load_folder(
                folder=trajectory_folder,
                pattern=&#34;*.csv&#34;,
                columns=column_mapping,
                scale=assign_scale,  # Use assign-specific scale factor
                motion_threshold=0.1
            )

            if not trajectories:
                st.error(f&#34;❌ No trajectories found in folder: {trajectory_folder}&#34;)
                return None

            # Show data cleaning summary
            total_trajectories = len(trajectories)
            st.info(f&#34;ℹ️ Loaded {total_trajectories} trajectories from folder&#34;)
            st.info(f&#34;ℹ️ NaN filtering applied during loading (built into load_folder function)&#34;)

            return trajectories

    except Exception as e:
        st.error(f&#34;❌ Failed to load assign trajectories: {str(e)}&#34;)
        return None</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.load_sample_data"><code class="name flex">
<span>def <span class="ident">load_sample_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Load sample data for demonstration</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_sample_data(self):
    &#34;&#34;&#34;Load sample data for demonstration&#34;&#34;&#34;
    try:
        with st.spinner(&#34;Loading sample data...&#34;):
            # Create sample trajectories
            np.random.seed(42)
            trajectories = []

            for i in range(10):
                # Create a sample trajectory
                n_points = np.random.randint(1000, 5000)
                t = np.linspace(0, 100, n_points)
                x = np.cumsum(np.random.normal(0, 0.5, n_points)) + 500
                z = np.cumsum(np.random.normal(0, 0.5, n_points)) + 200

                trajectory = Trajectory(
                    tid=str(i),
                    x=x,
                    z=z,
                    t=t
                )
                trajectories.append(trajectory)

            st.session_state.trajectories = trajectories
            st.success(f&#34;✅ Loaded {len(trajectories)} sample trajectories!&#34;)

    except Exception as e:
        st.error(f&#34;❌ Error loading sample data: {str(e)}&#34;)</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.load_sample_junctions"><code class="name flex">
<span>def <span class="ident">load_sample_junctions</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Load sample junctions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_sample_junctions(self):
    &#34;&#34;&#34;Load sample junctions&#34;&#34;&#34;
    sample_junctions = [
        Circle(cx=685, cz=170, r=30),
        Circle(cx=550, cz=-90, r=30),
        Circle(cx=730, cz=440, r=20),
        Circle(cx=520, cz=340, r=40),
        Circle(cx=500, cz=515, r=20),
        Circle(cx=575, cz=430, r=15),
        Circle(cx=500, cz=205, r=20)
    ]

    # Sample r_outer values
    sample_r_outer = [100.0, 50.0, 45.0, 45.0, 45.0, 30.0, 45.0]

    st.session_state.junctions = sample_junctions
    st.session_state.junction_r_outer = {i: r_outer for i, r_outer in enumerate(sample_r_outer)}
    st.success(&#34;✅ Loaded sample junctions with r_outer values!&#34;)
    st.rerun()</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.load_trajectory_data"><code class="name flex">
<span>def <span class="ident">load_trajectory_data</span></span>(<span>self, folder_path: str, x_col: str, z_col: str, t_col: str, scale: float, motion_threshold: float)</span>
</code></dt>
<dd>
<div class="desc"><p>Load trajectory data from folder using unified model</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_trajectory_data(self, folder_path: str, x_col: str, z_col: str, t_col: str, scale: float, motion_threshold: float):
    &#34;&#34;&#34;Load trajectory data from folder using unified model&#34;&#34;&#34;
    try:
        # Create progress bar
        progress_bar = st.progress(0)
        status_text = st.empty()

        status_text.text(&#34;🔄 Initializing data loading...&#34;)
        progress_bar.progress(10)

        # Build comprehensive column mapping for VR headset data
        column_mapping = {
            &#39;x&#39;: x_col,
            &#39;z&#39;: z_col,
            &#39;t&#39;: t_col,
            # VR headset gaze/physio columns
            &#39;head_forward_x&#39;: st.session_state.gaze_column_mappings.get(&#39;head_forward_x&#39;, &#39;Headset.Head.Forward.X&#39;),
            &#39;head_forward_y&#39;: st.session_state.gaze_column_mappings.get(&#39;head_forward_y&#39;, &#39;Headset.Head.Forward.Y&#39;),
            &#39;head_forward_z&#39;: st.session_state.gaze_column_mappings.get(&#39;head_forward_z&#39;, &#39;Headset.Head.Forward.Z&#39;),
            &#39;head_up_x&#39;: st.session_state.gaze_column_mappings.get(&#39;head_up_x&#39;, &#39;Headset.Head.Up.X&#39;),
            &#39;head_up_y&#39;: st.session_state.gaze_column_mappings.get(&#39;head_up_y&#39;, &#39;Headset.Head.Up.Y&#39;),
            &#39;head_up_z&#39;: st.session_state.gaze_column_mappings.get(&#39;head_up_z&#39;, &#39;Headset.Head.Up.Z&#39;),
            &#39;gaze_x&#39;: st.session_state.gaze_column_mappings.get(&#39;gaze_x&#39;, &#39;Headset.Gaze.X&#39;),
            &#39;gaze_y&#39;: st.session_state.gaze_column_mappings.get(&#39;gaze_y&#39;, &#39;Headset.Gaze.Y&#39;),
            &#39;pupil_l&#39;: st.session_state.gaze_column_mappings.get(&#39;pupil_l&#39;, &#39;Headset.PupilDilation.L&#39;),
            &#39;pupil_r&#39;: st.session_state.gaze_column_mappings.get(&#39;pupil_r&#39;, &#39;Headset.PupilDilation.R&#39;),
            &#39;heart_rate&#39;: st.session_state.gaze_column_mappings.get(&#39;heart_rate&#39;, &#39;Headset.HeartRate&#39;),
        }

        status_text.text(&#34;🔍 Scanning folder for CSV files...&#34;)
        progress_bar.progress(30)

        # Add a small delay to ensure progress bar is visible
        import time
        time.sleep(0.1)

        # Check what columns are available in the first CSV file and auto-detect gaze columns
        import glob
        import pandas as pd
        csv_files = glob.glob(os.path.join(folder_path, &#34;*.csv&#34;))
        if csv_files:
            sample_df = pd.read_csv(csv_files[0])

            # Auto-detect gaze columns if mappings are empty
            if not st.session_state.gaze_column_mappings:
                st.info(&#34;🔍 **Auto-detecting gaze columns from CSV file...**&#34;)

                # Try to find gaze columns by common patterns
                detected_mappings = {}

                # Look for pupil dilation columns
                pupil_cols = [col for col in sample_df.columns if &#39;pupil&#39; in col.lower() and (&#39;l&#39; in col.lower() or &#39;left&#39; in col.lower())]
                if pupil_cols:
                    detected_mappings[&#39;pupil_l&#39;] = pupil_cols[0]

                pupil_cols = [col for col in sample_df.columns if &#39;pupil&#39; in col.lower() and (&#39;r&#39; in col.lower() or &#39;right&#39; in col.lower())]
                if pupil_cols:
                    detected_mappings[&#39;pupil_r&#39;] = pupil_cols[0]

                # Look for heart rate columns
                hr_cols = [col for col in sample_df.columns if &#39;heart&#39; in col.lower() or &#39;hr&#39; in col.lower()]
                if hr_cols:
                    detected_mappings[&#39;heart_rate&#39;] = hr_cols[0]

                # Look for gaze columns
                gaze_x_cols = [col for col in sample_df.columns if &#39;gaze&#39; in col.lower() and (&#39;x&#39; in col.lower() or &#39;horizontal&#39; in col.lower())]
                if gaze_x_cols:
                    detected_mappings[&#39;gaze_x&#39;] = gaze_x_cols[0]

                gaze_y_cols = [col for col in sample_df.columns if &#39;gaze&#39; in col.lower() and (&#39;y&#39; in col.lower() or &#39;vertical&#39; in col.lower())]
                if gaze_y_cols:
                    detected_mappings[&#39;gaze_y&#39;] = gaze_y_cols[0]

                # Look for head forward columns
                head_fwd_x_cols = [col for col in sample_df.columns if &#39;head&#39; in col.lower() and &#39;forward&#39; in col.lower() and &#39;x&#39; in col.lower()]
                if head_fwd_x_cols:
                    detected_mappings[&#39;head_forward_x&#39;] = head_fwd_x_cols[0]

                head_fwd_z_cols = [col for col in sample_df.columns if &#39;head&#39; in col.lower() and &#39;forward&#39; in col.lower() and (&#39;z&#39; in col.lower() or &#39;depth&#39; in col.lower())]
                if head_fwd_z_cols:
                    detected_mappings[&#39;head_forward_z&#39;] = head_fwd_z_cols[0]

                # Update session state with detected mappings
                if detected_mappings:
                    st.session_state.gaze_column_mappings.update(detected_mappings)
                    st.success(f&#34;✅ **Auto-detected gaze columns:** {detected_mappings}&#34;)

                    # Update column mapping with detected values
                    column_mapping.update(detected_mappings)
                else:
                    st.warning(&#34;⚠️ **No gaze columns auto-detected**&#34;)

            # Check if gaze columns exist (using current mappings)
            gaze_columns = [&#39;Headset.Head.Forward.X&#39;, &#39;Headset.Head.Forward.Z&#39;, &#39;Headset.Gaze.X&#39;,
                          &#39;Headset.Gaze.Y&#39;, &#39;Headset.PupilDilation.L&#39;, &#39;Headset.PupilDilation.R&#39;, &#39;Headset.HeartRate&#39;]

            # Use detected mappings if available
            actual_gaze_columns = []
            for gaze_type in [&#39;head_forward_x&#39;, &#39;head_forward_z&#39;, &#39;gaze_x&#39;, &#39;gaze_y&#39;, &#39;pupil_l&#39;, &#39;pupil_r&#39;, &#39;heart_rate&#39;]:
                col_name = st.session_state.gaze_column_mappings.get(gaze_type, gaze_columns[[&#39;head_forward_x&#39;, &#39;head_forward_z&#39;, &#39;gaze_x&#39;, &#39;gaze_y&#39;, &#39;pupil_l&#39;, &#39;pupil_r&#39;, &#39;heart_rate&#39;].index(gaze_type)])
                actual_gaze_columns.append(col_name)

            missing_gaze_cols = [col for col in actual_gaze_columns if col not in sample_df.columns]
            if missing_gaze_cols:
                st.warning(f&#34;⚠️ **Missing gaze columns:** {missing_gaze_cols}&#34;)
            else:
                st.success(&#34;✅ **All gaze columns found in CSV!**&#34;)

        status_text.text(&#34;📊 Loading trajectory data...&#34;)
        progress_bar.progress(60)

        # Add a small delay to ensure progress bar is visible
        import time
        time.sleep(0.1)

        # Create progress callback function
        def update_progress(current, total, message):
            progress_percent = int(60 + (current / total) * 30)  # 60-90% range
            progress_bar.progress(progress_percent)
            status_text.text(message)

        # Use unified loader - always returns Trajectory objects with optional gaze fields
        trajectories = load_folder(
            folder=folder_path,
            pattern=&#34;*.csv&#34;,
            columns=column_mapping,
            require_time=False,
            scale=scale,
            motion_threshold=motion_threshold,
            progress_callback=update_progress
        )

        status_text.text(&#34;💾 Storing trajectories in session...&#34;)
        progress_bar.progress(90)

        # Add another small delay to show progress
        time.sleep(0.1)

        # Store unified trajectories
        st.session_state.trajectories = trajectories

        # Update status display
        st.session_state.data_loaded = True

        progress_bar.progress(100)
        status_text.text(&#34;✅ Data loading completed!&#34;)

        # Clear progress elements first
        progress_bar.empty()
        status_text.empty()

        # Queue success flash for top-of-page after rerun
        st.session_state.flash_message = (&#39;success&#39;, f&#34;🎉 Successfully loaded {len(trajectories)} trajectories!&#34;)

        # Force rerun to update status display and show flash at top
        st.rerun()

    except Exception as e:
        st.error(f&#34;❌ Error loading data: {str(e)}&#34;)
        if &#39;progress_bar&#39; in locals():
            progress_bar.empty()
        if &#39;status_text&#39; in locals():
            status_text.empty()</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.load_uploaded_files"><code class="name flex">
<span>def <span class="ident">load_uploaded_files</span></span>(<span>self, uploaded_files, x_col: str, z_col: str, t_col: str, scale: float, motion_threshold: float)</span>
</code></dt>
<dd>
<div class="desc"><p>Load trajectory data from uploaded files using unified model</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_uploaded_files(self, uploaded_files, x_col: str, z_col: str, t_col: str, scale: float, motion_threshold: float):
    &#34;&#34;&#34;Load trajectory data from uploaded files using unified model&#34;&#34;&#34;
    try:
        with st.spinner(&#34;Loading uploaded files...&#34;):
            import pandas as pd
            import io
            import numpy as np
            import tempfile
            import os
            from verta.verta_data_loader import Trajectory, TrajectoryLoader, ColumnMapping

            # Create progress bar
            progress_bar = st.progress(0)
            status_text = st.empty()

            status_text.text(&#34;🔄 Initializing file processing...&#34;)
            progress_bar.progress(5)

            # Build comprehensive column mapping for VR headset data (same as folder loading)
            column_mapping = {
                &#39;x&#39;: x_col,
                &#39;z&#39;: z_col,
                &#39;t&#39;: t_col,
                # VR headset gaze/physio columns
                &#39;head_forward_x&#39;: st.session_state.gaze_column_mappings.get(&#39;head_forward_x&#39;, &#39;Headset.Head.Forward.X&#39;),
                &#39;head_forward_y&#39;: st.session_state.gaze_column_mappings.get(&#39;head_forward_y&#39;, &#39;Headset.Head.Forward.Y&#39;),
                &#39;head_forward_z&#39;: st.session_state.gaze_column_mappings.get(&#39;head_forward_z&#39;, &#39;Headset.Head.Forward.Z&#39;),
                &#39;head_up_x&#39;: st.session_state.gaze_column_mappings.get(&#39;head_up_x&#39;, &#39;Headset.Head.Up.X&#39;),
                &#39;head_up_y&#39;: st.session_state.gaze_column_mappings.get(&#39;head_up_y&#39;, &#39;Headset.Head.Up.Y&#39;),
                &#39;head_up_z&#39;: st.session_state.gaze_column_mappings.get(&#39;head_up_z&#39;, &#39;Headset.Head.Up.Z&#39;),
                &#39;gaze_x&#39;: st.session_state.gaze_column_mappings.get(&#39;gaze_x&#39;, &#39;Headset.Gaze.X&#39;),
                &#39;gaze_y&#39;: st.session_state.gaze_column_mappings.get(&#39;gaze_y&#39;, &#39;Headset.Gaze.Y&#39;),
                &#39;pupil_l&#39;: st.session_state.gaze_column_mappings.get(&#39;pupil_l&#39;, &#39;Headset.PupilDilation.L&#39;),
                &#39;pupil_r&#39;: st.session_state.gaze_column_mappings.get(&#39;pupil_r&#39;, &#39;Headset.PupilDilation.R&#39;),
                &#39;heart_rate&#39;: st.session_state.gaze_column_mappings.get(&#39;heart_rate&#39;, &#39;Headset.HeartRate&#39;),
            }

            # Auto-detect gaze columns from first uploaded file (same as folder loading)
            if uploaded_files and not st.session_state.gaze_column_mappings:
                st.info(&#34;🔍 **Auto-detecting gaze columns from uploaded file...**&#34;)

                # Read first file to detect columns
                first_file = uploaded_files[0]
                df_sample = pd.read_csv(io.StringIO(first_file.read().decode(&#39;utf-8&#39;)))
                first_file.seek(0)  # Reset file pointer

                # Try to find gaze columns by common patterns (same logic as folder loading)
                detected_mappings = {}

                # Look for pupil dilation columns
                pupil_cols = [col for col in df_sample.columns if &#39;pupil&#39; in col.lower() and (&#39;l&#39; in col.lower() or &#39;left&#39; in col.lower())]
                if pupil_cols:
                    detected_mappings[&#39;pupil_l&#39;] = pupil_cols[0]

                pupil_cols = [col for col in df_sample.columns if &#39;pupil&#39; in col.lower() and (&#39;r&#39; in col.lower() or &#39;right&#39; in col.lower())]
                if pupil_cols:
                    detected_mappings[&#39;pupil_r&#39;] = pupil_cols[0]

                # Look for heart rate columns
                hr_cols = [col for col in df_sample.columns if &#39;heart&#39; in col.lower() or &#39;hr&#39; in col.lower()]
                if hr_cols:
                    detected_mappings[&#39;heart_rate&#39;] = hr_cols[0]

                # Look for gaze columns
                gaze_x_cols = [col for col in df_sample.columns if &#39;gaze&#39; in col.lower() and (&#39;x&#39; in col.lower() or &#39;horizontal&#39; in col.lower())]
                if gaze_x_cols:
                    detected_mappings[&#39;gaze_x&#39;] = gaze_x_cols[0]

                gaze_y_cols = [col for col in df_sample.columns if &#39;gaze&#39; in col.lower() and (&#39;y&#39; in col.lower() or &#39;vertical&#39; in col.lower())]
                if gaze_y_cols:
                    detected_mappings[&#39;gaze_y&#39;] = gaze_y_cols[0]

                # Look for head forward columns
                head_fwd_x_cols = [col for col in df_sample.columns if &#39;head&#39; in col.lower() and &#39;forward&#39; in col.lower() and &#39;x&#39; in col.lower()]
                if head_fwd_x_cols:
                    detected_mappings[&#39;head_forward_x&#39;] = head_fwd_x_cols[0]

                head_fwd_z_cols = [col for col in df_sample.columns if &#39;head&#39; in col.lower() and &#39;forward&#39; in col.lower() and (&#39;z&#39; in col.lower() or &#39;depth&#39; in col.lower())]
                if head_fwd_z_cols:
                    detected_mappings[&#39;head_forward_z&#39;] = head_fwd_z_cols[0]

                # Update session state with detected mappings
                if detected_mappings:
                    st.session_state.gaze_column_mappings.update(detected_mappings)
                    st.success(f&#34;✅ **Auto-detected gaze columns:** {detected_mappings}&#34;)

                    # Update column mapping with detected values
                    column_mapping.update(detected_mappings)
                else:
                    st.warning(&#34;⚠️ **No gaze columns auto-detected**&#34;)

            # Create temporary directory to store uploaded files
            with tempfile.TemporaryDirectory() as temp_dir:
                status_text.text(&#34;📁 Saving uploaded files to temporary directory...&#34;)
                progress_bar.progress(10)

                # Save uploaded files to temporary directory
                temp_files = []
                for i, uploaded_file in enumerate(uploaded_files):
                    temp_file_path = os.path.join(temp_dir, uploaded_file.name)
                    with open(temp_file_path, &#39;wb&#39;) as f:
                        f.write(uploaded_file.getvalue())
                    temp_files.append(temp_file_path)

                status_text.text(&#34;📊 Loading trajectories using unified loader...&#34;)
                progress_bar.progress(20)

                # Use the same unified loader as folder loading
                loader = TrajectoryLoader(ColumnMapping.from_dict(column_mapping))

                # Create progress callback function
                def update_progress(current, total, message):
                    progress_percent = int(20 + (current / total) * 70)  # 20-90% range
                    progress_bar.progress(progress_percent)
                    status_text.text(message)

                # Load trajectories using unified loader
                trajectories = loader.load_folder(
                    folder=temp_dir,
                    pattern=&#34;*.csv&#34;,
                    trajectory_class=Trajectory,
                    require_time=False,
                    scale=scale,
                    motion_threshold=motion_threshold,
                    progress_callback=update_progress
                )

            if trajectories:
                status_text.text(&#34;💾 Storing trajectories in session...&#34;)
                progress_bar.progress(90)

                st.session_state.trajectories = trajectories

                # Update status display
                st.session_state.data_loaded = True

                progress_bar.progress(100)
                status_text.text(&#34;✅ File processing completed!&#34;)

                # Clear progress elements first
                progress_bar.empty()
                status_text.empty()

                # Queue success flash for top-of-page after rerun
                st.session_state.flash_message = (
                    &#39;success&#39;,
                    f&#34;🎉 Successfully loaded {len(trajectories)} trajectories from {len(uploaded_files)} files!&#34;
                )

                # Force rerun to update status display and show flash at top
                st.rerun()
            else:
                st.error(&#34;❌ No valid trajectories could be loaded from uploaded files&#34;)

    except Exception as e:
        st.error(f&#34;❌ Error loading uploaded files: {str(e)}&#34;)</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.render_analysis"><code class="name flex">
<span>def <span class="ident">render_analysis</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Render the analysis interface</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render_analysis(self):
    &#34;&#34;&#34;Render the analysis interface&#34;&#34;&#34;
    st.markdown(&#39;&lt;h2 class=&#34;section-header&#34;&gt;📊 Analysis&lt;/h2&gt;&#39;, unsafe_allow_html=True)

    if not st.session_state.trajectories:
        st.warning(&#34;⚠️ Please load trajectory data first&#34;)
        return

    col1, col2 = st.columns([3, 1])

    with col1:
        st.markdown(&#34;### Analysis Configuration&#34;)

        # Analysis type selection
        analysis_type_options = {
            &#34;discover&#34;: &#34;🔍 Discover Branches - Find decision branches at junctions&#34;,
            &#34;assign&#34;: &#34;📊 Assign Trajectories - Assign trajectories to discovered branches&#34;,
            &#34;metrics&#34;: &#34;📈 Movement Metrics - Calculate trajectory movement patterns and timing&#34;,
            &#34;gaze&#34;: &#34;👁️ Gaze &amp; Physiology - Analyze eye tracking and physiological data&#34;,
            &#34;predict&#34;: &#34;🔮 Predict Choices - Predict junction choice patterns&#34;,
            &#34;intent&#34;: &#34;🧠 Intent Recognition - Predict route choices BEFORE decision points (ML)&#34;,
            &#34;enhanced&#34;: &#34;🚨 Enhanced Analysis - Evacuation planning, risk assessment, and efficiency metrics&#34;
        }

        # Reset analysis type if we&#39;re coming from another tab
        if st.session_state.current_step == &#34;analysis&#34;:
            # Clear any cached analysis type to ensure fresh selection
            if &#34;cached_analysis_type&#34; in st.session_state:
                del st.session_state.cached_analysis_type

        analysis_type = st.selectbox(
            &#34;Analysis Type:&#34;,
            list(analysis_type_options.keys()),
            format_func=lambda x: analysis_type_options[x],
            help=&#34;Select the type of analysis to perform&#34;,
            key=f&#34;analysis_type_select_{st.session_state.current_step}&#34;
        )

        # Store the selected analysis type
        st.session_state.cached_analysis_type = analysis_type

        # Initialize default values
        decision_mode = &#34;pathlen&#34;
        cluster_method = &#34;dbscan&#34;
        seed = 42

        # Initialize cluster parameters with defaults
        dbscan_eps = 0.5
        dbscan_min_samples = 5
        dbscan_angle_eps = 15.0
        kmeans_k = 3
        kmeans_k_min = 2
        kmeans_k_max = 6
        auto_k_min = 2
        auto_k_max = 6
        auto_min_sep_deg = 12.0
        auto_angle_eps = 15.0

        # Initialize decision mode parameters with defaults
        radial_r_outer = 50.0
        radial_epsilon = 0.05
        pathlen_path_length = 100.0
        pathlen_linger_delta = 0.0
        hybrid_r_outer = 50.0
        hybrid_path_length = 100.0
        hybrid_linger_delta = 0.0

        # Initialize metrics parameters with defaults
        metrics_decision_mode = &#34;pathlen&#34;
        metrics_distance = 100.0
        metrics_r_outer = 50.0
        metrics_trend_window = 5
        metrics_min_outward = 0.0

        # Initialize discover parameters with defaults
        discover_decision_mode = &#34;hybrid&#34;
        discover_r_outer = 50.0
        discover_epsilon = 0.05
        discover_path_length = 100.0
        discover_linger_delta = 0.0
        discover_hybrid_r_outer = 50.0
        discover_hybrid_path_length = 100.0

        # Show parameters based on analysis type
        if analysis_type == &#34;predict&#34;:
            # Predict analysis uses only spatial tracking - no parameters needed
            st.info(&#34;ℹ️ Predict analysis uses spatial tracking only. No additional parameters required.&#34;)

            # Set default values for compatibility (not used in analysis)
            cluster_method = &#34;kmeans&#34;
            seed = 42
            decision_mode = &#34;hybrid&#34;

            # REMOVED: All cluster method parameters - not needed for spatial tracking only

        elif analysis_type == &#34;intent&#34;:
            # Intent Recognition - ML-based early prediction
            st.markdown(&#34;#### 🧠 Intent Recognition Configuration&#34;)
            st.info(&#34;🤖 Machine Learning-based prediction of route choices **before** users reach decision points.&#34;)

            # Check if Discover has been run
            has_discover_results = (st.session_state.analysis_results and
                                   &#39;branches&#39; in st.session_state.analysis_results)

            if has_discover_results:
                st.success(&#34;✅ Will use branch assignments from your previous &#39;Discover Branches&#39; analysis&#34;)
            else:
                st.warning(&#34;⚠️ No &#39;Discover Branches&#39; results found. Will use default clustering parameters.&#34;)
                st.info(&#34;💡 **Recommended:** Run &#39;Discover Branches&#39; analysis first to control clustering settings!&#34;)

            # Prediction distances
            st.markdown(&#34;##### Prediction Distances&#34;)
            dist_col1, dist_col2, dist_col3, dist_col4 = st.columns(4)

            with dist_col1:
                intent_dist_100 = st.checkbox(&#34;100 units&#34;, value=True, help=&#34;Predict 100 units before junction&#34;)
            with dist_col2:
                intent_dist_75 = st.checkbox(&#34;75 units&#34;, value=True, help=&#34;Predict 75 units before junction&#34;)
            with dist_col3:
                intent_dist_50 = st.checkbox(&#34;50 units&#34;, value=True, help=&#34;Predict 50 units before junction&#34;)
            with dist_col4:
                intent_dist_25 = st.checkbox(&#34;25 units&#34;, value=True, help=&#34;Predict 25 units before junction&#34;)

            # Build prediction distances list
            intent_prediction_distances = []
            if intent_dist_100:
                intent_prediction_distances.append(100.0)
            if intent_dist_75:
                intent_prediction_distances.append(75.0)
            if intent_dist_50:
                intent_prediction_distances.append(50.0)
            if intent_dist_25:
                intent_prediction_distances.append(25.0)

            if not intent_prediction_distances:
                st.warning(&#34;⚠️ Select at least one prediction distance!&#34;)
                intent_prediction_distances = [50.0]  # Default

            # Model configuration
            st.markdown(&#34;##### Model Configuration&#34;)
            model_col1, model_col2 = st.columns(2)

            with model_col1:
                intent_model_type = st.selectbox(
                    &#34;ML Model:&#34;,
                    [&#34;random_forest&#34;, &#34;gradient_boosting&#34;],
                    index=0,
                    help=&#34;Random Forest: Fast, robust | Gradient Boosting: More accurate but slower&#34;
                )

            with model_col2:
                intent_cv_folds = st.number_input(
                    &#34;Cross-validation Folds:&#34;,
                    value=5,
                    min_value=2,
                    max_value=10,
                    help=&#34;Number of folds for cross-validation&#34;
                )

            # Feature configuration
            with st.expander(&#34;🔧 Advanced: Feature Configuration&#34;):
                st.markdown(&#34;**Features Used:**&#34;)
                st.markdown(&#34;&#34;&#34;
                - ✅ **Spatial**: Distance, approach angle, lateral offset
                - ✅ **Kinematic**: Speed, acceleration, curvature, sinuosity
                - ✅ **Gaze** (if available): Gaze angle, alignment, head rotation
                - ✅ **Physiological** (if available): Heart rate, pupil dilation
                - ✅ **Contextual**: Previous junction choices
                &#34;&#34;&#34;)

                intent_test_split = st.slider(
                    &#34;Test Set Size (%):&#34;,
                    min_value=10,
                    max_value=40,
                    value=20,
                    help=&#34;Percentage of data reserved for testing&#34;
                )

            # Store intent parameters in session state
            if &#39;intent_params&#39; not in st.session_state:
                st.session_state.intent_params = {}

            st.session_state.intent_params = {
                &#39;prediction_distances&#39;: intent_prediction_distances,
                &#39;model_type&#39;: intent_model_type,
                &#39;cv_folds&#39;: intent_cv_folds,
                &#39;test_split&#39;: intent_test_split / 100.0
            }

            # Set default values for compatibility
            cluster_method = &#34;kmeans&#34;
            seed = 42
            decision_mode = &#34;hybrid&#34;

        elif analysis_type == &#34;discover&#34;:
            # Clustering parameters (used by discover analysis)
            st.markdown(&#34;#### Clustering Parameters&#34;)
            col_method, col_seed = st.columns(2)

            with col_method:
                cluster_method = st.selectbox(
                    &#34;Cluster Method:&#34;,
                    [&#34;dbscan&#34;, &#34;kmeans&#34;, &#34;auto&#34;],
                    index=0,  # Default to DBSCAN
                    help=&#34;Clustering method for discover analysis&#34;
                )

            with col_seed:
                seed = st.number_input(
                    &#34;Random Seed:&#34;,
                    value=42,
                    min_value=0,
                    max_value=10000,
                    step=1,
                    help=&#34;Random seed for reproducibility&#34;
                )

            # Decision mode parameters (needed for discover analysis)
            st.markdown(&#34;#### Decision Mode Parameters&#34;)
            col_decision_mode, col_decision_param = st.columns(2)

            with col_decision_mode:
                discover_decision_mode = st.selectbox(
                    &#34;Decision Mode:&#34;,
                    [&#34;radial&#34;, &#34;pathlen&#34;, &#34;hybrid&#34;],
                    index=2,  # Default to hybrid
                    help=&#34;Decision mode for discover analysis&#34;
                )

            with col_decision_param:
                if discover_decision_mode == &#34;radial&#34;:
                    st.info(&#34;ℹ️ Using junction-specific r_outer values from the Junctions tab&#34;)
                    # r_outer will be overridden by junction-specific values
                    discover_r_outer = 50.0
                    discover_epsilon = st.number_input(
                        &#34;Epsilon:&#34;,
                        value=0.05,
                        min_value=0.01,
                        max_value=1.0,
                        step=0.01,
                        help=&#34;Epsilon parameter&#34;
                    )
                elif discover_decision_mode == &#34;pathlen&#34;:
                    discover_path_length = st.number_input(
                        &#34;Path Length:&#34;,
                        value=100.0,
                        min_value=10.0,
                        max_value=500.0,
                        step=10.0,
                        help=&#34;Path length for pathlen mode&#34;
                    )
                    discover_linger_delta = st.number_input(
                        &#34;Linger Delta:&#34;,
                        value=0.0,
                        min_value=0.0,
                        max_value=50.0,
                        step=1.0,
                        help=&#34;Linger distance beyond junction&#34;
                    )
                elif discover_decision_mode == &#34;hybrid&#34;:
                    st.info(&#34;ℹ️ Using junction-specific r_outer values from the Junctions tab&#34;)
                    # r_outer will be overridden by junction-specific values
                    discover_hybrid_r_outer = 50.0
                    discover_hybrid_path_length = st.number_input(
                        &#34;Hybrid Path Length:&#34;,
                        value=100.0,
                        min_value=10.0,
                        max_value=500.0,
                        step=10.0,
                        help=&#34;Path length for hybrid mode&#34;
                    )
                    discover_hybrid_linger_delta = st.number_input(
                        &#34;Hybrid Linger Delta:&#34;,
                        value=0.0,
                        min_value=0.0,
                        max_value=50.0,
                        step=1.0,
                        help=&#34;Linger distance beyond junction for hybrid mode&#34;
                    )

            # Dynamic parameters based on cluster method (for discover analysis)
            st.markdown(&#34;#### Cluster Method Parameters&#34;)
            if cluster_method == &#34;dbscan&#34;:
                col_eps, col_min_samples = st.columns(2)
                with col_eps:
                    dbscan_eps = st.number_input(
                        &#34;DBSCAN Epsilon (eps):&#34;,
                        value=0.5,
                        min_value=0.1,
                        max_value=10.0,
                        step=0.1,
                        help=&#34;Maximum distance between samples in the same neighborhood&#34;
                    )
                with col_min_samples:
                    dbscan_min_samples = st.number_input(
                        &#34;DBSCAN Min Samples:&#34;,
                        value=5,
                        min_value=1,
                        max_value=50,
                        step=1,
                        help=&#34;Minimum number of samples in a neighborhood&#34;
                    )

                # Add angle_eps parameter for DBSCAN
                dbscan_angle_eps = st.number_input(
                    &#34;DBSCAN Angle Epsilon (degrees):&#34;,
                    value=11.0,
                    min_value=1.0,
                    max_value=90.0,
                    step=1.0,
                    help=&#34;Angle epsilon for DBSCAN clustering (angular separation between clusters)&#34;
                )
            elif cluster_method == &#34;kmeans&#34;:
                col_k, col_k_range = st.columns(2)
                with col_k:
                    kmeans_k = st.number_input(
                        &#34;K-Means K (number of clusters):&#34;,
                        value=3,
                        min_value=2,
                        max_value=20,
                        step=1,
                        help=&#34;Number of clusters to form&#34;
                    )
                with col_k_range:
                    kmeans_k_min = st.number_input(
                        &#34;K-Means K Min:&#34;,
                        value=2,
                        min_value=2,
                        max_value=10,
                        step=1,
                        help=&#34;Minimum number of clusters for auto selection&#34;
                    )
                    kmeans_k_max = st.number_input(
                        &#34;K-Means K Max:&#34;,
                        value=6,
                        min_value=3,
                        max_value=20,
                        step=1,
                        help=&#34;Maximum number of clusters for auto selection&#34;
                    )
            elif cluster_method == &#34;auto&#34;:
                col_k_range, col_separation = st.columns(2)
                with col_k_range:
                    auto_k_min = st.number_input(
                        &#34;Auto K Min:&#34;,
                        value=2,
                        min_value=2,
                        max_value=10,
                        step=1,
                        help=&#34;Minimum number of clusters for auto selection&#34;
                    )
                    auto_k_max = st.number_input(
                        &#34;Auto K Max:&#34;,
                        value=6,
                        min_value=3,
                        max_value=20,
                        step=1,
                        help=&#34;Maximum number of clusters for auto selection&#34;
                    )
                with col_separation:
                    auto_min_sep_deg = st.number_input(
                        &#34;Auto Min Separation (degrees):&#34;,
                        value=12.0,
                        min_value=1.0,
                        max_value=90.0,
                        step=1.0,
                        help=&#34;Minimum separation in degrees between clusters&#34;
                    )
                    auto_angle_eps = st.number_input(
                        &#34;Auto Angle Epsilon (degrees):&#34;,
                        value=11.0,
                        min_value=1.0,
                        max_value=90.0,
                        step=1.0,
                        help=&#34;Angle epsilon for auto clustering&#34;
                    )

        elif analysis_type == &#34;enhanced&#34;:
            # Enhanced analysis parameters (same as discover since it uses discover_decision_chain)
            st.markdown(&#34;#### Enhanced Analysis Parameters&#34;)
            st.info(&#34;🚨 Enhanced analysis uses the same clustering and decision parameters as discover analysis, then performs evacuation planning, risk assessment, and efficiency analysis.&#34;)

            # Clustering parameters (same as discover)
            st.markdown(&#34;##### Clustering Parameters&#34;)
            col_method, col_seed = st.columns(2)

            with col_method:
                cluster_method = st.selectbox(
                    &#34;Cluster Method:&#34;,
                    [&#34;dbscan&#34;, &#34;kmeans&#34;, &#34;auto&#34;],
                    index=0,  # Default to DBSCAN
                    help=&#34;Clustering method for enhanced analysis&#34;
                )

            with col_seed:
                seed = st.number_input(
                    &#34;Random Seed:&#34;,
                    value=42,
                    min_value=0,
                    max_value=10000,
                    step=1,
                    help=&#34;Random seed for reproducibility&#34;
                )

            # Decision mode parameters (same as discover)
            st.markdown(&#34;##### Decision Mode Parameters&#34;)
            col_decision_mode, col_decision_param = st.columns(2)

            with col_decision_mode:
                discover_decision_mode = st.selectbox(
                    &#34;Decision Mode:&#34;,
                    [&#34;radial&#34;, &#34;pathlen&#34;, &#34;hybrid&#34;],
                    index=2,  # Default to hybrid
                    help=&#34;Decision mode for enhanced analysis&#34;
                )

            with col_decision_param:
                if discover_decision_mode == &#34;radial&#34;:
                    st.info(&#34;ℹ️ Using junction-specific r_outer values from the Junctions tab&#34;)
                    # r_outer will be overridden by junction-specific values
                    discover_r_outer = 50.0
                    discover_epsilon = st.number_input(
                        &#34;Epsilon:&#34;,
                        value=0.05,
                        min_value=0.01,
                        max_value=1.0,
                        step=0.01,
                        help=&#34;Epsilon parameter&#34;
                    )
                elif discover_decision_mode == &#34;pathlen&#34;:
                    discover_path_length = st.number_input(
                        &#34;Path Length:&#34;,
                        value=100.0,
                        min_value=10.0,
                        max_value=500.0,
                        step=10.0,
                        help=&#34;Path length for pathlen mode&#34;
                    )
                    discover_linger_delta = st.number_input(
                        &#34;Linger Delta:&#34;,
                        value=0.0,
                        min_value=0.0,
                        max_value=50.0,
                        step=1.0,
                        help=&#34;Linger distance beyond junction&#34;
                    )
                elif discover_decision_mode == &#34;hybrid&#34;:
                    st.info(&#34;ℹ️ Using junction-specific r_outer values from the Junctions tab&#34;)
                    # r_outer will be overridden by junction-specific values
                    discover_hybrid_r_outer = 50.0
                    discover_hybrid_path_length = st.number_input(
                        &#34;Hybrid Path Length:&#34;,
                        value=100.0,
                        min_value=10.0,
                        max_value=500.0,
                        step=10.0,
                        help=&#34;Path length for hybrid mode&#34;
                    )
                    discover_hybrid_linger_delta = st.number_input(
                        &#34;Hybrid Linger Delta:&#34;,
                        value=0.0,
                        min_value=0.0,
                        max_value=50.0,
                        step=1.0,
                        help=&#34;Linger distance beyond junction for hybrid mode&#34;
                    )

            # Dynamic parameters based on cluster method (same as discover)
            st.markdown(&#34;##### Cluster Method Parameters&#34;)
            if cluster_method == &#34;dbscan&#34;:
                col_eps, col_min_samples = st.columns(2)
                with col_eps:
                    dbscan_eps = st.number_input(
                        &#34;DBSCAN Epsilon (eps):&#34;,
                        value=0.5,
                        min_value=0.1,
                        max_value=10.0,
                        step=0.1,
                        help=&#34;Maximum distance between samples in the same neighborhood&#34;
                    )
                with col_min_samples:
                    dbscan_min_samples = st.number_input(
                        &#34;DBSCAN Min Samples:&#34;,
                        value=5,
                        min_value=1,
                        max_value=50,
                        step=1,
                        help=&#34;Minimum number of samples in a neighborhood&#34;
                    )

                # Add angle_eps parameter for DBSCAN
                dbscan_angle_eps = st.number_input(
                    &#34;DBSCAN Angle Epsilon (degrees):&#34;,
                    value=11.0,
                    min_value=1.0,
                    max_value=90.0,
                    step=1.0,
                    help=&#34;Angle epsilon for DBSCAN clustering (angular separation between clusters)&#34;
                )
            elif cluster_method == &#34;kmeans&#34;:
                col_k, col_k_range = st.columns(2)
                with col_k:
                    kmeans_k = st.number_input(
                        &#34;K-Means K (number of clusters):&#34;,
                        value=3,
                        min_value=2,
                        max_value=20,
                        step=1,
                        help=&#34;Number of clusters to form&#34;
                    )
                with col_k_range:
                    kmeans_k_min = st.number_input(
                        &#34;K-Means K Min:&#34;,
                        value=2,
                        min_value=2,
                        max_value=10,
                        step=1,
                        help=&#34;Minimum number of clusters for auto selection&#34;
                    )
                    kmeans_k_max = st.number_input(
                        &#34;K-Means K Max:&#34;,
                        value=6,
                        min_value=3,
                        max_value=20,
                        step=1,
                        help=&#34;Maximum number of clusters for auto selection&#34;
                    )
            elif cluster_method == &#34;auto&#34;:
                col_k_range, col_separation = st.columns(2)
                with col_k_range:
                    auto_k_min = st.number_input(
                        &#34;Auto K Min:&#34;,
                        value=2,
                        min_value=2,
                        max_value=10,
                        step=1,
                        help=&#34;Minimum number of clusters for auto selection&#34;
                    )
                    auto_k_max = st.number_input(
                        &#34;Auto K Max:&#34;,
                        value=6,
                        min_value=3,
                        max_value=20,
                        step=1,
                        help=&#34;Maximum number of clusters for auto selection&#34;
                    )
                with col_separation:
                    auto_min_sep_deg = st.number_input(
                        &#34;Auto Min Separation (degrees):&#34;,
                        value=12.0,
                        min_value=1.0,
                        max_value=90.0,
                        step=1.0,
                        help=&#34;Minimum separation in degrees between clusters&#34;
                    )
                    auto_angle_eps = st.number_input(
                        &#34;Auto Angle Epsilon (degrees):&#34;,
                        value=11.0,
                        min_value=1.0,
                        max_value=90.0,
                        step=1.0,
                        help=&#34;Angle epsilon for auto clustering&#34;
                    )

        elif analysis_type == &#34;metrics&#34;:
            # Metrics-specific parameters
            st.markdown(&#34;#### Metrics Parameters&#34;)
            st.info(&#34;Metrics analysis computes timing and distance metrics for trajectories. The decision mode determines how junction timing is calculated - &#39;pathlen&#39; measures time to reach a distance threshold, &#39;radial&#39; measures time to exit a radius, and &#39;hybrid&#39; tries radial first then falls back to pathlen.&#34;)

            col_metrics_mode, col_metrics_distance = st.columns(2)

            with col_metrics_mode:
                st.session_state.metrics_decision_mode = st.selectbox(
                    &#34;Decision Mode:&#34;,
                    [&#34;pathlen&#34;, &#34;radial&#34;, &#34;hybrid&#34;],
                    index=2,  # Default to hybrid
                    help=&#34;Decision mode for junction timing analysis&#34;
                )

            with col_metrics_distance:
                st.session_state.metrics_distance = st.number_input(
                    &#34;Distance Threshold:&#34;,
                    value=100.0,
                    min_value=10.0,
                    max_value=500.0,
                    step=10.0,
                    help=&#34;Path length for decision timing (pathlen mode) or outer radius (radial mode)&#34;
                )

            # Additional metrics parameters
            col_trend_window, col_min_outward = st.columns(2)

            with col_trend_window:
                st.session_state.metrics_trend_window = st.number_input(
                    &#34;Metrics Trend Window:&#34;,
                    value=5,
                    min_value=1,
                    max_value=20,
                    step=1,
                    help=&#34;Trend window for radial mode&#34;
                )

            st.session_state.metrics_min_outward = st.number_input(
                &#34;Metrics Min Outward:&#34;,
                value=0.0,
                min_value=0.0,
                max_value=10.0,
                step=0.1,
                help=&#34;Minimum outward movement for radial mode&#34;
            )

            # Show info about using junction-specific r_outer values
            if st.session_state.metrics_decision_mode in [&#34;radial&#34;, &#34;hybrid&#34;]:
                st.info(&#34;ℹ️ Using junction-specific r_outer values from the Junctions tab&#34;)

        elif analysis_type == &#34;gaze&#34;:
            # Gaze analysis parameters
            st.markdown(&#34;#### Gaze &amp; Physiological Analysis Parameters&#34;)

            # Prefer gaze trajectories if available
            active_trajs = st.session_state.trajectories

            # Check if we have proper gaze trajectory data
            has_gaze_data = self._check_for_gaze_data(active_trajs)

            # Show simple status message
            if active_trajs:
                from verta.verta_data_loader import has_vr_headset_data
                has_vr = any(has_vr_headset_data(traj) for traj in active_trajs)

                if has_vr:
                    st.success(&#34;✅ This dataset contains VR headset data!&#34;)

            # Show analysis options
            if has_gaze_data:

                # Check if we have existing branch assignments
                has_existing_assignments = (st.session_state.analysis_results is not None and
                                         &#34;branches&#34; in st.session_state.analysis_results)

                if has_existing_assignments:
                    st.success(&#34;✅ **Existing branch assignments found!**&#34;)
                    st.info(&#34;🔍 Gaze analysis will use existing branch assignments from previous discover analysis.&#34;)
                    st.write(&#34;**💡 To create new assignments:** Run &#39;🔍 Discover Branches&#39; analysis first, then return here for gaze analysis.&#34;)

                    # Always use existing assignments
                    st.session_state.use_existing_assignments = True
                    st.session_state.run_custom_discover = False

                else:
                    st.warning(&#34;⚠️ **No existing branch assignments found!**&#34;)
                    st.error(&#34;**Prerequisite:** You must run &#39;🔍 Discover Branches&#39; analysis first to create branch assignments.&#34;)
                    st.write(&#34;**Steps:**&#34;)
                    st.write(&#34;1. Go to the &#39;🔍 Discover Branches&#39; analysis&#34;)
                    st.write(&#34;2. Run the discover analysis to create branch assignments&#34;)
                    st.write(&#34;3. Return here to run gaze analysis&#34;)

                    # Disable gaze analysis if no assignments
                    st.session_state.use_existing_assignments = False
                    st.session_state.run_custom_discover = False

                    # Show a disabled button
                    st.button(&#34;Run Analysis&#34;, disabled=True, help=&#34;Run &#39;🔍 Discover Branches&#39; analysis first to create branch assignments&#34;)
                    return

                # Pupil Dilation Heatmap Settings
                st.markdown(&#34;---&#34;)
                st.markdown(&#34;#### 🗺️ Pupil Dilation Heatmap Settings&#34;)
                st.info(&#34;Configure spatial heatmap visualization of pupil dilation patterns&#34;)

                col_grid, col_norm = st.columns(2)

                with col_grid:
                    # Initialize session state with default value if not set
                    if &#39;pupil_heatmap_cell_size&#39; not in st.session_state:
                        st.session_state.pupil_heatmap_cell_size = 10.0

                    st.session_state.pupil_heatmap_cell_size = st.slider(
                        &#34;Cell Size (coordinate units):&#34;,
                        min_value=1.0,
                        max_value=200.0,
                        value=float(st.session_state.pupil_heatmap_cell_size),
                        step=1.0,
                        help=&#34;Size of each grid cell in coordinate units (smaller = finer resolution)&#34;
                    )

                with col_norm:
                    # Initialize session state with default value if not set
                    if &#39;pupil_heatmap_normalization&#39; not in st.session_state:
                        st.session_state.pupil_heatmap_normalization = &#39;relative&#39;

                    st.session_state.pupil_heatmap_normalization = st.selectbox(
                        &#34;Normalization:&#34;,
                        [&#34;relative&#34;, &#34;zscore&#34;],
                        index=0 if st.session_state.pupil_heatmap_normalization == &#39;relative&#39; else 1,
                        help=&#34;Relative: % change from baseline. Z-score: standard deviations from mean&#34;
                    )

                # Show expected grid dimensions
                if st.session_state.trajectories:
                    # Calculate approximate grid dimensions
                    all_x = np.concatenate([t.x for t in st.session_state.trajectories[:10] if hasattr(t, &#39;x&#39;)])
                    all_z = np.concatenate([t.z for t in st.session_state.trajectories[:10] if hasattr(t, &#39;z&#39;)])
                    x_range = np.max(all_x) - np.min(all_x)
                    z_range = np.max(all_z) - np.min(all_z)
                    cell_size = st.session_state.pupil_heatmap_cell_size
                    grid_x = int(np.ceil(x_range / cell_size))
                    grid_z = int(np.ceil(z_range / cell_size))

                    st.info(f&#34;📏 **Expected grid:** {grid_x} × {grid_z} cells ({cell_size}×{cell_size} units each)&#34;)

                # Run Analysis button
                if st.button(&#34;Run Analysis&#34;, type=&#34;primary&#34;):
                    # Run gaze analysis
                    self.run_analysis(&#34;gaze&#34;, &#34;hybrid&#34;, &#34;dbscan&#34;, 42)

            else:
                st.info(&#34;ℹ️ Gaze/Physiological analysis requires VR headset data with eye tracking.&#34;)

        elif analysis_type == &#34;assign&#34;:
            st.markdown(&#34;#### Assign Parameters&#34;)

            # Add scaling warning
            st.info(&#34;💡 **Important**: Ensure the scale factor used for trajectories matches the scale factor used during discover analysis. Mismatched scaling will cause assignment failures.&#34;)

            # Scale factor input for assign analysis
            st.markdown(&#34;**📏 Scale Factor for Assignment:**&#34;)
            col_scale_assign, col_scale_info = st.columns([1, 2])

            with col_scale_assign:
                assign_scale = st.number_input(
                    &#34;Scale Factor:&#34;,
                    value=st.session_state.get(&#34;scale_factor&#34;, 0.2),
                    min_value=0.01,
                    max_value=1.0,
                    step=0.01,
                    key=&#34;assign_scale_factor&#34;,
                    help=&#34;Scale factor to apply to trajectory coordinates&#34;
                )

            with col_scale_info:
                # Show scale factor from discover analysis if available
                analysis_results = st.session_state.get(&#34;analysis_results&#34;)
                if analysis_results and &#34;branches&#34; in analysis_results:
                    discover_scale = None
                    for junction_key, branch_data in analysis_results[&#34;branches&#34;].items():
                        if &#34;scale&#34; in branch_data:
                            discover_scale = branch_data[&#34;scale&#34;]
                            break

                    if discover_scale is not None:
                        st.info(f&#34;🔍 **Discover used scale**: {discover_scale:.2f}&#34;)
                        if abs(assign_scale - discover_scale) &gt; 0.01:
                            st.warning(f&#34;⚠️ **Scale mismatch detected!** Consider using {discover_scale:.2f} for consistency.&#34;)
                    else:
                        st.info(&#34;🔍 No discover scale factor found&#34;)
                else:
                    st.info(&#34;🔍 No discover analysis found&#34;)

            # Simplified data input options
            st.markdown(&#34;**📁 Data Input Options:**&#34;)

            # Trajectories input
            st.markdown(&#34;**Trajectories:**&#34;)
            trajectory_option = st.radio(
                &#34;Trajectory Source:&#34;,
                [&#34;Upload files&#34;, &#34;Select folder&#34;],
                key=&#34;assign_trajectory_option&#34;,
                help=&#34;Upload new trajectories to be assigned to existing branches&#34;
            )

            # Centers input
            st.markdown(&#34;**Junction Centers:**&#34;)
            centers_option = st.radio(
                &#34;Centers Source:&#34;,
                [&#34;Use session centers&#34;, &#34;Upload files&#34;, &#34;Select folder&#34;],
                key=&#34;assign_centers_option&#34;,
                help=&#34;Choose how to provide junction center data&#34;
            )

            # File upload and folder selection based on options
            if trajectory_option == &#34;Upload files&#34;:
                st.markdown(&#34;**📤 Upload Trajectory Files:**&#34;)
                trajectory_files = st.file_uploader(
                    &#34;Choose trajectory CSV files:&#34;,
                    type=[&#39;csv&#39;],
                    accept_multiple_files=True,
                    key=&#34;assign_trajectory_files&#34;,
                    help=&#34;Upload CSV files containing trajectory data to be assigned to existing branches&#34;
                )
            else:  # Select folder
                st.markdown(&#34;**📁 Select Trajectory Folder:**&#34;)
                trajectory_folder = st.text_input(
                    &#34;Trajectory folder path:&#34;,
                    key=&#34;assign_trajectory_folder&#34;,
                    help=&#34;Enter the path to the folder containing trajectory CSV files to be assigned to existing branches&#34;
                )

            if centers_option == &#34;Upload files&#34;:
                st.markdown(&#34;**📤 Upload Center Files:**&#34;)
                centers_files = st.file_uploader(
                    &#34;Choose center files (.npy or .zip):&#34;,
                    type=[&#39;npy&#39;, &#39;zip&#39;],
                    accept_multiple_files=True,
                    key=&#34;assign_centers_files&#34;,
                    help=&#34;Upload .npy files containing junction centers or .zip files with multiple centers&#34;
                )
            elif centers_option == &#34;Select folder&#34;:
                st.markdown(&#34;**📁 Select Centers Folder:**&#34;)
                centers_folder = st.text_input(
                    &#34;Centers folder path:&#34;,
                    key=&#34;assign_centers_folder&#34;,
                    help=&#34;Enter the path to the folder containing junction centers (will search subfolders for branch_centers_j*.npy files)&#34;
                )

            # Assignment parameters
            st.markdown(&#34;**⚙️ Assignment Parameters:**&#34;)
            # Decision mode selector (mirror discover logic with selectbox)
            # Default from discover if available
            default_mode = &#34;hybrid&#34;
            if centers_option == &#34;Use session centers&#34; and st.session_state.get(&#34;analysis_results&#34;) and &#34;branches&#34; in st.session_state.analysis_results:
                # Try to fetch from first matching junction block
                for _jk, _bd in st.session_state.analysis_results[&#34;branches&#34;].items():
                    if isinstance(_bd, dict) and &#34;decision_mode&#34; in _bd:
                        default_mode = _bd.get(&#34;decision_mode&#34;, default_mode)
                        break

            col_decision_mode, col_decision_param = st.columns(2)
            with col_decision_mode:
                assign_decision_mode = st.selectbox(
                    &#34;Decision Mode:&#34;,
                    [&#34;pathlen&#34;, &#34;radial&#34;, &#34;hybrid&#34;],
                    index=[&#34;pathlen&#34;,&#34;radial&#34;,&#34;hybrid&#34;].index(default_mode) if default_mode in [&#34;pathlen&#34;,&#34;radial&#34;,&#34;hybrid&#34;] else 2,
                    key=&#34;assign_decision_mode&#34;,
                    help=&#34;How to compute initial direction vectors for assignment&#34;
                )

            # Auto-rediscovery (always available when uploading new trajectories)
            st.markdown(&#34;**🧭 Auto-Discover New Branches (optional):**&#34;)
            auto_col1, auto_col2, auto_col3 = st.columns(3)
            with auto_col1:
                st.checkbox(
                    &#34;Enable auto-rediscover&#34;,
                    value=False,
                    key=&#34;assign_auto_rediscover&#34;,
                    help=&#34;If outlier assignments form a dense region of size ≥ min samples, rerun discovery for this junction using all trajectories (existing + newly uploaded).&#34;
                )
            with auto_col2:
                st.number_input(
                    &#34;Min samples for new branch&#34;,
                    value=5,
                    min_value=2,
                    max_value=100,
                    step=1,
                    key=&#34;assign_auto_min_samples&#34;,
                    help=&#34;Minimum outlier vectors required to trigger rediscovery.&#34;
                )
            with auto_col3:
                st.number_input(
                    &#34;Angle eps (deg)&#34;,
                    value=11.0,
                    min_value=1.0,
                    max_value=90.0,
                    step=1.0,
                    key=&#34;assign_auto_angle_eps&#34;,
                    help=&#34;Angular neighborhood size for detecting a dense outlier region.&#34;
                )

            # Decision mode parameters in second column (mirror discover UI)
            with col_decision_param:
                # Fetch defaults from discover if using session centers
                pref_path_length = 100.0
                pref_linger = 0.0
                pref_epsilon = 0.05
                if centers_option == &#34;Use session centers&#34; and st.session_state.get(&#34;analysis_results&#34;) and &#34;branches&#34; in st.session_state.analysis_results:
                    for _jk, _bd in st.session_state.analysis_results[&#34;branches&#34;].items():
                        if isinstance(_bd, dict):
                            if &#34;path_length&#34; in _bd:
                                pref_path_length = float(_bd.get(&#34;path_length&#34;, pref_path_length))
                            if &#34;linger_delta&#34; in _bd:
                                pref_linger = float(_bd.get(&#34;linger_delta&#34;, pref_linger))
                            if &#34;epsilon&#34; in _bd:
                                pref_epsilon = float(_bd.get(&#34;epsilon&#34;, pref_epsilon))
                            break

                if assign_decision_mode == &#34;radial&#34;:
                    st.info(&#34;ℹ️ Using r_outer from junctions (or stored discover results)&#34;)
                    assign_r_outer = None  # Will be fetched per junction
                    assign_epsilon = st.number_input(
                        &#34;Epsilon:&#34;,
                        value=pref_epsilon,
                        min_value=0.001,
                        max_value=1.0,
                        step=0.001,
                        format=&#34;%.3f&#34;,
                        key=&#34;assign_epsilon&#34;,
                        help=&#34;Minimum movement threshold&#34;
                    )
                    assign_path_length = 100.0
                    assign_linger_delta = 0.0
                elif assign_decision_mode == &#34;pathlen&#34;:
                    assign_path_length = st.number_input(
                        &#34;Path Length:&#34;,
                        value=pref_path_length,
                        min_value=10.0,
                        max_value=500.0,
                        step=10.0,
                        key=&#34;assign_path_length&#34;,
                        help=&#34;Path length for decision point&#34;
                    )
                    assign_linger_delta = st.number_input(
                        &#34;Linger Delta:&#34;,
                        value=pref_linger,
                        min_value=0.0,
                        max_value=200.0,
                        step=1.0,
                        key=&#34;assign_linger_delta&#34;,
                        help=&#34;Linger distance beyond junction&#34;
                    )
                    assign_epsilon = st.number_input(
                        &#34;Epsilon:&#34;,
                        value=pref_epsilon,
                        min_value=0.001,
                        max_value=1.0,
                        step=0.001,
                        format=&#34;%.3f&#34;,
                        key=&#34;assign_epsilon&#34;,
                        help=&#34;Minimum movement threshold&#34;
                    )
                    assign_r_outer = None
                elif assign_decision_mode == &#34;hybrid&#34;:
                    st.info(&#34;ℹ️ Using r_outer from junctions (or stored discover results)&#34;)
                    assign_r_outer = None  # Will be fetched per junction
                    assign_path_length = st.number_input(
                        &#34;Hybrid Path Length:&#34;,
                        value=pref_path_length,
                        min_value=10.0,
                        max_value=500.0,
                        step=10.0,
                        key=&#34;assign_path_length&#34;,
                        help=&#34;Path length for hybrid mode&#34;
                    )
                    assign_linger_delta = st.number_input(
                        &#34;Hybrid Linger Delta:&#34;,
                        value=pref_linger,
                        min_value=0.0,
                        max_value=200.0,
                        step=1.0,
                        key=&#34;assign_linger_delta&#34;,
                        help=&#34;Linger distance for hybrid mode&#34;
                    )
                    assign_epsilon = st.number_input(
                        &#34;Epsilon:&#34;,
                        value=pref_epsilon,
                        min_value=0.001,
                        max_value=1.0,
                        step=0.001,
                        format=&#34;%.3f&#34;,
                        key=&#34;assign_epsilon&#34;,
                        help=&#34;Minimum movement threshold&#34;
                    )

            # Junction parameters for external data (only show if not using session centers)
            if centers_option != &#34;Use session centers&#34;:
                st.markdown(&#34;**🎯 Junction Parameters (for external data):**&#34;)
                st.info(&#34;If using external trajectory data, you may need to specify junction parameters manually.&#34;)

                col_junction_cx, col_junction_cz, col_junction_r = st.columns(3)

                with col_junction_cx:
                    assign_junction_cx = st.number_input(
                        &#34;Junction Center X:&#34;,
                        value=0.0,
                        key=&#34;assign_junction_cx&#34;,
                        help=&#34;X coordinate of junction center&#34;
                    )

                with col_junction_cz:
                    assign_junction_cz = st.number_input(
                        &#34;Junction Center Z:&#34;,
                        value=0.0,
                        key=&#34;assign_junction_cz&#34;,
                        help=&#34;Z coordinate of junction center&#34;
                    )

                with col_junction_r:
                    assign_junction_r = st.number_input(
                        &#34;Junction Radius:&#34;,
                        value=50.0,
                        min_value=1.0,
                        max_value=200.0,
                        step=1.0,
                        key=&#34;assign_junction_r&#34;,
                        help=&#34;Radius of the junction area&#34;
                    )

            # Legacy code removed - using simplified interface above

    with col2:
        st.markdown(&#34;### Run Analysis&#34;)

        # Check if junctions are defined for analysis
        has_junctions = bool(st.session_state.junctions)

        if not has_junctions:
            st.warning(&#34;⚠️ **No junctions defined!** Please define junctions in the Junction Editor before running analysis.&#34;)
            st.info(&#34;💡 **Tip:** Go to the Junction Editor tab to define junctions for your analysis.&#34;)

        if st.button(&#34;🚀 Run Analysis&#34;, type=&#34;primary&#34;, disabled=not has_junctions):
            # Collect cluster method parameters
            cluster_params = {}
            if analysis_type == &#34;discover&#34; or analysis_type == &#34;enhanced&#34;:
                if cluster_method == &#34;dbscan&#34;:
                    cluster_params = {&#34;eps&#34;: dbscan_eps, &#34;min_samples&#34;: dbscan_min_samples, &#34;angle_eps&#34;: dbscan_angle_eps}
                elif cluster_method == &#34;kmeans&#34;:
                    cluster_params = {&#34;k&#34;: kmeans_k, &#34;k_min&#34;: kmeans_k_min, &#34;k_max&#34;: kmeans_k_max}
                elif cluster_method == &#34;auto&#34;:
                    cluster_params = {&#34;k_min&#34;: auto_k_min, &#34;k_max&#34;: auto_k_max, &#34;min_sep_deg&#34;: auto_min_sep_deg, &#34;angle_eps&#34;: auto_angle_eps}

            # Collect decision mode parameters
            decision_params = {}
            if analysis_type == &#34;predict&#34;:
                if decision_mode == &#34;radial&#34;:
                    decision_params = {&#34;r_outer&#34;: radial_r_outer, &#34;epsilon&#34;: radial_epsilon}
                elif decision_mode == &#34;pathlen&#34;:
                    decision_params = {&#34;path_length&#34;: pathlen_path_length, &#34;linger_delta&#34;: pathlen_linger_delta}
                elif decision_mode == &#34;hybrid&#34;:
                    decision_params = {&#34;r_outer&#34;: hybrid_r_outer, &#34;path_length&#34;: hybrid_path_length, &#34;linger_delta&#34;: hybrid_linger_delta}
            elif analysis_type == &#34;discover&#34; or analysis_type == &#34;enhanced&#34;:
                if discover_decision_mode == &#34;radial&#34;:
                    decision_params = {&#34;r_outer&#34;: discover_r_outer, &#34;epsilon&#34;: discover_epsilon}
                elif discover_decision_mode == &#34;pathlen&#34;:
                    decision_params = {&#34;path_length&#34;: discover_path_length, &#34;linger_delta&#34;: discover_linger_delta}
                elif discover_decision_mode == &#34;hybrid&#34;:
                    decision_params = {&#34;r_outer&#34;: discover_hybrid_r_outer, &#34;path_length&#34;: discover_hybrid_path_length, &#34;linger_delta&#34;: discover_hybrid_linger_delta}

            # Add metrics-specific parameters if analysis type is metrics
            if analysis_type == &#34;metrics&#34;:
                decision_params.update({
                    &#34;decision_mode&#34;: st.session_state.get(&#34;metrics_decision_mode&#34;, &#34;pathlen&#34;),
                    &#34;distance&#34;: st.session_state.get(&#34;metrics_distance&#34;, 100.0),
                    &#34;r_outer&#34;: st.session_state.get(&#34;metrics_r_outer&#34;, 50.0),
                    &#34;trend_window&#34;: st.session_state.get(&#34;metrics_trend_window&#34;, 5),
                    &#34;min_outward&#34;: st.session_state.get(&#34;metrics_min_outward&#34;, 0.0)
                })

            # Collect assign parameters if needed
            assign_params = {}
            if analysis_type == &#34;assign&#34;:
                # Get assignment parameters
                assign_path_length = st.session_state.get(&#34;assign_path_length&#34;, 100.0)
                assign_epsilon = st.session_state.get(&#34;assign_epsilon&#34;, 0.05)
                # Auto-rediscover controls
                assign_auto_rediscover = st.session_state.get(&#34;assign_auto_rediscover&#34;, False)
                assign_auto_min_samples = st.session_state.get(&#34;assign_auto_min_samples&#34;, 5)
                assign_auto_angle_eps = st.session_state.get(&#34;assign_auto_angle_eps&#34;, 15.0)

                # Get trajectory and centers options
                trajectory_option = st.session_state.get(&#34;assign_trajectory_option&#34;, &#34;Upload files&#34;)
                centers_option = st.session_state.get(&#34;assign_centers_option&#34;, &#34;Use session centers&#34;)

                # Collect trajectory data
                trajectory_files = None
                trajectory_folder = None
                if trajectory_option == &#34;Upload files&#34;:
                    trajectory_files = st.session_state.get(&#34;assign_trajectory_files&#34;)
                else:  # Select folder
                    trajectory_folder = st.session_state.get(&#34;assign_trajectory_folder&#34;)

                # Collect centers data
                centers_files = None
                centers_folder = None
                if centers_option == &#34;Upload files&#34;:
                    centers_files = st.session_state.get(&#34;assign_centers_files&#34;)
                elif centers_option == &#34;Select folder&#34;:
                    centers_folder = st.session_state.get(&#34;assign_centers_folder&#34;)

                assign_params = {
                    &#34;path_length&#34;: assign_path_length,
                    &#34;epsilon&#34;: assign_epsilon,
                    &#34;assign_scale&#34;: assign_scale,  # Add assign-specific scale factor
                    &#34;decision_mode&#34;: assign_decision_mode,
                    &#34;r_outer&#34;: assign_r_outer,
                    &#34;linger_delta&#34;: assign_linger_delta,
                    &#34;auto_rediscover&#34;: assign_auto_rediscover,
                    &#34;auto_min_samples&#34;: assign_auto_min_samples,
                    &#34;auto_angle_eps&#34;: assign_auto_angle_eps,
                    &#34;trajectory_option&#34;: trajectory_option,
                    &#34;centers_option&#34;: centers_option,
                    &#34;trajectory_files&#34;: trajectory_files,
                    &#34;trajectory_folder&#34;: trajectory_folder,
                    &#34;centers_files&#34;: centers_files,
                    &#34;centers_folder&#34;: centers_folder,
                    &#34;junction_cx&#34;: st.session_state.get(&#34;assign_junction_cx&#34;, 0.0),
                    &#34;junction_cz&#34;: st.session_state.get(&#34;assign_junction_cz&#34;, 0.0),
                    &#34;junction_r&#34;: st.session_state.get(&#34;assign_junction_r&#34;, 50.0)
                }

            self.run_analysis(analysis_type, decision_mode, cluster_method, seed, cluster_params, decision_params, assign_params, discover_decision_mode)

        if st.session_state.analysis_results:
            st.markdown(&#34;### Analysis Results&#34;)
            st.success(&#34;✅ Analysis completed successfully!&#34;)


            # Show summary based on analysis type
            if analysis_type == &#34;discover&#34;:
                if &#34;branches&#34; in st.session_state.analysis_results:
                    st.write(f&#34;**Branches discovered:** {len(st.session_state.analysis_results[&#39;branches&#39;])}&#34;)

            elif analysis_type == &#34;assign&#34;:
                if &#34;assignments&#34; in st.session_state.analysis_results:
                    st.write(f&#34;**Trajectories assigned:** {len(st.session_state.analysis_results[&#39;assignments&#39;])}&#34;)

                    # Show debug information if available
                    if &#34;assign_debug_info&#34; in st.session_state and st.session_state.assign_debug_info:
                        st.markdown(&#34;### 🔍 Debug Information&#34;)
                        for junction_key, debug_info in st.session_state.assign_debug_info.items():
                            with st.expander(f&#34;Debug Info for {junction_key}&#34;, expanded=False):
                                st.write(&#34;**Junction Parameters:**&#34;)
                                st.write(f&#34;- Center: {debug_info[&#39;junction_params&#39;][&#39;center&#39;]}&#34;)
                                st.write(f&#34;- Radius: {debug_info[&#39;junction_params&#39;][&#39;radius&#39;]}&#34;)
                                st.write(f&#34;- R_outer: {debug_info[&#39;junction_params&#39;][&#39;r_outer&#39;]}&#34;)

                                st.write(&#34;**Assignment Parameters:**&#34;)
                                st.write(f&#34;- Path length: {debug_info[&#39;assignment_params&#39;][&#39;path_length&#39;]}&#34;)
                                st.write(f&#34;- Epsilon: {debug_info[&#39;assignment_params&#39;][&#39;epsilon&#39;]}&#34;)

                                st.write(&#34;**Data Info:**&#34;)
                                st.write(f&#34;- Centers shape: {debug_info[&#39;data_info&#39;][&#39;centers_shape&#39;]}&#34;)
                                st.write(f&#34;- Trajectories: {debug_info[&#39;data_info&#39;][&#39;trajectories&#39;]}&#34;)

                                st.write(&#34;**Assignment Distribution:**&#34;)
                                total_trajectories = sum(debug_info[&#39;assignment_distribution&#39;].values())
                                for branch, count in debug_info[&#39;assignment_distribution&#39;].items():
                                    percentage = (count / total_trajectories) * 100
                                    st.write(f&#34;- Branch {branch}: {count} trajectories ({percentage:.1f}%)&#34;)

                                # Add troubleshooting info if most trajectories are -2/-1
                                neg2_count = debug_info[&#39;assignment_distribution&#39;].get(-2, 0)
                                neg1_count = debug_info[&#39;assignment_distribution&#39;].get(-1, 0)

                                if (neg2_count + neg1_count) / total_trajectories &gt; 0.8:  # More than 80% are -2/-1
                                    st.warning(&#34;⚠️ **Troubleshooting:** Most trajectories are getting -2/-1 assignments!&#34;)
                                    st.write(&#34;**Possible solutions:**&#34;)
                                    st.write(&#34;1. **Increase junction radius** - Current radius might be too small&#34;)
                                    st.write(&#34;2. **Adjust junction center** - Center might not match trajectory paths&#34;)
                                    st.write(&#34;3. **Check trajectory data** - Ensure trajectories actually pass through junction area&#34;)
                                    st.write(&#34;4. **Use manual junction parameters** - Try different center coordinates and radius&#34;)

                                st.write(&#34;**First 10 Assignments:**&#34;)
                                if debug_info[&#39;assignments_sample&#39;]:
                                    st.dataframe(pd.DataFrame(debug_info[&#39;assignments_sample&#39;]), width=&#39;stretch&#39;)

            elif analysis_type == &#34;metrics&#34;:
                if &#34;metrics&#34; in st.session_state.analysis_results:
                    st.write(f&#34;**Metrics computed:** {len(st.session_state.analysis_results[&#39;metrics&#39;])}&#34;)

                    # Show debug information for metrics
                    st.markdown(&#34;---&#34;)
                    st.markdown(&#34;### 🔍 Debug Information&#34;)

                    # Get debug information from the first few trajectories
                    if st.session_state.trajectories:
                        st.write(&#34;**Debug Status:**&#34;)
                        st.write(f&#34;- Total trajectories: {len(st.session_state.trajectories)}&#34;)

                        # Sample first 5 trajectories for debug
                        time_data_debug = []
                        for i, traj in enumerate(st.session_state.trajectories[:5]):
                            time_debug = {
                                &#34;trajectory_id&#34;: i,
                                &#34;time_data_type&#34;: str(type(traj.t)),
                                &#34;time_data_shape&#34;: traj.t.shape if traj.t is not None else None,
                                &#34;time_data_sample&#34;: traj.t[:3].tolist() if traj.t is not None and len(traj.t) &gt; 0 else None,
                                &#34;time_data_dtype&#34;: str(traj.t.dtype) if traj.t is not None else None,
                                &#34;time_is_none&#34;: traj.t is None,
                                &#34;time_length&#34;: len(traj.t) if traj.t is not None else 0,
                                # Add position data diagnostics
                                &#34;x_data_type&#34;: str(type(traj.x)),
                                &#34;x_data_shape&#34;: traj.x.shape if traj.x is not None else None,
                                &#34;x_data_sample&#34;: traj.x[:3].tolist() if traj.x is not None and len(traj.x) &gt; 0 else None,
                                &#34;x_data_dtype&#34;: str(traj.x.dtype) if traj.x is not None else None,
                                &#34;x_is_none&#34;: traj.x is None,
                                &#34;x_length&#34;: len(traj.x) if traj.x is not None else 0,
                                &#34;z_data_type&#34;: str(type(traj.z)),
                                &#34;z_data_shape&#34;: traj.z.shape if traj.z is not None else None,
                                &#34;z_data_sample&#34;: traj.z[:3].tolist() if traj.z is not None and len(traj.z) &gt; 0 else None,
                                &#34;z_data_dtype&#34;: str(traj.z.dtype) if traj.z is not None else None,
                                &#34;z_is_none&#34;: traj.z is None,
                                &#34;z_length&#34;: len(traj.z) if traj.z is not None else 0
                            }
                            time_data_debug.append(time_debug)

                        st.write(f&#34;- time_data_debug length: {len(time_data_debug)}&#34;)

                        if time_data_debug:
                            with st.expander(&#34;🔍 Trajectory Data Debug Information&#34;, expanded=True):
                                st.write(&#34;**First 5 trajectories data analysis:**&#34;)
                                for debug_info in time_data_debug:
                                    st.write(f&#34;**Trajectory {debug_info[&#39;trajectory_id&#39;]}:**&#34;)

                                    # Time data
                                    st.write(&#34;**Time Data:**&#34;)
                                    st.write(f&#34;- Is None: {debug_info[&#39;time_is_none&#39;]}&#34;)
                                    st.write(f&#34;- Length: {debug_info[&#39;time_length&#39;]}&#34;)
                                    st.write(f&#34;- Type: {debug_info[&#39;time_data_type&#39;]}&#34;)
                                    st.write(f&#34;- Shape: {debug_info[&#39;time_data_shape&#39;]}&#34;)
                                    st.write(f&#34;- Dtype: {debug_info[&#39;time_data_dtype&#39;]}&#34;)
                                    st.write(f&#34;- Sample: {debug_info[&#39;time_data_sample&#39;]}&#34;)

                                    # Position data
                                    st.write(&#34;**Position Data (X):**&#34;)
                                    st.write(f&#34;- Is None: {debug_info[&#39;x_is_none&#39;]}&#34;)
                                    st.write(f&#34;- Length: {debug_info[&#39;x_length&#39;]}&#34;)
                                    st.write(f&#34;- Type: {debug_info[&#39;x_data_type&#39;]}&#34;)
                                    st.write(f&#34;- Shape: {debug_info[&#39;x_data_shape&#39;]}&#34;)
                                    st.write(f&#34;- Dtype: {debug_info[&#39;x_data_dtype&#39;]}&#34;)
                                    st.write(f&#34;- Sample: {debug_info[&#39;x_data_sample&#39;]}&#34;)

                                    st.write(&#34;**Position Data (Z):**&#34;)
                                    st.write(f&#34;- Is None: {debug_info[&#39;z_is_none&#39;]}&#34;)
                                    st.write(f&#34;- Length: {debug_info[&#39;z_length&#39;]}&#34;)
                                    st.write(f&#34;- Type: {debug_info[&#39;z_data_type&#39;]}&#34;)
                                    st.write(f&#34;- Shape: {debug_info[&#39;z_data_shape&#39;]}&#34;)
                                    st.write(f&#34;- Dtype: {debug_info[&#39;z_data_dtype&#39;]}&#34;)
                                    st.write(f&#34;- Sample: {debug_info[&#39;z_data_sample&#39;]}&#34;)

                                    st.write(&#34;---&#34;)
                        else:
                            st.info(&#34;No time data debug information available&#34;)

            elif analysis_type == &#34;gaze&#34;:
                if &#34;gaze_results&#34; in st.session_state.analysis_results:
                    st.write(f&#34;**Gaze analysis completed**&#34;)

            elif analysis_type == &#34;predict&#34;:
                if &#34;choice_patterns&#34; in st.session_state.analysis_results:
                    st.write(f&#34;**Choice patterns analyzed**&#34;)</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.render_assign_visualizations"><code class="name flex">
<span>def <span class="ident">render_assign_visualizations</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Render assign analysis visualizations</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render_assign_visualizations(self):
    &#34;&#34;&#34;Render assign analysis visualizations&#34;&#34;&#34;
    st.markdown(&#34;### Assign Analysis Results&#34;)

    # Display assignment results for each junction
    for junction_key, assignments_data in st.session_state.analysis_results[&#34;assignments&#34;].items():
        st.markdown(f&#34;#### {junction_key.replace(&#39;_&#39;, &#39; &#39;).title()}&#34;)

        # Extract the actual assignments DataFrame from the nested structure
        if isinstance(assignments_data, dict) and &#34;assignments&#34; in assignments_data:
            assignments_df = assignments_data[&#34;assignments&#34;]
        else:
            assignments_df = assignments_data

        # Show assignment data
        if assignments_df is not None and hasattr(assignments_df, &#39;head&#39;):
            st.markdown(&#34;**Branch Assignments:**&#34;)
            st.dataframe(assignments_df.head(20), width=&#39;stretch&#39;)

            if len(assignments_df) &gt; 20:
                st.info(f&#34;Showing first 20 of {len(assignments_df)} assignments&#34;)

            # Show assignment statistics
            if &#39;branch&#39; in assignments_df.columns:
                branch_counts = assignments_df[&#39;branch&#39;].value_counts()
                st.markdown(&#34;**Branch Distribution:**&#34;)
                st.bar_chart(branch_counts)

                # Show detailed statistics
                st.markdown(&#34;**Assignment Statistics:**&#34;)
                total_trajectories = len(assignments_df)
                for branch, count in branch_counts.items():
                    percentage = (count / total_trajectories) * 100
                    st.write(f&#34;- Branch {branch}: {count} trajectories ({percentage:.1f}%)&#34;)
        else:
            st.info(f&#34;No assignment data available for {junction_key}&#34;)</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.render_conditional_probabilities"><code class="name flex">
<span>def <span class="ident">render_conditional_probabilities</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Render conditional probabilities</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render_conditional_probabilities(self):
    &#34;&#34;&#34;Render conditional probabilities&#34;&#34;&#34;
    st.markdown(&#34;### Conditional Probabilities&#34;)

    if &#34;conditional_probabilities&#34; in st.session_state.analysis_results:
        cond_probs = st.session_state.analysis_results[&#34;conditional_probabilities&#34;]

        # Create a DataFrame for better display
        df_data = []
        for junction_key, probs in cond_probs.items():
            junction_num = junction_key.split(&#39;_&#39;)[1] if &#39;_&#39; in junction_key else junction_key[1:]
            for origin, dest_probs in probs.items():
                for dest, prob in dest_probs.items():
                    df_data.append({
                        &#39;Junction&#39;: f&#39;J{junction_num}&#39;,
                        &#39;From&#39;: origin,
                        &#39;To&#39;: dest,
                        &#39;Probability&#39;: f&#34;{prob:.1%}&#34;
                    })

        if df_data:
            df = pd.DataFrame(df_data)
            st.dataframe(df, width=&#39;stretch&#39;)
        else:
            st.info(&#34;No conditional probabilities available&#34;)</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.render_data_upload"><code class="name flex">
<span>def <span class="ident">render_data_upload</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Render the data upload interface</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render_data_upload(self):
    &#34;&#34;&#34;Render the data upload interface&#34;&#34;&#34;
    st.markdown(&#39;&lt;h2 class=&#34;section-header&#34;&gt;📁 Data Upload&lt;/h2&gt;&#39;, unsafe_allow_html=True)

    col1, col2 = st.columns([2, 1])

    with col1:
        st.markdown(&#34;### Upload Trajectory Data&#34;)

        # File upload
        uploaded_files = st.file_uploader(
            &#34;Choose CSV files&#34;,
            type=[&#39;csv&#39;],
            accept_multiple_files=True,
            help=&#34;Upload one or more CSV files containing trajectory data&#34;
        )

        # Folder path input
        st.markdown(&#34;### Or specify folder path&#34;)
        folder_path = st.text_input(
            &#34;Folder path:&#34;,
            value=&#34;&#34;,
            help=&#34;Path to folder containing CSV files&#34;
        )

        # Column mapping
        st.markdown(&#34;### Column Mapping&#34;)
        col_x, col_z, col_t = st.columns(3)

        with col_x:
            x_col = st.text_input(&#34;X Column:&#34;, value=&#34;Headset.Head.Position.X&#34;)
        with col_z:
            z_col = st.text_input(&#34;Z Column:&#34;, value=&#34;Headset.Head.Position.Z&#34;)
        with col_t:
            t_col = st.text_input(&#34;Time Column:&#34;, value=&#34;Time&#34;)

        # New: Gaze/Physiology column mapping now lives here
        with st.expander(&#34;🔧 Gaze/Physiology Column Mapping&#34;, expanded=False):
            col_g1, col_g2 = st.columns(2)
            with col_g1:
                head_forward_x_col = st.text_input(&#34;Head Forward X&#34;, value=st.session_state.gaze_column_mappings.get(&#39;head_forward_x&#39;, &#39;Headset.Head.Forward.X&#39;))
                head_forward_z_col = st.text_input(&#34;Head Forward Z&#34;, value=st.session_state.gaze_column_mappings.get(&#39;head_forward_z&#39;, &#39;Headset.Head.Forward.Z&#39;))
                gaze_x_map = st.text_input(&#34;Gaze X&#34;, value=st.session_state.gaze_column_mappings.get(&#39;gaze_x&#39;, &#39;Headset.Gaze.X&#39;))
                gaze_y_map = st.text_input(&#34;Gaze Y&#34;, value=st.session_state.gaze_column_mappings.get(&#39;gaze_y&#39;, &#39;Headset.Gaze.Y&#39;))
            with col_g2:
                pupil_l_map = st.text_input(&#34;Pupil Left&#34;, value=st.session_state.gaze_column_mappings.get(&#39;pupil_l&#39;, &#39;Headset.PupilDilation.L&#39;))
                pupil_r_map = st.text_input(&#34;Pupil Right&#34;, value=st.session_state.gaze_column_mappings.get(&#39;pupil_r&#39;, &#39;Headset.PupilDilation.R&#39;))
                heart_rate_map = st.text_input(&#34;Heart Rate&#34;, value=st.session_state.gaze_column_mappings.get(&#39;heart_rate&#39;, &#39;Headset.HeartRate&#39;))

            st.session_state.gaze_column_mappings = {
                &#39;head_forward_x&#39;: head_forward_x_col.strip(),
                &#39;head_forward_z&#39;: head_forward_z_col.strip(),
                &#39;gaze_x&#39;: gaze_x_map.strip(),
                &#39;gaze_y&#39;: gaze_y_map.strip(),
                &#39;pupil_l&#39;: pupil_l_map.strip(),
                &#39;pupil_r&#39;: pupil_r_map.strip(),
                &#39;heart_rate&#39;: heart_rate_map.strip(),
            }

        # Analysis parameters
        st.markdown(&#34;### Analysis Parameters&#34;)
        col_scale, col_threshold = st.columns(2)

        with col_scale:
            scale = st.number_input(&#34;Scale Factor:&#34;, value=st.session_state.get(&#34;scale_factor&#34;, 0.2), min_value=0.01, max_value=1.0, step=0.01)
            st.session_state.scale_factor = scale  # Store scale factor in session state
        with col_threshold:
            motion_threshold = st.number_input(&#34;Motion Threshold:&#34;, value=0.1, min_value=0.01, max_value=1.0, step=0.01)

    with col2:
        st.markdown(&#34;### Quick Actions&#34;)

        if st.button(&#34;🔄 Load Data&#34;, type=&#34;primary&#34;):
            if uploaded_files and folder_path.strip():
                # Both provided - show warning and ask user to choose
                st.warning(&#34;⚠️ **Both file uploads and folder path are specified.**&#34;)
                st.info(&#34;**Current behavior:** File uploads will be processed (folder path will be ignored).&#34;)
                st.info(&#34;**To use folder path instead:** Clear the file uploads and click &#39;Load Data&#39; again.&#34;)

                # Process uploaded files (current behavior)
                self.load_uploaded_files(uploaded_files, x_col, z_col, t_col, scale, motion_threshold)
            elif uploaded_files:
                # Process uploaded files
                self.load_uploaded_files(uploaded_files, x_col, z_col, t_col, scale, motion_threshold)
            elif folder_path.strip():
                # Process folder path
                self.load_trajectory_data(folder_path, x_col, z_col, t_col, scale, motion_threshold)
            else:
                st.warning(&#34;⚠️ Please upload files or specify a folder path&#34;)

        if st.button(&#34;📋 Load Sample Data&#34;):
            self.load_sample_data()

        if st.session_state.trajectories:
            st.markdown(&#34;### Data Summary&#34;)
            st.write(f&#34;**Trajectories loaded:** {len(st.session_state.trajectories)}&#34;)

            if len(st.session_state.trajectories) &gt; 0:
                sample_traj = st.session_state.trajectories[0]
                st.write(f&#34;**Sample trajectory points:** {len(sample_traj.x)}&#34;)
                st.write(f&#34;**X range:** {min(sample_traj.x):.1f} to {max(sample_traj.x):.1f}&#34;)
                st.write(f&#34;**Z range:** {min(sample_traj.z):.1f} to {max(sample_traj.z):.1f}&#34;)</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.render_discover_visualizations"><code class="name flex">
<span>def <span class="ident">render_discover_visualizations</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Render discover analysis visualizations</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render_discover_visualizations(self):
    &#34;&#34;&#34;Render discover analysis visualizations&#34;&#34;&#34;
    st.markdown(&#34;### Discover Analysis Results&#34;)

    # Display decision intercepts for each junction
    for junction_key, branches_data in st.session_state.analysis_results[&#34;branches&#34;].items():
        if junction_key == &#34;chain_decisions&#34;:  # Skip the chain decisions data
            continue

        st.markdown(f&#34;#### {junction_key.replace(&#39;_&#39;, &#39; &#39;).title()}&#34;)

        # Show decision intercepts plot
        junction_num = junction_key.split(&#39;_&#39;)[1]
        junction_dir = os.path.join(&#34;gui_outputs&#34;, f&#34;junction_{junction_num}&#34;)

        # Display available plots
        intercepts_path = os.path.join(junction_dir, &#34;Decision_Intercepts.png&#34;)
        if os.path.exists(intercepts_path):
            st.image(intercepts_path, caption=f&#34;Decision Intercepts - {junction_key}&#34;, width=&#39;stretch&#39;)
        else:
            st.warning(f&#34;Decision intercepts plot not found for {junction_key}&#34;)

        # Check for other available plots that might be generated
        other_plots = [
            (&#34;Decision_Map.png&#34;, &#34;Decision Map&#34;),
            (&#34;Branch_Counts.png&#34;, &#34;Branch Counts&#34;),
            (&#34;Branch_Directions.png&#34;, &#34;Branch Directions&#34;)
        ]

        for plot_file, plot_name in other_plots:
            plot_path = os.path.join(junction_dir, plot_file)
            if os.path.exists(plot_path):
                st.image(plot_path, caption=f&#34;{plot_name} - {junction_key}&#34;, width=&#39;stretch&#39;)

        # Show branch summary
        if &#34;summary&#34; in branches_data and branches_data[&#34;summary&#34;] is not None:
            st.markdown(&#34;**Branch Summary:**&#34;)
            st.dataframe(branches_data[&#34;summary&#34;], width=&#39;stretch&#39;)

        # Show assignments preview
        if &#34;assignments&#34; in branches_data and branches_data[&#34;assignments&#34;] is not None:
            st.markdown(&#34;**Branch Assignments (first 20):**&#34;)
            st.dataframe(branches_data[&#34;assignments&#34;].head(20), width=&#39;stretch&#39;)</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.render_enhanced_visualizations"><code class="name flex">
<span>def <span class="ident">render_enhanced_visualizations</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Render enhanced analysis visualizations</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render_enhanced_visualizations(self):
    &#34;&#34;&#34;Render enhanced analysis visualizations&#34;&#34;&#34;
    st.markdown(&#34;### 🚨 Enhanced Analysis Results&#34;)

    # Check if enhanced analysis results exist
    if (st.session_state.analysis_results is None or
        &#34;enhanced&#34; not in st.session_state.analysis_results):
        st.info(&#34;No enhanced analysis results available. Run enhanced analysis first.&#34;)
        return

    enhanced_data = st.session_state.analysis_results[&#34;enhanced&#34;]

    # Create tabs for different analysis components
    tab1, tab2, tab3, tab4 = st.tabs([&#34;🚨 Evacuation Analysis&#34;, &#34;💡 Recommendations&#34;, &#34;⚠️ Risk Assessment&#34;, &#34;📊 Efficiency Metrics&#34;])

    with tab1:
        self._render_evacuation_analysis(enhanced_data[&#34;evacuation_analysis&#34;])

    with tab2:
        self._render_recommendations(enhanced_data[&#34;recommendations&#34;])

    with tab3:
        self._render_risk_assessment(enhanced_data[&#34;risk_assessment&#34;])

    with tab4:
        self._render_efficiency_metrics(enhanced_data[&#34;efficiency_metrics&#34;])</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.render_export"><code class="name flex">
<span>def <span class="ident">render_export</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Render the export interface</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render_export(self):
    &#34;&#34;&#34;Render the export interface&#34;&#34;&#34;
    st.markdown(&#39;&lt;h2 class=&#34;section-header&#34;&gt;💾 Export Results&lt;/h2&gt;&#39;, unsafe_allow_html=True)

    if not st.session_state.analysis_results:
        st.warning(&#34;⚠️ Please run an analysis first&#34;)
        return

    st.markdown(&#34;### Export Options&#34;)

    # Export format selection
    export_format = st.selectbox(
        &#34;Export Format:&#34;,
        [&#34;JSON&#34;, &#34;CSV&#34;, &#34;ZIP Archive&#34;],
        help=&#34;Select the format for exporting results&#34;
    )

    if st.button(&#34;📥 Export Results&#34;):
        self.export_results(export_format)</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.render_flow_graphs"><code class="name flex">
<span>def <span class="ident">render_flow_graphs</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Render flow graph visualizations</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render_flow_graphs(self):
    &#34;&#34;&#34;Render flow graph visualizations&#34;&#34;&#34;
    st.markdown(&#34;### Flow Graphs&#34;)

    col1, col2 = st.columns(2)

    with col1:
        st.markdown(&#34;#### Overall Flow Graph&#34;)
        if &#34;flow_graph_map&#34; in st.session_state.analysis_results:
            st.image(st.session_state.analysis_results[&#34;flow_graph_map&#34;], width=&#39;stretch&#39;)

    with col2:
        st.markdown(&#34;#### Per-Junction Flow Graph&#34;)
        if &#34;per_junction_flow_graph&#34; in st.session_state.analysis_results:
            st.image(st.session_state.analysis_results[&#34;per_junction_flow_graph&#34;], width=&#39;stretch&#39;)</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.render_gaze_visualizations"><code class="name flex">
<span>def <span class="ident">render_gaze_visualizations</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Render gaze analysis visualizations</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render_gaze_visualizations(self):
    &#34;&#34;&#34;Render gaze analysis visualizations&#34;&#34;&#34;
    st.markdown(&#34;### Gaze and Physiological Analysis Results&#34;)

    # Check if analysis results and gaze_results exist
    if (st.session_state.analysis_results is None or
        &#34;gaze_results&#34; not in st.session_state.analysis_results):
        st.info(&#34;No gaze analysis results available. Run gaze analysis first.&#34;)
        return

    # Display gaze results for each junction
    for junction_key, gaze_data in st.session_state.analysis_results[&#34;gaze_results&#34;].items():
        st.markdown(f&#34;#### {junction_key.replace(&#39;_&#39;, &#39; &#39;).title()}&#34;)

        if gaze_data is None:
            st.info(&#34;No gaze analysis data available for this junction&#34;)
            continue

        # Check if we have comprehensive gaze data or fallback data
        if isinstance(gaze_data, dict):
            if &#39;error&#39; in gaze_data:
                # Show error information
                st.error(f&#34;❌ **Gaze Analysis Failed for {junction_key}**&#34;)
                st.write(f&#34;**Error:** {gaze_data[&#39;error&#39;]}&#34;)
                st.write(f&#34;**Error Type:** {gaze_data[&#39;error_type&#39;]}&#34;)

                # Show suggestions based on error type
                if &#34;No assignments found&#34; in gaze_data[&#39;error&#39;]:
                    st.info(&#34;💡 **Solution:** Run &#39;🔍 Discover Branches&#39; analysis first to create proper assignments&#34;)
                elif &#34;trajectory&#34; in gaze_data[&#39;error&#39;].lower():
                    st.info(&#34;💡 **Solution:** Check if trajectories actually pass through this junction&#34;)
                elif &#34;column&#34; in gaze_data[&#39;error&#39;].lower():
                    st.info(&#34;💡 **Solution:** Check your gaze column mappings in the Data tab&#34;)

                # Show empty plots with error messages
                self._render_error_gaze_results(gaze_data, junction_key)
            elif &#39;physiological&#39; in gaze_data:
                # Comprehensive gaze analysis results
                self._render_comprehensive_gaze_results(gaze_data, junction_key)
            else:
                # Fallback movement pattern results
                st.warning(&#34;⚠️ Using fallback visualization&#34;)
                self._render_fallback_gaze_results(gaze_data, junction_key)
        else:
            # Fallback movement pattern results
            st.warning(&#34;⚠️ Using fallback visualization&#34;)
            self._render_fallback_gaze_results(gaze_data, junction_key)</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.render_header"><code class="name flex">
<span>def <span class="ident">render_header</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Render the main header</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render_header(self):
    &#34;&#34;&#34;Render the main header&#34;&#34;&#34;
    st.markdown(&#39;&lt;h1 class=&#34;main-header&#34;&gt;🗺️ VERTA&lt;/h1&gt;&#39;, unsafe_allow_html=True)
    st.markdown(&#34;&#34;&#34;
    &lt;div style=&#34;text-align: center; color: #666; margin-bottom: 2rem;&#34;&gt;
        Interactive analysis tool for VR trajectory data and junction-based choice prediction
    &lt;/div&gt;
    &#34;&#34;&#34;, unsafe_allow_html=True)
    # Show any pending flash message at the very top
    self._show_flash()</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.render_intent_visualizations"><code class="name flex">
<span>def <span class="ident">render_intent_visualizations</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Render intent recognition analysis visualizations</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render_intent_visualizations(self):
    &#34;&#34;&#34;Render intent recognition analysis visualizations&#34;&#34;&#34;
    st.markdown(&#34;### 🧠 Intent Recognition Results&#34;)

    if (st.session_state.analysis_results is None or
        &#34;intent_recognition&#34; not in st.session_state.analysis_results):
        st.info(&#34;No intent recognition results available.&#34;)
        return

    intent_data = st.session_state.analysis_results[&#34;intent_recognition&#34;]

    # Get successful junctions
    successful_junctions = {k: v for k, v in intent_data.items() if &#39;error&#39; not in v}

    if not successful_junctions:
        st.warning(&#34;⚠️ No successful intent recognition results to visualize&#34;)
        return

    # Junction selector
    junction_keys = list(successful_junctions.keys())
    if len(junction_keys) &gt; 1:
        selected_junction = st.selectbox(
            &#34;Select Junction:&#34;,
            junction_keys,
            format_func=lambda x: f&#34;Junction {x.replace(&#39;junction_&#39;, &#39;&#39;)}&#34;
        )
    else:
        selected_junction = junction_keys[0]

    junction_results = successful_junctions[selected_junction]
    junction_num = selected_junction.replace(&#39;junction_&#39;, &#39;&#39;)

    # Summary metrics
    st.markdown(f&#34;#### Junction {junction_num} Summary&#34;)

    models_trained = junction_results[&#39;training_results&#39;].get(&#39;models_trained&#39;, {})

    if models_trained:
        # Create metrics row
        cols = st.columns(len(models_trained))
        for idx, (dist, model_info) in enumerate(sorted(models_trained.items())):
            with cols[idx]:
                st.metric(
                    f&#34;{dist} units&#34;,
                    f&#34;{model_info[&#39;cv_mean_accuracy&#39;]:.1%}&#34;,
                    f&#34;n={model_info[&#39;n_samples&#39;]}&#34;
                )

        # Overall accuracy
        avg_acc = np.mean([m[&#39;cv_mean_accuracy&#39;] for m in models_trained.values()])
        st.markdown(f&#34;**Average Accuracy:** {avg_acc:.1%}&#34;)

        # Interpretation
        if avg_acc &gt; 0.85:
            st.success(&#34;🟢 Excellent Predictability&#34;)
        elif avg_acc &gt; 0.70:
            st.info(&#34;🟡 Good Predictability&#34;)
        else:
            st.warning(&#34;🔴 Moderate Predictability&#34;)

    # Feature Importance Plot
    st.markdown(&#34;#### Feature Importance&#34;)
    feature_importance_path = os.path.join(&#34;gui_outputs&#34;, &#34;intent_recognition&#34;,
                                           f&#34;junction_{junction_num}&#34;,
                                           &#34;intent_feature_importance.png&#34;)
    if os.path.exists(feature_importance_path):
        st.image(feature_importance_path, width=&#39;stretch&#39;)
    else:
        st.info(&#34;Feature importance plot not available&#34;)

    # Accuracy Analysis Plot
    st.markdown(&#34;#### Prediction Accuracy vs Distance&#34;)
    accuracy_path = os.path.join(&#34;gui_outputs&#34;, &#34;intent_recognition&#34;,
                                 f&#34;junction_{junction_num}&#34;,
                                 &#34;intent_accuracy_analysis.png&#34;)
    if os.path.exists(accuracy_path):
        st.image(accuracy_path, width=&#39;stretch&#39;)
        st.caption(&#34;This shows how prediction accuracy improves as users get closer to the junction&#34;)
    else:
        st.info(&#34;Accuracy analysis plot not available&#34;)

    # Test Predictions
    if &#39;test_predictions&#39; in junction_results:
        st.markdown(&#34;#### Sample Predictions&#34;)

        test_preds = junction_results[&#39;test_predictions&#39;]

        # Show a few example predictions
        example_count = min(5, len(test_preds))

        for traj_id in list(test_preds.keys())[:example_count]:
            pred_info = test_preds[traj_id]
            actual = pred_info[&#39;actual_branch&#39;]

            with st.expander(f&#34;Trajectory: {traj_id} (Actual: Branch {actual})&#34;):
                predictions = pred_info[&#39;predictions_by_distance&#39;]

                # Create visualization
                distances = []
                predicted_branches = []
                confidences = []
                correct_flags = []

                for dist in sorted(predictions.keys(), reverse=True):
                    p = predictions[dist]
                    distances.append(f&#34;{dist}u&#34;)
                    predicted_branches.append(f&#34;Branch {p[&#39;predicted_branch&#39;]}&#34;)
                    confidences.append(p[&#39;confidence&#39;])
                    correct_flags.append(&#34;✓&#34; if p[&#39;correct&#39;] else &#34;✗&#34;)

                # Create DataFrame
                pred_df = pd.DataFrame({
                    &#39;Distance Before&#39;: distances,
                    &#39;Predicted&#39;: predicted_branches,
                    &#39;Confidence&#39;: [f&#34;{c:.1%}&#34; for c in confidences],
                    &#39;Correct&#39;: correct_flags
                })

                st.dataframe(pred_df, width=&#39;stretch&#39;)

                # Confidence chart
                import plotly.graph_objects as go

                fig = go.Figure()
                fig.add_trace(go.Scatter(
                    x=[float(d.replace(&#39;u&#39;, &#39;&#39;)) for d in distances],
                    y=confidences,
                    mode=&#39;lines+markers&#39;,
                    name=&#39;Confidence&#39;,
                    line=dict(color=&#39;blue&#39;, width=3),
                    marker=dict(size=10)
                ))
                fig.update_layout(
                    title=&#34;Prediction Confidence Over Distance&#34;,
                    xaxis_title=&#34;Distance to Junction (units)&#34;,
                    yaxis_title=&#34;Confidence&#34;,
                    yaxis_range=[0, 1],
                    height=300
                )
                st.plotly_chart(fig, width=&#39;stretch&#39;, key=f&#34;intent_confidence_{junction_num}_{traj_id}&#34;)

    # Feature importance table
    if &#39;feature_importance&#39; in junction_results[&#39;training_results&#39;]:
        st.markdown(&#34;#### Feature Importance (Detailed)&#34;)

        with st.expander(&#34;View Feature Importance by Distance&#34;):
            feature_imp = junction_results[&#39;training_results&#39;][&#39;feature_importance&#39;]

            for dist in sorted(feature_imp.keys()):
                st.markdown(f&#34;**{dist} units before junction:**&#34;)

                importance_dict = feature_imp[dist]
                sorted_features = sorted(importance_dict.items(),
                                       key=lambda x: x[1], reverse=True)

                feat_df = pd.DataFrame(sorted_features[:10],
                                      columns=[&#39;Feature&#39;, &#39;Importance&#39;])
                feat_df[&#39;Importance&#39;] = feat_df[&#39;Importance&#39;].apply(lambda x: f&#34;{x:.3f}&#34;)

                st.dataframe(feat_df, width=&#39;stretch&#39;)
                st.markdown(&#34;---&#34;)

    # Download results
    st.markdown(&#34;#### Download Results&#34;)

    results_path = os.path.join(&#34;gui_outputs&#34;, &#34;intent_recognition&#34;,
                                f&#34;junction_{junction_num}&#34;,
                                &#34;intent_training_results.json&#34;)

    if os.path.exists(results_path):
        with open(results_path, &#39;r&#39;) as f:
            results_json = f.read()

        st.download_button(
            label=&#34;📥 Download Training Results (JSON)&#34;,
            data=results_json,
            file_name=f&#34;intent_recognition_junction_{junction_num}.json&#34;,
            mime=&#34;application/json&#34;
        )

    # Explanation
    with st.expander(&#34;ℹ️ Understanding Intent Recognition&#34;):
        st.markdown(&#34;&#34;&#34;
        **Intent Recognition** predicts which route users will choose **before** they reach decision points.

        **Key Insights:**
        - **Higher accuracy at closer distances**: Predictions improve as users approach junctions
        - **Feature importance**: Shows which trajectory features best predict choices
        - **Early prediction**: Enables proactive systems that respond before users act

        **Applications:**
        - 🗺️ Proactive wayfinding and navigation hints
        - 🎨 Adaptive UI that highlights likely options
        - 🚦 Congestion prediction and traffic management
        - ⚠️ Anomaly detection (unexpected behavior)
        - ⚡ Performance optimization (asset preloading)

        **Accuracy Interpretation:**
        - **&gt;85%**: Excellent - Highly predictable behavior
        - **70-85%**: Good - Clear patterns exist
        - **&lt;70%**: Moderate - Variable or exploratory behavior
        &#34;&#34;&#34;)</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.render_junction_editor"><code class="name flex">
<span>def <span class="ident">render_junction_editor</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Render the interactive junction editor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render_junction_editor(self):
    &#34;&#34;&#34;Render the interactive junction editor&#34;&#34;&#34;
    st.markdown(&#39;&lt;h2 class=&#34;section-header&#34;&gt;🎯 Junction Editor&lt;/h2&gt;&#39;, unsafe_allow_html=True)

    col1, col2 = st.columns([2, 1])

    with col1:
        st.markdown(&#34;### Interactive Junction Management&#34;)

        # Instructions
        st.info(&#34;&#34;&#34;
        **How to use:**
        1. **Add junctions** using the controls on the right
        2. **Edit existing junctions** by changing position, radius, or r_outer values
        3. **Hover over junctions** on the map to see their properties
        4. **Delete junctions** using the 🗑️ button
        &#34;&#34;&#34;)

        # Create interactive plot for junction editing
        if st.session_state.trajectories:
            self.render_junction_plot()
        else:
            st.warning(&#34;⚠️ Please load trajectory data first&#34;)

    with col2:
        st.markdown(&#34;### Junction Controls&#34;)

        # Add new junction section (always visible)
        with st.container():
            st.markdown(&#34;#### Add New Junction&#34;)
            col_x, col_z = st.columns(2)

            with col_x:
                new_x = st.number_input(&#34;X Position:&#34;, value=500.0, step=10.0)
            with col_z:
                new_z = st.number_input(&#34;Z Position:&#34;, value=300.0, step=10.0)

            col_radius, col_r_outer = st.columns(2)
            with col_radius:
                new_radius = st.number_input(&#34;Radius:&#34;, value=30.0, min_value=5.0, max_value=100.0, step=5.0)
            with col_r_outer:
                new_r_outer = st.number_input(&#34;R Outer:&#34;, value=50.0, min_value=10.0, max_value=200.0, step=5.0)

            if st.button(&#34;➕ Add Junction&#34;):
                new_junction = Circle(cx=new_x, cz=new_z, r=new_radius)
                st.session_state.junctions.append(new_junction)
                st.session_state.junction_r_outer[len(st.session_state.junctions)-1] = new_r_outer

                # Update junction state hash to force UI refresh
                st.session_state.junction_state_hash += 1

                st.success(f&#34;Added junction at ({new_x}, {new_z}) with r_outer={new_r_outer}&#34;)
                st.rerun()

        st.markdown(&#34;---&#34;)

        # Bulk operations (always visible)
        st.markdown(&#34;#### Bulk Operations&#34;)
        col_clear, col_sample = st.columns(2)

        with col_clear:
            if st.button(&#34;🗑️ Clear All&#34;):
                st.session_state.junctions = []
                st.session_state.junction_r_outer = {}

                # Update junction state hash to force UI refresh
                st.session_state.junction_state_hash += 1

                st.rerun()

        with col_sample:
            if st.button(&#34;📋 Load Sample&#34;):
                self.load_sample_junctions()

        st.markdown(&#34;---&#34;)

        # Scrollable junction list
        st.markdown(&#34;#### Current Junctions&#34;)
        if st.session_state.junctions:
            # Create a scrollable container for the junction list
            st.markdown(f&#34;**Total Junctions: {len(st.session_state.junctions)}**&#34;)
            with st.container():
                st.markdown(&#39;&lt;div class=&#34;junction-list-container&#34;&gt;&#39;, unsafe_allow_html=True)

                # Use a scrollable area for the junction list
                for i, junction in enumerate(st.session_state.junctions):
                    with st.expander(f&#34;Junction {i} - ({junction.cx:.1f}, {junction.cz:.1f})&#34;, expanded=False):
                        # Junction info and delete button
                        col_del, col_info = st.columns([1, 4])

                        with col_del:
                            if st.button(&#34;🗑️&#34;, key=f&#34;del_{i}&#34;, help=&#34;Delete this junction&#34;):
                                # Store the deleted junction info for debugging
                                deleted_junction = st.session_state.junctions[i]

                                # Remove the junction from the list
                                st.session_state.junctions.pop(i)

                                # Remove the corresponding r_outer entry
                                if i in st.session_state.junction_r_outer:
                                    del st.session_state.junction_r_outer[i]

                                # Reindex remaining junctions and r_outer values
                                new_r_outer = {}
                                for j, junction in enumerate(st.session_state.junctions):
                                    old_idx = j + (1 if j &gt;= i else 0)
                                    if old_idx in st.session_state.junction_r_outer:
                                        new_r_outer[j] = st.session_state.junction_r_outer[old_idx]
                                st.session_state.junction_r_outer = new_r_outer

                                # Show success message
                                st.success(f&#34;Deleted Junction {i} at ({deleted_junction.cx:.1f}, {deleted_junction.cz:.1f})&#34;)

                                # Update junction state hash to force UI refresh
                                st.session_state.junction_state_hash += 1

                                # Force a complete rerun to refresh all UI elements
                                st.rerun()

                        with col_info:
                            st.write(f&#34;Position: ({junction.cx:.1f}, {junction.cz:.1f})&#34;)
                            st.write(f&#34;Radius: {junction.r}&#34;)
                            st.write(f&#34;R_outer: {st.session_state.junction_r_outer.get(i, 50.0)}&#34;)

                        # Position editing
                        st.markdown(&#34;**Edit Position:**&#34;)
                        col_x_edit, col_z_edit = st.columns(2)

                        with col_x_edit:
                            new_x = st.number_input(
                                f&#34;X:&#34;,
                                value=float(junction.cx),
                                step=1.0,
                                key=f&#34;x_edit_{i}&#34;
                            )

                        with col_z_edit:
                            new_z = st.number_input(
                                f&#34;Z:&#34;,
                                value=float(junction.cz),
                                step=1.0,
                                key=f&#34;z_edit_{i}&#34;
                            )

                        # Radius editing
                        new_radius = st.number_input(
                            f&#34;Radius:&#34;,
                            value=float(junction.r),
                            min_value=5.0,
                            max_value=100.0,
                            step=1.0,
                            key=f&#34;radius_edit_{i}&#34;
                        )

                        # R_outer control
                        current_r_outer = st.session_state.junction_r_outer.get(i, 50.0)
                        new_r_outer = st.number_input(
                            f&#34;R Outer:&#34;,
                            value=current_r_outer,
                            min_value=10.0,
                            max_value=200.0,
                            step=5.0,
                            key=f&#34;r_outer_{i}&#34;
                        )

                        # Update junction if any values changed
                        if (new_x != junction.cx or new_z != junction.cz or
                            new_radius != junction.r or new_r_outer != current_r_outer):

                            # Update junction
                            st.session_state.junctions[i] = Circle(cx=new_x, cz=new_z, r=new_radius)
                            st.session_state.junction_r_outer[i] = new_r_outer

                            # Update junction state hash to force UI refresh
                            st.session_state.junction_state_hash += 1

                            st.rerun()

                st.markdown(&#39;&lt;/div&gt;&#39;, unsafe_allow_html=True)
        else:
            st.info(&#34;No junctions defined yet&#34;)</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.render_junction_plot"><code class="name flex">
<span>def <span class="ident">render_junction_plot</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Render interactive plot for junction editing</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render_junction_plot(self):
    &#34;&#34;&#34;Render interactive plot for junction editing&#34;&#34;&#34;
    if not st.session_state.trajectories:
        return

    # Create plot
    fig = go.Figure()

    # Add ALL trajectories (with sampling for performance)
    all_trajectories = st.session_state.trajectories
    for i, traj in enumerate(all_trajectories):
        # Sample every 20th point for performance with many trajectories
        sample_rate = max(1, len(traj.x) // 1000)  # Adaptive sampling
        fig.add_trace(go.Scatter(
            x=traj.x[::sample_rate],
            y=traj.z[::sample_rate],
            mode=&#39;lines&#39;,
            line=dict(color=&#39;lightgray&#39;, width=0.5),
            name=f&#39;Trajectory {i}&#39;,
            showlegend=False,
            opacity=0.6
        ))

    # Add junctions with r_outer circles
    for i, junction in enumerate(st.session_state.junctions):
        # Get r_outer for this junction
        r_outer = st.session_state.junction_r_outer.get(i, 50.0)

        # Junction center with hover info
        fig.add_trace(go.Scatter(
            x=[junction.cx],
            y=[junction.cz],
            mode=&#39;markers&#39;,
            marker=dict(size=20, color=&#39;red&#39;, symbol=&#39;circle&#39;),
            name=f&#39;J{i}&#39;,
            text=f&#39;J{i}&lt;br&gt;Pos: ({junction.cx:.1f}, {junction.cz:.1f})&lt;br&gt;Radius: {junction.r}&lt;br&gt;R_outer: {r_outer}&#39;,
            textposition=&#39;middle center&#39;,
            textfont=dict(color=&#39;white&#39;, size=14, family=&#34;Arial Black&#34;),
            hovertemplate=f&#39;&lt;b&gt;Junction {i}&lt;/b&gt;&lt;br&gt;&#39; +
                         f&#39;Position: ({junction.cx:.1f}, {junction.cz:.1f})&lt;br&gt;&#39; +
                         f&#39;Radius: {junction.r}&lt;br&gt;&#39; +
                         f&#39;R_outer: {r_outer}&lt;br&gt;&#39; +
                         &#39;&lt;extra&gt;&lt;/extra&gt;&#39;
        ))

        # Junction radius circle (decision radius)
        theta = np.linspace(0, 2*np.pi, 100)
        circle_x = junction.cx + junction.r * np.cos(theta)
        circle_z = junction.cz + junction.r * np.sin(theta)

        fig.add_trace(go.Scatter(
            x=circle_x,
            y=circle_z,
            mode=&#39;lines&#39;,
            line=dict(color=&#39;orange&#39;, width=3),
            showlegend=False,
            hovertemplate=f&#39;&lt;b&gt;Junction {i} - Decision Radius&lt;/b&gt;&lt;br&gt;&#39; +
                         f&#39;Position: ({junction.cx:.1f}, {junction.cz:.1f})&lt;br&gt;&#39; +
                         f&#39;Decision Radius: {junction.r}&lt;br&gt;&#39; +
                         f&#39;R_outer: {r_outer}&lt;br&gt;&#39; +
                         &#39;&lt;extra&gt;&lt;/extra&gt;&#39;,
            name=f&#39;J{i} Decision Radius&#39;
        ))

        # R_outer circle (analysis radius)
        r_outer_x = junction.cx + r_outer * np.cos(theta)
        r_outer_z = junction.cz + r_outer * np.sin(theta)

        fig.add_trace(go.Scatter(
            x=r_outer_x,
            y=r_outer_z,
            mode=&#39;lines&#39;,
            line=dict(color=&#39;blue&#39;, width=2, dash=&#39;dash&#39;),
            showlegend=False,
            hovertemplate=f&#39;&lt;b&gt;Junction {i} - Analysis Radius&lt;/b&gt;&lt;br&gt;&#39; +
                         f&#39;Position: ({junction.cx:.1f}, {junction.cz:.1f})&lt;br&gt;&#39; +
                         f&#39;Decision Radius: {junction.r}&lt;br&gt;&#39; +
                         f&#39;R_outer: {r_outer}&lt;br&gt;&#39; +
                         &#39;&lt;extra&gt;&lt;/extra&gt;&#39;,
            name=f&#39;J{i} R_outer&#39;
        ))

        # Add invisible junction area for better hover capture
        # Create a filled circle area that will capture hover events
        theta_dense = np.linspace(0, 2*np.pi, 200)
        area_x = junction.cx + r_outer * np.cos(theta_dense)
        area_z = junction.cz + r_outer * np.sin(theta_dense)

        fig.add_trace(go.Scatter(
            x=area_x,
            y=area_z,
            mode=&#39;lines&#39;,
            fill=&#39;toself&#39;,
            fillcolor=&#39;rgba(255, 0, 0, 0.05)&#39;,  # Very light red fill
            line=dict(width=0),  # No visible line
            showlegend=False,
            hovertemplate=f&#39;&lt;b&gt;Junction {i} Area&lt;/b&gt;&lt;br&gt;&#39; +
                         f&#39;Position: ({junction.cx:.1f}, {junction.cz:.1f})&lt;br&gt;&#39; +
                         f&#39;Decision Radius: {junction.r}&lt;br&gt;&#39; +
                         f&#39;R_outer: {r_outer}&lt;br&gt;&#39; +
                         &#39;&lt;extra&gt;&lt;/extra&gt;&#39;,
            name=f&#39;J{i} Area&#39;,
            hoveron=&#39;fills&#39;  # Only show hover on filled area
        ))

    # Update layout
    fig.update_layout(
        title=&#34;Interactive Junction Editor - All Trajectories&#34;,
        xaxis_title=&#34;X Position&#34;,
        yaxis_title=&#34;Z Position&#34;,
        hovermode=&#39;closest&#39;,
        showlegend=False
    )

    # Set equal aspect ratio for both axes
    fig.update_xaxes(scaleanchor=&#34;y&#34;, scaleratio=1)
    fig.update_yaxes(scaleanchor=&#34;x&#34;, scaleratio=1)

    # Add legend manually
    fig.add_trace(go.Scatter(
        x=[None], y=[None],
        mode=&#39;markers&#39;,
        marker=dict(size=10, color=&#39;red&#39;),
        name=&#39;Junction Center&#39;,
        showlegend=True
    ))
    fig.add_trace(go.Scatter(
        x=[None], y=[None],
        mode=&#39;lines&#39;,
        line=dict(color=&#39;orange&#39;, width=3),
        name=&#39;Decision Radius&#39;,
        showlegend=True
    ))
    fig.add_trace(go.Scatter(
        x=[None], y=[None],
        mode=&#39;lines&#39;,
        line=dict(color=&#39;blue&#39;, width=2, dash=&#39;dash&#39;),
        name=&#39;R_outer (Analysis Radius)&#39;,
        showlegend=True
    ))
    fig.add_trace(go.Scatter(
        x=[None], y=[None],
        mode=&#39;lines&#39;,
        line=dict(color=&#39;lightgray&#39;, width=1),
        name=&#39;Trajectories&#39;,
        showlegend=True
    ))

    st.plotly_chart(fig, config={&#39;displayModeBar&#39;: True}, width=&#39;stretch&#39;)</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.render_metrics_visualizations"><code class="name flex">
<span>def <span class="ident">render_metrics_visualizations</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Render metrics analysis visualizations</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render_metrics_visualizations(self):
    &#34;&#34;&#34;Render metrics analysis visualizations&#34;&#34;&#34;
    st.markdown(&#34;### Metrics Analysis Results&#34;)

    metrics_data = st.session_state.analysis_results[&#34;metrics&#34;]
    metrics_images = st.session_state.analysis_results.get(&#34;metrics_images&#34;, {})

    if metrics_data:
        # Convert to DataFrame for better display
        import pandas as pd
        df = pd.DataFrame(metrics_data)

        # Display metrics table
        st.markdown(&#34;**Trajectory Metrics:**&#34;)
        st.dataframe(df, width=&#39;stretch&#39;)

        # Create distribution visualizations (prefer pre-generated images)
        col1, col2 = st.columns(2)

        with col1:
            st.markdown(&#34;**Total Time Distribution**&#34;)
            img = metrics_images.get(&#34;total_time_distribution.png&#34;)
            if img and os.path.exists(img):
                st.image(img, width=&#39;stretch&#39;)
            else:
                if &#39;total_time&#39; in df.columns:
                    valid_times = df[&#39;total_time&#39;].dropna()
                    if len(valid_times) &gt; 0:
                        sorted_times = valid_times.sort_values().reset_index(drop=True)
                        st.bar_chart(sorted_times)
                        st.caption(f&#34;Range: {sorted_times.min():.1f}s - {sorted_times.max():.1f}s&#34;)
                        st.caption(f&#34;Mean: {sorted_times.mean():.1f}s, Median: {sorted_times.median():.1f}s&#34;)
                    else:
                        st.info(&#34;No valid time data available&#34;)

        with col2:
            st.markdown(&#34;**Average Speed Distribution**&#34;)
            img = metrics_images.get(&#34;average_speed_distribution.png&#34;)
            if img and os.path.exists(img):
                st.image(img, width=&#39;stretch&#39;)
            else:
                if &#39;average_speed&#39; in df.columns:
                    valid_speeds = df[&#39;average_speed&#39;].dropna()
                    if len(valid_speeds) &gt; 0:
                        sorted_speeds = valid_speeds.sort_values().reset_index(drop=True)
                        st.bar_chart(sorted_speeds)
                        st.caption(f&#34;Range: {sorted_speeds.min():.2f} - {sorted_speeds.max():.2f}&#34;)
                        st.caption(f&#34;Mean: {sorted_speeds.mean():.2f}, Median: {sorted_speeds.median():.2f}&#34;)
                    else:
                        st.info(&#34;No valid speed data available&#34;)

        # Add distance visualization
        col3, col4 = st.columns(2)

        with col3:
            st.markdown(&#34;**Total Distance Distribution**&#34;)
            img = metrics_images.get(&#34;total_distance_distribution.png&#34;)
            if img and os.path.exists(img):
                st.image(img, width=&#39;stretch&#39;)
            else:
                if &#39;total_distance&#39; in df.columns:
                    valid_distances = df[&#39;total_distance&#39;].dropna()
                    if len(valid_distances) &gt; 0:
                        sorted_distances = valid_distances.sort_values().reset_index(drop=True)
                        st.bar_chart(sorted_distances)
                        st.caption(f&#34;Range: {sorted_distances.min():.1f} - {sorted_distances.max():.1f}&#34;)
                        st.caption(f&#34;Mean: {sorted_distances.mean():.1f}, Median: {sorted_distances.median():.1f}&#34;)
                    else:
                        st.info(&#34;No valid distance data available&#34;)

        with col4:
            st.markdown(&#34;**Summary Statistics**&#34;)
            if len(df) &gt; 0:
                summary_stats = {
                    &#34;Total Trajectories&#34;: len(df),
                    &#34;Avg Total Time&#34;: f&#34;{df[&#39;total_time&#39;].mean():.2f}s&#34; if &#39;total_time&#39; in df.columns else &#34;N/A&#34;,
                    &#34;Avg Total Distance&#34;: f&#34;{df[&#39;total_distance&#39;].mean():.2f}&#34; if &#39;total_distance&#39; in df.columns else &#34;N/A&#34;,
                    &#34;Avg Speed&#34;: f&#34;{df[&#39;average_speed&#39;].mean():.2f}&#34; if &#39;average_speed&#39; in df.columns else &#34;N/A&#34;
                }
                for key, value in summary_stats.items():
                    st.metric(key, value)

        # Define junction columns early for use in speed analysis
        junction_cols = [col for col in df.columns if col.startswith(&#39;junction_&#39;) and col.endswith(&#39;_time&#39;)]

        # Speed analysis visualizations
        speed_cols = [col for col in df.columns if col.startswith(&#39;junction_&#39;) and col.endswith(&#39;_speed&#39;)]
        if speed_cols:
            st.markdown(&#34;### Junction Speed Analysis&#34;)

            # Create speed analysis summary
            speed_summary = []
            for col in speed_cols:
                junction_num = col.split(&#39;_&#39;)[1]
                speed_mode_col = f&#34;junction_{junction_num}_speed_mode&#34;
                entry_speed_col = f&#34;junction_{junction_num}_entry_speed&#34;
                exit_speed_col = f&#34;junction_{junction_num}_exit_speed&#34;
                avg_transit_col = f&#34;junction_{junction_num}_avg_transit_speed&#34;

                valid_speeds = df[col].dropna()
                total_trajectories = len(df)
                valid_count = len(valid_speeds)

                if valid_count &gt; 0:
                    speed_summary.append({
                        &#34;Junction&#34;: f&#34;Junction {junction_num}&#34;,
                        &#34;Avg Speed Through&#34;: f&#34;{valid_speeds.mean():.2f}&#34;,
                        &#34;Std Speed Through&#34;: f&#34;{valid_speeds.std():.2f}&#34;,
                        &#34;Valid Count&#34;: valid_count,
                        &#34;NaN Count&#34;: total_trajectories - valid_count
                    })

            if speed_summary:
                speed_df = pd.DataFrame(speed_summary)
                st.markdown(&#34;**Junction Speed Statistics:**&#34;)
                st.dataframe(speed_df, width=&#39;stretch&#39;)

                # One concise explanation above both diagrams
                st.markdown(&#34;### Speed Analysis&#34;)
                st.info(&#34;&#34;&#34;
                **Available Speed Metrics:** Entry (2–5 s before), Exit (2–5 s after), and Average Transit (inside junction).
                Use the selector in the correlation plot to switch the speed metric.
                &#34;&#34;&#34;)

                # Show correlation (left) and entry/exit bars (right) side-by-side
                col_speed1, col_speed2 = st.columns(2)

                with col_speed1:
                    st.markdown(&#34;**Speed vs Time Correlation**&#34;)
                    img = metrics_images.get(&#34;speed_vs_time_correlation.png&#34;)
                    if img and os.path.exists(img):
                        st.image(img, width=&#39;stretch&#39;)
                    else:
                        st.info(&#34;Correlation plot not available yet. Re-run metrics analysis to generate.&#34;)

                with col_speed2:
                    st.markdown(&#34;**Entry vs Exit Speed Analysis**&#34;)
                    st.caption(&#34;**Entry Speed**: Average speed in 2-5 second window before entering junction&#34;)
                    st.caption(&#34;**Exit Speed**: Average speed in 2-5 second window after leaving junction&#34;)
                    img = metrics_images.get(&#34;entry_exit_speed_by_junction.png&#34;)
                    if img and os.path.exists(img):
                        st.image(img, width=&#39;stretch&#39;)
                    else:
                        st.info(&#34;Entry/Exit bar chart not available yet. Re-run metrics analysis to generate.&#34;)

            # Detailed speed metrics table
            st.markdown(&#34;### Detailed Speed Metrics&#34;)
            speed_detail_cols = [col for col in df.columns if &#39;speed&#39; in col.lower()]
            if speed_detail_cols:
                speed_detail_df = df[speed_detail_cols + [&#39;trajectory_id&#39;, &#39;trajectory_tid&#39;]]
                st.dataframe(speed_detail_df, width=&#39;stretch&#39;)

        # Junction-specific metrics if available
        if junction_cols:
            st.markdown(&#34;### Junction Timing Analysis&#34;)

            # Check for NaN values and provide explanation
            total_junction_measurements = len(df) * len(junction_cols)
            valid_junction_measurements = sum(len(df[col].dropna()) for col in junction_cols)
            nan_count = total_junction_measurements - valid_junction_measurements

            if nan_count &gt; 0:
                st.info(f&#34;ℹ️ **Note**: {nan_count} out of {total_junction_measurements} junction timing measurements returned NaN. This typically means trajectories didn&#39;t pass through those junctions or timing couldn&#39;t be computed.&#34;)

            # Create junction timing summary
            junction_summary = []
            for col in junction_cols:
                junction_num = col.split(&#39;_&#39;)[1]
                mode_col = f&#34;junction_{junction_num}_mode&#34;
                if mode_col in df.columns:
                    valid_times = df[col].dropna()
                    total_trajectories = len(df)
                    valid_count = len(valid_times)
                    nan_count_junction = total_trajectories - valid_count

                    if valid_count &gt; 0:
                        junction_summary.append({
                            &#34;Junction&#34;: f&#34;Junction {junction_num}&#34;,
                            &#34;Avg Time&#34;: f&#34;{valid_times.mean():.2f}s&#34;,
                            &#34;Std Time&#34;: f&#34;{valid_times.std():.2f}s&#34;,
                            &#34;Valid Count&#34;: valid_count,
                            &#34;NaN Count&#34;: nan_count_junction
                        })
                    else:
                        junction_summary.append({
                            &#34;Junction&#34;: f&#34;Junction {junction_num}&#34;,
                            &#34;Avg Time&#34;: &#34;N/A&#34;,
                            &#34;Std Time&#34;: &#34;N/A&#34;,
                            &#34;Valid Count&#34;: 0,
                            &#34;NaN Count&#34;: total_trajectories
                        })

            if junction_summary:
                junction_df = pd.DataFrame(junction_summary)
                st.markdown(&#34;**Junction Statistics (Only trajectories that actually pass through each junction):**&#34;)
                st.dataframe(junction_df, width=&#39;stretch&#39;)

                # Junction timing visualization
                st.markdown(&#34;**Junction Timing Comparison**&#34;)
                img = metrics_images.get(&#34;junction_timing_comparison.png&#34;)
                if img and os.path.exists(img):
                    st.image(img, width=&#39;stretch&#39;)
                else:
                    st.info(&#34;Timing comparison chart not available yet. Re-run metrics analysis to generate.&#34;)

                # Show individual junction timing distributions
                st.markdown(&#34;**Individual Junction Timing Distributions**&#34;)
                if metrics_images:
                    # display per-junction histograms if present
                    for name, path in sorted(metrics_images.items()):
                        if name.startswith(&#34;timing_distribution_J&#34;) and os.path.exists(path):
                            jlabel = name.replace(&#34;timing_distribution_&#34;, &#34;&#34;).replace(&#34;.png&#34;, &#34;&#34;)
                            st.markdown(f&#34;**{jlabel.replace(&#39;_&#39;, &#39; &#39;)}**&#34;)
                            st.image(path, width=&#39;stretch&#39;)
                else:
                    for col in junction_cols:
                        junction_num = col.split(&#39;_&#39;)[1]
                        valid_times = df[col].dropna()
                        if len(valid_times) &gt; 0:
                            st.markdown(f&#34;**Junction {junction_num} Timing Distribution**&#34;)
                            sorted_times = valid_times.sort_values().reset_index(drop=True)
                            st.bar_chart(sorted_times)
                            st.caption(f&#34;Range: {sorted_times.min():.2f}s - {sorted_times.max():.2f}s, Mean: {sorted_times.mean():.2f}s&#34;)</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.render_navigation"><code class="name flex">
<span>def <span class="ident">render_navigation</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Render the navigation sidebar</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render_navigation(self):
    &#34;&#34;&#34;Render the navigation sidebar&#34;&#34;&#34;
    st.sidebar.title(&#34;Navigation&#34;)

    steps = {
        &#34;data_upload&#34;: &#34;📁 Data Upload&#34;,
        &#34;junction_editor&#34;: &#34;🎯 Junction Editor&#34;,
        &#34;analysis&#34;: &#34;📊 Analysis&#34;,
        &#34;visualization&#34;: &#34;📈 Visualization&#34;,
        &#34;export&#34;: &#34;💾 Export Results&#34;
    }

    for step_key, step_name in steps.items():
        if st.sidebar.button(step_name, key=f&#34;nav_{step_key}&#34;):
            st.session_state.current_step = step_key
            st.rerun()

    st.sidebar.markdown(&#34;---&#34;)
    st.sidebar.markdown(&#34;### Current Status&#34;)

    # Status indicators
    data_status = &#34;✅&#34; if (st.session_state.trajectories and getattr(st.session_state, &#39;data_loaded&#39;, False)) else &#34;❌&#34;
    junction_status = &#34;✅&#34; if st.session_state.junctions else &#34;❌&#34;

    st.sidebar.markdown(f&#34;{data_status} Data Loaded&#34;)
    st.sidebar.markdown(f&#34;{junction_status} Junctions Defined&#34;)

    if st.session_state.trajectories and st.session_state.junctions:
        st.sidebar.markdown(&#34;✅ Ready for Analysis&#34;)
    else:
        st.sidebar.markdown(&#34;⚠️ Complete setup steps&#34;)</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.render_pattern_analysis"><code class="name flex">
<span>def <span class="ident">render_pattern_analysis</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Render pattern analysis results</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render_pattern_analysis(self):
    &#34;&#34;&#34;Render pattern analysis results&#34;&#34;&#34;
    st.markdown(&#34;### Pattern Analysis&#34;)

    if &#34;choice_patterns&#34; in st.session_state.analysis_results:
        patterns = st.session_state.analysis_results[&#34;choice_patterns&#34;]

        # Display pattern statistics
        st.markdown(&#34;#### Pattern Statistics&#34;)
        for junction_key, pattern_data in patterns.items():
            junction_num = junction_key.split(&#39;_&#39;)[1] if &#39;_&#39; in junction_key else junction_key[1:]
            st.markdown(f&#34;**Junction {junction_num}:**&#34;)

            if &#34;total_trajectories&#34; in pattern_data:
                st.write(f&#34;- Total trajectories: {pattern_data[&#39;total_trajectories&#39;]}&#34;)

            if &#34;choice_counts&#34; in pattern_data:
                st.write(f&#34;- Choice counts: {pattern_data[&#39;choice_counts&#39;]}&#34;)</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.render_predict_visualizations"><code class="name flex">
<span>def <span class="ident">render_predict_visualizations</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Render predict analysis visualizations</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render_predict_visualizations(self):
    &#34;&#34;&#34;Render predict analysis visualizations&#34;&#34;&#34;
    st.markdown(&#34;### Predict Analysis Results&#34;)

    # Check if predict analysis results exist
    if (st.session_state.analysis_results is None or
        &#34;predictions&#34; not in st.session_state.analysis_results):
        st.info(&#34;No predict analysis results available. Run predict analysis first.&#34;)
        return

    predictions_data = st.session_state.analysis_results[&#34;predictions&#34;]

    # Display flow graphs
    st.markdown(&#34;#### Flow Graphs&#34;)
    col1, col2 = st.columns(2)

    with col1:
        st.markdown(&#34;##### Overall Flow Graph&#34;)
        flow_map_path = os.path.join(&#34;gui_outputs&#34;, &#34;Flow_Graph_Map.png&#34;)
        if os.path.exists(flow_map_path):
            st.image(flow_map_path, width=&#39;stretch&#39;)
        else:
            st.info(&#34;Flow graph map not available&#34;)

    with col2:
        st.markdown(&#34;##### Per-Junction Flow Graph&#34;)
        per_junction_path = os.path.join(&#34;gui_outputs&#34;, &#34;Per_Junction_Flow_Graph.png&#34;)
        if os.path.exists(per_junction_path):
            st.image(per_junction_path, width=&#39;stretch&#39;)
        else:
            st.info(&#34;Per-junction flow graph not available&#34;)

    # Display conditional probability heatmap
    st.markdown(&#34;#### Conditional Probability Analysis&#34;)
    heatmap_path = os.path.join(&#34;gui_outputs&#34;, &#34;conditional_probability_heatmap.png&#34;)
    if os.path.exists(heatmap_path):
        st.image(heatmap_path, width=&#39;stretch&#39;)
    else:
        st.info(&#34;Conditional probability heatmap not available&#34;)

    # Display behavioral pattern analysis
    st.markdown(&#34;#### Behavioral Pattern Distribution&#34;)
    pattern_path = os.path.join(&#34;gui_outputs&#34;, &#34;behavioral_patterns.png&#34;)
    if os.path.exists(pattern_path):
        st.image(pattern_path, width=&#39;stretch&#39;)
    else:
        st.info(&#34;Behavioral pattern analysis not available&#34;)

    # Display summary statistics
    if &#34;summary&#34; in predictions_data:
        st.markdown(&#34;#### Analysis Summary&#34;)
        summary = predictions_data[&#34;summary&#34;]

        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric(&#34;Total Trajectories&#34;, summary.get(&#34;total_trajectories&#34;, 0))
        with col2:
            st.metric(&#34;Total Junctions&#34;, summary.get(&#34;total_junctions&#34;, 0))
        with col3:
            st.metric(&#34;Total Transitions&#34;, summary.get(&#34;total_transitions&#34;, 0))
        with col4:
            st.metric(&#34;Unique Patterns&#34;, summary.get(&#34;unique_patterns&#34;, 0))

    # Interactive Junction Prediction Tool
    st.markdown(&#34;#### Interactive Junction Prediction&#34;)
    self._render_interactive_prediction_tool(predictions_data)</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.render_visualization"><code class="name flex">
<span>def <span class="ident">render_visualization</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Render the visualization interface</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def render_visualization(self):
    &#34;&#34;&#34;Render the visualization interface&#34;&#34;&#34;
    st.markdown(&#39;&lt;h2 class=&#34;section-header&#34;&gt;📈 Visualization&lt;/h2&gt;&#39;, unsafe_allow_html=True)

    if not st.session_state.analysis_results:
        st.warning(&#34;⚠️ Please run an analysis first&#34;)
        return

    # Debug: Show what analysis results are available
    with st.expander(&#34;🔍 Debug: Available Analysis Results&#34;, expanded=False):
        if st.session_state.analysis_results is not None:
            st.write(&#34;Analysis results keys:&#34;, list(st.session_state.analysis_results.keys()))
        else:
            st.write(&#34;No analysis results available&#34;)

    # Show different visualizations based on analysis type
    # If multiple analysis types are available, let user choose
    # Prioritize &#34;branches&#34; as the default selection
    available_analyses = []

    # Add &#34;branches&#34; first if available (for default selection)
    if &#34;branches&#34; in st.session_state.analysis_results:
        available_analyses.append(&#34;branches&#34;)

    # Add other analysis types
    if st.session_state.analysis_results is not None:
        if &#34;metrics&#34; in st.session_state.analysis_results:
            available_analyses.append(&#34;metrics&#34;)
        if &#34;assignments&#34; in st.session_state.analysis_results:
            available_analyses.append(&#34;assignments&#34;)
    if &#34;predictions&#34; in st.session_state.analysis_results:
        available_analyses.append(&#34;predictions&#34;)
    if &#34;choice_patterns&#34; in st.session_state.analysis_results:
        available_analyses.append(&#34;choice_patterns&#34;)
    if &#34;intent_recognition&#34; in st.session_state.analysis_results:
        available_analyses.append(&#34;intent_recognition&#34;)
    if &#34;enhanced&#34; in st.session_state.analysis_results:
        available_analyses.append(&#34;enhanced&#34;)
    if &#34;gaze_results&#34; in st.session_state.analysis_results:
        available_analyses.append(&#34;gaze_results&#34;)

    if len(available_analyses) &gt; 1:
        # Multiple analysis types available - let user choose
        st.markdown(&#34;### Multiple Analysis Results Available&#34;)
        selected_analysis = st.selectbox(
            &#34;Choose analysis to visualize:&#34;,
            available_analyses,
            help=&#34;Select which analysis results to display&#34;
        )

        if selected_analysis == &#34;metrics&#34;:
            self.render_metrics_visualizations()
        elif selected_analysis == &#34;assignments&#34;:
            self.render_assign_visualizations()
        elif selected_analysis == &#34;branches&#34;:
            self.render_discover_visualizations()
        elif selected_analysis == &#34;predictions&#34;:
            self.render_predict_visualizations()
        elif selected_analysis == &#34;choice_patterns&#34;:
            self.render_flow_graphs()
            self.render_conditional_probabilities()
            self.render_pattern_analysis()
        elif selected_analysis == &#34;intent_recognition&#34;:
            self.render_intent_visualizations()
        elif selected_analysis == &#34;enhanced&#34;:
            self.render_enhanced_visualizations()
        elif selected_analysis == &#34;gaze_results&#34;:
            self.render_gaze_visualizations()
    else:
        # Single analysis type - show automatically
        if st.session_state.analysis_results is not None:
            if &#34;metrics&#34; in st.session_state.analysis_results:
                self.render_metrics_visualizations()
            elif &#34;assignments&#34; in st.session_state.analysis_results:
                self.render_assign_visualizations()
            elif &#34;branches&#34; in st.session_state.analysis_results:
                self.render_discover_visualizations()
                # Also show flow graphs if available
                if &#34;flow_graph_map&#34; in st.session_state.analysis_results:
                    self.render_flow_graphs()
            elif &#34;predictions&#34; in st.session_state.analysis_results:
                self.render_predict_visualizations()
            elif &#34;intent_recognition&#34; in st.session_state.analysis_results:
                self.render_intent_visualizations()
            elif &#34;enhanced&#34; in st.session_state.analysis_results:
                self.render_enhanced_visualizations()
            elif &#34;gaze_results&#34; in st.session_state.analysis_results:
                self.render_gaze_visualizations()
            else:
                st.info(&#34;No visualizations available for this analysis type&#34;)
                st.write(&#34;Available analysis results:&#34;, list(st.session_state.analysis_results.keys()))
        else:
            st.info(&#34;No analysis results available. Please run an analysis first.&#34;)</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Main GUI run method</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(self):
    &#34;&#34;&#34;Main GUI run method&#34;&#34;&#34;
    # Add custom CSS for image aspect ratio preservation
    st.markdown(&#34;&#34;&#34;
    &lt;style&gt;
    .stImage &gt; img {
        object-fit: contain !important;
        max-width: 100% !important;
        height: auto !important;
    }
    .stImage &gt; div {
        display: flex !important;
        justify-content: center !important;
    }
    &lt;/style&gt;
    &#34;&#34;&#34;, unsafe_allow_html=True)

    self.render_header()
    self.render_navigation()

    # Render current step
    if st.session_state.current_step == &#34;data_upload&#34;:
        self.render_data_upload()
    elif st.session_state.current_step == &#34;junction_editor&#34;:
        self.render_junction_editor()
    elif st.session_state.current_step == &#34;analysis&#34;:
        self.render_analysis()
    elif st.session_state.current_step == &#34;visualization&#34;:
        self.render_visualization()
    elif st.session_state.current_step == &#34;export&#34;:
        self.render_export()</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.run_analysis"><code class="name flex">
<span>def <span class="ident">run_analysis</span></span>(<span>self, analysis_type: str, decision_mode: str, cluster_method: str, seed: int, cluster_params: dict = None, decision_params: dict = None, assign_params: dict = None, discover_decision_mode: str = 'hybrid')</span>
</code></dt>
<dd>
<div class="desc"><p>Run the selected analysis</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_analysis(self, analysis_type: str, decision_mode: str, cluster_method: str, seed: int, cluster_params: dict = None, decision_params: dict = None, assign_params: dict = None, discover_decision_mode: str = &#34;hybrid&#34;):
    &#34;&#34;&#34;Run the selected analysis&#34;&#34;&#34;
    try:
        with st.spinner(f&#34;Running {analysis_type} analysis...&#34;):

            if analysis_type == &#34;discover&#34;:
                # Unified multi-junction discovery for consistent decisions and assignments
                import os
                output_dir = &#34;gui_outputs&#34;
                os.makedirs(output_dir, exist_ok=True)

                # Cluster/decision parameters
                k_value = cluster_params.get(&#34;k&#34;, 3) if cluster_params else 3
                min_samples = cluster_params.get(&#34;min_samples&#34;, 5) if cluster_params else 5
                k_min = cluster_params.get(&#34;k_min&#34;, 2) if cluster_params else 2
                k_max = cluster_params.get(&#34;k_max&#34;, 6) if cluster_params else 6
                min_sep_deg = cluster_params.get(&#34;min_sep_deg&#34;, 12.0) if cluster_params else 12.0
                angle_eps = cluster_params.get(&#34;angle_eps&#34;, 15.0) if cluster_params else 15.0

                path_length = decision_params.get(&#34;path_length&#34;, 100.0) if decision_params else 100.0
                epsilon = decision_params.get(&#34;epsilon&#34;, 0.05) if decision_params else 0.05
                linger_delta = decision_params.get(&#34;linger_delta&#34;, 5.0) if decision_params else 5.0
                r_outer_list = [st.session_state.junction_r_outer.get(i, 50.0) for i in range(len(st.session_state.junctions))]

                # Run consolidated discovery
                chain_df, centers_list, decisions_chain_df = discover_decision_chain(
                    trajectories=st.session_state.trajectories,
                    junctions=st.session_state.junctions,
                    path_length=path_length,
                    epsilon=epsilon,
                    seed=seed,
                    decision_mode=discover_decision_mode,
                    r_outer_list=r_outer_list,
                    linger_delta=linger_delta,
                    out_dir=output_dir,
                    cluster_method=cluster_method,
                    k=k_value,
                    k_min=k_min,
                    k_max=k_max,
                    min_sep_deg=min_sep_deg,
                    angle_eps=angle_eps,
                    min_samples=min_samples,
                )

                # Build per-junction results view from chain_df/centers_list
                results = {}
                for i, junction in enumerate(st.session_state.junctions):
                    junction_key = f&#34;junction_{i}&#34;
                    col = f&#34;branch_j{i}&#34;
                    if col in chain_df.columns:
                        df_i = chain_df[[&#34;trajectory&#34;, col]].copy()
                        df_i = df_i.rename(columns={col: &#34;branch&#34;})
                        # summary counts for main branches (&gt;=0)
                        vc = df_i[df_i[&#34;branch&#34;] &gt;= 0][&#34;branch&#34;].value_counts().sort_index()
                        summary_i = vc.rename_axis(&#34;branch&#34;).to_frame(&#34;count&#34;).reset_index()
                        total_i = int(summary_i[&#34;count&#34;].sum()) if len(summary_i) else 0
                        summary_i[&#34;percent&#34;] = summary_i[&#34;count&#34;] / max(1, total_i) * 100.0
                    else:
                        import pandas as _pd
                        df_i = _pd.DataFrame(columns=[&#34;trajectory&#34;,&#34;branch&#34;])  # empty
                        summary_i = _pd.DataFrame(columns=[&#34;branch&#34;,&#34;count&#34;,&#34;percent&#34;])  # empty

                    results[junction_key] = {
                        &#34;assignments&#34;: df_i,
                        &#34;summary&#34;: summary_i,
                        &#34;centers&#34;: centers_list[i] if i &lt; len(centers_list) else None,
                        &#34;junction&#34;: junction,
                        &#34;r_outer&#34;: r_outer_list[i] if i &lt; len(r_outer_list) else None,
                        &#34;path_length&#34;: path_length,
                        &#34;epsilon&#34;: epsilon,
                        &#34;linger_delta&#34;: linger_delta,  # Store linger_delta for gaze analysis
                        &#34;decision_mode&#34;: discover_decision_mode,
                        &#34;scale&#34;: st.session_state.get(&#34;scale_factor&#34;, 1.0),
                    }

                # Flow graph generation removed - discover should only do discovery, not flow analysis

                # Persist results and decisions for gaze reuse
                if st.session_state.analysis_results is None:
                    st.session_state.analysis_results = {}
                st.session_state.analysis_results[&#34;branches&#34;] = results

                # Debug: Check chain_decisions DataFrame
                st.write(f&#34;🔍 **Chain Decisions Debug:**&#34;)
                st.write(f&#34;- decisions_chain_df is not None: {decisions_chain_df is not None}&#34;)
                if decisions_chain_df is not None:
                    st.write(f&#34;- decisions_chain_df length: {len(decisions_chain_df)}&#34;)
                    st.write(f&#34;- decisions_chain_df columns: {list(decisions_chain_df.columns)}&#34;)
                    if not decisions_chain_df.empty:
                        st.write(f&#34;- Junction indices in decisions_chain_df: {sorted(decisions_chain_df[&#39;junction_index&#39;].unique())}&#34;)
                    else:
                        st.write(&#34;- decisions_chain_df is empty!&#34;)
                else:
                    st.write(&#34;- decisions_chain_df is None!&#34;)

                # Store branch assignments (chain_df) as chain_decisions for gaze analysis
                if chain_df is not None and len(chain_df) &gt; 0:
                    st.session_state.analysis_results.setdefault(&#34;branches&#34;, {})
                    st.session_state.analysis_results[&#34;branches&#34;][&#34;chain_decisions&#34;] = chain_df
                    st.write(f&#34;✅ **Stored branch assignments (chain_df) with {len(chain_df)} rows in session state**&#34;)
                    st.write(f&#34;🔍 **Branch assignment columns:** {list(chain_df.columns)}&#34;)

                    # Debug: Check for branch_jX columns specifically
                    branch_cols = [col for col in chain_df.columns if col.startswith(&#39;branch_j&#39;)]
                    st.write(f&#34;🔍 **Branch columns found:** {branch_cols}&#34;)
                    if len(branch_cols) &gt; 0:
                        st.write(f&#34;🔍 **Sample branch data:** {chain_df[branch_cols].head()}&#34;)
                    else:
                        st.error(&#34;❌ **No branch_jX columns found in chain_df!**&#34;)
                else:
                    st.write(f&#34;❌ **Not storing branch assignments - chain_df is None or empty**&#34;)

                # Also store decision points separately for reference
                if decisions_chain_df is not None and len(decisions_chain_df) &gt; 0:
                    st.session_state.analysis_results.setdefault(&#34;branches&#34;, {})
                    st.session_state.analysis_results[&#34;branches&#34;][&#34;decision_points&#34;] = decisions_chain_df
                    st.write(f&#34;✅ **Stored decision points with {len(decisions_chain_df)} rows in session state**&#34;)
                else:
                    st.write(f&#34;❌ **Not storing decision points - DataFrame is None or empty**&#34;)

                # Add debugging information for flow analysis
                try:
                    st.markdown(&#34;#### 🔍 Flow Analysis Debug&#34;)

                    # Count trajectories that visit multiple junctions
                    multi_junction_trajectories = 0
                    junction_visit_counts = {}

                    for i, junction in enumerate(st.session_state.junctions):
                        junction_key = f&#34;junction_{i}&#34;
                        if junction_key in results and &#34;assignments&#34; in results[junction_key]:
                            assignments = results[junction_key][&#34;assignments&#34;]
                            if not assignments.empty:
                                visited_trajectories = set(assignments[&#34;trajectory&#34;].unique())
                                junction_visit_counts[i] = visited_trajectories

                    # Find trajectories that visit multiple junctions
                    all_trajectories = set()
                    for trajectories in junction_visit_counts.values():
                        all_trajectories.update(trajectories)

                    for traj_id in all_trajectories:
                        visited_junctions = [i for i, trajs in junction_visit_counts.items() if traj_id in trajs]
                        if len(visited_junctions) &gt; 1:
                            multi_junction_trajectories += 1

                    st.info(f&#34;📊 **Flow Analysis Summary:**&#34;)
                    st.write(f&#34;- Total trajectories: {len(st.session_state.trajectories)}&#34;)
                    st.write(f&#34;- Trajectories visiting multiple junctions: {multi_junction_trajectories}&#34;)
                    st.write(f&#34;- Junction visit counts: {[len(trajs) for trajs in junction_visit_counts.values()]}&#34;)

                    if multi_junction_trajectories == 0:
                        st.warning(&#34;⚠️ **No trajectories visit multiple junctions!** This explains the zero flow matrix.&#34;)
                        st.write(&#34;**Possible causes:**&#34;)
                        st.write(&#34;1. Trajectories are too short to reach multiple junctions&#34;)
                        st.write(&#34;2. Junction r_outer values are too small&#34;)
                        st.write(&#34;3. Junctions are too far apart&#34;)
                        st.write(&#34;4. Trajectory data needs different scaling&#34;)

                except Exception as e:
                    st.warning(f&#34;Debug analysis failed: {str(e)}&#34;)

                #st.success(f&#34;✅ Discover analysis completed successfully for all {len(st.session_state.junctions)} junctions!&#34;)
                self.generate_cli_command(&#34;discover&#34;, results, cluster_method, cluster_params, decision_mode, decision_params)

            elif analysis_type == &#34;assign&#34;:
                # Assign trajectories to branches using simplified interface
                import numpy as np

                # Load trajectories based on trajectory option
                trajectories = self.load_assign_trajectories(assign_params)
                if trajectories is None:
                    return

                # Load centers based on centers option
                centers_dict = self.load_assign_centers(assign_params)
                if centers_dict is None:
                    return

                # Validate that we have compatible data
                if not trajectories:
                    st.error(&#34;❌ No trajectories loaded for assignment&#34;)
                    return

                if not centers_dict:
                    st.error(&#34;❌ No junction centers loaded for assignment&#34;)
                    return

                # Get assignment parameters
                path_length = assign_params.get(&#34;path_length&#34;, 100.0) if assign_params else 100.0
                epsilon = assign_params.get(&#34;epsilon&#34;, 0.05) if assign_params else 0.05

                results = {}
                successful_assignments = 0

                # Process each junction
                for junction_key, centers in centers_dict.items():
                    try:
                        # Extract junction number from key (e.g., &#34;junction_0&#34; -&gt; 0)
                        junction_num = int(junction_key.split(&#39;_&#39;)[1])

                        # Get assignment parameters - use stored values from discover analysis if available
                        centers_option = assign_params.get(&#34;centers_option&#34;, &#34;Use session centers&#34;) if assign_params else &#34;Use session centers&#34;
                        if centers_option == &#34;Use session centers&#34;:
                            if junction_key in st.session_state.analysis_results[&#34;branches&#34;]:
                                branch_data = st.session_state.analysis_results[&#34;branches&#34;][junction_key]
                                # Use stored parameters from discover analysis
                                stored_path_length = branch_data.get(&#34;path_length&#34;, path_length)
                                stored_epsilon = branch_data.get(&#34;epsilon&#34;, epsilon)
                                stored_scale = branch_data.get(&#34;scale&#34;, 1.0)
                                st.info(f&#34;📊 Using assignment parameters from discover analysis: path_length={stored_path_length:.1f}, epsilon={stored_epsilon:.3f}, scale={stored_scale:.1f}&#34;)
                            else:
                                stored_path_length = path_length
                                stored_epsilon = epsilon
                                stored_scale = 1.0
                        else:
                            stored_path_length = path_length
                            stored_epsilon = epsilon
                            stored_scale = 1.0

                        # Get junction and r_outer - prioritize stored parameters from discover analysis
                        if centers_option == &#34;Use session centers&#34;:
                            # Try to get junction parameters from discover analysis first
                            if junction_key in st.session_state.analysis_results[&#34;branches&#34;]:
                                branch_data = st.session_state.analysis_results[&#34;branches&#34;][junction_key]
                                if &#34;junction&#34; in branch_data and &#34;r_outer&#34; in branch_data:
                                    junction = branch_data[&#34;junction&#34;]
                                    r_outer = branch_data[&#34;r_outer&#34;]
                                    stored_scale = branch_data.get(&#34;scale&#34;, 1.0)
                                    st.info(f&#34;📊 Using junction parameters from discover analysis: center=({junction.cx:.1f}, {junction.cz:.1f}), radius={junction.r:.1f}, r_outer={r_outer:.1f}&#34;)
                                    st.info(f&#34;📊 Scale factor from discover analysis: {stored_scale:.1f}&#34;)

                                    # Check if current trajectories use different scale factor
                                    if hasattr(st.session_state, &#39;trajectories&#39;) and st.session_state.trajectories:
                                        # Estimate scale factor from trajectory coordinates
                                        sample_traj = st.session_state.trajectories[0]
                                        if hasattr(sample_traj, &#39;x&#39;) and len(sample_traj.x) &gt; 0:
                                            # Simple heuristic: if coordinates are much larger than expected, scale might be different
                                            max_coord = max(abs(sample_traj.x.max()), abs(sample_traj.z.max()))
                                            if max_coord &gt; 1000 and stored_scale &lt; 0.5:
                                                st.warning(f&#34;⚠️ Scale factor mismatch detected! Discover used {stored_scale:.1f}, but current trajectories appear to use a different scale.&#34;)
                                                st.warning(f&#34;⚠️ This may cause assignment failures. Consider using the same scale factor as discover analysis.&#34;)
                                else:
                                    # Fallback to session state junctions
                                    if junction_num &lt; len(st.session_state.junctions):
                                        junction = st.session_state.junctions[junction_num]
                                        r_outer = st.session_state.junction_r_outer.get(junction_num, 50.0)
                                    else:
                                        st.error(f&#34;❌ No junction parameters found for {junction_key}&#34;)
                                        continue
                            else:
                                st.error(f&#34;❌ No discover analysis data found for {junction_key}&#34;)
                                continue
                        elif junction_num &lt; len(st.session_state.junctions):
                            junction = st.session_state.junctions[junction_num]
                            r_outer = st.session_state.junction_r_outer.get(junction_num, 50.0)
                        else:
                            # If using external data, use manual parameters or estimate from trajectory data
                            manual_cx = assign_params.get(&#34;junction_cx&#34;, 0.0)
                            manual_cz = assign_params.get(&#34;junction_cz&#34;, 0.0)
                            manual_r = assign_params.get(&#34;junction_r&#34;, 50.0)

                            if manual_cx != 0.0 or manual_cz != 0.0 or manual_r != 50.0:
                                # Use manual parameters
                                junction = Circle(cx=manual_cx, cz=manual_cz, r=manual_r)
                                r_outer = manual_r * 2.0
                                st.info(f&#34;📊 Using manual junction: center=({manual_cx:.1f}, {manual_cz:.1f}), radius={manual_r:.1f}&#34;)
                            else:
                                # Estimate junction from trajectory data
                                st.warning(f&#34;⚠️ No junction defined for {junction_key}. Attempting to estimate from trajectory data...&#34;)

                                # Estimate junction center from trajectory data
                                all_x = np.concatenate([tr.x for tr in trajectories])
                                all_z = np.concatenate([tr.z for tr in trajectories])

                                # Show trajectory data range for debugging
                                st.info(f&#34;📊 Trajectory data range:&#34;)
                                st.write(f&#34;- X range: {np.min(all_x):.1f} to {np.max(all_x):.1f}&#34;)
                                st.write(f&#34;- Z range: {np.min(all_z):.1f} to {np.max(all_z):.1f}&#34;)

                                # Use median as center (more robust than mean)
                                estimated_cx = float(np.median(all_x))
                                estimated_cz = float(np.median(all_z))

                                # Estimate radius based on data spread - use a more conservative approach
                                distances = np.sqrt((all_x - estimated_cx)**2 + (all_z - estimated_cz)**2)
                                estimated_r = float(np.percentile(distances, 75))  # Use 75th percentile for radius

                                junction = Circle(cx=estimated_cx, cz=estimated_cz, r=max(estimated_r, 20.0))
                                r_outer = estimated_r * 3.0  # Make r_outer much larger than junction radius

                                st.info(f&#34;📊 Estimated junction: center=({estimated_cx:.1f}, {estimated_cz:.1f}), radius={estimated_r:.1f}&#34;)
                                st.warning(f&#34;⚠️ Using estimated junction for {junction_key}. Consider defining junctions manually for better results.&#34;)
                                st.info(f&#34;💡 Tip: If trajectories still get -2/-1, try increasing the junction radius or adjusting the center coordinates.&#34;)

                        # Create output directory for this junction
                        import os
                        out_dir = os.path.join(&#34;gui_outputs&#34;, f&#34;junction_{junction_num}&#34;)
                        os.makedirs(out_dir, exist_ok=True)

                        # Run assignment
                        # Determine decision parameters to use
                        dm = assign_params.get(&#34;decision_mode&#34;, &#34;pathlen&#34;)
                        ld = assign_params.get(&#34;linger_delta&#34;, 0.0)
                        dm_r_outer = assign_params.get(&#34;r_outer&#34;, r_outer)

                        assignments = assign_branches(
                            trajectories=trajectories,
                            centers=centers,
                            junction=junction,
                            path_length=stored_path_length,
                            epsilon=stored_epsilon,
                            decision_mode=dm,
                            r_outer=dm_r_outer,
                            linger_delta=ld,
                            out_dir=out_dir
                        )

                        # Optional: auto-rediscover if outlier cluster among new assignments is large enough
                        try:
                            auto_flag = st.session_state.get(&#34;assign_auto_rediscover&#34;, False)
                            min_samples_new = int(st.session_state.get(&#34;assign_auto_min_samples&#34;, 5))
                            angle_eps_new = float(st.session_state.get(&#34;assign_auto_angle_eps&#34;, 15.0))
                            # Auto-rediscover: detect dense outlier regions among newly uploaded trajectories
                            if auto_flag and len(assignments) &gt; 0:
                                # Identify outliers (-1) that entered junction and have usable vectors
                                from verta.verta_decisions import compute_assignment_vectors
                                from verta.verta_clustering import cluster_angles_dbscan
                                # Compute vectors for these trajectories with same decision params
                                vec_df = compute_assignment_vectors(
                                    trajectories=trajectories,
                                    junction=junction,
                                    path_length=stored_path_length,
                                    decision_mode=dm,
                                    r_outer=dm_r_outer,
                                    epsilon=stored_epsilon,
                                )
                                # Merge to filter to current outliers only
                                outlier_ids = set(assignments[assignments[&#34;branch&#34;] == -1][&#34;trajectory&#34;].tolist())
                                if outlier_ids:
                                    use = vec_df[(vec_df[&#34;trajectory&#34;].isin(outlier_ids)) &amp; (vec_df[&#34;entered&#34;]) &amp; (vec_df[&#34;usable&#34;])].copy()
                                    if len(use) &gt;= min_samples_new:
                                        V = np.vstack([use[[&#34;vx&#34;,&#34;vz&#34;]].to_numpy()]) if len(use) else np.zeros((0,2))
                                        if V.size:
                                            labels_o, centers_o = cluster_angles_dbscan(V, eps_deg=angle_eps_new, min_samples=min_samples_new)
                                            # If any valid cluster exists (label &gt;=0), trigger rediscovery using all available trajectories
                                            if (labels_o &gt;= 0).any():
                                                st.info(f&#34;🔄 Auto-rediscover triggered for {junction_key}: detected dense outlier region (min_samples={min_samples_new}, angle_eps={angle_eps_new}°)&#34;)
                                                # Build combined trajectory set: existing session trajectories + newly provided
                                                all_trajs = []
                                                if hasattr(st.session_state, &#39;trajectories&#39;) and st.session_state.trajectories:
                                                    all_trajs.extend(st.session_state.trajectories)
                                                all_trajs.extend([t for t in trajectories if t not in all_trajs])
                                                # Rerun discovery for this junction
                                                from verta.verta_decisions import discover_branches
                                                new_assign, _sum, new_centers = discover_branches(
                                                    trajectories=all_trajs,
                                                    junction=junction,
                                                    k=centers.shape[0] if centers is not None and centers.size else 3,
                                                    path_length=stored_path_length,
                                                    epsilon=stored_epsilon,
                                                    seed=seed,
                                                    decision_mode=dm,
                                                    r_outer=dm_r_outer,
                                                    out_dir=out_dir,
                                                    cluster_method=cluster_method,
                                                    k_min=st.session_state.get(&#34;discover_k_min&#34;, 2),
                                                    k_max=st.session_state.get(&#34;discover_k_max&#34;, 6),
                                                    min_sep_deg=st.session_state.get(&#34;discover_min_sep_deg&#34;, 12.0),
                                                    angle_eps=st.session_state.get(&#34;discover_angle_eps&#34;, 15.0),
                                                    min_samples=min_samples_new,
                                                    junction_number=junction_num,
                                                    all_junctions=[junction]
                                                )
                                                # Update centers to the rediscovered ones and reassign current trajectories for display
                                                centers = new_centers
                                                assignments = assign_branches(
                                                    trajectories=trajectories,
                                                    centers=centers,
                                                    junction=junction,
                                                    path_length=stored_path_length,
                                                    epsilon=stored_epsilon,
                                                    decision_mode=dm,
                                                    r_outer=dm_r_outer,
                                                    linger_delta=ld,
                                                    out_dir=out_dir
                                                )
                                                st.warning(&#34;⚠️ Branch IDs may have been renumbered due to rediscovery.&#34;)
                        except Exception as _e:
                            # Keep assignment results even if auto-rediscover path fails
                            pass

                        # Enhanced debugging for assignment issues
                        if assignments is not None and len(assignments) &gt; 0:
                            # Check if assignments is a string (error message) or pandas DataFrame
                            if isinstance(assignments, str):
                                st.error(f&#34;🚨 **Assignment Error:** {assignments}&#34;)
                                st.error(&#34;This indicates the assign_branches function returned an error message instead of assignment results.&#34;)
                                st.error(&#34;Check the assign_branches function implementation or input parameters.&#34;)
                            elif hasattr(assignments, &#39;iterrows&#39;):  # pandas DataFrame
                                # Count assignment types from DataFrame
                                assignment_counts = assignments[&#39;branch&#39;].value_counts().to_dict()

                                total_trajectories = len(assignments)
                                neg2_count = assignment_counts.get(-2, 0)
                                neg1_count = assignment_counts.get(-1, 0)

                                # If most trajectories are -2/-1, provide enhanced debugging
                                if (neg2_count + neg1_count) / total_trajectories &gt; 0.8:
                                    st.error(f&#34;🚨 **Assignment Issue Detected for {junction_key}:**&#34;)
                                    st.error(f&#34;   -2 (never entered): {neg2_count} trajectories ({neg2_count/total_trajectories*100:.1f}%)&#34;)
                                    st.error(f&#34;   -1 (no usable vector): {neg1_count} trajectories ({neg1_count/total_trajectories*100:.1f}%)&#34;)

                                    # Enhanced debugging analysis
                                    st.info(f&#34;🔍 **Enhanced Debug Analysis:**&#34;)

                                    # Show trajectory data ranges
                                    all_x = []
                                    all_z = []
                                    for traj in trajectories:
                                        all_x.extend(traj.x)
                                        all_z.extend(traj.z)

                                    if all_x and all_z:
                                        st.info(f&#34;📊 **Trajectory Data Ranges:**&#34;)
                                        st.info(f&#34;   X: {min(all_x):.1f} to {max(all_x):.1f} (range: {max(all_x)-min(all_x):.1f})&#34;)
                                        st.info(f&#34;   Z: {min(all_z):.1f} to {max(all_z):.1f} (range: {max(all_z)-min(all_z):.1f})&#34;)
                                        st.info(f&#34;   Total points: {len(all_x)}&#34;)

                                        # Show how many trajectories actually pass through the junction area
                                        trajectories_in_junction = 0
                                        trajectories_with_usable_vectors = 0

                                        for traj in trajectories:
                                            # Check if trajectory passes through junction area
                                            distances = np.sqrt((traj.x - junction.cx)**2 + (traj.z - junction.cz)**2)
                                            if np.any(distances &lt;= junction.r):
                                                trajectories_in_junction += 1

                                                # Check if trajectory has usable vectors (length &gt; epsilon)
                                                if len(traj.x) &gt; 1:
                                                    dx = np.diff(traj.x)
                                                    dz = np.diff(traj.z)
                                                    movement = np.sqrt(dx**2 + dz**2)
                                                    if np.any(movement &gt; stored_epsilon):
                                                        trajectories_with_usable_vectors += 1

                                        st.info(f&#34;📊 **Junction Analysis:**&#34;)
                                        st.info(f&#34;   Trajectories passing through junction: {trajectories_in_junction}/{len(trajectories)}&#34;)
                                        st.info(f&#34;   Trajectories with usable vectors: {trajectories_with_usable_vectors}/{len(trajectories)}&#34;)

                                        # Analyze movement patterns for -1 assignments
                                        if neg1_count &gt; neg2_count:  # More -1 than -2 assignments
                                            st.error(f&#34;🚨 **Critical Issue: Most trajectories are -1 (entered junction but no usable vectors)!**&#34;)

                                            # Analyze movement patterns
                                            st.info(f&#34;🔍 **Movement Analysis:**&#34;)
                                            all_movements = []
                                            nan_trajectories = 0
                                            for traj in trajectories:
                                                if len(traj.x) &gt; 1:
                                                    # Check for NaN values
                                                    if np.any(np.isnan(traj.x)) or np.any(np.isnan(traj.z)):
                                                        nan_trajectories += 1
                                                        continue

                                                    dx = np.diff(traj.x)
                                                    dz = np.diff(traj.z)
                                                    movement = np.sqrt(dx**2 + dz**2)
                                                    # Filter out NaN movements
                                                    valid_movements = movement[~np.isnan(movement)]
                                                    all_movements.extend(valid_movements)

                                            if nan_trajectories &gt; 0:
                                                st.error(f&#34;🚨 **CRITICAL: {nan_trajectories} trajectories contain NaN coordinates!**&#34;)
                                                st.error(&#34;This will cause assignment failures. Check your trajectory data for missing/invalid coordinates.&#34;)

                                            if all_movements:
                                                percentile_5 = np.percentile(all_movements, 5)
                                                percentile_10 = np.percentile(all_movements, 10)
                                                percentile_25 = np.percentile(all_movements, 25)
                                                mean_movement = np.mean(all_movements)

                                                st.info(f&#34;📊 **Movement Statistics:**&#34;)
                                                st.info(f&#34;   Mean movement: {mean_movement:.4f}&#34;)
                                                st.info(f&#34;   5th percentile: {percentile_5:.4f}&#34;)
                                                st.info(f&#34;   10th percentile: {percentile_10:.4f}&#34;)
                                                st.info(f&#34;   25th percentile: {percentile_25:.4f}&#34;)
                                                st.info(f&#34;   Current epsilon: {stored_epsilon:.3f}&#34;)

                                                # Suggest epsilon adjustment
                                                if stored_epsilon &gt; percentile_25:
                                                    suggested_epsilon = percentile_10
                                                    st.warning(f&#34;⚠️ **Epsilon too high!** Try: {suggested_epsilon:.4f} (current: {stored_epsilon:.3f})&#34;)
                                                elif stored_epsilon &lt; percentile_5:
                                                    suggested_epsilon = percentile_10
                                                    st.warning(f&#34;⚠️ **Epsilon too low!** Try: {suggested_epsilon:.4f} (current: {stored_epsilon:.3f})&#34;)
                                                else:
                                                    suggested_epsilon = percentile_5
                                                    st.warning(f&#34;⚠️ **Try lower epsilon:** {suggested_epsilon:.4f} (current: {stored_epsilon:.3f})&#34;)
                                            else:
                                                st.error(f&#34;🚨 **NO VALID MOVEMENTS FOUND!**&#34;)
                                                st.error(&#34;All trajectories have NaN coordinates or invalid movement data.&#34;)
                                                st.error(&#34;This explains why all trajectories get -1 assignments.&#34;)
                                                st.error(&#34;**SOLUTION**: Check your trajectory data for missing/invalid coordinates.&#34;)

                                                # Show sample trajectory analysis
                                                st.info(f&#34;🔍 **Sample Trajectory Analysis (first 3):**&#34;)
                                                for i, traj in enumerate(trajectories[:3]):
                                                    if len(traj.x) &gt; 1:
                                                        # Check for NaN values
                                                        has_nan = np.any(np.isnan(traj.x)) or np.any(np.isnan(traj.z))
                                                        if has_nan:
                                                            st.error(f&#34;   Trajectory {i}: ⚠️ CONTAINS NaN COORDINATES!&#34;)
                                                            continue

                                                        dx = np.diff(traj.x)
                                                        dz = np.diff(traj.z)
                                                        movement = np.sqrt(dx**2 + dz**2)
                                                        max_movement = np.max(movement) if len(movement) &gt; 0 else 0
                                                        mean_movement = np.mean(movement) if len(movement) &gt; 0 else 0

                                                        # Check if trajectory passes through junction
                                                        distances = np.sqrt((traj.x - junction.cx)**2 + (traj.z - junction.cz)**2)
                                                        in_junction = np.any(distances &lt;= junction.r)
                                                        min_distance = np.min(distances)

                                                        st.info(f&#34;   Trajectory {i}: max_movement={max_movement:.3f}, mean_movement={mean_movement:.3f}, in_junction={in_junction}, min_distance={min_distance:.1f}&#34;)

                                        # Suggest junction radius adjustment
                                        if trajectories_in_junction &lt; len(trajectories) * 0.5:
                                            st.warning(f&#34;⚠️ **Low junction coverage!** Only {trajectories_in_junction}/{len(trajectories)} trajectories pass through the junction area.&#34;)

                                            # Calculate suggested radius to cover more trajectories
                                            all_distances = []
                                            for traj in trajectories:
                                                distances = np.sqrt((traj.x - junction.cx)**2 + (traj.z - junction.cz)**2)
                                                all_distances.extend(distances)

                                            suggested_radius = np.percentile(all_distances, 80)  # Cover 80% of trajectory points
                                            st.warning(f&#34;⚠️ **Suggested radius:** {suggested_radius:.1f} (current: {junction.r:.1f})&#34;)
                                            st.warning(f&#34;⚠️ **Suggested center:** ({junction.cx:.1f}, {junction.cz:.1f}) - verify this matches your junction location&#34;)
                            else:
                                st.warning(f&#34;⚠️ Unexpected assignment format: {type(assignments)} - {assignments}&#34;)
                                st.warning(&#34;Expected pandas DataFrame or string, but got something else.&#34;)

                        results[junction_key] = {
                            &#34;assignments&#34;: assignments,
                            &#34;centers&#34;: centers,
                            &#34;junction&#34;: junction,
                            &#34;path_length&#34;: path_length,
                            &#34;epsilon&#34;: epsilon
                        }

                        successful_assignments += 1
                        st.success(f&#34;✅ Completed assignment for {junction_key} ({len(assignments)} trajectories)&#34;)

                        # Store debug information in session state
                        branch_counts = assignments[&#39;branch&#39;].value_counts().sort_index()
                        debug_info = {
                            &#34;junction_params&#34;: {
                                &#34;center&#34;: f&#34;({junction.cx:.1f}, {junction.cz:.1f})&#34;,
                                &#34;radius&#34;: f&#34;{junction.r:.1f}&#34;,
                                &#34;r_outer&#34;: f&#34;{r_outer:.1f}&#34;
                            },
                            &#34;assignment_params&#34;: {
                                &#34;path_length&#34;: f&#34;{path_length:.1f}&#34;,
                                &#34;epsilon&#34;: f&#34;{epsilon:.3f}&#34;
                            },
                            &#34;data_info&#34;: {
                                &#34;centers_shape&#34;: str(centers.shape),
                                &#34;trajectories&#34;: len(trajectories)
                            },
                            &#34;assignment_distribution&#34;: dict(branch_counts),
                            &#34;assignments_sample&#34;: assignments.head(10).to_dict(&#39;records&#39;)
                        }

                        # Store in session state
                        if &#34;assign_debug_info&#34; not in st.session_state:
                            st.session_state.assign_debug_info = {}
                        st.session_state.assign_debug_info[junction_key] = debug_info

                    except Exception as e:
                        st.error(f&#34;❌ Assignment failed for {junction_key}: {str(e)}&#34;)
                        continue

                # Store results - preserve existing analysis results
                if st.session_state.analysis_results is None:
                    st.session_state.analysis_results = {}
                st.session_state.analysis_results[&#34;assignments&#34;] = results

                # Show summary
                total_junctions = len(centers_dict)
                if successful_assignments == total_junctions:
                    st.success(f&#34;✅ Assign analysis completed successfully for all {total_junctions} junctions!&#34;)
                else:
                    st.warning(f&#34;⚠️ Assign analysis completed for {successful_assignments}/{total_junctions} junctions&#34;)

                # Show assignment statistics
                if results:
                    st.markdown(&#34;### Assignment Summary&#34;)
                    total_trajectories = len(trajectories)
                    st.write(f&#34;**Total trajectories processed:** {total_trajectories}&#34;)
                    st.write(f&#34;**Junctions processed:** {successful_assignments}&#34;)

                    # Show branch distribution for first junction as example
                    first_junction = list(results.keys())[0]
                    assignments_df = results[first_junction][&#34;assignments&#34;]
                    branch_counts = assignments_df[&#34;branch&#34;].value_counts().sort_index()

                    st.markdown(f&#34;**Branch distribution for {first_junction}:**&#34;)
                    for branch, count in branch_counts.items():
                        percentage = (count / len(assignments_df)) * 100
                        st.write(f&#34;- Branch {branch}: {count} trajectories ({percentage:.1f}%)&#34;)

                # Generate CLI command for easy copying
                self.generate_cli_command(&#34;assign&#34;, results, cluster_method, cluster_params, decision_mode, decision_params)

            elif analysis_type == &#34;metrics&#34;:
                # Compute timing metrics
                metrics = []
                trajectories_with_time_data = 0
                trajectories_without_time_data = 0
                time_data_debug = []

                # Get junctions from session state or estimate from data (ONCE, before the loop)
                junctions_to_use = st.session_state.junctions if st.session_state.junctions else []

                # If no junctions defined, estimate from trajectory data
                if not junctions_to_use:
                    st.info(&#34;📊 No junctions defined. Estimating junctions from trajectory data...&#34;)

                    # Estimate junction from trajectory data (similar to assign analysis)
                    all_x = np.concatenate([tr.x for tr in st.session_state.trajectories])
                    all_z = np.concatenate([tr.z for tr in st.session_state.trajectories])

                    # Use median as center (more robust than mean)
                    estimated_cx = float(np.median(all_x))
                    estimated_cz = float(np.median(all_z))

                    # Estimate radius based on data spread
                    distances = np.sqrt((all_x - estimated_cx)**2 + (all_z - estimated_cz)**2)
                    estimated_r = float(np.percentile(distances, 75))  # Use 75th percentile for radius

                    # Create estimated junction
                    estimated_junction = Circle(cx=estimated_cx, cz=estimated_cz, r=max(estimated_r, 20.0))
                    junctions_to_use = [estimated_junction]

                    st.info(f&#34;📊 Estimated junction: center=({estimated_cx:.1f}, {estimated_cz:.1f}), radius={estimated_r:.1f}&#34;)

                    # Debug: Show trajectory data range
                    st.info(f&#34;📊 Trajectory data range:&#34;)
                    st.write(f&#34;- X range: {np.min(all_x):.1f} to {np.max(all_x):.1f}&#34;)
                    st.write(f&#34;- Z range: {np.min(all_z):.1f} to {np.max(all_z):.1f}&#34;)
                    st.write(f&#34;- Total trajectories: {len(st.session_state.trajectories)}&#34;)
                    st.write(f&#34;- Total coordinate points: {len(all_x)}&#34;)

                # Debug: Count how many trajectories pass through the estimated junction
                if junctions_to_use:
                    junction = junctions_to_use[0]
                    trajectories_through_junction = 0
                    for traj in st.session_state.trajectories:
                        dist = np.hypot(traj.x - junction.cx, traj.z - junction.cz)
                        if np.any(dist &lt;= junction.r):
                            trajectories_through_junction += 1

                    st.info(f&#34;📊 Junction Analysis:&#34;)
                    st.write(f&#34;- Trajectories passing through estimated junction: {trajectories_through_junction}/{len(st.session_state.trajectories)} ({trajectories_through_junction/len(st.session_state.trajectories)*100:.1f}%)&#34;)

                for i, traj in enumerate(st.session_state.trajectories):
                    try:
                        # Debug time data for first few trajectories
                        if i &lt; 5:  # Debug first 5 trajectories
                            time_debug = {
                        &#34;trajectory_id&#34;: i,
                                &#34;time_data_type&#34;: str(type(traj.t)),
                                &#34;time_data_shape&#34;: traj.t.shape if traj.t is not None else None,
                                &#34;time_data_sample&#34;: traj.t[:3].tolist() if traj.t is not None and len(traj.t) &gt; 0 else None,
                                &#34;time_data_dtype&#34;: str(traj.t.dtype) if traj.t is not None else None
                            }
                            time_data_debug.append(time_debug)

                        # Compute basic trajectory metrics
                        basic_metrics = compute_basic_trajectory_metrics(traj)

                        # Track time data availability
                        if basic_metrics[&#34;total_time&#34;] &gt; 0:
                            trajectories_with_time_data += 1
                        else:
                            trajectories_without_time_data += 1

                        # Compute junction-specific timing metrics
                        junction_metrics = {}

                        # Compute timing for each junction
                        for j, junction in enumerate(junctions_to_use):
                            try:
                                # First check if trajectory actually passes through this junction
                                entered, _ = entered_junction_idx(traj.x, traj.z, junction)

                                if not entered:
                                    # Trajectory doesn&#39;t pass through this junction, set NaN
                                    junction_metrics[f&#34;junction_{j}_time&#34;] = float(&#39;nan&#39;)
                                    junction_metrics[f&#34;junction_{j}_mode&#34;] = &#34;no_entry&#34;
                                    # Set speed metrics to NaN as well
                                    junction_metrics[f&#34;junction_{j}_speed&#34;] = float(&#39;nan&#39;)
                                    junction_metrics[f&#34;junction_{j}_speed_mode&#34;] = &#34;no_entry&#34;
                                    junction_metrics[f&#34;junction_{j}_entry_speed&#34;] = float(&#39;nan&#39;)
                                    junction_metrics[f&#34;junction_{j}_exit_speed&#34;] = float(&#39;nan&#39;)
                                    junction_metrics[f&#34;junction_{j}_avg_transit_speed&#34;] = float(&#39;nan&#39;)
                                    continue

                                # Use the selected decision mode from GUI (with defaults)
                                decision_mode = getattr(st.session_state, &#39;metrics_decision_mode&#39;, &#39;pathlen&#39;)
                                distance = getattr(st.session_state, &#39;metrics_distance&#39;, 100.0)
                                # Use r_outer from junction definitions like other functions do
                                r_outer_list = [st.session_state.junction_r_outer.get(i, 50.0) for i in range(len(st.session_state.junctions))]
                                r_outer = r_outer_list[j] if j &lt; len(r_outer_list) else 50.0
                                trend_window = getattr(st.session_state, &#39;metrics_trend_window&#39;, 5)
                                min_outward = getattr(st.session_state, &#39;metrics_min_outward&#39;, 0.0)

                                # Dynamic path length adjustment for all modes
                                if len(traj.x) &gt; 1:
                                    dx = np.diff(traj.x)
                                    dz = np.diff(traj.z)
                                    segments = np.hypot(dx, dz)
                                    total_distance = float(np.sum(segments))

                                    # Apply dynamic adjustment based on mode
                                    if decision_mode == &#34;pathlen&#34;:
                                        # Use 5% of total distance, but at least 0.1 and at most 10.0
                                        dynamic_distance = max(0.1, min(10.0, total_distance * 0.05))
                                        distance = dynamic_distance
                                    elif decision_mode == &#34;radial&#34;:
                                        # For radial mode, use a smaller r_outer based on trajectory data
                                        dynamic_r_outer = max(5.0, min(50.0, total_distance * 0.1))
                                        r_outer = dynamic_r_outer

                                    # Debug: Show dynamic parameters for first few trajectories
                                    if i &lt; 5:
                                        if decision_mode == &#34;pathlen&#34;:
                                            st.write(f&#34;🔍 Trajectory {i}: total_distance={total_distance:.2f}, dynamic_distance={dynamic_distance:.2f}&#34;)
                                        elif decision_mode == &#34;radial&#34;:
                                            st.write(f&#34;🔍 Trajectory {i}: total_distance={total_distance:.2f}, dynamic_r_outer={dynamic_r_outer:.2f}&#34;)

                                # Try the timing calculation with the dynamic parameters
                                t_val, mode_used = _timing_for_traj(
                                    tr=traj,
                                    junction=junction,
                                    decision_mode=decision_mode,
                                    distance=distance,
                                    r_outer=r_outer,
                                    trend_window=trend_window,
                                    min_outward=min_outward,
                                )

                                # Compute speed analysis for this junction
                                speed_val, speed_mode = speed_through_junction(
                                    tr=traj,
                                    junction=junction,
                                    decision_mode=decision_mode,
                                    path_length=distance,
                                    r_outer=r_outer,
                                    window=trend_window,
                                    min_outward=min_outward,
                                )

                                # Compute junction transit speeds
                                entry_speed, exit_speed, avg_transit_speed = junction_transit_speed(traj, junction)

                                # If still NaN and we have some movement, try with even smaller parameters
                                if np.isnan(t_val) and len(traj.x) &gt; 1:
                                    if decision_mode == &#34;pathlen&#34;:
                                        # Try with just 1% of total distance, minimum 0.01
                                        fallback_distance = max(0.01, total_distance * 0.01)
                                        t_val, mode_used = _timing_for_traj(
                                            tr=traj,
                                            junction=junction,
                                            decision_mode=decision_mode,
                                            distance=fallback_distance,
                                            r_outer=r_outer,
                                            trend_window=trend_window,
                                            min_outward=min_outward,
                                        )
                                    elif decision_mode == &#34;radial&#34;:
                                        # Try with even smaller r_outer
                                        fallback_r_outer = max(2.0, total_distance * 0.05)
                                        t_val, mode_used = _timing_for_traj(
                                            tr=traj,
                                            junction=junction,
                                            decision_mode=decision_mode,
                                            distance=distance,
                                            r_outer=fallback_r_outer,
                                            trend_window=trend_window,
                                            min_outward=min_outward,
                                        )

                                    if i &lt; 5 and not np.isnan(t_val):
                                        if decision_mode == &#34;pathlen&#34;:
                                            st.write(f&#34;🔍 Trajectory {i}: Fallback worked! fallback_distance={fallback_distance:.3f}&#34;)
                                        elif decision_mode == &#34;radial&#34;:
                                            st.write(f&#34;🔍 Trajectory {i}: Fallback worked! fallback_r_outer={fallback_r_outer:.3f}&#34;)

                                junction_metrics[f&#34;junction_{j}_time&#34;] = t_val
                                junction_metrics[f&#34;junction_{j}_mode&#34;] = mode_used
                                # Add speed analysis metrics
                                junction_metrics[f&#34;junction_{j}_speed&#34;] = speed_val
                                junction_metrics[f&#34;junction_{j}_speed_mode&#34;] = speed_mode
                                junction_metrics[f&#34;junction_{j}_entry_speed&#34;] = entry_speed
                                junction_metrics[f&#34;junction_{j}_exit_speed&#34;] = exit_speed
                                junction_metrics[f&#34;junction_{j}_avg_transit_speed&#34;] = avg_transit_speed
                            except Exception as e:
                                st.warning(f&#34;⚠️ Junction {j} timing failed for trajectory {i}: {e}&#34;)
                                junction_metrics[f&#34;junction_{j}_time&#34;] = float(&#39;nan&#39;)
                                junction_metrics[f&#34;junction_{j}_mode&#34;] = &#34;error&#34;

                        # Combine basic and junction metrics
                        combined_metrics = {
                            &#34;trajectory_id&#34;: i,
                            &#34;trajectory_tid&#34;: traj.tid,
                            **basic_metrics,
                            **junction_metrics
                        }
                        metrics.append(combined_metrics)

                    except Exception as e:
                        st.error(f&#34;❌ Failed to compute metrics for trajectory {i} ({traj.tid}): {e}&#34;)
                        # Add error entry to maintain consistency
                        error_metrics = {
                            &#34;trajectory_id&#34;: i,
                            &#34;trajectory_tid&#34;: traj.tid,
                            &#34;total_time&#34;: 0.0,
                            &#34;total_distance&#34;: 0.0,
                            &#34;average_speed&#34;: 0.0,
                            &#34;error&#34;: str(e)
                        }
                        metrics.append(error_metrics)

                # Show time data debug information (always show for metrics analysis)
                st.markdown(&#34;---&#34;)
                st.markdown(&#34;### 🔍 Debug Information&#34;)

                # Debug: Show what we have
                st.write(f&#34;**Debug Status:**&#34;)
                st.write(f&#34;- time_data_debug length: {len(time_data_debug)}&#34;)
                st.write(f&#34;- trajectories_without_time_data: {trajectories_without_time_data}&#34;)
                st.write(f&#34;- trajectories_with_time_data: {trajectories_with_time_data}&#34;)

                if time_data_debug:
                    with st.expander(&#34;🔍 Time Data Debug Information&#34;, expanded=True):
                        st.write(&#34;**First 5 trajectories time data analysis:**&#34;)
                        for debug_info in time_data_debug:
                            st.write(f&#34;**Trajectory {debug_info[&#39;trajectory_id&#39;]}:**&#34;)
                            st.write(f&#34;- Type: {debug_info[&#39;time_data_type&#39;]}&#34;)
                            st.write(f&#34;- Shape: {debug_info[&#39;time_data_shape&#39;]}&#34;)
                            st.write(f&#34;- Dtype: {debug_info[&#39;time_data_dtype&#39;]}&#34;)
                            st.write(f&#34;- Sample: {debug_info[&#39;time_data_sample&#39;]}&#34;)
                            st.write(&#34;---&#34;)
                else:
                    st.info(&#34;No time data debug information available&#34;)

                # Show detailed analysis of failing trajectories
                if trajectories_without_time_data &gt; 0:
                    with st.expander(&#34;🔍 Detailed Time Data Analysis&#34;, expanded=True):
                        st.write(f&#34;**Analysis of {trajectories_without_time_data} trajectories with invalid time data:**&#34;)

                        # Sample a few failing trajectories for detailed analysis
                        failing_samples = []
                        for i, traj in enumerate(st.session_state.trajectories):
                            if i &gt;= 10:  # Limit to first 10 for performance
                                break
                            try:
                                basic_metrics = compute_basic_trajectory_metrics(traj)
                                if basic_metrics[&#34;total_time&#34;] == 0:
                                    failing_samples.append({
                                        &#34;id&#34;: i,
                                        &#34;tid&#34;: traj.tid,
                                        &#34;time_data&#34;: traj.t[:5].tolist() if traj.t is not None and len(traj.t) &gt; 0 else None,
                                        &#34;time_dtype&#34;: str(traj.t.dtype) if traj.t is not None else None,
                                        &#34;time_shape&#34;: traj.t.shape if traj.t is not None else None,
                                        &#34;time_is_none&#34;: traj.t is None,
                                        &#34;time_length&#34;: len(traj.t) if traj.t is not None else 0
                                    })
                            except Exception as e:
                                failing_samples.append({
                                    &#34;id&#34;: i,
                                    &#34;tid&#34;: traj.tid,
                                    &#34;error&#34;: str(e)
                                })

                        if failing_samples:
                            st.write(&#34;**Sample failing trajectories:**&#34;)
                            for sample in failing_samples:
                                st.write(f&#34;**Trajectory {sample[&#39;id&#39;]} ({sample[&#39;tid&#39;]}):**&#34;)
                                if &#39;error&#39; in sample:
                                    st.write(f&#34;- Error: {sample[&#39;error&#39;]}&#34;)
                                else:
                                    st.write(f&#34;- Time data is None: {sample[&#39;time_is_none&#39;]}&#34;)
                                    st.write(f&#34;- Time data length: {sample[&#39;time_length&#39;]}&#34;)
                                    st.write(f&#34;- Time data sample: {sample[&#39;time_data&#39;]}&#34;)
                                    st.write(f&#34;- Time dtype: {sample[&#39;time_dtype&#39;]}&#34;)
                                    st.write(f&#34;- Time shape: {sample[&#39;time_shape&#39;]}&#34;)
                                st.write(&#34;---&#34;)

                        # Check if time data is completely missing
                        none_count = sum(1 for traj in st.session_state.trajectories[:10] if traj.t is None)
                        if none_count &gt; 0:
                            st.warning(f&#34;⚠️ {none_count} out of 10 sample trajectories have NO time data (t=None)&#34;)
                            st.write(&#34;**This suggests:**&#34;)
                            st.write(&#34;- Time column mapping is incorrect&#34;)
                            st.write(&#34;- Time column doesn&#39;t exist in CSV files&#34;)
                            st.write(&#34;- Time column is completely empty&#34;)

                            # Try to diagnose column mapping issue
                            st.write(&#34;**Column Mapping Diagnosis:**&#34;)
                            st.write(&#34;The GUI is looking for a time column, but it might not exist or be named differently.&#34;)
                            st.write(&#34;**Common time column names:**&#34;)
                            st.write(&#34;- &#39;Time&#39; (most common)&#34;)
                            st.write(&#34;- &#39;time&#39;&#34;)
                            st.write(&#34;- &#39;t&#39;&#34;)
                            st.write(&#34;- &#39;timestamp&#39;&#34;)
                            st.write(&#34;- &#39;Timestamp&#39;&#34;)
                            st.write(&#34;- &#39;TIME&#39;&#34;)
                            st.write(&#34;&#34;)
                            st.write(&#34;**To fix this:**&#34;)
                            st.write(&#34;1. Check the Data Upload tab&#34;)
                            st.write(&#34;2. Look at the &#39;Time Column&#39; field&#34;)
                            st.write(&#34;3. Make sure it matches a column name in your CSV files&#34;)
                            st.write(&#34;4. If unsure, try common names like &#39;Time&#39;, &#39;time&#39;, or &#39;t&#39;&#34;)

                        st.write(&#34;**Common time data issues:**&#34;)
                        st.write(&#34;- Empty strings or null values&#34;)
                        st.write(&#34;- Non-numeric text (e.g., &#39;Time: 1.23&#39;, &#39;1.23s&#39;)&#34;)
                        st.write(&#34;- Mixed data types in the same column&#34;)
                        st.write(&#34;- Missing or corrupted time data&#34;)
                        st.write(&#34;- Incorrect column mapping&#34;)

                # Store results - preserve existing analysis results
                if st.session_state.analysis_results is None:
                    st.session_state.analysis_results = {}
                st.session_state.analysis_results[&#34;metrics&#34;] = metrics

                # Save metrics to CSV file
                try:
                    import os
                    import pandas as pd
                    os.makedirs(&#34;gui_outputs&#34;, exist_ok=True)
                    df = pd.DataFrame(metrics)
                    csv_path = os.path.join(&#34;gui_outputs&#34;, &#34;metrics_results.csv&#34;)
                    df.to_csv(csv_path, index=False)
                    st.info(f&#34;📁 Metrics saved to: {csv_path}&#34;)
                except Exception as e:
                    st.warning(f&#34;⚠️ Could not save metrics to file: {e}&#34;)

                # Generate and save metrics plots for export and visualization reuse
                try:
                    import os
                    import glob
                    import pandas as pd
                    import matplotlib.pyplot as plt
                    import math

                    metrics_dir = os.path.join(&#34;gui_outputs&#34;, &#34;metrics&#34;)
                    os.makedirs(metrics_dir, exist_ok=True)

                    metrics_df = pd.DataFrame(metrics)

                    # Helper to safely save and close figures
                    def _save_fig(path):
                        plt.tight_layout()
                        plt.savefig(path, dpi=150)
                        plt.close()

                    # ---- Utilities for KDE and distribution fitting ----
                    def _kde_curve(values):
                        arr = np.asarray(values, dtype=float)
                        arr = arr[np.isfinite(arr)]
                        if len(arr) &lt; 2:
                            return None
                        std = np.std(arr)
                        if std == 0:
                            return None
                        n = len(arr)
                        # Silverman&#39;s rule of thumb
                        h = 1.06 * std * (n ** (-1.0 / 5.0))
                        xs = np.linspace(np.percentile(arr, 1), np.percentile(arr, 99), 200)
                        diffs = (xs[:, None] - arr[None, :]) / h
                        kernel = np.exp(-0.5 * diffs * diffs) / (math.sqrt(2.0 * math.pi))
                        density = np.sum(kernel, axis=1) / (n * h)
                        return xs, density

                    def _loglik_normal(arr, mu, sigma):
                        if sigma &lt;= 0:
                            return -np.inf
                        return np.sum(-0.5 * np.log(2 * np.pi) - np.log(sigma) - 0.5 * ((arr - mu) / sigma) ** 2)

                    def _loglik_lognormal(arr, mu_log, sigma_log):
                        if sigma_log &lt;= 0:
                            return -np.inf
                        if np.any(arr &lt;= 0):
                            return -np.inf
                        z = (np.log(arr) - mu_log) / sigma_log
                        return np.sum(-0.5 * np.log(2 * np.pi) - np.log(sigma_log) - np.log(arr) - 0.5 * z * z)

                    def _loglik_gamma(arr, k, theta):
                        if k &lt;= 0 or theta &lt;= 0:
                            return -np.inf
                        if np.any(arr &lt;= 0):
                            return -np.inf
                        # log pdf = (k-1)ln x - x/theta - k ln theta - lgamma(k)
                        return np.sum((k - 1) * np.log(arr) - arr / theta - k * np.log(theta) - math.lgamma(k))

                    def _fit_and_plot_overlays(ax, arr, xlabel, base_color, alt_color):
                        # KDE overlay
                        kde = _kde_curve(arr)
                        if kde is not None:
                            xs_kde, dens_kde = kde
                            ax.plot(xs_kde, dens_kde, color=alt_color, linewidth=2, alpha=0.9, label=&#34;KDE&#34;)

                        # Candidate distributions and AIC
                        candidates = []
                        n = len(arr)
                        if n &gt;= 2:
                            # Normal
                            mu = float(np.mean(arr))
                            sigma = float(np.std(arr))
                            ll_n = _loglik_normal(arr, mu, sigma) if sigma &gt; 0 else -np.inf
                            aic_n = 2 * 2 - 2 * ll_n  # 2 params
                            candidates.append({
                                &#34;name&#34;: &#34;Normal&#34;,
                                &#34;params&#34;: (mu, sigma),
                                &#34;aic&#34;: aic_n,
                                &#34;pdf&#34;: lambda x: (1.0 / (sigma * math.sqrt(2 * math.pi))) * np.exp(-0.5 * ((x - mu) / sigma) ** 2),
                                &#34;label&#34;: f&#34;Normal (μ={mu:.2f}, σ={sigma:.2f})&#34;
                            })

                            # Log-normal (positive values)
                            pos = arr[arr &gt; 0]
                            if len(pos) &gt;= 2 and np.std(np.log(pos)) &gt; 0:
                                mu_l = float(np.mean(np.log(pos)))
                                sigma_l = float(np.std(np.log(pos)))
                                ll_l = _loglik_lognormal(pos, mu_l, sigma_l)
                                aic_l = 2 * 2 - 2 * ll_l  # 2 params
                                candidates.append({
                                    &#34;name&#34;: &#34;LogNormal&#34;,
                                    &#34;params&#34;: (mu_l, sigma_l),
                                    &#34;aic&#34;: aic_l,
                                    &#34;pdf&#34;: lambda x: np.where(x &gt; 0, (1.0 / (x * sigma_l * math.sqrt(2 * math.pi))) * np.exp(-0.5 * ((np.log(x) - mu_l) / sigma_l) ** 2), 0.0),
                                    &#34;label&#34;: f&#34;LogNormal (μlog={mu_l:.2f}, σlog={sigma_l:.2f})&#34;
                                })

                            # Gamma (method of moments)
                            if np.all(arr &gt; 0) and np.var(arr) &gt; 0:
                                mean = float(np.mean(arr))
                                var = float(np.var(arr))
                                k = mean * mean / var
                                theta = var / mean
                                ll_g = _loglik_gamma(arr, k, theta)
                                aic_g = 2 * 2 - 2 * ll_g  # 2 params (k, theta)
                                candidates.append({
                                    &#34;name&#34;: &#34;Gamma&#34;,
                                    &#34;params&#34;: (k, theta),
                                    &#34;aic&#34;: aic_g,
                                    &#34;pdf&#34;: lambda x: np.where(x &gt; 0, (x ** (k - 1)) * np.exp(-x / theta) / (math.gamma(k) * (theta ** k)), 0.0),
                                    &#34;label&#34;: f&#34;Gamma (k={k:.2f}, θ={theta:.2f})&#34;
                                })

                        if candidates:
                            best = min(candidates, key=lambda c: c[&#34;aic&#34;])
                            xs = np.linspace(np.percentile(arr, 1), np.percentile(arr, 99), 200)
                            ax.plot(xs, best[&#34;pdf&#34;](xs), color=base_color, linewidth=2.5, label=f&#34;Best: {best[&#39;label&#39;]} (AIC {best[&#39;aic&#39;]:.1f})&#34;)
                            ax.legend()

                    # 1) Total Time Distribution (+ KDE and best-fit distribution)
                    if &#34;total_time&#34; in metrics_df.columns and metrics_df[&#34;total_time&#34;].notna().any():
                        plt.figure(figsize=(8, 4))
                        vals = metrics_df[&#34;total_time&#34;].dropna().to_numpy()
                        ax = plt.gca()
                        ax.hist(vals, bins=30, color=&#34;#4C78A8&#34;, alpha=0.55, density=True, edgecolor=&#34;none&#34;)
                        _fit_and_plot_overlays(ax, vals, xlabel=&#34;Seconds&#34;, base_color=&#34;#1F77B4&#34;, alt_color=&#34;#4C78A8&#34;)
                        ax.set_title(&#34;Total Time Distribution (s)&#34;)
                        ax.set_xlabel(&#34;Seconds&#34;)
                        ax.set_ylabel(&#34;Density&#34;)
                        _save_fig(os.path.join(metrics_dir, &#34;total_time_distribution.png&#34;))

                    # 2) Average Speed Distribution (+ KDE and best-fit distribution)
                    if &#34;average_speed&#34; in metrics_df.columns and metrics_df[&#34;average_speed&#34;].notna().any():
                        plt.figure(figsize=(8, 4))
                        vals = metrics_df[&#34;average_speed&#34;].dropna().to_numpy()
                        ax = plt.gca()
                        ax.hist(vals, bins=30, color=&#34;#F58518&#34;, alpha=0.55, density=True, edgecolor=&#34;none&#34;)
                        _fit_and_plot_overlays(ax, vals, xlabel=&#34;Speed&#34;, base_color=&#34;#DD8452&#34;, alt_color=&#34;#F58518&#34;)
                        ax.set_title(&#34;Average Speed Distribution&#34;)
                        ax.set_xlabel(&#34;Speed&#34;)
                        ax.set_ylabel(&#34;Density&#34;)
                        _save_fig(os.path.join(metrics_dir, &#34;average_speed_distribution.png&#34;))

                    # 3) Total Distance Distribution (+ KDE and best-fit distribution)
                    if &#34;total_distance&#34; in metrics_df.columns and metrics_df[&#34;total_distance&#34;].notna().any():
                        plt.figure(figsize=(8, 4))
                        vals = metrics_df[&#34;total_distance&#34;].dropna().to_numpy()
                        ax = plt.gca()
                        ax.hist(vals, bins=30, color=&#34;#54A24B&#34;, alpha=0.55, density=True, edgecolor=&#34;none&#34;)
                        _fit_and_plot_overlays(ax, vals, xlabel=&#34;Distance&#34;, base_color=&#34;#2CA02C&#34;, alt_color=&#34;#54A24B&#34;)
                        ax.set_title(&#34;Total Distance Distribution&#34;)
                        ax.set_xlabel(&#34;Distance&#34;)
                        ax.set_ylabel(&#34;Density&#34;)
                        _save_fig(os.path.join(metrics_dir, &#34;total_distance_distribution.png&#34;))

                    # Discover junction-related columns
                    junction_time_cols = [c for c in metrics_df.columns if c.startswith(&#34;junction_&#34;) and c.endswith(&#34;_time&#34;)]
                    speed_cols = [c for c in metrics_df.columns if c.startswith(&#34;junction_&#34;) and c.endswith(&#34;_speed&#34;)]

                    # 4) Speed vs Time Correlation (means per junction for selected speed metrics)
                    # Use average transit speed if available; else fall back to generic _speed
                    suffix_candidates = [&#34;_avg_transit_speed&#34;, &#34;_speed&#34;]
                    chosen_speed_cols = []
                    for sfx in suffix_candidates:
                        candidate = [c for c in speed_cols if c.endswith(sfx)]
                        if candidate:
                            chosen_speed_cols = candidate
                            break
                    if chosen_speed_cols and junction_time_cols:
                        means_speed = []
                        means_time = []
                        labels = []
                        for sc in chosen_speed_cols:
                            jn = sc.split(&#34;_&#34;)[1]
                            tc = f&#34;junction_{jn}_time&#34;
                            if tc in metrics_df.columns:
                                df_pair = metrics_df[[sc, tc]].dropna()
                                if len(df_pair) &gt; 0:
                                    means_speed.append(df_pair[sc].mean())
                                    means_time.append(df_pair[tc].mean())
                                    labels.append(f&#34;J{jn}&#34;)
                        if means_speed and means_time:
                            plt.figure(figsize=(6, 6))
                            plt.scatter(means_time, means_speed, c=&#34;#4C78A8&#34;)
                            for x, y, lab in zip(means_time, means_speed, labels):
                                plt.annotate(lab, (x, y), xytext=(5, 5), textcoords=&#39;offset points&#39;, fontsize=8)
                            plt.title(&#34;Speed vs Time (means per junction)&#34;)
                            plt.xlabel(&#34;Time (s)&#34;)
                            plt.ylabel(&#34;Speed&#34;)
                            _save_fig(os.path.join(metrics_dir, &#34;speed_vs_time_correlation.png&#34;))

                    # 5) Entry vs Exit Speed by Junction (grouped bars)
                    entry_cols = [c for c in metrics_df.columns if c.endswith(&#34;_entry_speed&#34;)]
                    exit_cols = [c for c in metrics_df.columns if c.endswith(&#34;_exit_speed&#34;)]
                    if entry_cols and exit_cols:
                        data = []
                        labels_j = []
                        for ec in entry_cols:
                            jn = ec.split(&#34;_&#34;)[1]
                            xc = f&#34;junction_{jn}_exit_speed&#34;
                            if xc in metrics_df.columns:
                                e_vals = metrics_df[ec].dropna()
                                x_vals = metrics_df[xc].dropna()
                                if len(e_vals) &gt; 0 and len(x_vals) &gt; 0:
                                    data.append((e_vals.mean(), x_vals.mean()))
                                    labels_j.append(f&#34;J{jn}&#34;)
                        if data:
                            entry_means = [d[0] for d in data]
                            exit_means = [d[1] for d in data]
                            x = np.arange(len(labels_j))
                            width = 0.35
                            plt.figure(figsize=(max(6, len(labels_j) * 0.6), 4))
                            plt.bar(x - width/2, entry_means, width, label=&#39;Entry&#39;, color=&#39;#72B7B2&#39;)
                            plt.bar(x + width/2, exit_means, width, label=&#39;Exit&#39;, color=&#39;#E45756&#39;)
                            plt.xticks(x, labels_j)
                            plt.title(&#34;Entry vs Exit Speed by Junction&#34;)
                            plt.xlabel(&#34;Junction&#34;)
                            plt.ylabel(&#34;Speed&#34;)
                            plt.legend()
                            _save_fig(os.path.join(metrics_dir, &#34;entry_exit_speed_by_junction.png&#34;))

                    # 6) Junction Timing Comparison (average times)
                    if junction_time_cols:
                        jt_labels = []
                        jt_means = []
                        for tc in sorted(junction_time_cols, key=lambda c: int(c.split(&#39;_&#39;)[1])):
                            jn = tc.split(&#34;_&#34;)[1]
                            vals = metrics_df[tc].dropna()
                            if len(vals) &gt; 0:
                                jt_labels.append(f&#34;J{jn}&#34;)
                                jt_means.append(vals.mean())
                        if jt_labels:
                            x = np.arange(len(jt_labels))
                            plt.figure(figsize=(max(6, len(jt_labels) * 0.6), 4))
                            plt.bar(x, jt_means, color=&#34;#4C78A8&#34;)
                            plt.xticks(x, jt_labels)
                            plt.title(&#34;Average Junction Timing&#34;)
                            plt.xlabel(&#34;Junction&#34;)
                            plt.ylabel(&#34;Time (s)&#34;)
                            _save_fig(os.path.join(metrics_dir, &#34;junction_timing_comparison.png&#34;))

                    # 7) Individual Junction Timing Distributions (per junction) (+ KDE and best-fit distribution)
                    if junction_time_cols:
                        for tc in sorted(junction_time_cols, key=lambda c: int(c.split(&#39;_&#39;)[1])):
                            jn = tc.split(&#34;_&#34;)[1]
                            vals = metrics_df[tc].dropna().to_numpy()
                            if len(vals) &gt; 0:
                                plt.figure(figsize=(8, 4))
                                ax = plt.gca()
                                ax.hist(vals, bins=30, color=&#34;#B279A2&#34;, alpha=0.55, density=True, edgecolor=&#34;none&#34;)
                                _fit_and_plot_overlays(ax, vals, xlabel=&#34;Seconds&#34;, base_color=&#34;#A05FA3&#34;, alt_color=&#34;#B279A2&#34;)
                                ax.set_title(f&#34;Junction {jn} Timing Distribution (s)&#34;)
                                ax.set_xlabel(&#34;Seconds&#34;)
                                ax.set_ylabel(&#34;Density&#34;)
                                _save_fig(os.path.join(metrics_dir, f&#34;timing_distribution_J{jn}.png&#34;))

                    # Store paths in session for downstream tabs
                    try:
                        images = {}
                        for p in glob.glob(os.path.join(metrics_dir, &#34;*.png&#34;)):
                            images[os.path.basename(p)] = p
                        st.session_state.analysis_results.setdefault(&#34;metrics_images&#34;, images)
                    except Exception:
                        pass

                except Exception as e:
                    st.warning(f&#34;⚠️ Could not generate metrics plots: {e}&#34;)

                # Provide detailed success message
                success_msg = f&#34;✅ Computed metrics for {len(metrics)} trajectories&#34;
                if trajectories_with_time_data &gt; 0:
                    success_msg += f&#34; ({trajectories_with_time_data} with time data)&#34;
                if trajectories_without_time_data &gt; 0:
                    success_msg += f&#34; ({trajectories_without_time_data} without valid time data)&#34;

                st.success(success_msg)

                # Show warning if many trajectories lack time data
                if trajectories_without_time_data &gt; len(metrics) * 0.5:
                    st.warning(f&#34;⚠️ {trajectories_without_time_data} trajectories lack valid time data. This may indicate time data format issues. Check the debug information above.&#34;)

                    # Provide suggestions for fixing time data issues
                    with st.expander(&#34;💡 Suggestions for Fixing Time Data Issues&#34;, expanded=False):
                        st.write(&#34;**To fix time data issues, try these solutions:**&#34;)
                        st.write(&#34;&#34;)
                        st.write(&#34;**1. Check Column Mapping:**&#34;)
                        st.write(&#34;- Verify the time column is correctly mapped&#34;)
                        st.write(&#34;- Look for columns like &#39;Time&#39;, &#39;timestamp&#39;, &#39;t&#39;, etc.&#34;)
                        st.write(&#34;&#34;)
                        st.write(&#34;**2. Check Data Format:**&#34;)
                        st.write(&#34;- Time data should be numeric (e.g., 0, 1.5, 2.3)&#34;)
                        st.write(&#34;- Avoid text formats like &#39;Time: 1.23&#39; or &#39;1.23s&#39;&#34;)
                        st.write(&#34;- Remove quotes around time values&#34;)
                        st.write(&#34;&#34;)
                        st.write(&#34;**3. Data Cleaning:**&#34;)
                        st.write(&#34;- Remove empty rows or null values&#34;)
                        st.write(&#34;- Ensure consistent data types in time column&#34;)
                        st.write(&#34;- Check for mixed formats in the same column&#34;)
                        st.write(&#34;&#34;)
                        st.write(&#34;**4. File Format:**&#34;)
                        st.write(&#34;- Ensure CSV files are properly formatted&#34;)
                        st.write(&#34;- Check for encoding issues (UTF-8 recommended)&#34;)
                        st.write(&#34;- Verify file is not corrupted&#34;)
                        st.write(&#34;&#34;)
                        st.write(&#34;**5. Manual Inspection:**&#34;)
                        st.write(&#34;- Open the CSV file in a text editor&#34;)
                        st.write(&#34;- Look at the first few rows of the time column&#34;)
                        st.write(&#34;- Check for patterns in the data format&#34;)

                # Generate CLI command for easy copying
                # Build results dict for CLI command generation
                metrics_results = {}
                for i, junction in enumerate(junctions_to_use):
                    junction_key = f&#34;junction_{i}&#34;
                    metrics_results[junction_key] = {
                        &#34;junction&#34;: junction,
                        &#34;r_outer&#34;: st.session_state.junction_r_outer.get(i, 50.0) if i &lt; len(st.session_state.junctions) else 50.0,
                        &#34;decision_mode&#34;: getattr(st.session_state, &#39;metrics_decision_mode&#39;, &#39;pathlen&#39;),
                        &#34;distance&#34;: getattr(st.session_state, &#39;metrics_distance&#39;, 100.0),
                        &#34;r_outer_value&#34;: st.session_state.junction_r_outer.get(i, 50.0) if i &lt; len(st.session_state.junctions) else 50.0,
                        &#34;trend_window&#34;: getattr(st.session_state, &#39;metrics_trend_window&#39;, 5),
                        &#34;min_outward&#34;: getattr(st.session_state, &#39;metrics_min_outward&#39;, 0.0),
                    }
                self.generate_cli_command(&#34;metrics&#34;, metrics_results, cluster_method, cluster_params, decision_mode, decision_params)

            elif analysis_type == &#34;gaze&#34;:
                # Analyze gaze and physiological data
                gaze_results = {}

                # Create dedicated debug container for gaze analysis
                gaze_debug_container = st.empty()

                def update_gaze_debug_display():
                    &#34;&#34;&#34;Update the persistent gaze debug display&#34;&#34;&#34;

                # Initialize gaze debug info
                st.session_state[&#39;gaze_debug_info&#39;] = {}

                # Check if we have proper gaze trajectory data (prefer trajectories that actually carry data)
                active_trajs = st.session_state.trajectories
                # Build a filtered list that actually has gaze OR physio data
                try:
                    from verta.verta_data_loader import has_gaze_data as _has_gaze, has_physio_data as _has_physio
                    trajs_with_signals = [t for t in active_trajs if (_has_gaze(t) or _has_physio(t))]

                    # Debug: Show filtering results
                    st.info(f&#34;🔍 **Trajectory Filtering Debug:**&#34;)
                    st.write(f&#34;- Total trajectories: {len(active_trajs)}&#34;)
                    st.write(f&#34;- Trajectories with gaze/physio data: {len(trajs_with_signals)}&#34;)

                    if len(trajs_with_signals) &lt; len(active_trajs):
                        skipped_count = len(active_trajs) - len(trajs_with_signals)
                        st.info(f&#34;ℹ️ Skipped {skipped_count} trajectories without gaze/physiological data&#34;)

                except Exception as e:
                    st.warning(f&#34;⚠️ Error filtering trajectories: {e}&#34;)
                    trajs_with_signals = active_trajs

                # Use the filtered list if it has any valid trajectories
                if trajs_with_signals and len(trajs_with_signals) &gt; 0:
                    active_trajs = trajs_with_signals
                else:
                    st.warning(&#34;⚠️ No trajectories with gaze/physiological data found&#34;)
                    active_trajs = []
                has_gaze_data = self._check_for_gaze_data(active_trajs)
                # If global check still fails but we have some physio data, allow comprehensive path to proceed
                try:
                    from verta.verta_data_loader import has_physio_data as _has_physio
                    has_any_physio = any(_has_physio(t) for t in active_trajs)
                except Exception:
                    has_any_physio = False

                # Get column mappings from session state (if they were specified)
                column_mappings = self._get_gaze_column_mappings()

                if not has_gaze_data and not has_any_physio and not column_mappings:
                    st.warning(&#34;⚠️ **No Gaze Data Available**&#34;)
                    st.info(&#34;&#34;&#34;
                    **Gaze analysis requires trajectories with:**
                    - Head tracking data (`head_forward_x`, `head_forward_z`)
                    - Eye tracking data (`gaze_x`, `gaze_y`)
                    - Physiological data (`pupil_l`, `pupil_r`, `heart_rate`)

                    **Current trajectories only have position data (x, z, t).**

                    Proceeding with movement-only fallback so visualizations still render.
                    &#34;&#34;&#34;)

                    # Compute movement-only fallback per junction
                    for i, junction in enumerate(st.session_state.junctions):
                        junction_key = f&#34;junction_{i}&#34;
                        r_outer = st.session_state.junction_r_outer.get(i, 50.0)
                        try:
                            movement_df = self._analyze_movement_patterns_optimized(
                                active_trajs, junction, r_outer, decision_mode, path_length=100.0, epsilon=0.05
                            )
                            # Tag as movement analysis for downstream UI
                            if hasattr(movement_df, &#39;assign&#39;):
                                movement_df = movement_df.assign(analysis_type=&#39;movement&#39;)
                            gaze_results[junction_key] = movement_df
                        except Exception as e:
                            st.warning(f&#34;⚠️ Movement fallback failed for {junction_key}: {e}&#34;)
                            gaze_results[junction_key] = None
                else:
                    # Perform gaze analysis (use the active trajectories we already determined)

                    # Debug: Track session state before analysis
                    st.session_state.debug_session_state = {
                        &#39;trajectories_count&#39;: len(st.session_state.trajectories) if st.session_state.trajectories else 0,
                        &#39;gaze_trajectories_count&#39;: 0,
                        &#39;last_modified&#39;: &#39;before_gaze_analysis&#39;
                    }

                    # Create global heatmap ONCE (outside junction loop)
                    global_heatmap_data = None
                    cell_size = st.session_state.get(&#39;pupil_heatmap_cell_size&#39;, 50.0)
                    normalization = st.session_state.get(&#39;pupil_heatmap_normalization&#39;, &#39;relative&#39;)

                    # Define output directory for global plots
                    import os
                    global_out_dir = os.path.join(&#34;gui_outputs&#34;, &#34;gaze_plots&#34;)
                    os.makedirs(global_out_dir, exist_ok=True)

                    st.info(&#34;🗺️ Creating global pupil dilation heatmap...&#34;)
                    try:
                        global_heatmap_data = create_pupil_dilation_heatmap(
                            trajectories=active_trajs,
                            junctions=st.session_state.junctions,
                            cell_size=cell_size,
                            normalization=normalization
                        )
                        st.success(&#34;✅ Global heatmap created&#34;)

                        # Store global heatmap data for consistent scaling calculation
                        st.session_state[&#39;global_heatmap_data&#39;] = global_heatmap_data

                        # Generate global heatmap plot
                        try:
                            from verta.verta_gaze import plot_pupil_dilation_heatmap
                            import matplotlib.pyplot as plt
                            import os

                            # Create global heatmap plot
                            global_plot_path = os.path.join(global_out_dir, &#34;global_pupil_heatmap.png&#34;)

                            fig = plot_pupil_dilation_heatmap(
                                heatmap_data=global_heatmap_data,
                                junctions=st.session_state.junctions,
                                trajectories=active_trajs,
                                all_trajectories=active_trajs,
                                title=&#34;Global Pupil Dilation Heatmap&#34;,
                                show_sample_counts=False,
                                show_minimap=False,
                                vmin=None,  # Let the function determine scaling
                                vmax=None
                            )

                            # Save the plot
                            fig.savefig(global_plot_path, dpi=150, bbox_inches=&#34;tight&#34;)
                            plt.close(fig)

                            st.info(f&#34;📁 Global heatmap plot saved to: {global_plot_path}&#34;)
                            st.write(f&#34;🔍 **Debug:** Global plot saved to: `{global_plot_path}`&#34;)
                            st.write(f&#34;🔍 **Debug:** File exists after save: {os.path.exists(global_plot_path)}&#34;)

                        except Exception as plot_e:
                            st.warning(f&#34;⚠️ Could not generate global heatmap plot: {plot_e}&#34;)

                    except Exception as e:
                        st.warning(f&#34;⚠️ Could not create global heatmap: {e}&#34;)

                    # Debug: Check junctions
                    st.info(f&#34;🔍 **Junction Debug:**&#34;)
                    st.write(f&#34;- Number of junctions: {len(st.session_state.junctions)}&#34;)
                    if st.session_state.junctions:
                        for i, junction in enumerate(st.session_state.junctions):
                            r_outer = st.session_state.junction_r_outer.get(i, 50.0)
                            st.write(f&#34;- Junction {i}: Circle(cx={junction.cx}, cz={junction.cz}, r={junction.r}), r_outer={r_outer}&#34;)
                    else:
                        st.error(&#34;❌ **No junctions defined!** Gaze analysis requires junctions to be defined.&#34;)
                        st.write(&#34;**Solution:** Go to the Junction Editor tab and define at least one junction.&#34;)
                        return

                    # CRITICAL FIX: Perform comprehensive gaze analysis for ALL junctions at once
                    # This ensures all junctions have access to the complete assignments DataFrame
                    st.info(&#34;🔍 **Performing comprehensive gaze analysis for all junctions...**&#34;)

                    # Create output directory for all junctions
                    import os
                    import pandas as pd
                    out_dir = os.path.join(&#34;gui_outputs&#34;, &#34;gaze_analysis&#34;)
                    os.makedirs(out_dir, exist_ok=True)

                    # Get r_outer values for all junctions
                    r_outer_list = [st.session_state.junction_r_outer.get(i, 50.0) for i in range(len(st.session_state.junctions))]

                    # Perform comprehensive gaze analysis for ALL junctions at once
                    try:
                        gaze_data_all = self._perform_comprehensive_gaze_analysis_all_junctions(
                            trajectories=active_trajs,
                            junctions=st.session_state.junctions,
                            r_outer_list=r_outer_list,
                            decision_mode=decision_mode,
                            path_length=100.0,
                            epsilon=0.05,
                            linger_delta=0.0,
                            out_dir=out_dir
                        )
                    except Exception as e:
                        st.error(f&#34;❌ **Comprehensive gaze analysis failed:** {e}&#34;)
                        st.write(f&#34;**Error type:** {type(e).__name__}&#34;)
                        st.write(f&#34;**Error message:** {str(e)}&#34;)

                        # Fall back to individual junction analysis
                        st.info(&#34;🔄 **Falling back to individual junction analysis...**&#34;)
                        for i, junction in enumerate(st.session_state.junctions):
                            junction_key = f&#34;junction_{i}&#34;
                            r_outer = r_outer_list[i]

                            try:
                                gaze_data = self._perform_gaze_analysis_with_mappings(
                                    trajectories=active_trajs,
                                    junction=junction,
                                    r_outer=r_outer,
                                    decision_mode=decision_mode,
                                    path_length=100.0,
                                    epsilon=0.05,
                                    linger_delta=0.0,
                                    out_dir=out_dir,
                                    column_mappings=column_mappings,
                                    scale_factor=1.0
                                )

                                # Normalize columns so downstream plots find expected names
                                if isinstance(gaze_data, dict):
                                    gaze_data = self._normalize_gaze_result_frames(gaze_data)

                                # Store the comprehensive gaze analysis results
                                gaze_results[junction_key] = gaze_data

                                st.success(f&#34;✅ Completed fallback gaze analysis for {junction_key}&#34;)

                                # Generate gaze plots immediately after analysis
                                try:
                                    st.info(f&#34;📊 Generating gaze plots for {junction_key}...&#34;)
                                    self._generate_gaze_plots_during_analysis(gaze_data, junction_key, out_dir)
                                    st.success(f&#34;✅ Generated plots for {junction_key}&#34;)
                                except Exception as e:
                                    st.warning(f&#34;⚠️ Plot generation failed for {junction_key}: {e}&#34;)

                                # Update debug display after each junction
                                update_gaze_debug_display()

                            except Exception as e:
                                st.warning(f&#34;⚠️ Fallback gaze analysis failed for {junction_key}: {e}&#34;)

                                # Store error information
                                gaze_results[junction_key] = {
                                    &#39;error&#39;: str(e),
                                    &#39;error_type&#39;: type(e).__name__,
                                    &#39;junction&#39;: junction,
                                    &#39;r_outer&#39;: r_outer,
                                    &#39;physiological&#39;: None,
                                    &#39;pupil_dilation&#39;: None,
                                    &#39;head_yaw&#39;: None
                                }

                                # Update debug display even on error
                                update_gaze_debug_display()
                                continue

                        # Skip the rest of the multi-junction processing
                        gaze_data_all = None

                    # The comprehensive analysis returns data for all junctions
                    # We need to split it by junction for storage
                    if gaze_data_all is not None and isinstance(gaze_data_all, dict) and &#39;head_yaw&#39; in gaze_data_all:
                            # Split the results by junction
                            head_yaw_df = gaze_data_all[&#39;head_yaw&#39;]
                            physio_df = gaze_data_all.get(&#39;physiological&#39;)
                            pupil_df = gaze_data_all.get(&#39;pupil_dilation&#39;)
                            all_heatmaps = gaze_data_all.get(&#39;pupil_heatmap_junction&#39;, {})

                            # Process each junction&#39;s data
                            for i, junction in enumerate(st.session_state.junctions):
                                junction_key = f&#34;junction_{i}&#34;

                                # Filter data for this junction
                                junction_head_yaw = head_yaw_df[head_yaw_df[&#39;junction&#39;] == i] if not head_yaw_df.empty else pd.DataFrame()
                                junction_physio = physio_df[physio_df[&#39;junction&#39;] == i] if physio_df is not None and not physio_df.empty else None
                                junction_pupil = pupil_df[pupil_df[&#39;junction&#39;] == i] if pupil_df is not None and not pupil_df.empty else None

                                # Get heatmap data for this junction
                                junction_heatmap = all_heatmaps.get(i) if all_heatmaps else None

                                # Create junction-specific gaze data
                                gaze_data = {
                                    &#39;head_yaw&#39;: junction_head_yaw,
                                    &#39;physiological&#39;: junction_physio,
                                    &#39;pupil_dilation&#39;: junction_pupil,
                                    &#39;pupil_heatmap_junction&#39;: {i: junction_heatmap} if junction_heatmap else {},
                                    &#39;junction&#39;: junction,
                                    &#39;r_outer&#39;: r_outer_list[i]
                                }

                                st.info(f&#34;🔍 **Processing junction {i}: {junction_key}**&#34;)
                                st.write(f&#34;- Junction: Circle(cx={junction.cx}, cz={junction.cz}, r={junction.r})&#34;)
                                st.write(f&#34;- R_outer: {r_outer_list[i]}&#34;)
                                st.write(f&#34;- Head yaw records: {len(junction_head_yaw)}&#34;)
                                st.write(f&#34;- Physiological records: {len(junction_physio) if junction_physio is not None else 0}&#34;)
                                st.write(f&#34;- Pupil records: {len(junction_pupil) if junction_pupil is not None else 0}&#34;)

                                # Normalize columns so downstream plots find expected names
                                if isinstance(gaze_data, dict):
                                    gaze_data = self._normalize_gaze_result_frames(gaze_data)

                                # Store the comprehensive gaze analysis results
                                gaze_results[junction_key] = gaze_data

                                # Debug: Verify data was saved correctly
                                st.info(f&#34;🔍 **Data Storage Verification for {junction_key}:**&#34;)
                                if isinstance(gaze_data, dict):
                                    for data_type, data in gaze_data.items():
                                        if data is not None:
                                            if hasattr(data, &#39;shape&#39;):
                                                st.write(f&#34;- {data_type}: {data.shape} DataFrame&#34;)
                                            elif isinstance(data, list):
                                                st.write(f&#34;- {data_type}: {len(data)} records&#34;)
                                            else:
                                                st.write(f&#34;- {data_type}: {type(data).__name__}&#34;)
                                        else:
                                            st.write(f&#34;- {data_type}: None&#34;)
                                else:
                                    st.write(f&#34;- Raw data type: {type(gaze_data).__name__}&#34;)

                                st.success(f&#34;✅ Completed gaze analysis for {junction_key}&#34;)

                                # Generate gaze plots immediately after analysis
                                try:
                                    st.info(f&#34;📊 Generating gaze plots for {junction_key}...&#34;)
                                    self._generate_gaze_plots_during_analysis(gaze_data, junction_key, out_dir)
                                    st.success(f&#34;✅ Generated plots for {junction_key}&#34;)
                                except Exception as e:
                                    st.warning(f&#34;⚠️ Plot generation failed for {junction_key}: {e}&#34;)

                                # Update debug display after each junction
                                update_gaze_debug_display()
                    else:
                        st.error(&#34;❌ **Comprehensive gaze analysis failed to return expected data structure**&#34;)
                        st.write(f&#34;Returned data type: {type(gaze_data_all)}&#34;)
                        if isinstance(gaze_data_all, dict):
                            st.write(f&#34;Keys: {list(gaze_data_all.keys())}&#34;)

                        # Fall back to individual junction analysis
                        st.info(&#34;🔄 **Falling back to individual junction analysis...**&#34;)
                        for i, junction in enumerate(st.session_state.junctions):
                            junction_key = f&#34;junction_{i}&#34;
                            r_outer = r_outer_list[i]

                            try:
                                gaze_data = self._perform_gaze_analysis_with_mappings(
                                    trajectories=active_trajs,
                                    junction=junction,
                                    r_outer=r_outer,
                                    decision_mode=decision_mode,
                                    path_length=100.0,
                                    epsilon=0.05,
                                    linger_delta=0.0,
                                    out_dir=out_dir,
                                    column_mappings=column_mappings,
                                    scale_factor=1.0
                                )

                                # Normalize columns so downstream plots find expected names
                                if isinstance(gaze_data, dict):
                                    gaze_data = self._normalize_gaze_result_frames(gaze_data)

                                # Store the comprehensive gaze analysis results
                                gaze_results[junction_key] = gaze_data

                                st.success(f&#34;✅ Completed fallback gaze analysis for {junction_key}&#34;)

                                # Generate gaze plots immediately after analysis
                                try:
                                    st.info(f&#34;📊 Generating gaze plots for {junction_key}...&#34;)
                                    self._generate_gaze_plots_during_analysis(gaze_data, junction_key, out_dir)
                                    st.success(f&#34;✅ Generated plots for {junction_key}&#34;)
                                except Exception as e:
                                    st.warning(f&#34;⚠️ Plot generation failed for {junction_key}: {e}&#34;)

                                # Update debug display after each junction
                                update_gaze_debug_display()

                            except Exception as e:
                                st.warning(f&#34;⚠️ Fallback gaze analysis failed for {junction_key}: {e}&#34;)

                                # Store error information
                                gaze_results[junction_key] = {
                                    &#39;error&#39;: str(e),
                                    &#39;error_type&#39;: type(e).__name__,
                                    &#39;junction&#39;: junction,
                                    &#39;r_outer&#39;: r_outer,
                                    &#39;physiological&#39;: None,
                                    &#39;pupil_dilation&#39;: None,
                                    &#39;head_yaw&#39;: None
                                }

                                # Update debug display even on error
                                update_gaze_debug_display()
                                continue

                # Store results - preserve existing analysis results
                if st.session_state.analysis_results is None:
                    st.session_state.analysis_results = {}
                st.session_state.analysis_results[&#34;gaze_results&#34;] = gaze_results

                # Store global heatmap separately (only once)
                if global_heatmap_data is not None:
                    st.session_state.analysis_results[&#34;pupil_heatmap_global&#34;] = global_heatmap_data

                # Show summary
                successful_junctions = len([k for k, v in gaze_results.items() if v is not None])
                total_junctions = len(st.session_state.junctions)
                if successful_junctions == total_junctions:
                    st.success(f&#34;✅ Gaze analysis completed successfully for all {total_junctions} junctions!&#34;)
                else:
                    st.warning(f&#34;⚠️ Gaze analysis completed for {successful_junctions}/{total_junctions} junctions&#34;)

                # Generate CLI command for easy copying
                # Build results dict for CLI command generation
                gaze_results_dict = {}
                for i, junction in enumerate(st.session_state.junctions):
                    junction_key = f&#34;junction_{i}&#34;
                    gaze_results_dict[junction_key] = {
                        &#34;junction&#34;: junction,
                        &#34;r_outer&#34;: st.session_state.junction_r_outer.get(i, 50.0),
                        &#34;decision_mode&#34;: decision_mode,
                        &#34;path_length&#34;: decision_params.get(&#34;path_length&#34;, 100.0) if decision_params else 100.0,
                        &#34;epsilon&#34;: decision_params.get(&#34;epsilon&#34;, 0.05) if decision_params else 0.05,
                        &#34;linger_delta&#34;: decision_params.get(&#34;linger_delta&#34;, 5.0) if decision_params else 5.0,
                    }
                self.generate_cli_command(&#34;gaze&#34;, gaze_results_dict, cluster_method, cluster_params, decision_mode, decision_params)

            elif analysis_type == &#34;predict&#34;:
                # Run prediction analysis using spatial tracking only
                # Create output directory for prediction results
                import os
                output_dir = &#34;gui_outputs&#34;
                os.makedirs(output_dir, exist_ok=True)

                # Skip discover_decision_chain - use spatial tracking only
                # Create empty chain_df for compatibility
                import pandas as pd
                chain_df = pd.DataFrame(columns=[&#39;trajectory&#39;])
                chain_df[&#39;trajectory&#39;] = [i for i in range(len(st.session_state.trajectories))]

                print(f&#34;🔍 DEBUG: Using spatial tracking only - skipping discover_decision_chain&#34;)
                print(f&#34;🔍 DEBUG: Created empty chain_df with {len(chain_df)} trajectories&#34;)

                # Define r_outer_list for predict analysis
                r_outer_list = [st.session_state.junction_r_outer.get(i, 50.0) for i in range(len(st.session_state.junctions))]

                print(f&#34;🔍 DEBUG: Starting analyze_junction_choice_patterns with spatial tracking only...&#34;)

                # Debug: Check what trajectories visit multiple junctions
                print(f&#34;\n🔍 DEBUG: Analyzing trajectory junction visits...&#34;)
                multi_junction_trajectories = 0
                consecutive_junction_trajectories = 0

                # COMMENTED OUT: This code referenced norm_df which we removed
                # for idx, row in norm_df.iterrows():
                #     traj_id = row[&#39;trajectory&#39;]
                #     visited_junctions = []
                #     for i in range(7):  # 7 junctions
                #         col = f&#39;branch_j{i}&#39;
                #         if pd.notna(row[col]) and row[col] &gt;= 0:  # Valid branch assignment
                #             visited_junctions.append(i)
                #
                #     if len(visited_junctions) &gt; 1:
                #         multi_junction_trajectories += 1
                #         # Check if junctions are consecutive (for flow analysis)
                #         if len(visited_junctions) &gt;= 2:
                #             consecutive_junction_trajectories += 1
                #             if consecutive_junction_trajectories &lt;= 5:  # Show first 5 examples
                #                 print(f&#34;🔍 DEBUG: Trajectory {traj_id} visits junctions: {visited_junctions}&#34;)

                print(f&#34;🔍 DEBUG: Trajectories visiting multiple junctions: {multi_junction_trajectories}&#34;)
                print(f&#34;🔍 DEBUG: Trajectories with consecutive visits: {consecutive_junction_trajectories}&#34;)

                # Debug: Check r_outer_list values
                print(f&#34;\n🔍 DEBUG: r_outer_list values: {r_outer_list}&#34;)
                print(f&#34;🔍 DEBUG: r_outer_list length: {len(r_outer_list)}&#34;)
                print(f&#34;🔍 DEBUG: r_outer_list types: {[type(r) for r in r_outer_list]}&#34;)

                # Debug: Check junction radii
                print(f&#34;\n🔍 DEBUG: Junction radii:&#34;)
                for i, junction in enumerate(st.session_state.junctions):
                    print(f&#34;  Junction {i}: radius={junction.r}, r_outer={r_outer_list[i] if i &lt; len(r_outer_list) else &#39;N/A&#39;}&#34;)

                # Debug: Test trajectory sequence tracking with multiple trajectories
                print(f&#34;\n🔍 DEBUG: Testing trajectory sequence tracking...&#34;)
                from verta.verta_plotting import _track_trajectory_junction_sequence

                # Test trajectories 1-3 instead of trajectory 0 (which has limited range)
                test_trajectories = st.session_state.trajectories[1:4]  # Trajectories 1-3

                for test_idx, test_traj in enumerate(test_trajectories):
                    traj_id = getattr(test_traj, &#39;tid&#39;, test_idx + 1)
                    print(f&#34;\n🔍 DEBUG: === Testing Trajectory {traj_id} ===&#34;)

                    # Debug: Check trajectory data
                    print(f&#34;🔍 DEBUG: Trajectory {traj_id} data:&#34;)
                    print(f&#34;  - Length: {len(test_traj.x)} points&#34;)
                    print(f&#34;  - X range: {min(test_traj.x):.2f} to {max(test_traj.x):.2f}&#34;)
                    print(f&#34;  - Z range: {min(test_traj.z):.2f} to {max(test_traj.z):.2f}&#34;)

                    # Debug: Check junction data
                    print(f&#34;🔍 DEBUG: Junction data:&#34;)
                    for i, junction in enumerate(st.session_state.junctions):
                        print(f&#34;  Junction {i}: center=({junction.cx:.2f}, {junction.cz:.2f}), radius={junction.r}, r_outer={r_outer_list[i]}&#34;)

                    # Test spatial tracking
                    test_sequence = _track_trajectory_junction_sequence(test_traj, st.session_state.junctions, r_outer_list)
                    print(f&#34;🔍 DEBUG: Trajectory {traj_id} sequence: {test_sequence}&#34;)

                    if len(test_sequence) &gt; 0:
                        print(f&#34;🔍 DEBUG: ✅ Trajectory {traj_id} has valid sequence!&#34;)
                        print(f&#34;🔍 DEBUG: Stopping debug testing - spatial tracking is working!&#34;)
                        break
                    else:
                        print(f&#34;🔍 DEBUG: ❌ Trajectory {traj_id} has no sequence&#34;)

                # If no trajectories worked, test trajectory 0 as fallback
                if all(len(_track_trajectory_junction_sequence(traj, st.session_state.junctions, r_outer_list)) == 0 for traj in test_trajectories):
                    print(f&#34;\n🔍 DEBUG: === Testing Trajectory 0 as fallback ===&#34;)
                    test_traj = st.session_state.trajectories[0]
                    test_sequence = _track_trajectory_junction_sequence(test_traj, st.session_state.junctions, r_outer_list)
                    print(f&#34;🔍 DEBUG: Trajectory 0 sequence: {test_sequence}&#34;)

                # Debug: Show sample of norm_df data
                # print(f&#34;\n🔍 DEBUG: Sample norm_df data (first 10 rows):&#34;)
                # print(norm_df.head(10))

                # Then analyze junction choice patterns
                print(&#34;\n&#34; + &#34;=&#34;*60)
                print(&#34;🔍 PREDICT ANALYSIS - FLOW GRAPH DEBUG OUTPUT&#34;)
                print(&#34;=&#34;*60)

                # Run prediction analysis using spatial tracking only
                results = analyze_junction_choice_patterns(
                    trajectories=st.session_state.trajectories,
                    chain_df=chain_df,  # Empty chain_df for compatibility
                    junctions=st.session_state.junctions,
                    output_dir=output_dir,
                    r_outer_list=r_outer_list,
                    gui_mode=False  # Enable console debug output
                )

                print(&#34;=&#34;*60)
                print(&#34;✅ Predict analysis completed&#34;)
                print(&#34;=&#34;*60 + &#34;\n&#34;)

                # Store results - preserve existing analysis results
                if st.session_state.analysis_results is None:
                    st.session_state.analysis_results = {}
                st.session_state.analysis_results[&#34;predictions&#34;] = results

                # Generate CLI command for easy copying
                self.generate_cli_command(&#34;predict&#34;, results, cluster_method, cluster_params, decision_mode, decision_params)

            elif analysis_type == &#34;intent&#34;:
                # Run intent recognition analysis
                import pandas as pd
                import numpy as np
                import os

                st.info(&#34;🧠 Running Intent Recognition Analysis...&#34;)

                # Get parameters
                intent_params = st.session_state.get(&#39;intent_params&#39;, {
                    &#39;prediction_distances&#39;: [100.0, 75.0, 50.0, 25.0],
                    &#39;model_type&#39;: &#39;random_forest&#39;,
                    &#39;cv_folds&#39;: 5,
                    &#39;test_split&#39;: 0.2
                })

                # Check if we have time data
                has_time = all(tr.t is not None and len(tr.t) &gt; 0 for tr in st.session_state.trajectories[:5])
                if not has_time:
                    st.warning(&#34;⚠️ Time data not detected. Intent recognition requires temporal information for velocity/acceleration features.&#34;)
                    st.info(&#34;💡 Tip: Ensure your CSV files have a time column specified in column mapping.&#34;)

                # Check for sklearn
                try:
                    import sklearn
                except ImportError:
                    st.error(&#34;❌ scikit-learn not installed!&#34;)
                    st.markdown(&#34;&#34;&#34;
                    Intent recognition requires scikit-learn. Install with:
                    ```bash
                    pip install scikit-learn
                    ```
                    &#34;&#34;&#34;)
                    return

                # Create output directory
                output_dir = &#34;gui_outputs/intent_recognition&#34;
                os.makedirs(output_dir, exist_ok=True)

                # For each junction, run intent recognition
                intent_results = {}

                progress_bar = st.progress(0)
                status_text = st.empty()

                for junction_idx, junction in enumerate(st.session_state.junctions):
                    status_text.text(f&#34;Analyzing junction {junction_idx + 1}/{len(st.session_state.junctions)}...&#34;)

                    try:
                        # Create junction output directory
                        junction_output = os.path.join(output_dir, f&#34;junction_{junction_idx}&#34;)
                        os.makedirs(junction_output, exist_ok=True)

                        # Try to load existing branch assignments from previous Discover analysis
                        assignments_df = None
                        centers = None

                        # Check if we have results from a previous analysis
                        if st.session_state.analysis_results and &#39;branches&#39; in st.session_state.analysis_results:
                            branch_results = st.session_state.analysis_results[&#39;branches&#39;]
                            junction_key = f&#34;junction_{junction_idx}&#34;

                            if junction_key in branch_results:
                                assignments_df = branch_results[junction_key].get(&#39;assignments&#39;)
                                centers = branch_results[junction_key].get(&#39;centers&#39;)

                                if assignments_df is not None:
                                    st.info(f&#34;📋 Using existing branch assignments from previous Discover analysis for Junction {junction_idx}&#34;)

                        # If no existing assignments, run discovery
                        if assignments_df is None:
                            st.warning(f&#34;⚠️ No existing branch assignments found for Junction {junction_idx}&#34;)
                            st.info(f&#34;🔍 Running branch discovery with default parameters...&#34;)
                            st.info(&#34;💡 **Tip:** Run &#39;Discover Branches&#39; analysis first to control clustering parameters!&#34;)

                            r_outer = st.session_state.junction_r_outer.get(junction_idx, 50.0)

                            # Run branch discovery with default parameters
                            assignments_df, summary_df, centers = discover_branches(
                                trajectories=st.session_state.trajectories,
                                junction=junction,
                                k=3,
                                decision_mode=&#34;hybrid&#34;,
                                r_outer=r_outer,
                                path_length=100.0,
                                epsilon=0.05,
                                cluster_method=&#34;auto&#34;,
                                out_dir=junction_output
                            )

                        # Filter valid branches (&gt;= 0)
                        valid_assignments = assignments_df[assignments_df[&#39;branch&#39;] &gt;= 0]

                        if len(valid_assignments) &lt; 10:
                            st.warning(f&#34;⚠️ Junction {junction_idx}: Insufficient valid trajectories ({len(valid_assignments)}). Skipping intent analysis.&#34;)
                            intent_results[f&#34;junction_{junction_idx}&#34;] = {
                                &#39;error&#39;: &#39;insufficient_data&#39;,
                                &#39;n_valid_trajectories&#39;: len(valid_assignments)
                            }
                            continue

                        # Count unique branches
                        n_branches = len(assignments_df[assignments_df[&#39;branch&#39;] &gt;= 0][&#39;branch&#39;].unique())
                        st.success(f&#34;✅ Using {n_branches} branches with {len(valid_assignments)} valid trajectories&#34;)

                        # Run intent recognition
                        st.info(f&#34;🤖 Training intent recognition models...&#34;)

                        results = analyze_intent_recognition(
                            trajectories=st.session_state.trajectories,
                            junction=junction,
                            actual_branches=assignments_df,
                            output_dir=junction_output,
                            prediction_distances=intent_params[&#39;prediction_distances&#39;],
                            previous_choices=None  # TODO: Could add multi-junction support
                        )

                        if &#39;error&#39; in results:
                            st.error(f&#34;❌ Junction {junction_idx}: {results[&#39;error&#39;]}&#34;)
                            intent_results[f&#34;junction_{junction_idx}&#34;] = results
                        else:
                            st.success(f&#34;✅ Junction {junction_idx}: Intent recognition complete!&#34;)

                            # Display quick summary
                            models_trained = results[&#39;training_results&#39;].get(&#39;models_trained&#39;, {})
                            if models_trained:
                                avg_acc = np.mean([m[&#39;cv_mean_accuracy&#39;] for m in models_trained.values()])
                                st.metric(f&#34;Junction {junction_idx} Avg Accuracy&#34;, f&#34;{avg_acc:.1%}&#34;)

                            intent_results[f&#34;junction_{junction_idx}&#34;] = results

                    except Exception as e:
                        st.error(f&#34;❌ Junction {junction_idx} failed: {str(e)}&#34;)
                        intent_results[f&#34;junction_{junction_idx}&#34;] = {
                            &#39;error&#39;: str(e),
                            &#39;error_type&#39;: type(e).__name__
                        }

                    progress_bar.progress((junction_idx + 1) / len(st.session_state.junctions))

                status_text.text(&#34;✅ Intent recognition analysis complete!&#34;)
                progress_bar.empty()

                # Store results
                if st.session_state.analysis_results is None:
                    st.session_state.analysis_results = {}
                st.session_state.analysis_results[&#34;intent_recognition&#34;] = intent_results

                # Display summary
                st.markdown(&#34;### 📊 Intent Recognition Summary&#34;)

                successful_junctions = [k for k, v in intent_results.items()
                                      if &#39;error&#39; not in v]

                if successful_junctions:
                    st.success(f&#34;✅ Successfully analyzed {len(successful_junctions)}/{len(st.session_state.junctions)} junctions&#34;)

                    # Create summary table
                    summary_data = []
                    for junction_key in successful_junctions:
                        results = intent_results[junction_key]
                        models = results[&#39;training_results&#39;].get(&#39;models_trained&#39;, {})

                        for dist, model_info in models.items():
                            summary_data.append({
                                &#39;Junction&#39;: junction_key.replace(&#39;junction_&#39;, &#39;J&#39;),
                                &#39;Distance (units)&#39;: dist,
                                &#39;Accuracy&#39;: f&#34;{model_info[&#39;cv_mean_accuracy&#39;]:.1%}&#34;,
                                &#39;Std Dev&#39;: f&#34;±{model_info[&#39;cv_std_accuracy&#39;]:.1%}&#34;,
                                &#39;Samples&#39;: model_info[&#39;n_samples&#39;]
                            })

                    if summary_data:
                        summary_df = pd.DataFrame(summary_data)
                        st.dataframe(summary_df, width=&#39;stretch&#39;)

                    # Show interpretation
                    st.markdown(&#34;#### 💡 Interpretation&#34;)
                    avg_accuracy = np.mean([float(d[&#39;Accuracy&#39;].strip(&#39;%&#39;)) / 100 for d in summary_data])

                    if avg_accuracy &gt; 0.85:
                        st.success(&#34;🟢 **Excellent Predictability**: User intent is highly predictable. Early intervention systems will be very effective!&#34;)
                    elif avg_accuracy &gt; 0.70:
                        st.info(&#34;🟡 **Good Predictability**: Clear patterns exist. Adaptive systems can benefit users.&#34;)
                    else:
                        st.warning(&#34;🔴 **Moderate Predictability**: Behavior is variable. Consider per-user models or additional features.&#34;)
                else:
                    st.error(&#34;❌ Intent recognition failed for all junctions&#34;)

                st.info(f&#34;📁 Detailed results saved to: {output_dir}&#34;)

                # Generate CLI command for easy copying
                # Build results dict for CLI command generation
                intent_results_dict = {}
                for i, junction in enumerate(st.session_state.junctions):
                    junction_key = f&#34;junction_{i}&#34;
                    intent_results_dict[junction_key] = {
                        &#34;junction&#34;: junction,
                        &#34;r_outer&#34;: st.session_state.junction_r_outer.get(i, 50.0),
                        &#34;decision_mode&#34;: decision_mode,
                        &#34;path_length&#34;: decision_params.get(&#34;path_length&#34;, 100.0) if decision_params else 100.0,
                        &#34;epsilon&#34;: decision_params.get(&#34;epsilon&#34;, 0.05) if decision_params else 0.05,
                        &#34;linger_delta&#34;: decision_params.get(&#34;linger_delta&#34;, 5.0) if decision_params else 5.0,
                        &#34;prediction_distances&#34;: intent_params.get(&#39;prediction_distances&#39;, [100.0, 75.0, 50.0, 25.0]),
                        &#34;model_type&#34;: intent_params.get(&#39;model_type&#39;, &#39;random_forest&#39;),
                        &#34;cv_folds&#34;: intent_params.get(&#39;cv_folds&#39;, 5),
                        &#34;test_split&#34;: intent_params.get(&#39;test_split&#39;, 0.2),
                    }
                self.generate_cli_command(&#34;intent&#34;, intent_results_dict, cluster_method, cluster_params, decision_mode, decision_params)

            elif analysis_type == &#34;enhanced&#34;:
                # Run enhanced analysis for evacuation planning and risk assessment
                import os
                output_dir = &#34;gui_outputs&#34;
                os.makedirs(output_dir, exist_ok=True)

                # First run discovery to get chain data
                k_value = cluster_params.get(&#34;k&#34;, 3) if cluster_params else 3
                min_samples = cluster_params.get(&#34;min_samples&#34;, 5) if cluster_params else 5
                k_min = cluster_params.get(&#34;k_min&#34;, 2) if cluster_params else 2
                k_max = cluster_params.get(&#34;k_max&#34;, 6) if cluster_params else 6
                min_sep_deg = cluster_params.get(&#34;min_sep_deg&#34;, 12.0) if cluster_params else 12.0
                angle_eps = cluster_params.get(&#34;angle_eps&#34;, 15.0) if cluster_params else 15.0

                path_length = decision_params.get(&#34;path_length&#34;, 100.0) if decision_params else 100.0
                epsilon = decision_params.get(&#34;epsilon&#34;, 0.05) if decision_params else 0.05
                linger_delta = decision_params.get(&#34;linger_delta&#34;, 5.0) if decision_params else 5.0
                r_outer_list = [st.session_state.junction_r_outer.get(i, 50.0) for i in range(len(st.session_state.junctions))]

                # Run discovery first
                chain_df, centers_list, decisions_chain_df = discover_decision_chain(
                    trajectories=st.session_state.trajectories,
                    junctions=st.session_state.junctions,
                    path_length=path_length,
                    epsilon=epsilon,
                    seed=seed,
                    decision_mode=discover_decision_mode,
                    r_outer_list=r_outer_list,
                    linger_delta=linger_delta,
                    out_dir=output_dir,
                    cluster_method=cluster_method,
                    k=k_value,
                    k_min=k_min,
                    k_max=k_max,
                    min_sep_deg=min_sep_deg,
                    angle_eps=angle_eps,
                    min_samples=min_samples,
                )

                # Run enhanced analysis
                enhanced_results = self._run_enhanced_analysis(
                    trajectories=st.session_state.trajectories,
                    chain_df=chain_df,
                    junctions=st.session_state.junctions,
                    r_outer_list=r_outer_list,
                    centers_list=centers_list,
                    decisions_df=decisions_chain_df
                )

                # Store results
                if st.session_state.analysis_results is None:
                    st.session_state.analysis_results = {}
                st.session_state.analysis_results[&#34;enhanced&#34;] = enhanced_results

            #st.success(f&#34;✅ {analysis_type.capitalize()} analysis completed!&#34;)
            #st.rerun()

    except Exception as e:
        st.error(f&#34;❌ Analysis failed: {str(e)}&#34;)
        st.exception(e)</code></pre>
</details>
</dd>
<dt id="verta.verta_gui.RouteAnalyzerGUI.run_quick_analysis"><code class="name flex">
<span>def <span class="ident">run_quick_analysis</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Run a quick analysis with default parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_quick_analysis(self):
    &#34;&#34;&#34;Run a quick analysis with default parameters&#34;&#34;&#34;
    if not st.session_state.trajectories or not st.session_state.junctions:
        st.warning(&#34;⚠️ Please load data and define junctions first&#34;)
        return

    st.session_state.current_step = &#34;analysis&#34;
    st.rerun()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#verta-web-gui">VERTA Web GUI</a></li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="verta" href="index.html">verta</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="verta.verta_gui.main" href="#verta.verta_gui.main">main</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="verta.verta_gui.RouteAnalyzerGUI" href="#verta.verta_gui.RouteAnalyzerGUI">RouteAnalyzerGUI</a></code></h4>
<ul class="">
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.clear_all_data" href="#verta.verta_gui.RouteAnalyzerGUI.clear_all_data">clear_all_data</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.export_results" href="#verta.verta_gui.RouteAnalyzerGUI.export_results">export_results</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.generate_cli_command" href="#verta.verta_gui.RouteAnalyzerGUI.generate_cli_command">generate_cli_command</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.initialize_session_state" href="#verta.verta_gui.RouteAnalyzerGUI.initialize_session_state">initialize_session_state</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.load_assign_centers" href="#verta.verta_gui.RouteAnalyzerGUI.load_assign_centers">load_assign_centers</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.load_assign_trajectories" href="#verta.verta_gui.RouteAnalyzerGUI.load_assign_trajectories">load_assign_trajectories</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.load_sample_data" href="#verta.verta_gui.RouteAnalyzerGUI.load_sample_data">load_sample_data</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.load_sample_junctions" href="#verta.verta_gui.RouteAnalyzerGUI.load_sample_junctions">load_sample_junctions</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.load_trajectory_data" href="#verta.verta_gui.RouteAnalyzerGUI.load_trajectory_data">load_trajectory_data</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.load_uploaded_files" href="#verta.verta_gui.RouteAnalyzerGUI.load_uploaded_files">load_uploaded_files</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.render_analysis" href="#verta.verta_gui.RouteAnalyzerGUI.render_analysis">render_analysis</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.render_assign_visualizations" href="#verta.verta_gui.RouteAnalyzerGUI.render_assign_visualizations">render_assign_visualizations</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.render_conditional_probabilities" href="#verta.verta_gui.RouteAnalyzerGUI.render_conditional_probabilities">render_conditional_probabilities</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.render_data_upload" href="#verta.verta_gui.RouteAnalyzerGUI.render_data_upload">render_data_upload</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.render_discover_visualizations" href="#verta.verta_gui.RouteAnalyzerGUI.render_discover_visualizations">render_discover_visualizations</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.render_enhanced_visualizations" href="#verta.verta_gui.RouteAnalyzerGUI.render_enhanced_visualizations">render_enhanced_visualizations</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.render_export" href="#verta.verta_gui.RouteAnalyzerGUI.render_export">render_export</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.render_flow_graphs" href="#verta.verta_gui.RouteAnalyzerGUI.render_flow_graphs">render_flow_graphs</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.render_gaze_visualizations" href="#verta.verta_gui.RouteAnalyzerGUI.render_gaze_visualizations">render_gaze_visualizations</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.render_header" href="#verta.verta_gui.RouteAnalyzerGUI.render_header">render_header</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.render_intent_visualizations" href="#verta.verta_gui.RouteAnalyzerGUI.render_intent_visualizations">render_intent_visualizations</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.render_junction_editor" href="#verta.verta_gui.RouteAnalyzerGUI.render_junction_editor">render_junction_editor</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.render_junction_plot" href="#verta.verta_gui.RouteAnalyzerGUI.render_junction_plot">render_junction_plot</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.render_metrics_visualizations" href="#verta.verta_gui.RouteAnalyzerGUI.render_metrics_visualizations">render_metrics_visualizations</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.render_navigation" href="#verta.verta_gui.RouteAnalyzerGUI.render_navigation">render_navigation</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.render_pattern_analysis" href="#verta.verta_gui.RouteAnalyzerGUI.render_pattern_analysis">render_pattern_analysis</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.render_predict_visualizations" href="#verta.verta_gui.RouteAnalyzerGUI.render_predict_visualizations">render_predict_visualizations</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.render_visualization" href="#verta.verta_gui.RouteAnalyzerGUI.render_visualization">render_visualization</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.run" href="#verta.verta_gui.RouteAnalyzerGUI.run">run</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.run_analysis" href="#verta.verta_gui.RouteAnalyzerGUI.run_analysis">run_analysis</a></code></li>
<li><code><a title="verta.verta_gui.RouteAnalyzerGUI.run_quick_analysis" href="#verta.verta_gui.RouteAnalyzerGUI.run_quick_analysis">run_quick_analysis</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>